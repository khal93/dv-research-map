{
	"grants":[
	{
		"grant":0,
		"ID": "EP/E027261/1",
		"Title": "Semiconductor Research at the Materials-Device Interface",
		"PIID": "6674",
		"Scheme": "Platform Grants",
		"StartDate": "01/05/2007",
		"EndDate": "31/10/2012",
		"Value": "800579",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "6674", "Role": "Principal Investigator"},
		{"ID": "29195", "Role": "Co Investigator"},
		{"ID": "90639", "Role": "Co Investigator"},
		{"ID": "101342", "Role": "Co Investigator"},
		{"ID": "12223", "Role": "Co Investigator"},
		{"ID": "45348", "Role": "Co Investigator"},
		{"ID": "96538", "Role": "Co Investigator"},
		{"ID": "10965", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal concerns research into electronic materials, and the development of experimental methods designed to improve our measurement capability on the nm scale.  Semiconductor materials and devices are central to manufacturing, healthcare, security, administration and leisure. This pivotal position in our lives has developed gradually but is due in the main to dramatic changes that have occurred quite recently. Over the last decade semiconductor technology has begun to experience a revolution in terms of functionality based on decreased size and increased complexity, and this trend will define the future for the entire manufacturing sector. This presents immense challenges to both researchers and to manufacturers of semiconductors because the key issues are no longer the properties of bulk materials or even two-dimensional structures but the properties of small heterogeneous clusters of atoms (semiconductor, dielectric and metal) that constitute today's functional device. To put this into context, the next generation silicon NMOS transistor (45nm node) is only half the size of an influenza virus and for most applications will work in conjunction with tens of millions of similar devices. For research, development and control in manufacture the electronic and physical properties of small atomic clusters need to be probed and interactions with structures in close proximity understood.As materials and device sub-structures become more complex the experimental task of obtaining precise information becomes ever more challenging. In particular the atomic organisation and local chemistry can have a profound effect on electronic behaviour and there is a growing need to develop measurement methods which can both image structures and link shape with local spectroscopic information. In our work we are pushing forward such methods by combining x-ray spectroscopy with scanning probe imaging, using both national and international synchrotron radiation sources. In a complementary approach, we are extending electron energy loss techniques in scanning transmission electron microscopy to link chemical and structural information. Optical spectroscopy is an invaluable tool for characterising condensed matter and we are developing free electron laser pumped Raman spectroscopy in order to directly probe electron states in ultra small semiconductors.Almost all emerging device technologies are limited by these materials issues and much of our work is guided by measuring and understanding these. For example, ultra high speed, low noise detectors and amplifiers are desperately needed by radio-astronomers for the next generation of telescopes. Such devices demand near perfect material and interface properties and form part of our programme. Similarly future THz emitters are hugely challenging in terms of materials physics. One of the key developments in electronic materials in the last decade is the ability to synthesise quantum dots which give three dimensional control over quantum size effects and hold the promise of highly tuneable materials. Measuring the collective electrical properties has proved a major task and the information required to build many devices is missing. We are extending and adapting point defect measurement methods to close this gap. The increasing complexity of materials raises many issues for the device and circuit designer. An important feature of our proposed work is that we aim to include device design concepts at the materials level, and will use this work to guide our experimental programme."
	},
	{
		"grant":1,
		"ID": "EP/E051804/1",
		"Title": "Structural Nanoprobes of Organic Semiconductor Devices",
		"PIID": "49195",
		"Scheme": "Advanced Fellowship",
		"StartDate": "10/09/2007",
		"EndDate": "09/09/2012",
		"Value": "654090",
		"ResearchArea": "Materials For Energy Applications",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "24",
		"Investigators":[
		{"ID": "49195", "Role": "Principal Investigator"},
		{"ID": "-155242", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "North Carolina State University"}
		],
		"Summary": "Organic semiconductors are an exciting new class of material that combine the electronic properties traditionally only associated with inorganic materials, with the mechanical properties and processibility of polymers (plastics) and small organic molecules. In particular, the ability to process active semiconductor layers through solution processing has led to the commercialisation of organic light-emitting diode-based displays. Commercial potential has also been demonstrated by organic transistors and organic solar cells, where both technologies have the advantage of low-cost processing and the ability to be incorporated into flexible architectures.However, as organic semiconductors are a relatively new class of material, there are still many fundamental questions governing key processes that affect device performance. For example, organic semiconductor films are typically less ordered than their inorganic counterparts and the influence of domain structure, molecular orientation and molecular alignment on charge transport is not fully understood. Additionally, for organic solar cells, where typically two different materials are blended together to form efficient networks for charge separation and transport, the influence of material mixing on charge separation and transport are still being discovered.Since organic semiconductors have vastly different properties compared to inorganic semiconductors, the development and application of new techniques to probe the properties of this new class of material is required. This research programme will adapt state-of-the-art microscopes and utilize advanced X-ray analytical techniques to probe structure and device action in organic devices with unprecedented precision and clarity. This further understanding of device operation will allow for the identification of physical processes that limit device performance and hence promote future device optimisation."
	},
	{
		"grant":2,
		"ID": "EP/E051847/1",
		"Title": "SCAffolding Rich Learning Experiences through Technology: SCARLET",
		"PIID": "70737",
		"Scheme": "Advanced Fellowship",
		"StartDate": "01/10/2007",
		"EndDate": "31/12/2012",
		"Value": "758092",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Culture, Communication and Media",
		"OrgID": "79",
		"Investigators":[
		{"ID": "70737", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "British Educational Communications"},
		{"Number": "1", "Name": "Centre For Self Managed Learning"},
		{"Number": "2", "Name": "intoMedia"},
		{"Number": "3", "Name": "Little Horsted Church of England School"},
		{"Number": "4", "Name": "University of Sussex"}
		],
		"Summary": "The nature of technology has changed since scaffolding was conceptualised as an educational approach, and software scaffolding systems were first developed.  Technology is smaller, more mobile, networked, pervasive and often ubiquitous as well as being provided by the standard desktop PC.  This offers the potential for technology supported learning wherever and whenever learners need and want it.  However, in order to take advantage of this potential for greater flexibility we need to develop modelling and scaffolding techniques that go beyond a single subject and place.   The development of such models and techniques is the subject of the SCARLET project proposal.The concept of scaffolding was introduced to describe the sorts of support that can be offered to a learner to help them bridge the gap between what they want to achieve and what they are currently able to achieve by themselves. These face to face scaffolding techniques were then applied to educational software design. The requirements for successful scaffolding remain the same whether the scaffolder is a person or technology. The scaffolder needs to know about their learners as well as the subject to be learnt so that they can provide and remove support as and when appropriate to the learners' needs.  For example, the scaffolder needs to know how much the learners currently understand about the subject, how motivated to learn they are and how confident they feel. Scaffolding software dynamically creates such learner models, activities are completed and information from teachers, peers and collaborators can be added.  To complement these learner models, the scaffolder also needs a good model of the subject to be learnt.  This model needs to identify the types of task that the learner can complete, the sorts of resources they can access and the types of assistance that can be offered. For example, simulated science laboratory software might ask children to explore food web problems by adding animals and plants to a virtual lab and then selecting actions, such as one animal to eat another.  A scaffolding component could offer advice contingent upon:* The organisms available to the learner, domain resource contingency* The actions she tries to make happen once she has selected her organisms, task contingency. * The time at which support is offered, temporal contingency Initially software scaffolding systems concentrated upon using artificial intelligence to build such models of the learners' knowledge development and implemented scaffolding based upon the contingencies described above. More recently this modelling activity has involved exploring further types of contingency relating to learners' metacognitive awareness (what learners know and believe about their own learning), learners' motivation, and learners' confidence. To date however this scaffolding has been implemented to support learning within the context provided by the software.The increasing ubiquity of technology brings with it the need to explore new types of contingency. We now need to be able to model the context beyond that created by a single piece of software as well as the learner and the subjects being learnt.  But what are the new types of contingency and how can we scaffold them?  What types of technology can we use to develop new forms of scaffolding?  These are the questions that the SCARLET project will explore. Such contingencies might include interface contingency for example. We also need to explore a different granularity of scaffolding support. If we consider scaffolding in the real world then the domain resource contingency discussed earlier might relate to resources such as a museum, park, an environmental expert or certain books in the library.  The SCARLET project will explore ways in which we can use technology to offer advice about the resources that can be used to support learning across multiple locations, subjects and times."
	},
	{
		"grant":3,
		"ID": "EP/E052029/2",
		"Title": "The Synthesis of Probabilistic Prediction & Mechanistic Modelling within a Computational & Systems Biology Context",
		"PIID": "84313",
		"Scheme": "Advanced Fellowship",
		"StartDate": "08/11/2010",
		"EndDate": "30/04/2013",
		"Value": "343928",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Statistical Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "84313", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Beatson Institute for Cancer Research"},
		{"Number": "1", "Name": "Institute of Cancer Research"},
		{"Number": "2", "Name": "Mosaiques Diagnostics AG"},
		{"Number": "3", "Name": "University of Bristol"},
		{"Number": "4", "Name": "University of Glasgow"}
		],
		"Summary": "The synergistic advances that can be made by the multidisciplinary interplay between abstracted computational modelling and biological experimental investigation within a system biology context are poised to make major contributions to our understanding of some of the most important biological systems implicated in the genesis of many serious diseases such as cancer.  However, due to the unavoidable inherent levels of uncertainty, noise and relative scarcity of biological data it is vital that sound evidential based scientific reasoning be enabled within a systems biology context by formally embedding mechanistic models within a probabilistic inferential framework. The synthesis of mechanistic modelling & probabilistic inference provides outstanding opportunities to make further significant advances in understanding biological systems and processes at multiple levels, by defining system components and inferring how they dynamically interact. There is a major role that statistical machine learning methodology has to play in both computational & systems biology research and a number of important methodological challenges are presented by applications working at this interface.However, one of  the most important aspects of successful computational & systems biology research is that it must be conducted in direct collaboration with world-class experimental biologists.   An outstanding feature of this Fellowship is that it has set in place six exciting collaborations with internationally leading cancer researchers, proteomics technologists, biochemists and plant biologists who are all fully committed to successfully driving forward a potentially groundbreaking multidisciplinary systems biology research programme as detailed in this proposal. Three important application areas within biological science will shape and direct the research to be undertaken during this Fellowship. The applications are distinct, yet overlap in terms of the modelling & inferential issues which each present and this is important in ensuring a consistent and coherent line of research.   They have also been selected for their major importance in the study of cellular mechanisms which are fundamental to cell function, some of which are implicated in certain serious diseases. In addition, the applicant has substantive ongoing collaborations with world-class laboratories engaged in these biological investigations. This ensures the proposed research programme is focused on realistic methodological problems which will have a direct impact on the major scientific questions being asked within each area, as well contributing to the computational and inferential sciences. The first application will develop the inferential tools required by cancer biologists when reasoning about the structures underlying the observed dynamics of the MAPK pathway and these tools will be employed in a large scale study of this pathway in collaboration with the Beatson Institute of Cancer Research. The second application, to be conducted with the Plant Sciences group at the University of Glasgow, will seek to elucidate, in a model-based inferential manner, the remarkable observed phenomenon of organ specificity of the circadian clock in soybean and Arabidopsis, in addition a study of models of transcriptional regulation in the cell-cycle will be conducted. The final application will investigate a number of open issues associated with clinical transcriptomics and proteomics where the identification of possible target genes and proteins is of vital importance to cancer researchers in their studies of, in this case breast and ovarian cancer. This study will be conducted in direct conjunction with the Institute of Cancer Research where an ongoing study of BRCA1&2 mutations implicated in breast and ovarian cancer is underway."
	},
	{
		"grant":4,
		"ID": "EP/E052819/1",
		"Title": "Foundational Structures and Methods for Quantum Informatics",
		"PIID": "35",
		"Scheme": "Senior Fellowship",
		"StartDate": "01/10/2007",
		"EndDate": "30/09/2012",
		"Value": "548725",
		"ResearchArea": "Quantum Optics and Information",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "35", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Quantum Mechanics offers radically new possibilities for information processing, with phenomena such as entanglement --- the possibility of strong instantaneous correlations between spatially separated particles, leading to highly counter-intuitive non-local effects --- playing a central role.Current methods for dealing with these ideas, and with the subtle interplays and information flows between a quantum system and its classical observer, are cumbersome, and somewhat like the early days of programming computers in machine code.In this proposal, building on our previous work, we aim to use ideas and methods which have already proved useful in Computer Science to develop an elegant, conceptual approach, both to the foundations of Quantum Mechanics itself, and to the description, design and analysis of quantum information processing systems. The effect is something like having a high-level programming language --- in which the `computer' is the physical world!While the underlying mathematics we use (category theory) is quite abstract, it is accompanied by a very intuitive diagrammatic formalism, which is useful both as a practical tool for calculations, and for bringing the ideas to life in a very vivid and visual fashion.This formalism also leads to fascinating and unexpected links with logic --- our diagrams can be seen as graphical representations of formal proofs --- and with geometric ideas such as knots and braids."
	},
	{
		"grant":5,
		"ID": "EP/E053041/2",
		"Title": "Scalable Program Analysis for Software Verification",
		"PIID": "-161956",
		"Scheme": "Advanced Fellowship",
		"StartDate": "01/05/2011",
		"EndDate": "30/09/2012",
		"Value": "120683",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-161956", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Recent years have seen a renaissance in automatic program verification, based on advances in program analysis (abstract interpretation).Tools such as Microsoft's Static Driver Verifiercan automatically verify certain lightweight properties (e.g., protocol properties)of interfaces between program components.There is, though, a fundamental problem:Scalable methods are lacking.Current tools are based on a closed world assumption,where a complete program is available,and they work over a system's entire global state.This lack of modularity impedes scalability, and  wider applicability. This research proposes an attack on the scalability problem.Our thesis is that progress in three directions, localization,isolation and generalization, can lead to much more scalable analyses.The idea of the first two of these isto reduce the cost for analyzing each component once,while  the third aims to ensure thatone analysis result of a program component can be reused in manydifferent contexts. We will develop a general framework and concreteinstances of analyses that achieve these three goals.We will test our ideas by developing prototype toolsthat we will apply to widely-used open-source infrastructure software,such as network software and operating system components.Scalability is the core problem in the automatic verification of software.Success on the problems in this research would have a majorimpact on the use of automatic techniques for theanalysis and verification of significant, real-world code."
	},
	{
		"grant":6,
		"ID": "EP/E055672/1",
		"Title": "Evaluation and harnessing of noise in telecommunication systems",
		"PIID": "118538",
		"Scheme": "Advanced Fellowship",
		"StartDate": "01/10/2007",
		"EndDate": "30/09/2012",
		"Value": "552188",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12",
		"Investigators":[
		{"ID": "118538", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Bell Laboratories"},
		{"Number": "1", "Name": "France Telecom R and D"}
		],
		"Summary": "Enhanced capability in digital communication is recognized as a pivotal element in a future economy, education, commercial and social activities. The role of fibre communications, currently providing for a healthy fraction of the total information traffic, will considerably increase in the near future. This will be driven by the introduction of and advances in new services such as videophony, video on demand, computer based banking, high speed computer communications and other elements of economical and social infrastructure. Light-wave fibre communication systems form the backbone of the present day high-speed data transmission links. The information is encoded and transmitted through an optical channel as a digital bit stream. One of the principal issues to be addressed by the researchers and system designers is to estimate the probability of errors (bit error rate) occurring in such bit stream due to the intrinsic noise in a transmission system. Noisy communication links need special solutions - primarily special coding techniques to minimise noise/errors in the information bit-stream. Knowledge of noise statistics of nonlinear fibre channels is crucial for the design of efficient and adequate coding techniques and practical realizations of coding devices. Therefore, novel mathematical, theoretical and numerical approaches are required to understand error statistics in order to create advanced future communication systems. Another challenging and exciting task is to find the way to use the always present intrinsic noise to our advantage. This seems like quite a paradoxical idea: for generations noise has been considered as nuisance that must be eradicated at all cost. However in certain nonlinear systems, including electronic circuits and biological sensory systems the presence of noise can enhance the detection of weak signals. The phenomenon is known as stochastic resonance and is of great interest to electronic instrumentation and telecommunications. The question arises whether this phenomenon can be used in communications and can the new devices be constructed based on this effect. In the current project we first plan to apply a novel simulation technique for studying the statistics of the signal output in optical fibres. This approach will allow us to obtain precise and explicit description of the statistics of the transmitted signal. These in turn, are crucial for determining the bit error rate and system performance. The advantage of the proposed algorithm is that it allows one to model the statistics of extremely rare events, something, which is utterly impossible to achieve with the conventional Monte Carlo techniques that are widely used at present. Therefore we expect that with this new powerful tool we will be able to calculate numerically the error probabilities, which are of the order of 1 error event per 1,000,000,000 transmitted bits. Our method will allow us to obtain these results without significant CPU time penalty, which makes the proposed approach very practical. This technique is crucial for the correct estimation of the bit error rate and, therefore, the overall system performance. We will then develop the techniques that will help to improve the quality of detection reducing the probability of mistaking logical  ones  for  zeros  and vice versa. The final and most challenging stage of the project is to propose and develop nonlinear electronic devices that make use of the phenomenon of stochastic resonance. During this stage of the project we will use the data about signal statistics collected at the initial stages of the project by Monte Carlo simulations. Currently the proposed host institution (Aston University) possesses state-of-the-art facilities for the project (including 144 node Cray XD1 supercomputer) and the proposed research is a logical development of the existing techniques and has the potential to produce a de-facto standard for modelling of noise in telecommunications."
	},
	{
		"grant":7,
		"ID": "EP/E055818/1",
		"Title": "Adaptive optics for three-dimensional microscopy and photonic engineering",
		"PIID": "106108",
		"Scheme": "Advanced Fellowship",
		"StartDate": "31/03/2008",
		"EndDate": "30/03/2013",
		"Value": "865927",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "106108", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "RIKEN"},
		{"Number": "1", "Name": "Swinburne University of Technology"}
		],
		"Summary": "Light is a versatile tool for imaging and engineering on microscopic scales. Optical microscopes use focused light so that we can view specimens with high resolution. These microscopes are widely used in the life sciences to permit the visualisation of cellular structures and sub-cellular processes.  However, the resolution of an optical microscope is often adversely affected by the very presence of the specimen it images. Variations in the optical properties of the specimen introduce optical distortions, known as aberrations, that compromise image quality.  This is a particular problem when imaging deep into thick specimens such as skin or brain tissue.  Ultimately, the aberrations restrict the amount of the specimen that can be observed by the microscope, the depth often being limited to a few cellular layers near the surface. This is a serious limitation if one wants to observe cells and their processes in their natural environment, rather than on a microscope slide.  I am developing microscopes that will remove the problematic aberrations and enable high resolution imaging deep in specimens.Focused light also has other less well-known uses. It can be used to initiate chemical reactions that create polymer or metal building blocks for fabrication on the sub-micrometre scale. These blocks, with sizes as small as a few tens of nanometers, can be built into structures in a block-by-block fashion. Alternatively, larger blocks of material can be sculpted into shape using the high intensities of focused lasers.  These optical methods of fabrication show potential for use in the manufacture of nanotechnological devices.  When manufacturing such devices, the laser must be focused through parts of the pre-fabricated structure.  The greater the overall size and complexity of the structures, the more the effects of aberrations degrade the precision of the fabrication system. My research centres on the use of advanced techniques to measure and correct such distortions, restoring the accuracy of these optical systems.Traditional optical systems consist mainly of static elements, e.g. lenses for focusing, mirrors for reflecting and scanning, and prisms for separating different wavelengths. However, in the systems I use the aberrations are changing constantly. Therefore they require an adaptive method of correction in which the aberrations are dynamically compensated. These adaptive optics techniques were originally developed for astronomical and military purposes, for stabilising and de-blurring telescope images of stars and satellites.  Such images are affected by the aberrations introduced by turbulence in the Earth's atmosphere.  The most obvious manifestation of this is the twinkling of stars seen by the naked eye. Recent technological developments, such as compact and affordable deformable mirrors for compensating the optical distortions, mean that this technology is now being developed for more down-to-Earth reasons. This has opened up the possibility of using adaptive optics in smaller scale applications.In conjunction with researchers in Japan and Australia, I will develop adaptive optical fabrication systems that will be able to produce complex micrometre-scale structures with greater accuracy than was previously possible.  With biologists in the University of Oxford, I will use adaptive optics to increase the capabilities of microscopes in imaging deep into thick specimens.  This will enable biologists to learn more about the processes that occur within cells and the development of organisms.  The aberration correction technology will also have use in other areas such as medical imaging, optical communications and astronomy."
	},
	{
		"grant":8,
		"ID": "EP/E056091/1",
		"Title": "Semantics of Nondeterminism: Functions, Strategies and Bisimulation",
		"PIID": "98879",
		"Scheme": "Advanced Fellowship",
		"StartDate": "01/01/2008",
		"EndDate": "31/12/2012",
		"Value": "416684",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "98879", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A key question in the theory of programming is: when are two programs equivalent?  The answer will depend on what we think a program means.  For example, we can think of a program as a function: the user gives all the required input, and then the computer behaves accordingly.  Or we can think of it as a  strategy for a game between the computer and the user: the computer gives some information, the user responds, the computer gives some more information, the user responds, and so on.  These viewpoints have been extremely fruitful in recent years.To reason about a computer system, it is often necessary to idealize it as nondeterministic, i.e.  possessing a range of possible behaviours. The factors that determine its actual behaviour are too low-level and complex to consider explicitly. But this apparently simple idea has ramifications for the theory of programming language semantics that are not well understood. They centre on the same questions: when are two programs equivalent, and what do programs mean?  Previous research has used mathematical structures known from the theory of deterministic programs. But these have limited applicability to nondeterministic programs, and lead to somewhat awkward notions of equivalence. This research will proceed in the opposite direction: begin with certain computationally natural notions of equivalence, and investigate what structures they lead to.  In some cases (thinking of programs as strategies), these are likely to be structures that we already know, but, by proceeding in this way, we aim to relate them more closely to the way programs actually behave.In other cases (thinking of programs as functions), completely new structures will be required.  Some mysterious theorems have been proved that show that, in a sense, all programs (of a certain kind) share some behaviour---yet they do not tell us what this behaviour is.  We will therefore undertake a careful examination of programs' behaviour to solve this mystery, and thereby obtain the required structures."
	},
	{
		"grant":9,
		"ID": "EP/E059600/1",
		"Title": "Measurement-based quantum computing and its relation to other quantum models",
		"PIID": "-111291",
		"Scheme": "Advanced Fellowship",
		"StartDate": "01/03/2008",
		"EndDate": "31/07/2013",
		"Value": "400906",
		"ResearchArea": "Logic and Combinatorics",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-111291", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "My research proposal focuses on the field of quantum computation and information theory, a rapidly growing cross-disciplinary field of great importance from both a fundamental and technological perspective. As predicted by Moore's law, the miniaturisation has been the key element in the highly successful quest for more powerful information processing devices in the recent decades. However these advances reach now a fundamental limit where one can no longer ignore microscopic quantum phenomena, and needs quantum engineering to take the challenge.Fortunately, various physical implementations bring quantum computers closer to a reality, while theoretical results exhibit several remarkable advantages of quantum computers over classical ones. It has been predicted that any large-scale overhaul of information science in the 21st century will have to include quantum information. This is evident from the launch of start-up companies which conduct research on quantum information processing such at Quantique, MagiQ and D-Waves, as well as from the creation of quantum research teams within well established firms such as IBM, Microsoft and HP.This research proposal specifically targets a novel form of quantum information processing, called measurement-based quantum computing (MQC), where the key twin notions that distinguish quantum information processing from its classical counterpart, that is Entanglement (creating non-local correlations between quantum elements),and Measurement (observing a quantum system), are the explicit driving force of computation. Such a new paradigm has been so far mainly investigated by physicists with a specific focus on implementation, involving many research groups in UK. I propose to investigate the more computational and mathematical sides, and exploit the main characteristic of this model, namely that any computation can be broken down into a round of global operations (involving more than one quantum element), and a subsequent round of only local ones (together with classical communication). This has potential consequences in the particular questions I wish to address: what is the depth complexity of such computations (how many low-level operations can be applied simultaneously), are there hitherto unknown classes of computations one can realise with strong constraints on the needed quantum resources (using a polynomial number of quantum elements), can we design new commitment protocols (some party is committing to a choice only to be revealed at a later time of his choice), and hiding protocols (some party is drawing on another's computing resources without revealing what for), and perhaps more ambitiously new MQC-specific schemes for redundant computations that will protect computations from errors induced by unavoidable contacts with the environment.  A positive answer to one of the above questions would lend further credence in the MQC model as a strong contender in the elusive search for a scalable implementation of quantum computing."
	},
	{
		"grant":10,
		"ID": "EP/E065317/1",
		"Title": "UK Silicon Photonics",
		"PIID": "48924",
		"Scheme": "Standard Research",
		"StartDate": "19/02/2008",
		"EndDate": "18/08/2013",
		"Value": "266915",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "48924", "Role": "Principal Investigator"},
		{"ID": "12074", "Role": "Co Investigator"},
		{"ID": "111210", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Intel Corporation"},
		{"Number": "1", "Name": "QinetiQ (Malvern)"}
		],
		"Summary": "Silicon Photonics is a field that has seen rapid growth and dramatic changes in the past 5 years. According to the MIT Communications Technology Roadmap, which aims to establish a common architecture platform across market sectors with a potential $20B in annual revenue, silicon photonics is among the top ten emerging technologies. This has in part been a consequence of the recent involvement of large semiconductor companies in the USA such as Intel and IBM, who have realised the enormous potential of the technology, as well as large investment in the field by DARPA in the USA under the Electronic and Photonic Integrated Circuit (EPIC) initiative. Significant investment in the technology has also followed in Japan, Korea, and to a lesser extent in the European Union (IMEC and LETI).  The technology offers an opportunity to revolutionise a range of application areas by providing excellent performance at moderate cost due primarily to the fact that silicon is a thoroughly studied material, and unsurpassed in quality of fabrication with very high yield due to decades of investment from the microelectronics industry. The proposed work is a collaboration between 5 UK Universities (Surrey, St. Andrews, Leeds, Warwick and Southampton) with input from the industrial sector both in the UK and the USA. We will target primarily the interconnect applications, as they are receiving the most attention worldwide and have the largest potential for wealth creation, based on the scalability of silicon-based processes. However, we will ensure that our approach is more broadly applicable to other applications. This can be achieved by targeting device functions that are generic, and introducing specificity only when a particular application is targeted. The generic device functions we envisage are as follows: Optical modulation; coupling from fibre to sub-micron silicon waveguides; interfacing of optical signals within sub micron waveguides; optical filtering; optical/electronic integration; optical detection; optical amplification. In each of these areas we propose to design, fabricate, and test devices that will improve the current state of the art.  Subsequently we will integrate these optical devices with electronics to further improve the state of the art in optical/electronic integration in silicon.We have included in our list of objectives, benchmark targets for each of our proposed devices to give a clear and unequivocal statement of ambition and intent.We believe we have assembled an excellent consortium to deliver the proposed work, and to enable the UK to compete on an international level.  The combination of skills and expertise is unique in the UK and entirely complementary within the consortium.  Further, each member of the consortium is recognised as a leading international researcher in their field.The results of this work have the potential to have very significant impact to wealth creation opportunities within the UK and around the world.  For example emerging applications such as optical interconnect, both intra-chip, and inter-chip, as well as board to board and rack to rack, and Fibre To The Home for internet and other large bandwidth applications, will require highly cost effective and mass production solutions.  Silicon Photonics is a seen as a leading candidate technology in these application areas if suitable performance can be achieved."
	},
	{
		"grant":11,
		"ID": "EP/E501214/1",
		"Title": "Bristol Centre for Complexity Sciences",
		"PIID": "27122",
		"Scheme": "Capacity Build in Complexity Science",
		"StartDate": "30/10/2006",
		"EndDate": "29/10/2014",
		"Value": "3982000",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Mathematics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "27122", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hewlett-Packard Ltd"},
		{"Number": "1", "Name": "ICERA Inc"},
		{"Number": "2", "Name": "LSCITS"},
		{"Number": "3", "Name": "Myriad-Technology"},
		{"Number": "4", "Name": "Systems Thinking UK"},
		{"Number": "5", "Name": "Toshiba Research Europe Ltd"}
		],
		"Summary": "The Bristol Centre for Complexity Sciences (BCCS) will meet the present national need to address the implications of Complexity within the cognate applications areas of Engineering, Life & Molecular Sciences by setting up an integrated multidisciplinary research and training environment in complexity science. The selected application areas in aspects of biology, chemistry and engineering offer major challenges to complexity but their resolution requires innovative theory to be developed. The vision for the BCCS is that it will grow a dynamic hub of theoretical and computational expertise that interacts with and connects the application areas, which in turn link to cross-cutting University research themes."
	},
	{
		"grant":12,
		"ID": "EP/E501311/1",
		"Title": "Capacity Building in Complexity Science at Warwick",
		"PIID": "41081",
		"Scheme": "Capacity Build in Complexity Science",
		"StartDate": "01/12/2006",
		"EndDate": "30/11/2014",
		"Value": "4142427",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Mathematics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "41081", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hewlett-Packard Ltd"},
		{"Number": "1", "Name": "IBM United Kingdom Limited"},
		{"Number": "2", "Name": "Land Rover Group Ltd"},
		{"Number": "3", "Name": "NCCRCD"},
		{"Number": "4", "Name": "NHS Institute for Innovation and Improve"},
		{"Number": "5", "Name": "RAND Europe Cambridge Ltd"},
		{"Number": "6", "Name": "The British Antarctic Survey"}
		],
		"Summary": "We group our doctoral programme under six key areas of Complexity Science:agent-based modelling;networks and emergent behaviour;self-organisation and assembly;nonlinear dynamics and modelling in the presence of noise;spatio-temporal complexity, quantification and modelling;management and bounding of complexity.The development of these themes and applications proposed within them addresses societal, financial and technical performance at the system level, key for national competitiveness. Particular key current societal problems are also addressed, such as crime, terrorism, epidemics, computer viruses, and understanding how to control their spread.We propose medically related projects of clear importance to the NHS (diabetes, back pain, consultation effectiveness), financial applications address the effective management of UK markets and financial intervention, and we include a diverse range of environmental interests for the UK, from habitats to our skies and space weather. Our neuro-related network projects and intelligent agent and trusted agent researches relate to close UK priorities identified by DTI Foresight programme, and we also look to longer term possibilities such as self-assembling molecular-scale technology."
	},
	{
		"grant":13,
		"ID": "EP/E501680/1",
		"Title": "LSI Doctoral Training Centres-University of Oxford",
		"PIID": "53062",
		"Scheme": "LSI Doctoral Training Centres",
		"StartDate": "01/10/2007",
		"EndDate": "30/09/2012",
		"Value": "1109942",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "53062", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Summary of themes and need for Centre: The Oxford Life Sciences Interface Doctoral Training Centre provides training at the LSI, and facilitates leading-edge research, in the mathematical, physical and engineering science techniques that underpin three flagship application projects: the joint EPSRC/MRC IRC  From Medical Signals to Useful Clinical Information ; the joint EPSRC/BBSRC/MRC IRC in Bionanotechnology; and the JIF-funded Oxford Centre for Gene Function. These projects each involve extensive interdisciplinary collaborations between world-leading theoretical and experimental research groups drawn from across the physical and life sciences. These projects have substantial scientific overlap and complementarily, and share an enormous demand for researchers from a physical science background. Each of the application areas enjoys strong industrial support and collaborations from across the biotechnology, pharmaceutical and biomedical sectors, and the demand for researchers with a physical sciences background but with an appreciation and understanding of the wider issues in life science research both within these industrial sectors and within the wider academic community, is huge."
	},
	{
		"grant":14,
		"ID": "EP/F001096/1",
		"Title": "LSCITS-RPv2: Large-Scale Complex IT Systems Initiative - Research Programme v2",
		"PIID": "84767",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2007",
		"EndDate": "30/06/2013",
		"Value": "5055294",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "84767", "Role": "Principal Investigator"},
		{"ID": "10482", "Role": "Co Investigator"},
		{"ID": "92244", "Role": "Co Investigator"},
		{"ID": "9080", "Role": "Co Investigator"},
		{"ID": "14819", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We intend to establish a UK National strategic coordinated research and training initiative centred on issues in the science and engineering of Large-Scale Complex IT Systems (LSCITS: pronounced else-its). We propose the creation of a coordinated national network of researchers in industry and academia with the skills and knowledge appropriate to dealing with the problems of designing, managing, and main-taining current and future LSCITS across their life-cycles. Pursu-ng research that addresses these problems, and training the researchers who will undertake that research or apply its findings in practice, are two major strategic needs at the national level. The proposed Initiative's programme of work is intended to last for five years, but it is our intent that this be viewed as a period of pump-priming  ramp-up , establishing at steady-state a well-coordinated community of interacting researchers, self-sustaining by generating ongoing financial support from public funds and from industrial sponsorship and collaborations. Thus we include here a discussion of our plans for continuing the Initiative beyond its fifth year. Total funds requested from EPSRC are approx 9.6m, of which 4m is dedicated to establishing and running an Engineering Doctorate (EngD) training programme, and 5.6m is dedicated to funding a coordinated set of research work-packages. Both the EngD and the research programme are intended to be conducted with close involvement from industry. The EngD proposal will be submitted separately, once the outcome of this application for 5.6m funding of the research programme is known.Our proposal marks the formation of a new partnership between leading academics from five UK universities, who each have significant histories of research and education leadership in complementary aspects of the science and engineering of LSCITS. Additionally, all five of the authors of this proposal have very strong links into relevant companies across several major sectors of UK industry. Companies who have indicated an intent to be-come involved in significant ways at this stage include Accenture, BAE Systems, BT, DSTL, Hewlett-Packard, IBM, Praxis, Qinetiq, and Rolls-Royce. Our Case For Support document presents a broad picture of our proposal and sets out what we plan to achieve. It should be read in conjunction with its extensive appendices which provide inter alia a significantly more detailed discussion of the background, research work-plan, EngD, exploitation/IPR strategies, and community-building activities.Our overall approach can be characterised as follows. The complexity that is inherent in large-scale systems stems from a variety of causes. These systems are often designed to address so-called 'wicked problems'  which, by their very nature cannot be completely defined: they have to meet the (rapidly changing) needs of diverse stakeholders; they must integrate with a range of other legacy systems, processes and policies; they may be critical systems that have to deliver both a high level of performance and dependability; and they are profoundly affected by political influences in the organisations developing and procuring the system, and in the broader system's environment. While it would be simplistic to suggest that we can address all of the problems, we believe that we can make significant progress by altering our perspective on the engineering of large-scale complex IT systems. Rather than considering this to be a problem of specifying, developing, deploying and operating a large-scale system, , we believe that we should look at the problem as being a system of systems problem. By examining the relationships between the different systems that make up and interact with each other, and the systems involved in procuring, deploying and operating software, we believe that we can make headway in tackling the issue of complexity."
	},
	{
		"grant":15,
		"ID": "EP/F001428/2",
		"Title": "UK Silicon Photonics",
		"PIID": "13096",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2012",
		"EndDate": "31/12/2013",
		"Value": "806545",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "13096", "Role": "Principal Investigator"},
		{"ID": "-165020", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "QinetiQ (Malvern)"}
		],
		"Summary": "Silicon Photonics is a field that has seen rapid growth and dramatic changes in the past 5 years. According to the MIT Communications Technology Roadmap, which aims to establish a common architecture platform across market sectors with a potential $20B in annual revenue, silicon photonics is among the top ten emerging technologies. This has in part been a consequence of the recent involvement of large semiconductor companies in the USA such as Intel and IBM, who have realised the enormous potential of the technology, as well as large investment in the field by DARPA in the USA under the Electronic and Photonic Integrated Circuit (EPIC) initiative. Significant investment in the technology has also followed in Japan, Korea, and to a lesser extent in the European Union (IMEC and LETI).  The technology offers an opportunity to revolutionise a range of application areas by providing excellent performance at moderate cost due primarily to the fact that silicon is a thoroughly studied material, and unsurpassed in quality of fabrication with very high yield due to decades of investment from the microelectronics industry. The proposed work is a collaboration between 5 UK Universities (Surrey, St. Andrews, Leeds, Warwick and Southampton) with input from the industrial sector both in the UK and the USA. We will target primarily the interconnect applications, as they are receiving the most attention worldwide and have the largest potential for wealth creation, based on the scalability of silicon-based processes. However, we will ensure that our approach is more broadly applicable to other applications. This can be achieved by targeting device functions that are generic, and introducing specificity only when a particular application is targeted. The generic device functions we envisage are as follows: Optical modulation; coupling from fibre to sub-micron silicon waveguides; interfacing of optical signals within sub micron waveguides; optical filtering; optical/electronic integration; optical detection; optical amplification. In each of these areas we propose to design, fabricate, and test devices that will improve the current state of the art. Subsequently we will integrate these optical devices with electronics to further improve the state of the art in optical/electronic integration in silicon.We have included in our list of objectives, benchmark targets for each of our proposed devices to give a clear and unequivocal statement of ambition and intent.We believe we have assembled an excellent consortium to deliver the proposed work, and to enable the UK to compete on an international level. The combination of skills and expertise is unique in the UK and entirely complementary within the consortium. Further, each member of the consortium is recognised as a leading international researcher in their field.The results of this work have the potential to have very significant impact to wealth creation opportunities within the UK and around the world. For example emerging applications such as optical interconnect, both intra-chip, and inter-chip, as well as board to board and rack to rack, and Fibre To The Home for internet and other large bandwidth applications, will require highly cost effective and mass production solutions. Silicon Photonics is a seen as a leading candidate technology in these application areas if suitable performance can be achieved."
	},
	{
		"grant":16,
		"ID": "EP/F001622/1",
		"Title": "UK Silicon Photonics",
		"PIID": "41641",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2008",
		"EndDate": "30/11/2013",
		"Value": "1155935",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "41641", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Intel Corporation"},
		{"Number": "1", "Name": "QinetiQ (Malvern)"}
		],
		"Summary": "Silicon Photonics is a field that has seen rapid growth and dramatic changes in the past 5 years. According to the MIT Communications Technology Roadmap, which aims to establish a common architecture platform across market sectors with a potential $20B in annual revenue, silicon photonics is among the top ten emerging technologies. This has in part been a consequence of the recent involvement of large semiconductor companies in the USA such as Intel and IBM, who have realised the enormous potential of the technology, as well as large investment in the field by DARPA in the USA under the Electronic and Photonic Integrated Circuit (EPIC) initiative. Significant investment in the technology has also followed in Japan, Korea, and to a lesser extent in the European Union (IMEC and LETI).  The technology offers an opportunity to revolutionise a range of application areas by providing excellent performance at moderate cost due primarily to the fact that silicon is a thoroughly studied material, and unsurpassed in quality of fabrication with very high yield due to decades of investment from the microelectronics industry. The proposed work is a collaboration between 5 UK Universities (Surrey, St. Andrews, Leeds, Warwick and Southampton) with input from the industrial sector both in the UK and the USA. We will target primarily the interconnect applications, as they are receiving the most attention worldwide and have the largest potential for wealth creation, based on the scalability of silicon-based processes. However, we will ensure that our approach is more broadly applicable to other applications. This can be achieved by targeting device functions that are generic, and introducing specificity only when a particular application is targeted. The generic device functions we envisage are as follows: Optical modulation; coupling from fibre to sub-micron silicon waveguides; interfacing of optical signals within sub micron waveguides; optical filtering; optical/electronic integration; optical detection; optical amplification. In each of these areas we propose to design, fabricate, and test devices that will improve the current state of the art. Subsequently we will integrate these optical devices with electronics to further improve the state of the art in optical/electronic integration in silicon.We have included in our list of objectives, benchmark targets for each of our proposed devices to give a clear and unequivocal statement of ambition and intent.We believe we have assembled an excellent consortium to deliver the proposed work, and to enable the UK to compete on an international level. The combination of skills and expertise is unique in the UK and entirely complementary within the consortium. Further, each member of the consortium is recognised as a leading international researcher in their field.The results of this work have the potential to have very significant impact to wealth creation opportunities within the UK and around the world. For example emerging applications such as optical interconnect, both intra-chip, and inter-chip, as well as board to board and rack to rack, and Fibre To The Home for internet and other large bandwidth applications, will require highly cost effective and mass production solutions. Silicon Photonics is a seen as a leading candidate technology in these application areas if suitable performance can be achieved"
	},
	{
		"grant":17,
		"ID": "EP/F001894/1",
		"Title": "UK Silicon Photonics",
		"PIID": "107181",
		"Scheme": "Standard Research",
		"StartDate": "05/02/2008",
		"EndDate": "04/08/2013",
		"Value": "110120",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "107181", "Role": "Principal Investigator"},
		{"ID": "-111019", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Intel Corporation"},
		{"Number": "1", "Name": "QinetiQ (Malvern)"}
		],
		"Summary": "Silicon Photonics is a field that has seen rapid growth and dramatic changes in the past 5 years. According to the MIT Communications Technology Roadmap, which aims to establish a common architecture platform across market sectors with a potential $20B in annual revenue, silicon photonics is among the top ten emerging technologies. This has in part been a consequence of the recent involvement of large semiconductor companies in the USA such as Intel and IBM, who have realised the enormous potential of the technology, as well as large investment in the field by DARPA in the USA under the Electronic and Photonic Integrated Circuit (EPIC) initiative. Significant investment in the technology has also followed in Japan, Korea, and to a lesser extent in the European Union (IMEC and LETI).  The technology offers an opportunity to revolutionise a range of application areas by providing excellent performance at moderate cost due primarily to the fact that silicon is a thoroughly studied material, and unsurpassed in quality of fabrication with very high yield due to decades of investment from the microelectronics industry. The proposed work is a collaboration between 5 UK Universities (Surrey, St. Andrews, Leeds, Warwick and Southampton) with input from the industrial sector both in the UK and the USA. We will target primarily the interconnect applications, as they are receiving the most attention worldwide and have the largest potential for wealth creation, based on the scalability of silicon-based processes. However, we will ensure that our approach is more broadly applicable to other applications. This can be achieved by targeting device functions that are generic, and introducing specificity only when a particular application is targeted. The generic device functions we envisage are as follows: Optical modulation; coupling from fibre to sub-micron silicon waveguides; interfacing of optical signals within sub micron waveguides; optical filtering; optical/electronic integration; optical detection; optical amplification. In each of these areas we propose to design, fabricate, and test devices that will improve the current state of the art.  Subsequently we will integrate these optical devices with electronics to further improve the state of the art in optical/electronic integration in silicon.We have included in our list of objectives, benchmark targets for each of our proposed devices to give a clear and unequivocal statement of ambition and intent.We believe we have assembled an excellent consortium to deliver the proposed work, and to enable the UK to compete on an international level.  The combination of skills and expertise is unique in the UK and entirely complementary within the consortium.  Further, each member of the consortium is recognised as a leading international researcher in their field.The results of this work have the potential to have very significant impact to wealth creation opportunities within the UK and around the world.  For example emerging applications such as optical interconnect, both intra-chip, and inter-chip, as well as board to board and rack to rack, and Fibre To The Home for internet and other large bandwidth applications, will require highly cost effective and mass production solutions.  Silicon Photonics is a seen as a leading candidate technology in these application areas if suitable performance can be achieved."
	},
	{
		"grant":18,
		"ID": "EP/F002548/1",
		"Title": "UK Silicon Photonics",
		"PIID": "32627",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2008",
		"EndDate": "31/12/2013",
		"Value": "588714",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "32627", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Intel Corporation"},
		{"Number": "1", "Name": "QinetiQ (Malvern)"}
		],
		"Summary": "Silicon Photonics is a field that has seen rapid growth and dramatic changes in the past 5 years. According to the MIT Communications Technology Roadmap, which aims to establish a common architecture platform across market sectors with a potential $20B in annual revenue, silicon photonics is among the top ten emerging technologies. This has in part been a consequence of the recent involvement of large semiconductor companies in the USA such as Intel and IBM, who have realised the enormous potential of the technology, as well as large investment in the field by DARPA in the USA under the Electronic and Photonic Integrated Circuit (EPIC) initiative. Significant investment in the technology has also followed in Japan, Korea, and to a lesser extent in the European Union (IMEC and LETI).  The technology offers an opportunity to revolutionise a range of application areas by providing excellent performance at moderate cost due primarily to the fact that silicon is a thoroughly studied material, and unsurpassed in quality of fabrication with very high yield due to decades of investment from the microelectronics industry. The proposed work is a collaboration between 5 UK Universities (Surrey, St. Andrews, Leeds, Warwick and Southampton) with input from the industrial sector both in the UK and the USA. We will target primarily the interconnect applications, as they are receiving the most attention worldwide and have the largest potential for wealth creation, based on the scalability of silicon-based processes. However, we will ensure that our approach is more broadly applicable to other applications. This can be achieved by targeting device functions that are generic, and introducing specificity only when a particular application is targeted. The generic device functions we envisage are as follows: Optical modulation; coupling from fibre to sub-micron silicon waveguides; interfacing of optical signals within sub micron waveguides; optical filtering; optical/electronic integration; optical detection; optical amplification. In each of these areas we propose to design, fabricate, and test devices that will improve the current state of the art.  Subsequently we will integrate these optical devices with electronics to further improve the state of the art in optical/electronic integration in silicon.We have included in our list of objectives, benchmark targets for each of our proposed devices to give a clear and unequivocal statement of ambition and intent.We believe we have assembled an excellent consortium to deliver the proposed work, and to enable the UK to compete on an international level.  The combination of skills and expertise is unique in the UK and entirely complementary within the consortium.  Further, each member of the consortium is recognised as a leading international researcher in their field.The results of this work have the potential to have very significant impact to wealth creation opportunities within the UK and around the world.  For example emerging applications such as optical interconnect, both intra-chip, and inter-chip, as well as board to board and rack to rack, and Fibre To The Home for internet and other large bandwidth applications, will require highly cost effective and mass production solutions.  Silicon Photonics is a seen as a leading candidate technology in these application areas if suitable performance can be achieved."
	},
	{
		"grant":19,
		"ID": "EP/F00897X/1",
		"Title": "Liquid Crystal Photonics",
		"PIID": "10928",
		"Scheme": "Platform Grants",
		"StartDate": "01/04/2008",
		"EndDate": "31/03/2013",
		"Value": "908161",
		"ResearchArea": "Displays",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "10928", "Role": "Principal Investigator"},
		{"ID": "45311", "Role": "Co Investigator"},
		{"ID": "27827", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Advance Nano Tech Inc"},
		{"Number": "1", "Name": "Alps Electric UK Ltd"},
		{"Number": "2", "Name": "Dow Corning Ltd"},
		{"Number": "3", "Name": "Ericsson Ltd"}
		],
		"Summary": "Liquid crystal devices have come of age, having fulfilled their promise of several decades ago by increasingly dominating the market for displays. The industry has become global and the manufacturing is mostly in the Far East. This is not the end, but the beginning and UK scientists and engineers that have played a distinguished role in these developments must work with the global industry and develop strategies that enable us to remain engaged.We note that innovation continues rapidly and that the massive investment in this technology has produced a remarkable diversity of materials and electro-optic phenomena that are now starting to be applied in photonic devices in communications and the biosciences.The title  Liquid Crystal Photonics  is used to suggest that opto-electronics and displays should be embraced under one heading, reliant as they are on closely related optical functionality in similar materials. The strategic importance of phase-only real time holography by liquid crystal components is emerging into the marketplace in both optical communications and in displays.  In displays the changes are probably going to be disruptive, producing highly miniature micro projectors with flexible control of all image attributes.  Initially these are destined for 'micro projectors' for mobile phones etc., but will ultimately move to rear projection high definition TV.  In optical communications the integration of several functions into software controlled modules matches closely the requirements of the now crucial metropolitan area network. Flexible, compact and low cost optical routers and add-drop-multiplexers for wavelength division (WDM) multiplexed systems may become a common sight in urban areas.  The deep-sub-micron silicon CMOS technology that is used for liquid crystal over silicon (LCOS) backplanes is now mass producing complex low-cost integrated circuits with a minimum feature size below 100nm. We can therefore now electrically address liquid crystals using nano structure electrodes to open up applications requiring sub-wavelength photonic crystal structures (e.g. exhibiting electrically switchable surface alignment of liquid crystals, form birefringence and optical band gaps).  As in the case of 'conventional' phase-only holography, the unique advantages resulting from the use of silicon CMOS backplanes are programmability and software control. It may be possible to enhance the already remarkable electro-optic properties of liquid crystals, enabling such properties as negative refractive index, programmable scattering and ultra-high-speed switching to be obtained.In general, liquid crystals respond dramatically to nano structures in the range from tens to hundreds of nanometres with or without electrical fields, e.g. liquid crystal director fields are aligned in contact with surface topography in this range.  The interactions that occur between free particles embedded in nematic liquid crystals (due to both elastic interactions and Casimir interactions) are important issues in polymer based nano-composite materials and director deformations on this scale are important in structured dielectrics, semiconductors and conductors in the advance of polymer electronics.  These are substantial areas of scientific and technological interest where the infra structure of liquid science and technology (that has been driven by the display industry) will be a major factor in future developments."
	},
	{
		"grant":20,
		"ID": "EP/F02827X/1",
		"Title": "Visual Media Research Platform Grant",
		"PIID": "45542",
		"Scheme": "Platform Grants",
		"StartDate": "01/03/2009",
		"EndDate": "31/05/2014",
		"Value": "861905",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51",
		"Investigators":[
		{"ID": "45542", "Role": "Principal Investigator"},
		{"ID": "56864", "Role": "Co Investigator"},
		{"ID": "-115276", "Role": "Co Investigator"},
		{"ID": "8936", "Role": "Co Investigator"},
		{"ID": "94934", "Role": "Co Investigator"},
		{"ID": "8006", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Visual Media Platform Grant awarded in 2003 has successfully underpinned the expansion of Visual Media Research within CVSSP by supporting key experienced post-doctoral research personnel on a stable long-term basis. Strategically the platform grant has allowed the flexibility to pursue adventurous research providing the foundation to initiate a number of new research projects and collaborations with both industry and international research groups. Stable support of a critical-mass of experience has led to the groups research grant portfolio doubling over the period 2003-07. In addition the platform grant has directly contributed to pioneering two new technologies leading to commercial exploitation: 3D video capture of facial dynamics; and whole-body surface motion capture. The platform grant has allowed the initiation of a number of international collaborations with leading research teams in the US, Europe and Japan. EPSRC platform grant support for Visual Media Research has been pivotal to meeting the strategic objective of increasing the international profile and impact of the research group and individual researchers by enabling adventurous research contributing step changes to the state-of-the-art.The strategic objective of the platform grant renewal is for the Visual Media Research within CVSSP to lead fundamental research internationally to pioneer new technologies which impact directly on industry practice. This will build on the proven success of the platform grant awarded in 2003 as a mechanism for supporting long-term research continuity of key post-doctoral researchers, pursuing adventurous research to initiate new research directions and contributing advances to the leading edge internationally . Renewal of the platform grant is requested to continue to reinforce the critical mass of expertise and knowledge of specialist facilities required to contribute advances in both fundamental understanding and pioneering new technology.  Strategically the platform grant renewal aims to build on recent advances in dynamic scene analysis towards understanding and modelling real-world scenes. Increasing relevance and collaboration with the UK entertainment and communications industries, and focusing on international collaborations initiated under the existing platform grant with leading groups in the US (Washington), Japan (Kyoto) and Europe(INRIA,MPI,EPFL,KTH,CTU)."
	},
	{
		"grant":21,
		"ID": "EP/F032641/1",
		"Title": "Securing the Future: Expanding the cs4fn (Computer Science for Fun) Project",
		"PIID": "60141",
		"Scheme": "PPE",
		"StartDate": "01/05/2008",
		"EndDate": "30/09/2013",
		"Value": "661645",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "60141", "Role": "Principal Investigator"},
		{"ID": "32809", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "British Computer Society"},
		{"Number": "1", "Name": "Higher Education Academy"}
		],
		"Summary": "Computer Science is facing an international crisis. For many reasons, the number of students applying to University to study the subject has been dropping for several years. There are too few Computer Scientists to meet the country's future needs. This problem was singled out as the most significant risk to the UK computing industry in the recent EPSRC International Review. The possible harmful impact to the UK economy is great. To overcome this requires urgent and major, long-term effort.A critical part of the problem is that many children and teachers do not fully understand what Computer Science is. Many do not see either the intellectual challenge or the relevance of computer science to the world around them and are largely unaware of the many subjects computer science touches on. To choose it as a career, students need not only to understand what it involves but must also be enthused. This kind of problem cannot be solved in a short project, but only through consolidated effort over a long period.The ongoing cs4fn project, that this proposal supports, combines a website, magazine and live shows on computer science research using a special off-beat style. We have already shown that cs4fn enthuses school children and helps them understand.This can be seen by the many positive comments of students and teachers we have received as well as, for example, from the numbers of people visiting the website and asking for the magazine. cs4fn has also helped successfully increased University applications locally.cs4fn tackles the significant problems in motivating school students to choose computer science as a subject to study, demonstrating for example the difference between school ICT and Computer Science. It also provides an effective link between school and university study, introducing university level topics through fun activities.cs4fn introduces its readers to the virtues of computational thinking. This is a key part of a computer science education, and many have argued it is the key intellectual skill for the 21st century, preparing students for a wide range of computing and non-computing careers. cs4fn makes readers think about ethical issues and the way new technology shapes society. It also provides a resource to excite students about science and engineering more generally, helping to deal with similar issues in other subjects.This proposal seeks funds to both continue and expand cs4fn to help solve these strategically significant national issues by creating a national marketing campaign around it. We believe that by providing this national resource the general level of applications can be improved across the UK just as we have already achieved locally.To have a wide and lasting impact it is vitally important that the project lasts at least 5 years, not only to reach successive years of school children, but also to reinforce the message both to those children and the teachers who advise them. This extended time is also needed to help other Universities build up skills in communicating the excitement of Computer Science to the public.The effect of the project over the 5 years will be independently evaluated and the lessons learned as to what works, and how, will be shared with others. A business plan for its continued stability will also be developed."
	},
	{
		"grant":22,
		"ID": "EP/F033206/1",
		"Title": "Verifying Interoperability Requirements in Pervasive Systems",
		"PIID": "15688",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2008",
		"EndDate": "28/02/2013",
		"Value": "470410",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "15688", "Role": "Principal Investigator"},
		{"ID": "110250", "Role": "Co Investigator"},
		{"ID": "60345", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Kelvin Connect Ltd"},
		{"Number": "1", "Name": "Spartan Solutions Ltd"}
		],
		"Summary": "The success of pervasive computing depends crucially on the ability to build, maintain and augment interoperable systems: components from different manufacturers built at different times are required to interact to achieve the user's overall goals.Pervasive systems often contain devices which must operate in very different environments and connect together in different ways,  e.g., over ad-hoc wireless connections to a variety of systems, and still satisfy all the desired security and performance properties.  Our approach to verifying these properties is to identify interoperability requirements for the interaction between the devices and their environment.  These requirements introduce also an important layer of abstraction because they allow modularity in the verification process: it suffices to show that each mobile device or fixed component meets the interoperability requirements, and that the interoperability requirements entail the desired high-level properties.We argue that this verification framework makes it possible to adapt and extend techniques (such as model checking and process algebras) which have traditionally been used for verifying properties of small homogeneous systems, to large heterogenous systems.  To support this thesis, we will develop techniques to verify properties concerning important aspects of heterogenous systems' security, individual and collective behaviour, performance and privacy.  We will use the formal techniques to verify the consequent interoperability requirements, and evaluate their effectiveness through case studies.Note that our focus is on the verification of designs; in particular we focus on the design of basic component behaviours and the protocols which dictate access to them and interaction between them.  It is important to note our intention is not to develop pervasive computing systems as such, but rather to draw motivation from, and test our ideas in, a number of planned and existing systems.Three  case studies are planned; two are with industrial collaborators. The case studies will be drawn from three layers typical within pervasive systems: application, infrastructure and network. One industrial case study will be  a healthcare application.  One of its crucial features is the need for the monitoring device to operate in different environments. Hence a careful analysis of the necessary interoperability requirements is mandatory for this application. We will develop and apply our techniques as the system is designed, thus influencing directly the design of the application, motivating our techniques as we develop them,  and gaining  real life  experience of applying our techniques in the field.  In addition, our past experience indicates that we will also bring in further case studies, as the project develops.  Drawing on the variety of expertise of the members of the consortium, we hope to make a step change in verification technology by developing novel techniques and learning which techniques are most effective in different contexts. The outcomes will directly benefit  system designers, and indirectly, end users. They will include techniques applicable to a wide range of application domains, and results and lessons learned from three specific applications including a healthcare data capture system and RFID system infrastructure."
	},
	{
		"grant":23,
		"ID": "EP/F033370/1",
		"Title": "Holistic Design of Power Amplifiers for Future Wireless Systems",
		"PIID": "106698",
		"Scheme": "Standard Research",
		"StartDate": "18/08/2008",
		"EndDate": "17/08/2013",
		"Value": "774411",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "106698", "Role": "Principal Investigator"},
		{"ID": "10525", "Role": "Co Investigator"},
		{"ID": "1080", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Power amplifiers are one of the main fundamental building blocks of all modern wireless communications systems. They are used in all base stations and all the mobile units which are currently available. To maintain the required levels of system performance current commercially available amplifiers are designed to operate with extremely poor levels of efficiency which means they consume far more energy than is strictly necessary. For example current base stations in the UK operate at an efficiency level of approximately 12% this results in over 609,000 of CO2 emissions into the atmosphere on an annual basis. If these base stations were to be 50% efficient CO2 emissions could be cut by over 450,000 tons per year. The design of highly efficient and highly linear power amplifiers is an extremely complex process. At present the design of highly linear amplifiers is carried out using a trial-and-error based approach where designs are drawn up, then a prototype is produced and design issues are identified. The whole process is repeated until an optimum solution is reached. This has lead manufacturers to take a very risk adverse approach to amplifier design which has resulted in very inefficient systems. There is an increasing need to develop wireless communications systems with increased digital data throughput.  For example, in recent years we have seen the roll out of 3rd generation 3G mobile communication systems and the increasing use of wireless LAN systems. It is highly likely that the future so-called 4th generation systems will contain modulation schemes which also use wireless LAN technology. With the introduction of 3G systems it became clear that the existing design methodologies for the development and optimisation of amplifiers are labour intensive and time consuming. The present approach has become a key hindrance in the evaluation, development, and testing of modern communication systems. This proposal seeks to overcome these fundamental design issues through the establishment of a scientifically robust fully interlinked design methodology for nonlinear circuits. By combining the world-class power amplifier design expertise in Bristol and waveform measurement/engineering expertise /introduced and pioneered in Cardiff/ a scientifically robust nonlinear design methodology will be established in which the measured waveforms and waveform engineering will facilitate new methods of amplifier design and linearisation. The aim being a one pass design process for future communications systems which will result in the exploitation of this technology within a commercial setting."
	},
	{
		"grant":24,
		"ID": "EP/F033540/1",
		"Title": "Verifying Interoperability Requirements in Pervasive Systems",
		"PIID": "54555",
		"Scheme": "Standard Research",
		"StartDate": "08/10/2008",
		"EndDate": "07/03/2013",
		"Value": "418296",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "54555", "Role": "Principal Investigator"},
		{"ID": "45401", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Kelvin Connect Ltd"},
		{"Number": "1", "Name": "Spartan Solutions Ltd"}
		],
		"Summary": "The success of pervasive computing depends crucially on the ability to build, maintain and augment interoperable systems: components from different manufacturers built at different times are required to interact to achieve the user's overall goals.Pervasive systems often contain devices which must operate in very different environments and connect together in different ways,  e.g., over ad-hoc wireless connections to a variety of systems, and still satisfy all the desired security and performance properties.  Our approach to verifying these properties is to identify interoperability requirements for the interaction between the devices and their environment.  These requirements introduce also an important layer of abstraction because they allow modularity in the verification process: it suffices to show that each mobile device or fixed component meets the interoperability requirements, and that the interoperability requirements entail the desired high-level properties.We argue that this verification framework makes it possible to adapt and extend techniques (such as model checking and process algebras) which have traditionally been used for verifying properties of small homogeneous systems, to large heterogenous systems.  To support this thesis, we will develop techniques to verify properties concerning important aspects of heterogenous systems' security, individual and collective behaviour, performance and privacy.  We will use the formal techniques to verify the consequent interoperability requirements, and evaluate their effectiveness through case studies.Note that our focus is on the verification of designs; in particular we focus on the design of basic component behaviours and the protocols which dictate access to them and interaction between them.  It is important to note our intention is not to develop pervasive computing systems as such, but rather to draw motivation from, and test our ideas in, a number of planned and existing systems.Three  case studies are planned; two are with industrial collaborators. The case studies will be drawn from three layers typical within pervasive systems: application, infrastructure and network. One industrial case study will be  a healthcare application.  One of its crucial features is the need for the monitoring device to operate in different environments. Hence a careful analysis of the necessary interoperability requirements is mandatory for this application. We will develop and apply our techniques as the system is designed, thus influencing directly the design of the application, motivating our techniques as we develop them,  and gaining  real life  experience of applying our techniques in the field.  In addition, our past experience indicates that we will also bring in further case studies, as the project develops.  Drawing on the variety of expertise of the members of the consortium, we hope to make a step change in verification technology by developing novel techniques and learning which techniques are most effective in different contexts. The outcomes will directly benefit  system designers, and indirectly, end users. They will include techniques applicable to a wide range of application domains, and results and lessons learned from three specific applications including a healthcare data capture system and RFID system infrastructure."
	},
	{
		"grant":25,
		"ID": "EP/F033567/1",
		"Title": "Verifying Interoperability Requirements in Pervasive Systems",
		"PIID": "29766",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "31/03/2013",
		"Value": "438609",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "29766", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Kelvin Connect Ltd"},
		{"Number": "1", "Name": "Spartan Solutions Ltd"}
		],
		"Summary": "The success of pervasive computing depends crucially on the ability to build, maintain and augment interoperable systems: components from different manufacturers built at different times are required to interact to achieve the user's overall goals.Pervasive systems often contain devices which must operate in very different environments and connect together in different ways,  e.g., over ad-hoc wireless connections to a variety of systems, and still satisfy all the desired security and performance properties.  Our approach to verifying these properties is to identify interoperability requirements for the interaction between the devices and their environment.  These requirements introduce also an important layer of abstraction because they allow modularity in the verification process: it suffices to show that each mobile device or fixed component meets the interoperability requirements, and that the interoperability requirements entail the desired high-level properties.We argue that this verification framework makes it possible to adapt and extend techniques (such as model checking and process algebras) which have traditionally been used for verifying properties of small homogeneous systems, to large heterogenous systems.  To support this thesis, we will develop techniques to verify properties concerning important aspects of heterogenous systems' security, individual and collective behaviour, performance and privacy.  We will use the formal techniques to verify the consequent interoperability requirements, and evaluate their effectiveness through case studies.Note that our focus is on the verification of designs; in particular we focus on the design of basic component behaviours and the protocols which dictate access to them and interaction between them.  It is important to note our intention is not to develop pervasive computing systems as such, but rather to draw motivation from, and test our ideas in, a number of planned and existing systems.Three  case studies are planned; two are with industrial collaborators. The case studies will be drawn from three layers typical within pervasive systems: application, infrastructure and network. One industrial case study will be  a healthcare application.  One of its crucial features is the need for the monitoring device to operate in different environments. Hence a careful analysis of the necessary interoperability requirements is mandatory for this application. We will develop and apply our techniques as the system is designed, thus influencing directly the design of the application, motivating our techniques as we develop them,  and gaining  real life  experience of applying our techniques in the field.  In addition, our past experience indicates that we will also bring in further case studies, as the project develops.  Drawing on the variety of expertise of the members of the consortium, we hope to make a step change in verification technology by developing novel techniques and learning which techniques are most effective in different contexts. The outcomes will directly benefit  system designers, and indirectly, end users. They will include techniques applicable to a wide range of application domains, and results and lessons learned from three specific applications including a healthcare data capture system and RFID system infrastructure."
	},
	{
		"grant":26,
		"ID": "EP/F033702/1",
		"Title": "Holistic Design of Power Amplifiers for Future Wireless Systems",
		"PIID": "40551",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2008",
		"EndDate": "30/09/2013",
		"Value": "1014860",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "28",
		"Investigators":[
		{"ID": "40551", "Role": "Principal Investigator"},
		{"ID": "-83354", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Power amplifiers are one of the main fundamental building blocks of all modern wireless communications systems. They are used in all base stations and all the mobile units which are currently available. To maintain the required levels of system performance current commercially available amplifiers are designed to operate with extremely poor levels of efficiency which means they consume far more energy than is strictly necessary. For example current base stations in the UK operate at an efficiency level of approximately 12% this results in over 609,000 of CO2 emissions into the atmosphere on an annual basis. If these base stations were to be 50% efficient CO2 emissions could be cut by over 450,000 tons per year. The design of highly efficient and highly linear power amplifiers is an extremely complex process. At present the design of highly linear amplifiers is carried out using a trial-and-error based approach where designs are drawn up, then a prototype is produced and design issues are identified. The whole process is repeated until an optimum solution is reached. This has lead manufacturers to take a very risk adverse approach to amplifier design which has resulted in very inefficient systems.There is an increasing need to develop wireless communications systems with increased digital data throughput.  For example, in recent years we have seen the roll out of 3rd generation 3G mobile communication systems and the increasing use of wireless LAN systems. It is highly likely that the future so-called 4th generation systems will contain modulation schemes which also use wireless LAN technology. With the introduction of 3G systems it became clear that the existing design methodologies for the development and optimisation of amplifiers are labour intensive and time consuming. The present approach has become a key hindrance in the evaluation, development, and testing of modern communication systems.This proposal seeks to overcome these fundamental design issues through the establishment of a scientifically robust fully interlinked design methodology for nonlinear circuits. By combining the world-class power amplifier design expertise in Bristol and waveform measurement/engineering expertise /introduced and pioneered in Cardiff/ a scientifically robust nonlinear design methodology will be established in which the measured waveforms and waveform engineering will facilitate new methods of amplifier design and linearisation. The aim being a one pass design process for future communications systems which will result in the exploitation of this technology within a commercial setting."
	},
	{
		"grant":27,
		"ID": "EP/F03430X/1",
		"Title": "PDP-squared: Meaningful PDP language models using parallel distributed processors.",
		"PIID": "-23788",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "31/03/2014",
		"Value": "812212",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Psychological Sciences",
		"OrgID": "93",
		"Investigators":[
		{"ID": "-23788", "Role": "Principal Investigator"},
		{"ID": "5628", "Role": "Co Investigator"},
		{"ID": "121916", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Parallel Distributed Processing (PDP) is a form of computation where a large number of processing units performing simple calculations can be employed all together to solve much more complex problems. Perhaps the best example of this is the human brain, which contains approximately one hundred billion neurones. Individually these neurones simply have to decide whether to fire or not, and they do this based upon how many other neurones that are connected to them have fired recently. When this simple local computation is distributed over billions of neurones it is capable of supporting all the extremely complex behaviours that humans exhibit / talking, reading, walking, running etc / behaviours that are well beyond the abilities of more traditional computers. For this and other reasons, many psychologists believe that PDP models are the best way of describing human cognition. Unfortunately, at the moment these models are invariably simulated using standard PCs, which means that each unit in the model has to be dealt with one after the other in a serial process. This serial processing imposes severe limitations upon the complexity of problems that can be tackled. Our goal is to us to understand how the brain supports language function, how this breaks down after brain damage and the mechanisms that support recovery/rehabilitation. This will require a model of language that is capable of simulating speech, repetition, comprehension, naming and reading. To train such a model using existing pc-based simulators would take far too long /possibly more than a lifetime. So the first objective of this project is to produce a parallel distributed processing machine that is truly parallel (PDP-squared). We intend to use an array of 10,000 ARM processors incorporated into a machine that will be able to run our simulations of human behaviour 500-1000 times faster than is currently possible on a single pc. Once we have successfully produced this machine (Phase1 of the project), we will use it to build a model of normal human language function that can support reading (both aloud and for meaning), comprehension, speech, naming and repetition for all of the single monosyllabic words in English. We will validate this model by showing that damaging it can lead to the same patterns of behaviour as found in brain damaged individuals (Phase 2). Finally we will use the model to predict the results of different speech therapy strategies and will test these predictions in a population of stroke patients who have linguistic problems."
	},
	{
		"grant":28,
		"ID": "EP/F036345/1",
		"Title": "Reasoning with Relaxed Memory Models",
		"PIID": "61326",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2008",
		"EndDate": "30/11/2012",
		"Value": "813748",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "61326", "Role": "Principal Investigator"},
		{"ID": "132053", "Role": "Co Investigator"},
		{"ID": "-27360", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "INRIA Paris - Rocquencourt"}
		],
		"Summary": "Computer Science is undergoing a difficult transition.  The continual performance improvements of past decades were achieved primarily by speeding up sequential computation.  Constraints in device manufacture, especially the problem of power consumption, are driving a shift to ubiquitous concurrent computation, with multicore processors becoming commonplace.  Programming these, however, to deliver high-performance and reliable systems, remains very challenging. There are two key difficulties, which we address here.  Firstly, the concurrent algorithms that are being developed, such as non-blocking datastructures and implementations of software transactional memory, are very subtle, so informal reasoning cannot give high confidence in their correctness.  Secondly, the extensive prior work on software verification for concurrency (including temporal logics, rely-guarantee reasoning, separation logic, and process calculi) neglects what is now a key  phenomenon: relaxed memory models.  For performance reasons, typical multiprocessors do not provide a sequentially consistent memory model.  Instead, memory accesses may be reordered in various constrained ways, making it still harder to reason about executions.  In this project we will establish accurate semantics for the behaviour of real-world processors, such as x86, PowerPC, and ARM architectures, covering their memory models and fragments of their instruction sets.  We will experimentally validate these, building on our previous experience with realistic large-scale semantics.  Above these, we will develop theoretical and practical tools for specifying and proving correctness of modern algorithms, building on our experience with separation logic, mechanized reasoning, and algorithm design.  We will thereby lay the groundwork for verified compilation targeting real multicore processors, providing both high performance and high confidence for future applications."
	},
	{
		"grant":29,
		"ID": "EP/F036361/1",
		"Title": "Game semantics, recursion schemes and collapsible pushdown automata: a new approach to the algorithmics of infinite structures",
		"PIID": "34203",
		"Scheme": "Standard Research",
		"StartDate": "15/07/2008",
		"EndDate": "14/04/2013",
		"Value": "522778",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "34203", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Despite considerable progress over the past decade, the computer-aided verification of software remains a hugely challenging problem. There are two main reasons. First, programs are infinite-state systems, but verification tools of industrial scale are essentially finite-state technologies. Secondly, modern programs -- in which unbounded data types, complex memory operations, non-local control flow, higher-order constructs, and name bindings of various kinds etc. are key features -- can only be accurately modelled using highly-structured mathematical models, as studied in semantics. Relatively little is known about the algorithmic properties of such rich, often  higher-order'', mathematical structures.This project concerns an approach to software verification by developing automatic techniques which deal directly with infinite-state systems that are highly accurate models of programs. Striking examples of such infinite-state models are fully abstract game semantics of higher-order procedural programs; these models can be represented as (variant classes of higher-order) pushdown automata. In recent years, there have been remarkable advances in the study of algorithmic properties of hierarchies of generically-defined infinite structures. A notable result is that ranked trees that are generated by higher-order recursion schemes have decidable monadic second-order theories (Ong LICS'06). Further there is a new variant hierarchy of higher-order pushdown automata, called collapsible pushdown automata, that are equi-expressive with higher-order recursion schemes (in the sense that they generate the same class of trees), subsuming well-known structures at low orders: trees at order 0 and 1 are respectively the regular (Rabin 1969) and algebraic (Courcelle 1995)  trees. The discovery of this rich, unifying and robust hierarchy of trees with excellent model-checking properties sparked the present research proposal.We have two general goals: - First we plan to study the connexions between the two closely-related higher-order families of generators (i.e. recursion schemes and collapsible pushdown automata), to explore the logical-algorithmic theory of  infinite structures generated by them, and to derive (local and global) model checking algorithms. - Secondly we aim to develop verification techniques and construct efficient implementations of symbolic model-chekcers of reachability and temporal logics for these infinite structures of low orders. Why is it important to pursue these goals?- First, these hierarchies of infinite structures lie at the very frontier of infinite-state verification. The family of ranked trees thus generated is, to date, the largest generically-defined family of trees known to have decidable monadic second-order (MSO) theories; the family of transition graphs thus generated (does not have decidable MSO theories but) is, to date, the largest that is known to have decidable modal mu-calculus theories. These are significant indicators of our understanding of the subject, as MSO logic is the gold standard of languages for describing model-checking properties in computer-aided verification. - Secondly, these hierarchies of infinite structures are (representations of) highly accurate models of computation for higher-order procedural programs (such as OCAML, Haskell and F#). Recent results in (algorithmic) game semantics have shown that they underpin the computer-aided verification of these programs, an important and challenging direction for the next generation of software model checkers."
	},
	{
		"grant":30,
		"ID": "EP/F036884/1",
		"Title": "Support for the UK Car-Parrinello Consortium",
		"PIID": "61373",
		"Scheme": "Standard Research",
		"StartDate": "01/08/2008",
		"EndDate": "31/12/2012",
		"Value": "393767",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "55",
		"Investigators":[
		{"ID": "61373", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many technological advances in modern day life are dependent upon the development of new materials or better control and understanding of existing materials. Understanding the detailed properties of materials has therefore never been more important. The development of high quality computer simulation techniques has played an increasing significant role in this endeavour over recent years. The UK has been at the forefront of this new wave, and the UKCP consortium has played an important part, in both developing computer codes and algorithms, and exploiting these new advances to increase our understanding of many industrially relevant materials and processes.The research described in this proposal will make significant impacts on many areas of future technology, such as the development of new materials for hydrogen storage which will be necessary for zero-pollution cars in the future, the development of new materials for alternative computer memory technologies, and the development of new carbon-based nano-sized electronic components that could replace silicon altogether.Other parts of this proposal seek to develop new algorithms and theoretical improvements that will increase our simulation abilities, either by increasing the accuracy and reliability of calculations, or by enabling us to simulate bigger systems for longer. These will enable the next generation of simulations and further widen our computational horizons.The research proposed does not easily fit into any of the traditional categories of 'physics' or 'chemistry' etc. Instead, the UKCP is a multi-disciplinary consortium using a common theoretical foundation to advance many different areas of materials-based science."
	},
	{
		"grant":31,
		"ID": "EP/F03718X/1",
		"Title": "Support for the UK Car-Parrinello Consortium",
		"PIID": "37788",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "92574",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst for Materials Research",
		"OrgID": "114",
		"Investigators":[
		{"ID": "37788", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many technological advances in modern day life are dependent upon the development of new materials or better control and understanding of existing materials. Understanding the detailed properties of materials has therefore never been more important. The development of high quality computer simulation techniques has played an increasing significant role in this endeavour over recent years. The UK has been at the forefront of this new wave, and the UKCP consortium has played an important part, in both developing computer codes and algorithms, and exploiting these new advances to increase our understanding of many industrially relevant materials and processes.The research described in this proposal will make significant impacts on many areas of future technology, such as the development of new materials for hydrogen storage which will be necessary for zero-pollution cars in the future, the development of new materials for alternative computer memory technologies, and the development of new carbon-based nano-sized electronic components that could replace silicon altogether.Other parts of this proposal seek to develop new algorithms and theoretical improvements that will increase our simulation abilities, either by increasing the accuracy and reliability of calculations, or by enabling us to simulate bigger systems for longer. These will enable the next generation of simulations and further widen our computational horizons.The research proposed does not easily fit into any of the traditional categories of 'physics' or 'chemistry' etc. Instead, the UKCP is a multi-disciplinary consortium using a common theoretical foundation to advance many different areas of materials-based science."
	},
	{
		"grant":32,
		"ID": "EP/F03721X/1",
		"Title": "Support for the UK Car-Parrinello Consortium",
		"PIID": "45",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "20712",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Physics and Astronomy",
		"OrgID": "41",
		"Investigators":[
		{"ID": "45", "Role": "Principal Investigator"},
		{"ID": "9965", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many technological advances in modern day life are dependent upon the development of new materials or better control and understanding of existing materials. Understanding the detailed properties of materials has therefore never been more important. The development of high quality computer simulation techniques has played an increasing significant role in this endeavour over recent years. The UK has been at the forefront of this new wave, and the UKCP consortium has played an important part, in both developing computer codes and algorithms, and exploiting these new advances to increase our understanding of many industrially relevant materials and processes.The research described in this proposal will make significant impacts on many areas of future technology, such as the development of new materials for hydrogen storage which will be necessary for zero-pollution cars in the future, the development of new materials for alternative computer memory technologies, and the development of new carbon-based nano-sized electronic components that could replace silicon altogether.Other parts of this proposal seek to develop new algorithms and theoretical improvements that will increase our simulation abilities, either by increasing the accuracy and reliability of calculations, or by enabling us to simulate bigger systems for longer. These will enable the next generation of simulations and further widen our computational horizons.The research proposed does not easily fit into any of the traditional categories of 'physics' or 'chemistry' etc. Instead, the UKCP is a multi-disciplinary consortium using a common theoretical foundation to advance many different areas of materials-based science."
	},
	{
		"grant":33,
		"ID": "EP/F038038/1",
		"Title": "Support for the UK Car-Parrinello Consortium",
		"PIID": "-115235",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "89146",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Chemistry",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-115235", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many technological advances in modern day life are dependent upon the development of new materials or better control and understanding of existing materials. Understanding the detailed properties of materials has therefore never been more important. The development of high quality computer simulation techniques has played an increasing significant role in this endeavour over recent years. The UK has been at the forefront of this new wave, and the UKCP consortium has played an important part, in both developing computer codes and algorithms, and exploiting these new advances to increase our understanding of many industrially relevant materials and processes.The research described in this proposal will make significant impacts on many areas of future technology, such as the development of new materials for hydrogen storage which will be necessary for zero-pollution cars in the future, the development of new materials for alternative computer memory technologies, and the development of new carbon-based nano-sized electronic components that could replace silicon altogether.Other parts of this proposal seek to develop new algorithms and theoretical improvements that will increase our simulation abilities, either by increasing the accuracy and reliability of calculations, or by enabling us to simulate bigger systems for longer. These will enable the next generation of simulations and further widen our computational horizons.The research proposed does not easily fit into any of the traditional categories of 'physics' or 'chemistry' etc. Instead, the UKCP is a multi-disciplinary consortium using a common theoretical foundation to advance many different areas of materials-based science."
	},
	{
		"grant":34,
		"ID": "EP/F040784/1",
		"Title": "On-Chip milliKelvin Electronic Refrigerator for Astronomical and Quantum Device Applications",
		"PIID": "48924",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/11/2013",
		"Value": "1069459",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "48924", "Role": "Principal Investigator"},
		{"ID": "106491", "Role": "Co Investigator"},
		{"ID": "12074", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "VTT Technical Research Centre of Finland"}
		],
		"Summary": "We intend to develop a new user-friendly technology that would enable small devices to be cooled to exceedingly low temperatures (<100mk).  Such a capability will allow diverse and futuristic applications to flourish.  These include the detection of black holes, cancer detection and quantum computing.  We propose to do this by using an electronic cooling process where relatively energetic (hot) carriers (electrons or holes) quantum mechanically tunnel out of a medium, thereby causing the average electronic temperature in the medium to decrease.  The application of this process to realise extremely low temperatures is very new, and we want to greatly improve its efficiency by introducing a new generation semiconductor SiGe into the design of the electronic cooler and, along with it, the well developed silicon processing techniques - so that, ultimately, such coolers can be produced economically and to industrial standards.  Coolers will be fabricated around the periphery of a small silicon  chip  with thermal links to the active device ( payload ) mounted in the centre of the chip.  This requires very good thermal design such that the electronic cooler efficiently cools the payload.  However, in some cases, it is only necessary to cool the electrons / not the lattice atoms; here SiGe gives a lot of flexibility in controlling the thermal coupling between the electrons and the lattice.  Such electronic coolers can operate from a starting temperature of 0.3K, which can be produced by a cryogenic fluid-free closed-cycle helium cryostat, so that a turn-switch technology can be envisaged enabling access to ~10mK working environments.  This will be a huge technology step forward, as existing techniques require massive and complex cryogenic fluid-based equipment.During the first phase of the project we will examine several approaches to the realisation of effective electronic cooling, exploiting the wide range of fundamental electronic conditions that can be obtained at very low temperatures in SiGe with its associated metal silicides / thereby enhancing carrier transport and thermoelectric effects.  The new coolers will then be tested in two areas of great topical interest, namely radiation detectors and quantum information devices.  They could dramatically enhance our ability to detect, for example, the photons that emanate from the earliest black holes, with satellite-based detectors operating at <100mK.  And, very significantly, such detectors could revolutionize the fluorescence light detection that is used extensively in biomedical research, enabling advances in our understanding of genetically-based diseases (e.g. cancer) and the workings of a single cell.  Furthermore, the computational vista that is opened-up by the quantum computing era requiring  qubit  devices operating at 10-20mK, is truly awe inspiring. Warwick is co-ordinating the project and has assembled a tightly knit consortium of scientists and engineers with appropriate expertise from four UK universities -Warwick, Cardiff, Leicester and London(Royal Holloway) - and four leading-edge companies, concerned with the development of this technology and the demonstration of its applicability and advantages in two key areas.  We are also working closely with Europe's leading centre on mK coolers (Helsinki University of Technology).  The UK is exceedingly well positioned to derive benefit from this genuinely new and exciting technology, and this project will sow the seeds for its realisation."
	},
	{
		"grant":35,
		"ID": "EP/F041128/1",
		"Title": "On-Chip milliKelvin Electronic Refrigerator for Astronomical and Quantum Device Applications",
		"PIID": "29563",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2013",
		"Value": "275882",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "43",
		"Investigators":[
		{"ID": "29563", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "VTT Technical Research Centre of Finland"}
		],
		"Summary": "We intend to develop a new user-friendly technology that would enable small devices to be cooled to exceedingly low temperatures (<100mk).  Such a capability will allow diverse and futuristic applications to flourish.  These include the detection of black holes, cancer detection and quantum computing.  We propose to do this by using an electronic cooling process where relatively energetic (hot) carriers (electrons or holes) quantum mechanically tunnel out of a medium, thereby causing the average electronic temperature in the medium to decrease.  The application of this process to realise extremely low temperatures is very new, and we want to greatly improve its efficiency by introducing a new generation semiconductor SiGe into the design of the electronic cooler and, along with it, the well developed silicon processing techniques - so that, ultimately, such coolers can be produced economically and to industrial standards.  Coolers will be fabricated around the periphery of a small silicon  chip  with thermal links to the active device ( payload ) mounted in the centre of the chip.  This requires very good thermal design such that the electronic cooler efficiently cools the payload.  However, in some cases, it is only necessary to cool the electrons / not the lattice atoms; here SiGe gives a lot of flexibility in controlling the thermal coupling between the electrons and the lattice.  Such electronic coolers can operate from a starting temperature of 0.3K, which can be produced by a cryogenic fluid-free closed-cycle helium cryostat, so that a turn-switch technology can be envisaged enabling access to ~10mK working environments.  This will be a huge technology step forward, as existing techniques require massive and complex cryogenic fluid-based equipment.During the first phase of the project we will examine several approaches to the realisation of effective electronic cooling, exploiting the wide range of fundamental electronic conditions that can be obtained at very low temperatures in SiGe with its associated metal silicides / thereby enhancing carrier transport and thermoelectric effects.  The new coolers will then be tested in two areas of great topical interest, namely radiation detectors and quantum information devices.  They could dramatically enhance our ability to detect, for example, the photons that emanate from the earliest black holes, with satellite-based detectors operating at <100mK.  And, very significantly, such detectors could revolutionize the fluorescence light detection that is used extensively in biomedical research, enabling advances in our understanding of genetically-based diseases (e.g. cancer) and the workings of a single cell.  Furthermore, the computational vista that is opened-up by the quantum computing era requiring  qubit  devices operating at 10-20mK, is truly awe inspiring. Warwick is co-ordinating the project and has assembled a tightly knit consortium of scientists and engineers with appropriate expertise from four UK universities -Warwick, Cardiff, Leicester and London(Royal Holloway) - and four leading-edge companies, concerned with the development of this technology and the demonstration of its applicability and advantages in two key areas.  We are also working closely with Europe's leading centre on mK coolers (Helsinki University of Technology).  The UK is exceedingly well positioned to derive benefit from this genuinely new and exciting technology, and this project will sow the seeds for its realisation."
	},
	{
		"grant":36,
		"ID": "EP/F041470/1",
		"Title": "On-Chip milliKelvin Electronic Refrigerator for Astronomical and Quantum Device Applications",
		"PIID": "5526",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2008",
		"EndDate": "31/10/2013",
		"Value": "272134",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "66",
		"Investigators":[
		{"ID": "5526", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "VTT Technical Research Centre of Finland"}
		],
		"Summary": "We intend to develop a new user-friendly technology that would enable small devices to be cooled to exceedingly low temperatures (<100mk).  Such a capability will allow diverse and futuristic applications to flourish.  These include the detection of black holes, cancer detection and quantum computing.  We propose to do this by using an electronic cooling process where relatively energetic (hot) carriers (electrons or holes) quantum mechanically tunnel out of a medium, thereby causing the average electronic temperature in the medium to decrease.  The application of this process to realise extremely low temperatures is very new, and we want to greatly improve its efficiency by introducing a new generation semiconductor SiGe into the design of the electronic cooler and, along with it, the well developed silicon processing techniques - so that, ultimately, such coolers can be produced economically and to industrial standards.  Coolers will be fabricated around the periphery of a small silicon  chip  with thermal links to the active device ( payload ) mounted in the centre of the chip.  This requires very good thermal design such that the electronic cooler efficiently cools the payload.  However, in some cases, it is only necessary to cool the electrons / not the lattice atoms; here SiGe gives a lot of flexibility in controlling the thermal coupling between the electrons and the lattice.  Such electronic coolers can operate from a starting temperature of 0.3K, which can be produced by a cryogenic fluid-free closed-cycle helium cryostat, so that a turn-switch technology can be envisaged enabling access to ~10mK working environments.  This will be a huge technology step forward, as existing techniques require massive and complex cryogenic fluid-based equipment.During the first phase of the project we will examine several approaches to the realisation of effective electronic cooling, exploiting the wide range of fundamental electronic conditions that can be obtained at very low temperatures in SiGe with its associated metal silicides / thereby enhancing carrier transport and thermoelectric effects.  The new coolers will then be tested in two areas of great topical interest, namely radiation detectors and quantum information devices.  They could dramatically enhance our ability to detect, for example, the photons that emanate from the earliest black holes, with satellite-based detectors operating at <100mK.  And, very significantly, such detectors could revolutionize the fluorescence light detection that is used extensively in biomedical research, enabling advances in our understanding of genetically-based diseases (e.g. cancer) and the workings of a single cell.  Furthermore, the computational vista that is opened-up by the quantum computing era requiring  qubit  devices operating at 10-20mK, is truly awe inspiring. Warwick is co-ordinating the project and has assembled a tightly knit consortium of scientists and engineers with appropriate expertise from four UK universities -Warwick, Cardiff, Leicester and London(Royal Holloway) - and four leading-edge companies, concerned with the development of this technology and the demonstration of its applicability and advantages in two key areas.  We are also working closely with Europe's leading centre on mK coolers (Helsinki University of Technology).  The UK is exceedingly well positioned to derive benefit from this genuinely new and exciting technology, and this project will sow the seeds for its realisation."
	},
	{
		"grant":37,
		"ID": "EP/F048041/1",
		"Title": "Creating, detecting and exploiting quantum states of light",
		"PIID": "2220",
		"Scheme": "Platform Grants",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "960579",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "2220", "Role": "Principal Investigator"},
		{"ID": "-188684", "Role": "Co Investigator"},
		{"ID": "21659", "Role": "Co Investigator"},
		{"ID": "-189537", "Role": "Co Investigator"},
		{"ID": "-149220", "Role": "Co Investigator"},
		{"ID": "76390", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The last decade has seen a world-wide expansion in investigation into quantum states of light. Much of the expansion in this subject area has been stimulated by the emergence of quantum cryptography or quantum key distribution (QKD) / first proposed in 1984 - which offers unconditionally secure information transport guaranteed by quantum-mechanical laws. Whilst QKD remains a fertile subject of exciting laboratory and field research, experimental progress in free-space and optical fibre transmission media have taken quantum cryptography to the fringes of commercial exploitation and real-world application. Concurrently, a number of other developments in quantum information research have also been highly significant, such as quantum computing algorithms that, if realised, would make today's public-key based data security system obsolete.  The building blocks, or quantum components, of these quantum-based systems require considerable research effort and this is the subject matter of this proposal.  Significantly and perhaps in a more short-term manner, a number of these components will be utilised in other applications outside the quantum information processing sphere; these applications include including low-light level communications (eg as proposed in the NASA Mars Communications Programme), in remote sensing, imaging and quantum-based metrology.This Platform Grant application from the Heriot-Watt group centres on leading edge research into the creation, detection and exploitation of the quantum states of light. This project will be used to make strategic decisions regarding research in these fast-moving fields. At the time of application, a number of exciting projects have been highlighted for investigation, although these projects are not meant to represent a comprehensive and exclusive list of research topics.  Some areas worth immediate investigation include prototype quantum devices such as quantum dots for entangled photon pair production; or quantum imaging in remote sensing, or single-photon sources constructed from carbon nanotubes.  Whilst this grant will not provide the full resources for long-term investigations into all these areas, this project will permit rapid start-up and allow the group to collaborate more effectively with other groups, including overseas researchers."
	},
	{
		"grant":38,
		"ID": "EP/F048084/1",
		"Title": "Support for the UK Car-Parrinello Consortium",
		"PIID": "42450",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2008",
		"EndDate": "31/10/2012",
		"Value": "23712",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials",
		"OrgID": "77",
		"Investigators":[
		{"ID": "42450", "Role": "Principal Investigator"},
		{"ID": "43972", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many technological advances in modern day life are dependent upon the development of new materials or better control and understanding of existing materials. Understanding the detailed properties of materials has therefore never been more important. The development of high quality computer simulation techniques has played an increasing significant role in this endeavour over recent years. The UK has been at the forefront of this new wave, and the UKCP consortium has played an important part, in both developing computer codes and algorithms, and exploiting these new advances to increase our understanding of many industrially relevant materials and processes.The research described in this proposal will make significant impacts on many areas of future technology, such as the development of new materials for hydrogen storage which will be necessary for zero-pollution cars in the future, the development of new materials for alternative computer memory technologies, and the development of new carbon-based nano-sized electronic components that could replace silicon altogether.Other parts of this proposal seek to develop new algorithms and theoretical improvements that will increase our simulation abilities, either by increasing the accuracy and reliability of calculations, or by enabling us to simulate bigger systems for longer. These will enable the next generation of simulations and further widen our computational horizons.The research proposed does not easily fit into any of the traditional categories of 'physics' or 'chemistry' etc. Instead, the UKCP is a multi-disciplinary consortium using a common theoretical foundation to advance many different areas of materials-based science."
	},
	{
		"grant":39,
		"ID": "EP/F056745/1",
		"Title": "Access to Nanoscience and Nanotechnology Equipment at Cardiff",
		"PIID": "35966",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "31/12/2012",
		"Value": "526757",
		"ResearchArea": "Functional Ceramics and Inorganics",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "28",
		"Investigators":[
		{"ID": "35966", "Role": "Principal Investigator"},
		{"ID": "7937", "Role": "Principal Investigator"},
		{"ID": "69416", "Role": "Co Investigator"},
		{"ID": "-190766", "Role": "Co Investigator"},
		{"ID": "1534", "Role": "Co Investigator"},
		{"ID": "32124", "Role": "Co Investigator"},
		{"ID": "64992", "Role": "Co Investigator"},
		{"ID": "-171046", "Role": "Co Investigator"},
		{"ID": "12398", "Role": "Co Investigator"},
		{"ID": "55336", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This programme for free access to nanotechnology facilities, some of which unique in the UK, will be coordinated by the Manufacturing Engineering Centre (MEC) at Cardiff University (CU).  The Centre is part of the CU Innovative Manufacturing Research Centre (CUIMRC) funded by the EPSRC, and has been a Centre of Excellence for Technology and Industrial Collaboration (CETIC) in Wales in the last six years and was a Centre for Manufacturing Excellence in Phase 1 of the DTI Manufacturing Advisory Service (MAS). In 2001, the MEC initiated a Micro and Nano Manufacturing (MNM) R&D programme with a focus on advanced manufacturing research for micro and nano-systems, one of the priority areas in the UK Technology Strategy. In 2005, in recognition of its research achievements and state-of-the-art facilities in Micro and Nano Technology the Centre became one of the main nodes within the UK MNT Network with the award of a 3.5M capital grant from the DTI and Welsh Assembly Government. Currently, the MEC MNM programme includes a portfolio of two National and eight EC funded research projects with a total budget in excess of 11M. The state-of-the-art nanoscience and nanotechnology equipment that underpins the MEC MNM programme will be made available through this programme to the academic community in the UK.The R&D services offered under this programme will focus on fabrication, processing and characterisation processes which were identified to have the greatest demand in NanoEquip2007, a recently conducted survey by the EPSRC.. In particular, the following services will be made available to the UK academic community through this programme.- Zeiss XB1540 (50% spare capacity): 2D and 3D FIB nanostructuring/lithography; FIB milling with simultaneous SEM; FIB assisted material deposition; prototyping and small batch fabrication of nanodevices, NEMS and nanoelectronics; preparation of microscopy samples; preparation of specimens for atom probe analysis; preparation of TEM lamellae; advanced imaging and compositional analysis (EDX, EBSD and STEM); fabrication of 2D and 3D replication masters; failure analysis and repair of semiconductor devices; high resolution SEM with FE emitter.- Imprio 55 (60%): UV imprinting of nanostructures on 4'' and 8'' wafers; batch fabrication of planner nanodevices, e.g. FETs, sensors and optical components; development of new UV curable functional materials, e.g. organic semiconductors.- XE-100 (60%): nanolithography for fabrication of nanodevices; AFM/STM imaging for characterisation of nanostructures and nanoparticles; structure and morphology characterisation of nanomaterials.- MicroXAM-100-HR (60%): 3D profiling measurements with nanometre resolution in the vertical range; failure analysis of materials; measurements of step heights from 1 nm to 3 mm.- SA/1m (60%): fabrication of nanodevices in nickel from UV imprinted masters; fabrication of masters for thermal imprinting.- Mikrotechnik HEX03 (60%): thermal imprinting of nanostructures on organic thin sheets; batch fabrication of planar nanodevices, e.g.  FETs, sensors and optical components; characterisation of new functional organic materials, e.g. organic semiconductors.- Microsystem 50 (60%): replication of nanostructures in thermoplastics; characterisation of new functional thermoplastics for serial manufacture of nanostructures/nanodevices."
	},
	{
		"grant":40,
		"ID": "EP/F05999X/1",
		"Title": "Hybrid organic semiconductor/gallium nitride/CMOS smart pixel arrays",
		"PIID": "45306",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "1670326",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Photonics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "45306", "Role": "Principal Investigator"},
		{"ID": "61164", "Role": "Co Investigator"},
		{"ID": "12371", "Role": "Co Investigator"},
		{"ID": "50661", "Role": "Co Investigator"},
		{"ID": "-24412", "Role": "Co Investigator"},
		{"ID": "48498", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Organic semiconductors are a very exciting category of optoelectronic materials, in the development of which, over the past 10-15 years, the UK has played a vital and widely acknowledged role. These materials offer efficient light emission across the visible spectrum whilst being amenable to a wide range of simple and scalable processing methods such as ink-jet printing. These attractive properties have led to the rapid development of efficient, electrically-driven light emitting diodes (LED's) at red, green and blue wavelengths, that are already having significant commercial impact in areas such as mobile phones and large area flat-panel displays. Laboratories around the world have shown that high-performance laser action and optical amplfication is also possible in these materials, opening up an entirely new approach to visible-wavelength lasers - a region of the spectrum that has proven difficult to cover fully with more established solid-state laser technology. This opens up many new applications in areas as diverse as optical communications, instrumentation, metrology, spectroscopy and bio- and chemical-sensing. However these devices currently require separate lasers for pumping and are not available in compact, integrated form. Here, we propose a novel approach to the development of integrated organic semiconductor lasers, utilising a gallium nitride inorganic semiconductor optoelectronic interface to produce compact formats of organic device under electronic control. The gallium nitride devices, as proposed, produce blue-violet pump light for the organic lasers when driven by silicon CMOS electronics. These technologies can all be made planar and integrated one above the other, thus bringing the performance of the organic lasers under computer control for the first time.This offers the prospects of a very versatile optical interconnect technology that can either couple in-plane organic elements together in novel planar lightwave circuits taking true advantage of the versatile processing potential of the organics or relay the pattern-programmable output to other applications interfaces such as bio-instrumentation. In addition, the CMOS design offers highly-sensitive on-chip photodetection in the wavelength range, down to the single-photon level, of both the gallium nitride and the organics, thus opening up novel methods of active feedback, modulation and control. These attributes offer potential linkages in emerging areas of computation and communications including quantum information processing and bio-computing.Accomplishing these ambitious goals, which draw together a range of hitherto largely disparate technologies, requires a substantial and co-ordinated programme. We have assembled a partnership of leading researchers with the complementary skills and experience required, who also have an established track record of working together successfully on interdisciplinary research."
	},
	{
		"grant":41,
		"ID": "EP/F060041/1",
		"Title": "Artificial Biochemical Networks: Computational Models and Architectures",
		"PIID": "16159",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2008",
		"EndDate": "30/09/2013",
		"Value": "631292",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics",
		"OrgID": "55",
		"Investigators":[
		{"ID": "16159", "Role": "Principal Investigator"},
		{"ID": "37618", "Role": "Co Investigator"},
		{"ID": "74909", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Previous work by ourselves and others has shown how the structure and organisation of biological organisms can motivate the design of computer hardware and software, with the aim of capturing useful properties such as complex information processing and resistance to environmental perturbation. This proposal focuses upon one of the most complex sets of structures found in biological systems: biochemical networks. These structures are fundamental to the development, function and evolution of biological organisms, and are the main factor underlying the complexity seen within higher organisms. Previous attempts to build hardware and software systems motivated by these structures has led to a group of computer architectures which we collectively refer to as artificial biochemical network models. The best known of these is the artificial genetic network, which has shown itself to be an effective means of expressing complex computational behaviours, particularly within robotic control. Nevertheless, this field of research has received relatively little attention, and little is known about the computational properties of these architectures. The aim of the proposed work is to develop better artificial biochemical network models, which we will do by both bringing together existing work and introducing new understanding from the biological sciences. We will also develop a theoretical framework to better understand what these computational architectures are capable of, and show how how these models can be applied to the difficult problem of controlling a robot in real world environments. It is expected that this work will also produce insights into the function and evolution of the biological systems on which the architectures are modelled."
	},
	{
		"grant":42,
		"ID": "EP/F064551/1",
		"Title": "Structural Vulnerability Measures for Networks and Graphs",
		"PIID": "-26269",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "493476",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-26269", "Role": "Principal Investigator"},
		{"ID": "-26259", "Role": "Co Investigator"},
		{"ID": "-26266", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Networks appear in many different applications and settings, the more known ones being telecommunication networks, computer networks, the internet, road and rail networks, and other logistic networks, like gas and electricity networks. In all applications of networks vulnerability and reliability are crucial and important features. From a practical point of view, in many network applications the first thing one wants to know is how well the network performs with regard to node or link failures: are the remaining nodes still connected if some of the nodes or links break down? This is captured by the concepts of connectivity and edge connectivity that are well-studied within the research area of graph theory. Due to existing knowledge from this area the level of connectivity and edge connectivity is closely related to the existence of certain sets of nodes and links that separate the network in disconnected parts, as well as to the existence of certain connecting paths between pairs of nodes. These structural results have very nice algorithmic implications, namely that the level of connectivity and edge connectivity, as well as the connecting paths between pairs of nodes can be determined by fast algorithms. So at first sight everything seems to be satisfactorily settled. However, in practice the measures connectivity and edge connectivity are too simple and too rude: they do not capture the effect of node or link failures on networks well enough. Depending on the type of application, one would like to take into account other effects of node or link failure (vertex or edge deletions), like the number of resulting components, the size of the largest (smallest) component, a split in (almost) equally sized parts, etc. In the proposed research we study various vulnerability measures that capture such effects. We will develop and extend the knowledge base for these measures by analysing their structural properties. We will also consider algorithmic aspects that will help us in answering the question how easy or difficult these measures can be computed in (large) networks. These structural and algorithmic properties of vulnerability measures can also have an impact on solving other difficult optimization problems for networks. If one can break a large network into smaller networks by deleting certain sets of nodes or links, then under some conditions the solutions for the optimization problem on the smaller networks can be combined to a solution for the optimization problem on the larger network. This approach for solving optimization problems is known as divide-and-conquer."
	},
	{
		"grant":43,
		"ID": "EP/F064578/1",
		"Title": "Actuated Acoustic Sensor Networks for Industrial Processes (AASN4IP)",
		"PIID": "-5678",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "846909",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "-5678", "Role": "Principal Investigator"},
		{"ID": "28186", "Role": "Co Investigator"},
		{"ID": "17685", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Nexia Solutions"},
		{"Number": "1", "Name": "Phoenix Inspection Systems Ltd"}
		],
		"Summary": "This project aims to develop wireless technology to support the internal monitoring of industrial processes involving conducting liquids (e.g. water). Such processes are common in many sectors including the chemical, pharmaceutical and nuclear industries. The technology will be based on wireless sensor networks (WSN) which consist of collections of 'nodes' containing sensors, communications transceivers and an embedded computer system. Nodes organise themselves into a computer network, which is used to send sensor readings to a base station. The present proposal seeks to establish the UK as a centre of excellence for the development and application of this technology, and will significantly extend the work that we are currently carrying out in its application to grain processing.The project will research the technologies necessary to construct a network of small nodes that can be immersed within a process enclosure and which can sense local conditions and communicate readings through the network to a base station outside the vessel. An important and novel aspect of this work is the use of acoustic techniques, in a confined space, for communications and the determination of node position. The nodes will contain small scale buoyancy and propulsion systems enabling them to be manoeuvred to selected positions for measurement purposes. Software will also be developed to enable nodes to explore the process, a capability that is very important in a demonstrator system that will be developed with one of our industrial collaborators (Nexia).The use of a demonstrator system provides a focus to the generic research that will be carried out within the project. It is concerned with measuring the conditions within nuclear waste storage ponds, providing crucial information that will enable a carefully planned material removal and disposal programme to be carried out. This is clearly a timely application, given increasing public concern about the long-term storage of nuclear waste. However, the motivation for the research goes beyond a single application, and stems from the desire to overcome the limitations of current process measurement technology, and to provide much more accurate and detailed information about process dynamics than can be obtained at present. Access to such information will provide opportunities for increased plant agility, reduced raw materials uptake, reduced energy usage, reduced environmental impact, reduced waste generation and reduced occupational exposure via improved knowledge of the process.The key research challenges include the use of acoustic techniques within confined and potentially cluttered underwater environments, the development of very small scale buoyancy and propulsion systems, energy husbandry, and efficient exploration strategies. Clearly this requires a broad range of expertise. The team making the proposal includes three academic investigators from the University of Manchester, and one from the University of Oxford. The Manchester academics are from two research groups in the School of Electrical & Electronic Engineering:  Microwave and Communication Systems  and  Sensing, Imaging and Signal Processing . They provide skills in communications (physical layers and protocols), embedded systems, sensing, and electronic systems. In addition, the lead investigator has experience in mechanical engineering. The investigator from Oxford is a member of the Software Engineering Group in the Computing Laboratory, and provides expertise in exploration algorithms and protocols. Four postdoctoral research assistants will be employed to support the work and to deliver the demonstrator. In addition two research students will explore the areas of mobility and power management, and exploration algorithms. Four support staff will contribute about three years of effort to the project."
	},
	{
		"grant":44,
		"ID": "EP/F065825/1",
		"Title": "Reverse Engineering State Machine Hierarchies by Grammar Inference (REGI)",
		"PIID": "87124",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "315209",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "87124", "Role": "Principal Investigator"},
		{"ID": "-111126", "Role": "Co Investigator"},
		{"ID": "7501", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Catholic University of Louvain"},
		{"Number": "1", "Name": "Delft University of Technology"},
		{"Number": "2", "Name": "Genesys Solutions"}
		],
		"Summary": "Software systems pervade modern life; they control everything from fly-by-wire aircraft and financial transfer systems to ABS breaking systems in cars and cooking modes in microwaves. It is an accepted fact that, as software systems evolve and their requirements change, they become increasingly complex and difficult to maintain. Different developers carry out (sometimes conflicting) changes to address different bugs or add different features, and before long the original design of the system is neglected and it becomes impossible to understand exactly how the system operates.One way around this problem is to keep a high-level design document that specifies exactly how the system is supposed to behave. A major benefit of developing such specifications along with the actual software itself is the fact that powerful testing and verification techniques exist that can be used to determine whether the actual system conforms to the specification. However, one major problem hampers the routine use of such specifications: as the software system evolves, they often tedious and time-consuming to generate and maintain separately from the code. With this project we will produce a technique that addresses the above problem by automating the process of generating a specification. Given a system that has no specification (or only a partial one), our technique will analyse and probe the system by both running it and looking at its static structure. It will produce a complete document specifying the behaviour of the system as a collection of state machines. Often, depending on the facet of behaviour that is of interest, it is useful to display certain aspects at a greater level of detail than others (for a financial system for example, the developer might be interested in details sub-prime loan management and not the transaction processing system). For this reason, state machines that are generated will be presented as a hierarchy. Devising a practical technique to automatically reverse engineer such specifications is challenging; the system in question needs to be suitably sampled to identify relevant behaviour, and the final specification will have to be a valid generalisation of these samples. How do we identify the set(s) of samples that can be used as a basis for generating the specification? How do we go about building an accurate specification of the system from this set of samples? One key realisation that will be investigated in this work is the fact that identical challenges to these arise in the field of grammar inference, where the challenge is to build a grammar (which can be represented by a state machine) from a sample of sentences in the target language. Grammar inference is a very mature field, with many very powerful techniques, but has never been linked to the similar problem of reverse-engineering specifications from software systems.This work will explore the relationship between the two fields of reverse-engineering and grammar inference, and will produce a set of approaches based on and extending existing techniques that, when combined, will produce a practical technique that generates complete and accurate hierarchy of state machines of a software system."
	},
	{
		"grant":45,
		"ID": "EP/F067607/1",
		"Title": "Logic of Interaction and Information Flow",
		"PIID": "35",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2012",
		"Value": "305955",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "35", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our current understanding of computation has widened enormously beyond the original ``closed world'' picture of numerical calculation in isolation from the environment. In the age of the Internet and the Web, and now of pervasive and ubiquitous computing, it has become clear that interaction and information flow between multiple agents are essential features of computation. The standard unit of description or design, whether at a micro-scale of procedure call-return interfaces or hardware components, or a macro-scale of software agents on the Web, becomes a process or agent, the essence of whose behaviour ishow it interacts with its environment across some defined interface.These developments have required the introduction of novel mathematical models of interactive computation. The traditional view whereby a program is seen as computing a function or relation from inputs to outputs is no longer sufficient: what function does the Internet compute?One of the compelling ideas which has taken root is toconceptualize the interaction between the System and the Environment as a two-person game. A program specifying how the System should behave in the face of all possible actions by the Environment is then a strategy for the player corresponding to the System.These ideas have led to the development of Game semantics, which both provides powerful tools for analyzing many different forms of computation, and leads to new perspectives on logic itself.For example, consider the ``copy-cat strategy'', whereby we can beat a Grand-Master at chess, by the power of logic. The idea is that we play against two Grand-Masters on two boards, one as White and one as Black (corresponding to ``A or not A''). By simply copying moves back and forth, we end up playing one grand-master off against the other --- and hence do  as well as our opponents! This is a kind of ``dynamic tautology'': we achieve a strong logical effect by a simple copying process. Note only do the considerations arising from this and related ideas lead to powerful applications in computation and logic, striking connections are also emerging to geometric ideas, related to knots and braids, and to information flow in entangled quantum systems.Thus one aim of the proposal is build a unified theory of interaction and information flow covering all these examples and more. Another to is to connect this ``dynamic'' or ``intrinsic'' approach to more traditional approaches to logics of interaction, and indeed to find useful ways of combining them."
	},
	{
		"grant":46,
		"ID": "EP/F067828/1",
		"Title": "Platform Renewal Proposal: MULTIFUNCTIONAL OXIDES  MATERIALS TO DEVICES",
		"PIID": "36081",
		"Scheme": "Platform Grants",
		"StartDate": "01/02/2009",
		"EndDate": "31/07/2014",
		"Value": "830692",
		"ResearchArea": "Functional Ceramics and Inorganics",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials",
		"OrgID": "77",
		"Investigators":[
		{"ID": "36081", "Role": "Principal Investigator"},
		{"ID": "48855", "Role": "Co Investigator"},
		{"ID": "-188954", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Academy of Sciences of Czech Republic"},
		{"Number": "1", "Name": "Antenova Ltd"},
		{"Number": "2", "Name": "Ericsson"},
		{"Number": "3", "Name": "Polytechnic University of Madrid UPM"},
		{"Number": "4", "Name": "Powerwave UK Ltd"},
		{"Number": "5", "Name": "St Petersburg Electrotechnical Uni"},
		{"Number": "6", "Name": "Warsaw University of Technology"}
		],
		"Summary": "The original objectives of the Platform grant were:1. development of new materials2. characterisation3. theory and modelling development4. device developmentOur achivements against these objectives were:1 Microwave dielectric ceramics / Niobates / Pb Free pezoelectrics based on silver niobate / multiferroic/magnetoelectric materials including BiFeO32 First characterisation of BiFeO3 at microwave frequencies,  rigorous models to determine properties in thin ferroelectric films, scanning evanescent wave microscopy3 Density Functional Perturbation Theory, mode matching for accurate values of loss and permittivity4 Devices / piezoelectrically tuned dielectric resonator filters Extra Outputs  not anticipated: Development and patenting of core-less transformers (no ferromagnetic core at all) using layered pcb geometry.The Forward LookIn the new Platform the objectives are:1 To use the Platform flexibility to carry out speculative and adventurous research2 To develop thin film multilayers with particular emphasis on interfaces3 To develop novel devices, prototypes and applications4 To ensure that the expertise is maintained and that key postdoctoral staff can develop their careers and move to more senior positionsThe areas of research that we intend to explore are:*    Fundamental chemistry of functional ceramicsThere is a need to focus on and understand the chemistry, crystal structure and physical properties of ceramics. This knowledge is vital as a reference point for the production of thin films, which are after all made from bulk ceramics targets. We will concentrate on three main groups of ceramics:I. Microwave dielectrics:  II. Piezoelectrics: III.             Multiferroics / magnetoelectrics: This builds on the group's expertise in the solid state chemistry and reactions of electronic and magnetic ceramics.*   Thin functional oxide films / advanced characterisation methodsThe future trend will be towards nanoscale structures. Our core areas of research are: Materials development; thin film deposition; structural and electrical characterisation; device development. The future strategy requires extra expertise in the area of TEM (Professor McComb), electron holography (Harrison).  *   New device structures to test material propertiesWhilst a material's structural and electrical properties can and will be tested during development, a very useful method of testing a material is to assess its performance in a prototype device.  This enables us to evaluate the different influences on performance. We will examine  ultra High Q structures and frequency agile devices*  Modelling of structures using density functional theoryLinear scaling DFT codes will faciltiate the study of the electrical properties of large superlattices and multilayered thin-films.  The influence of substitutions and defects in bulk ceramic systems will also be accessible as will be the properties of large unit-cell crystals such as spinels and ferrites.  Modelling will also complement the advanced characterisation techniques and fundamental solid-state chemistry areas of research."
	},
	{
		"grant":47,
		"ID": "EP/F069421/1",
		"Title": "Adaptive cognition for automated sports video annotation (ACASVA)",
		"PIID": "8936",
		"Scheme": "Standard Research",
		"StartDate": "05/05/2009",
		"EndDate": "30/09/2013",
		"Value": "1415482",
		"ResearchArea": "Human Communication in ICT",
		"Theme": "Information and Communication Technologies",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51",
		"Investigators":[
		{"ID": "8936", "Role": "Principal Investigator"},
		{"ID": "-114528", "Role": "Co Investigator"},
		{"ID": "37812", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The development of a machine that can autonomously understand and interpret patterns of real-world events remains a challenging goal in AI. Humans are able to achieve this by developing sophisticated internal representational structures for object and events and the  grammars  that connect them.  ACASVA aims to investigate the interaction between visual and linguistic grammars in learning by developing grammars in a scenario where the number of different events is constrained, by a set of rules, to be small: a sport.  We will analyse video footage of a game (e.g. tennis) and use computer vision techniques to progressively  understand  it as a sequence of (possibly overlapping) events, and build a grammar of events.  We will do a similar audio/linguistic analysis on the commentary on the game.  Both of these grammars will be used to build a representational structure for understanding the game.  Visual representations are additionally constrained by the inference of game rules so that object-classification mechanisms are preferentially tuned to game-relevant entities like 'player' rather than game-irrelevant entities like 'crowd-member'. We will also investigate how the two modes, sight and sound, can influence each other in the learning process; interpretation of the video is affected by the linguistic grammar and vice versa.  Furthermore, this coupling of modes will lead to improved recognition of both audio and video events when the grammars from the video modes are used to influence the audio recognition, and vice versa.  The psychological component of the ACASVA correspondingly attempts to learn how these capabilities are developed in humans; how visual grammars are organized and employed in the learning problem, how these grammars are modified by prior linguistic knowledge of the domain, how visual grammars map onto linguistic grammars, and how game rule-inferences influence lower-level visual learning (determined via gaze-behaviour). These results will feedback into the machine-learning problem and vice versa, as well as providing a performance benchmark for the system.Potential beneficiaries of ACASVA (in addition to the knowledge beneficiaries within the fields of science and engineering) include the broadcasting and on-line video search industries."
	},
	{
		"grant":48,
		"ID": "EP/F069502/1",
		"Title": "Algorithmic Mechanism Design and Optimization Problems with Economic Applications",
		"PIID": "-194260",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "287199",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-194260", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our day-to-day life activities are influenced by various communication networking facilities, like electronic mail, electronic trade and banking systems. Their common underlying infrastructure is the Internet, probably one of the mostinfluential human creations of the last century. The Internet is decentralized, i.e.,created and operated by independent entities (humans, servers, companies)which mostly realize their own, usually selfish, goals. In classic optimization problems which are mostly NP-hard, the complete input data is available to the algorithm. The selfish setting raises new, challenging optimization problems withthe added difficulty that part of the input data is private data of these entities to be elicited. The very successful theory of approximation algorithms is usually used to treat NP-hard problems. On the other hand, the standard tool to cope with selfish behavior is economic non-cooperative game theory and mechanism design, with the prominent notion of Nash equilibria. The synergy between these two areas,algorithmic game theory (AGT), has been a major research area in computer science recently. This vibrant research area is establishing the mathematical platform for problems arising in context of the Internet. Despite efforts, many problems still await their solutions, and many applications call for newmathematical models. We propose to contribute to this fascinating and important research endeavor from many different perspectives, by attacking both fundamental, basic problems and opening new lines of thinking.Our main high level goal is to develop new techniques for mathematical analysis of algorithmic models, and to develop new models, motivated by economic and network design applications, using as basis approximation algorithms and linear programming (duality) techniques. The main classes of problems which we plan to study include basic auction settings (like combinatorial auctions, adwords auctions), revenue maximizing product pricing problems, and network mechanism design problems."
	},
	{
		"grant":49,
		"ID": "EP/F500041/1",
		"Title": "LSI Doctoral Training Centres: University of Warwick",
		"PIID": "39898",
		"Scheme": "LSI Doctoral Training Centres",
		"StartDate": "01/10/2007",
		"EndDate": "30/09/2012",
		"Value": "1117411",
		"ResearchArea": "Analytical Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Chemistry",
		"OrgID": "31",
		"Investigators":[
		{"ID": "39898", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Life Sciences research in the UK is increasingly focused on the opportunities presented by the extensive genomic and post-genomic data now available. However, the abundance and complexity of these resources has already highlighted major deficiencies in the skills required to acquire and interpret these data. The aim of this DTC is to provide advanced training in (i) the mathematical and computational skills required for a detailed and quantitative understanding of core Life Sciences research areas and (ii) the instrumentation technologies that will be increasingly used to analyse macromolecules of central importance."
	},
	{
		"grant":50,
		"ID": "EP/F500084/1",
		"Title": "LSI Doctoral Training Centres: University College London",
		"PIID": "30452",
		"Scheme": "LSI Doctoral Training Centres",
		"StartDate": "01/10/2007",
		"EndDate": "30/09/2012",
		"Value": "1150364",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Cell and Developmental Biology",
		"OrgID": "81",
		"Investigators":[
		{"ID": "30452", "Role": "Principal Investigator"},
		{"ID": "16570", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "1. To train high quality scientists to apply state-of-the-art techniques from the mathematical/engineering/computer/physical sciences to solve genuine biological problems.2. To create a cohort of researchers working at the Life Sciences Interface who can act as a focus of interdisciplinary activities and disseminate the importance of such research throughout the UK. 3. To encourage more mathematical/engineering/computer/physical scientists to work in the Life Sciences Interface, and to stimulate new collaborations with life scientists.4. To identify new life sciences problems that will be of interest to mainstream mathematical/engineering/ computer/physical scientists."
	},
	{
		"grant":51,
		"ID": "EP/F501374/1",
		"Title": "EngD in Large-Scale Complex IT Systems",
		"PIID": "10482",
		"Scheme": "Engineering Doctorate",
		"StartDate": "01/04/2008",
		"EndDate": "30/09/2016",
		"Value": "4099287",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "10482", "Role": "Principal Investigator"},
		{"ID": "95204", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed EngD will be the major training activity of the LSCITS Initiative, whose research component is already funded by EPSRC. This Initiative was prompted by various reports by the Royal Academy of Engineering and others, which established an industrial need for specialist education in LSCITS. The specific EngD themes are those that were indentified by the Initiative as the   LSCITS Stack  that also guides the Initiative reserach programme, and spans from Mathematical Foundations,via Predicatable Software Systems, High-integrity Software Engineering, to Compexity in Organisations"
	},
	{
		"grant":52,
		"ID": "EP/G004021/1",
		"Title": "Enforcement of Constraints on XML Streams",
		"PIID": "-185626",
		"Scheme": "Standard Research",
		"StartDate": "30/03/2009",
		"EndDate": "29/07/2013",
		"Value": "533789",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-185626", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The eXtensible Markup Language (XML) has become a ubiquitous format for exchanging data. Enterprise data from industries as diverse as finance, healthcare, and genomics are routinely exchanged as XML. Much of this XML-encoded information has to be queried on-the-fly as it arrives -- that is, as an XML stream.   News and event information, for example, is available in the form of XML feeds; applications that react to these events must process the feeds in streaming fashion. Communication and messaging protocols also make use of XML, and the corresponding protocol handlers are thus also  XML stream-processors.A crucial aspect of processing any form of data is validation: before data is made available to applications, it must be in a  sane'' state. In the context of data being exchanged over networks data corruption is ubiquitous, because messages are received from untrusted or even unknown parties. Indeed, many or even most of the data being sent to web-accessible application servers may be from malicious or compromised hosts.The XML community has already developed standardized means for describing constraints on the structure of XML documents. On the one hand, there are schema-based constraints, such as Document Type Definitions (DTDs) giving limitations on the tags that can occur within a document. Qualifiers in the XML query language XPath provide a more flexible method for adding application-specific constraints. But how can a firewall enforce these constraints efficiently on large collections of parallel feeds? This is a critical issue, whether the XML streams represent signalling messages, event feeds,  or web service calls. This project will study which constraints can and cannot be enforced efficiently, and will provide tools and technologies to effectively monitor XML streams for violation of both schema constraints and application-specific constraints."
	},
	{
		"grant":53,
		"ID": "EP/G006555/2",
		"Title": "Illuminating the Path of Video Visualization",
		"PIID": "22018",
		"Scheme": "Standard Research",
		"StartDate": "30/06/2011",
		"EndDate": "29/09/2013",
		"Value": "371464",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Oxford e-Research Centre",
		"OrgID": "106",
		"Investigators":[
		{"ID": "22018", "Role": "Principal Investigator"},
		{"ID": "-107736", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The notion of video visualization was coined by the PI and his postgraduate student in their 2003 IEEE VIS paper. It is a technology drawing the concepts and methodologies from volume and flow visualization, image and video processing, and vision science. It extracts meaningful information from a video data set and conveys the extracted information to users in appropriate visual representations. It is not intended to provide fully automatic solutions to the traditional problems in video processing, but involves human in the loop of intelligent reasoning while reducing the burden of viewing videos. In the subsequent work in collaboration with Stuttgart, the PI and CI introduced the concept of visual signatures in video visualization, and reported a major user study conducted at Swansea involving some 92 subjects [IEEE TVCG 2006]. This work offered an important scientific insight as to how human observers may learn to recognize visual signatures of events depicted in an abstract visual representation of a video.[Tsotsos01] stated that a bounded visual search (e.g., looking for all moving pixel clusters with 20-60 pixels) can be achieved in linear time, whist an unbounded visual search (e.g., looking for something abnormal in a video) is NP-complete. For most practical problems in video processing and computer vision, we rarely have perfectly bounded visual search. We often search simultaneously for entities (e.g., objects, motions or events) in different classes. The models that are used to guide a search are usually incomplete and may lead to uncertainty or errors in detection, segmentation andclassification. The dynamic and unpredictable nature of the input videos instigates mechanisms for heuristic reasoning and iterative decision optimization, which further depart from linear or polynomial performance.In contrast, the human eye-brain system is undeniable more powerful than any current vision system in performing visual searches, especially unbounded visual searches. Even we suppose that the human eye-brain system is a Turing machine, its 100 billion neurons and 100-500 trillion synaptic connections between neurons will unlikely to be matched by computers in the near future. Hence this raises the possibility that using video visualization to aid unbound visual search may provide a more scalable means for dealing with large volumes of video datasets.Video visualization can be deployed in many application areas, such as scientific experimentation and computation, security industry and media and entertainment industry. However, in traditional visualization (e.g., medical visualization), the users are normally familiar with the 3D objects (e.g., bones or organs) depicted in a visual representation. In contrast, human observers are not familiar with the 3D objects depicted in a visual representation of a video because one spatial dimension of these objects shows the temporal dimension. The problem is further complicated by the fact that, in most videos, each 2D frame is the projective view of a 3D scene. Hence, a visual representation of a video on a computer display is, in effect, a 2D projective view of a 4D spatiotemporal domain. In order to for us to see 'time' without using 'time', we need to address a range of challenges in science, technology, visual perception and applications. This project is intended to continue the UK's leadership in tackling these challenges by building on the existing expertise and excellence in video visualization."
	},
	{
		"grant":54,
		"ID": "EP/G008329/1",
		"Title": "Digital imaging enhanced by plasmon resonance elements",
		"PIID": "71158",
		"Scheme": "Standard Research",
		"StartDate": "02/03/2009",
		"EndDate": "01/03/2013",
		"Value": "482449",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Electrical Engineering",
		"OrgID": "49",
		"Investigators":[
		{"ID": "71158", "Role": "Principal Investigator"},
		{"ID": "110614", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Sharp Laboratories of Europe Ltd"}
		],
		"Summary": "In this project we will combine the CMOS imager design skills at Oxford University and the thin-film technology of Sharp Laboratories Europe with the nanofabrication and nano-optics expertise at Glasgow University to, for the first time, implement plasmon enhanced technologies for use in imaging and displays.  The proposed technology can provide both wavelength and polarisation control in a single fabricated layer on the surface of otherwise standard technologies.  This is a major step-forward from present day technologies that rely on multiple layers of processing to achieve less powerful effects.Optical resonances occur at the surface of metal films due to the dielectric dispersion relation.  This phenomenon leads to surface plasmon resonances (SPR).  Surface plasmons are non-radiative electromagnetic surface waves that cause fluctuations in the surface electron density.  The simplest exploitation of this phenomenon is in thin films where the dispersion relation is close to resonance, leading to the enhancement of the electric field of a propagating light wave.  Surface inhomogeneity, such as a deliberately-created periodic undulation on the metal surface, is used to improve the coupling of the light to the plasmons [1] hence increasing the enhancement.  More recent work has shown how nanoparticle structures made by techniques ranging from colloidal suspensions to direct-write lithography can lead to further SPR enhancement in small structures.CMOS integrated circuits are now the dominant technology for optical imaging, including digital cameras, microscopes and a range of optical instruments.  Similarly, active display technologies have become dominant, and widespread, in industrial and commercial sectors.  These electronic devices combine high performance with low cost and also enable designers to implement signal processing functions on to the same substrate as the imaging sensor to reduce cost, pixel size and power consumption. However, current technologies suffer from a number of drawbacks that are limiting progress in traditional markets.  Furthermore, little is being done to enable the core technology to expand its functionality, and hence use, in new and emerging markets.  The aim of this project is to use the emerging field of plasmonics to study the potential for using back-end-of-line (BEOL) processing at the silicon foundry to enable both enhancement and diversification of the capabilities of electronic optical detectors, imagers and displays."
	},
	{
		"grant":55,
		"ID": "EP/G008523/1",
		"Title": "Embedded Broadband Ultrasonic Sensing for Robust and Scalable Positioning",
		"PIID": "129562",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "183286",
		"ResearchArea": "Mobile Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "129562", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Microsoft Research Ltd"}
		],
		"Summary": "Important applications spanning the fields of ubiquitous computing and sensor networking require the spontaneous provision of reliable, accurate, and up-to-date location information in unprepared environments. Examples include simultaneous localisation and mapping of new environments using resource-constrained devices; impromptu location-aware collaborative work and data sharing between proximate users; and emergency search and rescue and live coordination of personnel in disaster sites.  Embedded, ad hoc positioning systems are a crucial ingredient for such applications. In such systems, wireless and battery-powered sensing devices can be deployed in a matter of minutes to collaboratively measure and estimate their locations, and supply the required information.While solutions for minimal deployment and ad hoc positioning have been put forth for ubiquitous computing and sensor networks, none of the systems to date have been able to provide the high-fidelity information the above applications require. Due to their interactive and real-time nature, they tend to require location readings which are robust (consistently delivered accuracy of a few tens of centimetres or better) and up-to-date for all participating nodes (several location readings per second for each locatable device). The aim of this project is to develop broadband ultrasonic signalling and processing methods for ad hoc, embedded positioning systems.  This will allow them to (1) produce robust estimates (through noise-resistant coding and measurement of multiple physical quantities such as range, bearing, and velocity) even when only a small number of devices are present; and (2) maintain high location update rates (through multiple access signalling) when a large number of devices are present.  Our approach is to couple broadband ultrasonics (piezopolymer transducers) with real-time signal processing implemented using reconfigurable fabrics (field programmable gate arrays)."
	},
	{
		"grant":56,
		"ID": "EP/G009635/1",
		"Title": "An Interactive and Intuitive Object Deformation Framework for Interactive Multimedia Applications",
		"PIID": "-185357",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/04/2009",
		"EndDate": "31/12/2012",
		"Value": "101535",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-185357", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Object deformation is an important aspect of Interactive Multimedia Applications (IMAs) that are prevalent over a number of different applications, such as product and engineering design, facial animation in MPEG-4, medical simulation and computer gaming. In such applications, users may interact with and manipulate deformable objects while expecting to receive interactive responses. However, because processing deformable objects is time consuming and computationally expensive, rigid (i.e. non-deformable) objects are still being used in place of deformable objects in many existing applications. This reduces both the level of realism and the expressive power in an application.There are two critical issues to address involving object deformation in IMAs. First, object deformation requires the management and evaluation of both the shape changes and the constraint settings over a number of domain spaces, such as geometric space and motion space. These are large in terms of data size and complexity. Second, an object can be deformed provided that certain constraints and manipulators have been set up, which may have very different natures and be difficult to specify.The research in this proposal will develop a new object deformation framework, which will comprise of a set of new techniques to give an improved object deformation performance and quality over existing techniques and to offer a novel interface to allow deformation constraints and manipulators to be specified in an intuitive manner."
	},
	{
		"grant":57,
		"ID": "EP/G011737/1",
		"Title": "AMPS: Analysis of Massively Parallel Stochastic Systems",
		"PIID": "-27192",
		"Scheme": "Standard Research",
		"StartDate": "24/08/2009",
		"EndDate": "23/06/2013",
		"Value": "543607",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-27192", "Role": "Principal Investigator"},
		{"ID": "95522", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "IBM UK Labs Ltd"},
		{"Number": "1", "Name": "Uni of Illinois at Urbana Champaign"}
		],
		"Summary": "A fast response time and a reliable service are important quality of service criteria for almost all computer and communication systems.  Indeed, a system which does not meet its performance and dependability requirements - its system performability - is, in practical terms, as ineffectual as a system that does not meet its correctness requirements.This proposal aims to develop a range of novel techniques and software tools for the performability analysis of massively parallel systems, that is systems which are constructed from large groups of identical interdependent distributed components. The analysis of these systems is a difficult, yet practically important problem in many diverse fields, including the biological sciences - e.g. the dynamics of insect colonies and rate of spread of infections, environmental sciences - e.g. population growth and crowd dynamics, economics - e.g. fluctuations of financial markets as a result of individual trader behaviour, and computer science - e.g. diffusion of computer worms and viruses, dynamic load modelling in computational Grids, peer-to-peer networks, and mobile ad hoc networks.From this wide range of possible application areas, we will focus on massively parallel computer-communication systems.  Many such systems, such as distributed mobile publish-subscribe architectures, peer-to-peer filesharing networks and network worm infestations, are having an increasing economic, social and technological impact on our society. Yet, while great strides have been made in our ability to concisely describe these systems and their dynamic behaviour by means of compositional modelling formalisms, much less progress has been made on our ability to analyse these systems quantitatively.  This is because almost all traditional performance analysis techniques are based on the idea of (explicitly or implicitly) constructing and solving a Markov chain made up of all possible system behaviours or states.  Because the number of states explodes combinatorially with an increasing number of components, the number of states found in a massively parallel system is typically far beyond the feasible limit of direct quantitative analytical study (currently the state of the art is of the order 100 million states).  This means that usually the only practical alternative for tackling such models is discrete-event simulation.  However, for large models, even long-running simulations often suffer from low state-space coverage, which in turn leads to problems with stability and accuracy of performance metrics (especially where rare-events are not taken into account).An exciting recent development in the performance analysis of such massively parallel systems when represented in stochastic process algebras (such as PEPA), is to use a fluid approximation of the state space. We aim to significantly develop this new paradigm, in collaboration with compositional techniques, by exploring how the fluid approximations can be captured precisely using ordinary and stochastic differential equations (ODEs and SDEs). This gives us the potential to explore the emergent behaviour of a massively parallel system based on the discrete agent description of the underlying components."
	},
	{
		"grant":58,
		"ID": "EP/G012407/1",
		"Title": "Structural Comparison of Labelled Graph Data",
		"PIID": "27991",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2012",
		"Value": "81067",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer and Information Sciences",
		"OrgID": "48",
		"Investigators":[
		{"ID": "27991", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Semistructured data is an important data format, essentially embodied by the XML standard. It is increasingly used in many critical applications, especially in business-to-business communications and peer-to-peer traffic on the Internet.The main advantage of the semistructured format is that it increases the flexibility of the way the data may be structured: as new situations arise, the structure of the data may evolve as well as the values.For example, given an established stream of business messages about car insurance between multiple brokers and underwriters, one underwriter may decide that the colour of a car is a significant factor not currently included in the data being supplied. They can advertise this fact, and brokers may choose to start asking their clients for the colour of their cars. From this point onwards, a <colour> field may start to appear in the messages between brokers and underwriters, the field being optionally included by brokers and optionally acted upon by underwriters where it is present.When much of this kind of activity takes place, it becomes important to consider the structural attributes of the whole, potentially large, pool of data, as well as the individual items. For example, given a set of data items: do they have anything much in common with each other?; do they all have at least something in common, and if so what?; how different is one given item from another, and are there any others exactly the same as this one?; can one or more clusters of similarly or identically-structured items be identified within the pool?; is the pool of data itself evolving or becoming quiescent, that is, over time, are individual items becoming, on the whole, more different or more similar?Recent work we have done on the inherent complexity, and thus regularity, of semistructured data items has led us to an observation that we believe will give great insights into how to answer these and other similar questions. Using some long-established results from Information Theory, we have applied the concept of mechanical entropy to the domain of semistructured data to give a metric for the complexity of individual data items. We have also discovered an efficient way of calculating this, by use of a data structure, the structural fingerprint, which represents the essential structure of the item. We now believe that the reapplication of this work into the above context will give a great leverage in terms of producing useful, quantified answers to the above questions and others, while the use of the structural fingerprint will make it computationally feasible to perform these calculations upon large pools of semistructured data in the global domain of the Internet."
	},
	{
		"grant":59,
		"ID": "EP/G012458/1",
		"Title": "Dynamics of Spin-VCSELs",
		"PIID": "30827",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "254609",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30",
		"Investigators":[
		{"ID": "30827", "Role": "Principal Investigator"},
		{"ID": "42440", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We propose to investigate the dynamic properties of optically injected spin-polarised, vertical-cavity surface-emitting lasers (VCSELs). In general spin-polarised lasers offer many attractive advantages, including enhanced intensity and polarisation stability as well as a reduced threshold current, and they have potential applications in cryptographic communications and reconfigurable optical interconnects. Moreover the nonlinear dynamics of optically-injected semiconductor lasers has been a fertile research topic for many years and the benefits are now emerging in stable injection-locked sources with very high bandwidths as well as in potential applications in encrypted communications systems based on chaos. However, the novelty of this proposal lies in combining the hitherto disparate fields of spin-polarised VCSELs and optical injected lasers, examining particularly the effects of polarised optical injection on the nonlinear dynamics of spin-polarised lasers."
	},
	{
		"grant":60,
		"ID": "EP/G012962/1",
		"Title": "Solving Parity Games and Mu-Calculi",
		"PIID": "29466",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2009",
		"EndDate": "31/12/2012",
		"Value": "279162",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Lab. for Foundations of Computer Science",
		"OrgID": "41",
		"Investigators":[
		{"ID": "29466", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modal mu-calculus is a logical language used for describing the behaviour of hardware and software systems. It is widely used, but we still don't know some of its main mathematical properties. In particular, we don't know whether it's always cheap to work out whether a system has some behaviour described in this language, or whether very complicated properties are really expensive to check. People have been trying to solve this problem for more than 20 years now.In this grant proposal, we want to explore some new approaches to this problem. Our main aim is to use a branch of mathematics called descriptive set theory, which looks at the difficulty of describing - writing down in mathematical notation - sets of numbers. Some sets require very complicated formulae to write them down, whereas others have simple descriptions. In earlier work, we have made a connection between this difficulty of describing sets, and the complexity of properties in modal mu-calculus, which let us solve another well known problem about modal mu-calculus. Now we want to take these ideas much further, and see if we can get closer to a solution to the problem described above.Another of the surprising things about this problem is that there are ways of solving it which look as though they are fast, but which nobody can yet prove really are fast. So we will also try to understand these solutions more deeply, hoping to show either that they really are always fast, or that we can come up with bad examples on which they take a long time.We are also trying to solve another problem about modal mu-calculus, which is basically this: if you give me a formula that looks very complicated, can I work out whether it is really a complicated property, or whether it can be turned into an equivalent but much simpler formula? This has also been around for a long time, with only limited answers. We think that this problem is well suited to an approach with our techniques, and we are optimistic about solving it."
	},
	{
		"grant":61,
		"ID": "EP/G013500/1",
		"Title": "Advanced Processor Technologies Platform Grant",
		"PIID": "5628",
		"Scheme": "Platform Grants",
		"StartDate": "01/10/2008",
		"EndDate": "30/09/2013",
		"Value": "1154981",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "5628", "Role": "Principal Investigator"},
		{"ID": "16653", "Role": "Co Investigator"},
		{"ID": "116096", "Role": "Co Investigator"},
		{"ID": "-154576", "Role": "Co Investigator"},
		{"ID": "4702", "Role": "Co Investigator"},
		{"ID": "34959", "Role": "Co Investigator"},
		{"ID": "1784", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This Platform Grant is to support the activities of the Advanced Processor Technologies research group in the School of Computer Science, University of Manchester.The activities of the group are concerned with the advancement of techniques to construct high-performance processors of the future. This covers the structure of future systems which will both be highly parallel and need to cope with the variability of circuits resulting from their continuing miniaturisation. Other areas of concern are the need for tools to design these complex systems and the way in which programs are written and controlled on the future hardware.This is a rapidly developing area and the resources of the Platform Grant will enable the group to develop their research to maintain a position at the forefront of the subject."
	},
	{
		"grant":62,
		"ID": "EP/G013829/1",
		"Title": "Non-interactive Zero-Knowledge Proofs",
		"PIID": "-187034",
		"Scheme": "First Grant Scheme",
		"StartDate": "10/06/2009",
		"EndDate": "09/09/2012",
		"Value": "301726",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Adastral Park Campus",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-187034", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Society is becoming increasingly digitalized and interconnected. When building the future society it is important to think about security; we need protection against criminal organizations, hostile nations and other types of adversaries that may use techniques such as eavesdropping and wiretapping, hacking, attempts of impersonation, etc. In short, we need secure protocols.Two important issues frequently come up when designing secure protocols between two or more parties: verifiability and privacy. As an example, consider a protocol by which an internet bank customer gains access to her account. On one hand the internet bank wants to verify that it is talking to its customer, on the other hand the customer wants her password to remain private. As a more general example, verifiability and privacy issues come up whenever we need to verify that somebody is following a particular protocol correctly, yet that person or entity has some secrets that cannot be revealed.This proposal relates to techniques in the field of cryptography known as zero-knowledge proofs, which simultaneously provide verification and privacy. A zero-knowledge proof allows one party to convince another party that a certain statement is true without leaking any other information. The internet bank customer can for instance convince the bank that she should get access to her account without even sending a password or any other private information over the internet. Or in the more general example, somebody we are interacting with can convince us that they are following the protocol without divulging their private information.Zero-knowledge proofs can be both interactive and non-interactive. Whereas the two parties exchange messages back and forth in standard zero-knowledge proofs, a non-interactive zero-knowledge proof consists of a single message that is sent from one party to the other. This distinction is important since there are many examples of non-interactive tasks, where only one party acts. For instance, we can make a digital signature on a document without interacting with other parties. Non-interactive zero-knowledge proofs can be used in connection with such non-interactive tasks.In this research project we intend to improve state of the art in non-interactive zero-knowledge proofs. We will construct more efficient non-interactive zero-knowledge proofs. We will construct non-interactive zero-knowledge proofs with additional advanced security properties. We will base our constructions on as sound security assumptions as possible. We will extend their range of applicability to more and different settings than are currently known how to handle. In addition to these improvements, we will also demonstrate the advances made by giving concrete applications."
	},
	{
		"grant":63,
		"ID": "EP/G015635/1",
		"Title": "Multiparty Session Types: Theory and Conversation-Oriented Programming",
		"PIID": "84749",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2009",
		"EndDate": "31/12/2012",
		"Value": "344294",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "84749", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Driven by the industrial and social needs, communication is rapidlybecoming the central element of software development, as found in webservices, business protocols, multicore CPUs and corporateintegration. In this environment, a programmer will have hundreds ofcomputing nodes per application at her/his disposal, which will becomea norm in a few years: communication and concurrency is becomingcentral to computing. However a mature understanding on how software can best harness thepower and richness of this coming environment, is still lacking in thecomputing community, industry and academia alike.  This current lackof the understanding --- hence lack of agreement --- on this issuewill lead to a dire confusion in programming, in modelling, inoptimisation technologies, and in the development of diverseinfrastructures including middle-ware. This project aims to contributeto the development of some of the key technical elements forharnessing the power of communication and concurrency in programming.The project centres on the idea of organising communication asstructured conversations, based on one of the most advanced theoriesin this area, the multiparty session types, recently introduced by thePIs. We develop a generalisation of the theory of multiparty sessiontypes, then apply the developed theory to a design and implementationof an extension of the well-known object-oriented language, Java,extending the code base which is also developed by the PIs.The resulting language, Java with multiparty session types, will beused to implement real-world complex financial/business protocols inthe two industry standards, ISO UNIFI (ISO TC68 WG4 ISO20022 UNIversalFinancial Industry message scheme) and W3C CDL (ChoreographyDescription Language). The theory of multiparty session types itselfwas motivated by the PIs' dialogue with key personnel of thesestandards. Through implementing advanced financial/business protocolsin the developed language, validator and compiler, not only can weexamine the practical usability of the developed theory vis-a-vis someof the most complex conversation patterns found in practice, but alsowe shall make it viable to use the developed technical ideas includingnotations and implementations as part of these standardisationefforts, for which the PIs have been working as official invitedmembers."
	},
	{
		"grant":64,
		"ID": "EP/G015740/1",
		"Title": "Biologically-Inspired Massively Parallel Architectures - computing beyond a million processors",
		"PIID": "5628",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2009",
		"EndDate": "14/06/2014",
		"Value": "2707120",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "5628", "Role": "Principal Investigator"},
		{"ID": "34959", "Role": "Co Investigator"},
		{"ID": "29759", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "ARM Ltd"},
		{"Number": "1", "Name": "Silistix Ltd"}
		],
		"Summary": "The human brain remains as one of the great frontiers of science - how does this organ upon which we all depend so critically actually do its job? A great deal is known about the underlying technology - the neuron - and we can observe large-scale brain activity through techniques such as magnetic resonance imaging, but this knowledge barely starts to tell us how the brain works. Something is happening at the intermediate levels of processing that we have yet to begin to understand, but the essence of the brain's information processing function probably lies in these intermediate levels. To get at these middle layers requires that we build models of very large systems of spiking neurons, with structures inspired by the increasingly detailed findings of neuroscience, in order to investigate the emergent behaviours, adaptability and fault-tolerance of those systems.Our goal in this project is to deliver machines of unprecedented cost-effectiveness for this task, and to make them readily accessible to as wide a user base as possible. We will also explore the applicability of the unique architecture that has emerged from the pursuit of this goal to other important application domains."
	},
	{
		"grant":65,
		"ID": "EP/G019622/1",
		"Title": "Extending the Applications and Improving the Efficiency of Positioning Through the Exploitation of New GNSS Signals",
		"PIID": "119985",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2009",
		"EndDate": "30/06/2013",
		"Value": "733900",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Civil Environmental and Geomatic Eng",
		"OrgID": "81",
		"Investigators":[
		{"ID": "119985", "Role": "Principal Investigator"},
		{"ID": "3606", "Role": "Principal Investigator"},
		{"ID": "119985", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Air Semiconductor Ltd"},
		{"Number": "1", "Name": "EADS Astrium"},
		{"Number": "2", "Name": "Leica Geosystems Ltd"},
		{"Number": "3", "Name": "Nottingham Scientific Ltd"},
		{"Number": "4", "Name": "Ordnance Survey"},
		{"Number": "5", "Name": "QinetiQ Ltd"},
		{"Number": "6", "Name": "S T Microelectronics"},
		{"Number": "7", "Name": "Thales Research and Technology UK Ltd"},
		{"Number": "8", "Name": "The Civil Aviation Authority"}
		],
		"Summary": "Over the past three decades the US GPS (Global Positioning System) has evolved from a system designed to provide metre-level positioning for military applications to one that is used for a diverse range of unforeseen, and mainly civilian, applications. This evolution has been both driven and underpinned by fundamental research, including that carried out at UK universities, especially in the fields of error modeling, receiver design and sensor integration. However, GPS and its current augmentations still cannot satisfy the ever increasing demands for higher performance. For instance there is insufficient coverage in many urban areas, it is not accurate enough for some engineering applications such as the laying of road pavements and receivers cannot reliably access signals indoors.However things are changing rapidly. Over the next few years the current GNSSs (Global Satellite Navigation Systems) are scheduled to evolve into new and enhanced forms. Modernised GPS and GLONASS (Russia's equivalent to GPS) will bring new signals to complement those that we have been using from GPS for the last 30 years. Also we will see the gradual deployment of new GNSSs including Europe's Galileo and China's Compass systems, so leading to at least a tripling of the number of satellite available today by about 2013 - all with signals significantly different from, and more sophisticated than, those used today.These new signals have the potential to extend the applications of GNSS into those areas that GPS alone cannot satisfy. They will also enable the invention of new positioning concepts that will significantly increase the efficiency of positioning for many of today's applications and stimulate new ones, especially those that will develop in conjunction with the anticipated fourth generation communication networks to provide the location based services that will be essential for economic development across the whole world, including the open oceans. This proposal seeks to undertake a number of specific aspects of the research that is necessary to exploit the new signals and to enable these new applications. They include those related to the design of new GNSS sensors, the modeling of various data error sources to improve positioning accuracy, and the integration of GNSSs with each other and with other positioning-related inputs such as inertial sensors, the eLORAN navigation system, and a wide rage of pseudolite and ultra-wide band radio systems. We are also seeking to find new ways to measure the quality of integrated systems so that we can realistically assess their fitness for specific purposes (especially for safety-critical and legally-critical applications). As part of our work we will build an evaluation platform to test our ideas and validate our discoveries.The proposal builds on the unique legacy of the SPACE (Seamless Positioning in All Conditions and Environments) project, which was a successful EPRSC-funded research collaboration framework that brought together the leading academic GNSS research centres in the UK, with many of the most important industrial organisations and user agencies in the field. The project laid the foundation for an effective, long-term virtual academic team with an efficient interface to access industry's needs and experience. The research proposed here will be carried out within a new collaboration framework (based on SPACE) involving four universities (UCL, Imperial, Nottingham and Westminster) and nine industrial partners (EADS Astrium, Ordnance Survey, Leica Geosystems, Air Semiconductors, ST Microsystems, Thales Research and Technology, QinetiQ,  Civil Aviation Authority and NSL). The industrial partners have pledged almost 2M of in-kind support and the proposed management structure, led by one of the industrial partners, is carefully designed to foster collaboration and to bring to bear our combined facilities and resources in the most effective manner."
	},
	{
		"grant":66,
		"ID": "EP/G020604/1",
		"Title": "Quantified Constraints and Generalisations",
		"PIID": "15129",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2009",
		"EndDate": "31/08/2012",
		"Value": "247539",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "15129", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computational complexity is the study of the resources needed for the computational solution of problems, with these resources usually being time and space (that is, memory).  The subject originated about 40 years ago when it was realised that just because a computer can solve a problem in theory, it does not mean to say that it can solve the problem in practice.  The concept of NP-completeness arose and ever since NP-completeness has provided a barrier to efficient computation, even though there still remains the possibility that all NP-complete problems might all be efficiently solvable.  This famous P versus NP question is one of the most important open problems in mathematics and computer science.Computer scientists, whilst continuing to try and resolve the P versus NP question, have looked at ways of getting round the question by restricting attention to certain classes of problems.  The class of constraint satisfaction problems forms a very significant class of problems within NP, and very often problems from this class have been shown to be susceptible to reasonable solution using a variety of heuristic methods.  As such, the study of constraint satisfaction problems has resulted in extremely powerful tools and techniques for the good solution of many real-world problems.  Fairly recently, a more theoretical analysis of constraint satisfaction problems (CSPs) has been undertaken.  The driving force behind this analysis (which uses myriad techniques and methods from computability, combinatorics, logic and algebra) has been Feder and Vardi's conjecture that all (non-uniform) constraint satisfaction problems are either in P or NP-complete (irrespective of whether P is equal to NP).  The situation with regard to NP is very different for if P is not equal to NP then there is an infinite world of distinct equivalence classes of problems within NP (unlike the world of constraint satisfaction problems where if Feder and Vardi's conjecture is true then there are just 2 distinct equivalence classes).  This theoretical analysis of constraint satisfaction has practical spin-offs, as we are identifying more and more classes of CSPs that can provably be solved efficiently.A natural counterpart to the study of CSPs is the study of SAT-solving, whereupon a problem is reduced to the Satisfiability Problem and solved using some SAT-solver with the results being interpreted so as to solve the original problem.  SAT-solvers are extremely powerful and can solve large instances of many real-world problems.  SAT-solving has naturally expanded into QSAT-solving where the Quantified Satisfiability Problem plays a role identical to that of the Satisfiability Problem.  Powerful QSAT-solvers are now beginning to emerge.In this proposal, we intended to study quantified constraint satisfaction problems (QCSPs), where a QCSP is obtained from a CSP in a manner similar to how the Quantified Satisfiability Problem is obtained from the Satisfiability Problem.  We hope to better understand the structure of QCSPs and related concepts, particularly with regard to their computational complexity."
	},
	{
		"grant":67,
		"ID": "EP/G02085X/1",
		"Title": "ConDOR: Consequence-Driven Ontology Reasoning",
		"PIID": "71179",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2009",
		"EndDate": "30/09/2012",
		"Value": "359707",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "71179", "Role": "Principal Investigator"},
		{"ID": "-192991", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Ontologies, and ontology based vocabularies, are becoming increasingly important. They provide a common vocabulary together with computer accessible descriptions of the meaning of relevant terms through relationships with other terms. For example, in an ontology describing human anatomy the vocabulary could include terms such as [Organ], [Circulatory System], [Heart], etc., and one can define a term [Muscular Organ] as an [Organ] that is a part of the [Muscular System] and a term [Heart] as a [Muscular Organ] that is a part of the [Circulatory System].Ontologies play a major role in the Semantic Web and in e-Science where they are widely used in, e.g., bio-informatics, medical terminologies and other knowledge management applications. One of the most important aspects of ontologies is that they contain knowledge structured in a special way. The users of ontologies are typically interested in obtaining information about relationships between concepts described in ontologies and querying the ontologies. Both tasks require reasoning tools that can derive new knowledge from the knowledge explicitly stated in ontologies. For example a reasoning tool should be able to derive that [Heart] is a part of the [Muscular System] which is not explicitly stated in the anatomical ontology but is a logical consequence of the above definitions for [Heart] and [Muscular Organ].Most existing ontology reasoners do not derive logical consequences of ontological axioms explicitly, but instead they check whether it is possible to construct a model of the ontology where the target consequence does not hold, e.g., they try to construct a situation where [Heart] would be a part of the [Circulatory System] but not a part of the [Muscular System]. If such a situation is not possible, then it is concluded that the target consequence follows from the axioms in the ontology. One problem with this technique is that when an ontology expresses long and possibly cyclic dependencies between terms, e.g., [Heart] is a part of [Circulatory System] which has a part [Lung]  which is a part of [Respiratory System] which has a part [Trachea], etc., then the reasoner has to construct very large models. For some existing medical ontologies, the models are so big that they do not fit into the main memory of a computer. Another problem is that the ontology may potentially have a large number of different models, each of which must be independently explored by the reasoner. Ontology languages provide for constructors called 'number restrictions', which result in a particularly large number of models. Number restrictions are used to specify quantitative information in ontologies and are often used in bio-chemical ontologies, for example to express that a molecule of [Ethanol] contains {exactly 6} [Hydrogen Atoms]. These limitations of model-building reasoners, therefore, pose a serious problem for the development of large medical and bio-chemical ontologies---without efficient reasoning tools, for example, the users of such ontologies may not be able to obtain the information that they are interested in.In this project we investigate alternative  consequence driven  reasoning procedures that do not build models but explicitly derive logical consequences of ontological axioms. Our preliminary investigations suggest that both problems mentioned above can be avoided for consequence-driven reasoning procedures: there is no need to keep track of large models, and the number of logical consequences of ontological axioms is typically much smaller than the sizes and the number of the models."
	},
	{
		"grant":68,
		"ID": "EP/G023018/1",
		"Title": "Reliable Distributed Algorithms for Dynamic Communication in Ad Hoc Networks",
		"PIID": "-115066",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2009",
		"EndDate": "29/08/2012",
		"Value": "278844",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-115066", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The last several years has seen a substantial increase in the deployment of wireless networks as a result of the growth in popularity of mobile laptops, cell phones, PDAs, and sensor devices. Despite the unquestionable advantages of wireless communication, there are also significant new challenges posed by this development. One of them is to handle the mobility of devices, other is to overcome the impact of various kinds of failures that are common in wireless environment. The examples of typical faults are:(1) unreliable communication: in real-world deployments, electromagnetic interference is ubiquitous and can often prevent communication; (2) fault-prone devices: wireless devices are often small, fragile, and have limited battery life, resulting in frequent devices failures; (3) malicious network disruption: wireless device networks are often deployed in public venues, facilitating easy attacks.Malicious adversaries pose a particularly great threat to wireless networks: the sharednature of the communication medium allows an adversary to disrupt or prevent anyinformation exchange between honest processes by ``jamming'' the channel with noise or by propagating corrupted data instead.In this project we address the problem of reliable and efficient communication in wireless networks prone to failures. Such communication is typically dynamic, which means that communication tasks are generated in ad hoc manner by applications run by the devices. Network topology is also ad hoc, due to the mobility of devices. All these issues, together with various kinds of failures, raise a complex challenge for protocol designers. In this context, we plan to address fundamental communication tasks, such asrouting, information dissemination and aggregation (e.g., broadcast, convergecast), information exchange (e.g., gossip, group communication), as well as related problems such as agreement and leader election. There has been relatively little work done on development and analysis of algorithms for fault-prone wireless environments, especially in the presence of malicious adversaries.Moreover, all these works consider only static communication, i.e., where there is only one task triggered by each device. Such an assumption is a significant simplification of more realistic scenarios, where the tasks may interfere with each other. The main goal of this project is to develop new algorithmic techniques, accompanied by a comprehensive theoretical analysis and simulations, for coping with unreliable dynamic communication, faulty devices, and malicious disruption in ad hoc wireless networks.More specific objectives include:(a) development of a more comprehensive theoretical model for dynamic communication in fault-prone ad hoc wireless networks,(b) design of new algorithms for fundamental communication problems, and(c) evaluation of new algorithmic solutions in the theoretical model and through software simulations, followed by comparison with widely used wireless protocols.This project focuses on development of algorithmic foundations of reliable and dynamic communication in ad hoc wireless networks. A complementary, more engineering-oriented approach, is not addressed in this proposal.The expertise of Dr. Kowalski in the algorithmic aspects of fault-tolerant distributed computing, network communication and mobile computing, as well as the broad network of collaborating research groups and individual researchers, is key to the success of the proposed project. It is expected that new developments within this project will improve reliability and stability of communication protocols forming a part of future wireless distributed systems."
	},
	{
		"grant":69,
		"ID": "EP/G023360/2",
		"Title": "Automated Modelling and Reformulation in Planning",
		"PIID": "46163",
		"Scheme": "Standard Research",
		"StartDate": "02/11/2011",
		"EndDate": "01/12/2012",
		"Value": "102118",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Informatics",
		"OrgID": "78",
		"Investigators":[
		{"ID": "46163", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Although AI Planning and Constraint Programming share many techniques and approaches, an important difference lies in the approach to modelling. In CP and also in Operations Research, modellers spend considerable time and effort evaluating alternative models and selecting representations of a problem that will make it most amenable to solution by existing technology. In Planning, researchers typically spend little time considering alternative models and are content to work with the first model they construct, working instead on improving the planning technology to try to tackle the problem, whatever its form. The reason for the strategy of planning researchers is that the intention is to avoid the need for expert planning knowledge in order to exploit a planner. However, the price for this strategy is that there is very little accumulated research expertise in the problem of modelling and no systematic comparison of the performance of planners using alternative models of the same problem. Although avoiding the need for expert planning knowledge in order to use a planner is an important goal, there is clearly a lost opportunity to identify ways in which models might be structured to be most amenable to solution. We propose to combine these strategies by exploring the automatic reformulation of planning problems in order to better exploit the existing planning technology by restructuring models to expose the information that can make a planner make more intelligent choices."
	},
	{
		"grant":70,
		"ID": "EP/G026009/1",
		"Title": "Electrically Pumped Broad Band and Vertical Cavity Semiconductor Dilute Nitride Amplifiers for Metro and Acess Networks",
		"PIID": "56944",
		"Scheme": "Standard Research",
		"StartDate": "02/03/2009",
		"EndDate": "31/08/2012",
		"Value": "298897",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "56944", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Optical fibre communications are used for transmission of voice, data and video throughout the world today. As the demand for broadband services in the access segment of the industry continues to increase, network operators face increased challenges to deliver higher bandwidths since customers are often not prepared to pay significantly more than at present for these services. Cost-effective, well-managed metropolitan networks are therefore required that have sufficient capacity and flexibility to respond to future demand. For future optical metro and access networks it is essential to develop cheap, reliable components with good performance at the wavelength of 1.3 micron that allows transmission of high bandwidths over fibre. Such networks are very cost-sensitive, since some components serve just one customer instead of being shared by large numbers of users as in, for example, a trans-oceanic cable. There is therefore a pressing need for optical components that can offer the required functionality at low cost with high bandwidth. In this context, components based on the dilute nitride (GaInNAs/GaAs) system are predicted to offer significant advantages over devices using the more conventional GaInAs/InP system. In particular the broad gain spectrum of GaInNAs in the wavelength range 1.3 - 1.55 micron makes it especially suitable for use in planar semiconductor optical amplifiers, whilst the aspects of growth on GaAs and integration with GaAs/AlGaAs DBRs are attractive for applications in vertical-cavity devices. Initial work in this area has been successful, with the demonstration of edge-emitting lasers and vertical-cavity surface-emitting lasers (VCSELs) with good light output and fast modulation speed. The current proposal seeks to further exploit the device potential of dilute nitrides by focussing on the design and characterization of two specific photonic devices: an edge emitting broad band semiconductor optical amplifier (BBSOA) and an electrically pumped vertical cavity semiconductor optical amplifier (VCSOA). This joint proposal between Essex and Bristol will be productive, cost-effective and wide-ranging, covering both VCSOAs and BBSOAs for different metro and access applications in the 1.3 micron communications window."
	},
	{
		"grant":71,
		"ID": "EP/G026238/1",
		"Title": "myGrid: A Platform for e-Biology Renewal",
		"PIID": "6007",
		"Scheme": "Platform Grants",
		"StartDate": "10/01/2009",
		"EndDate": "09/01/2014",
		"Value": "1124438",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "6007", "Role": "Principal Investigator"},
		{"ID": "83408", "Role": "Co Investigator"},
		{"ID": "-7965", "Role": "Co Investigator"},
		{"ID": "98122", "Role": "Co Investigator"},
		{"ID": "4049", "Role": "Co Investigator"},
		{"ID": "8693", "Role": "Co Investigator"},
		{"ID": "54170", "Role": "Co Investigator"},
		{"ID": "88057", "Role": "Co Investigator"},
		{"ID": "1867", "Role": "Co Investigator"},
		{"ID": "110304", "Role": "Co Investigator"},
		{"ID": "5579", "Role": "Co Investigator"},
		{"ID": "12161", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "SummaryThe myGrid Consortium is a multi-institutional, multi-disciplinary, internationally leading research group focusing on the challenges of e Science-the use of computational resources that allows scientists around the world to collaborate to produce and analyse the vast amounts of complex data in disciplines as diverse as biology, chemistry, astronomy, physics, music and social science. This platform grant enables the consortium to sustain an internationally leading team of researchers working on the foundations of e-Science. The consortium delivers e-Laboratory environments in which scientists perform virtual or in silico experiments. The consortium's flagship tools include Taverna, myExperiment and Utopia. Taverna is used to develop the scientific workflows that scientists use to gather and analyse data - these represent the experiments on, for example, the genes and proteins involved in diseases. The myExperiment Virtual Research Environment is social web site software for the social curation and sharing of scientific research objects, including workflows and in silico experiments. UTOPIA is a suite of scientific visualisation and analysis tools that brings together disparate data sources in an easy to use unified interface. Together these enable scientific investigations to be undertaken in a way that enables the scientist to concentrate on the science, a feat that requires basic research in computer science.These E-Science tools are world leaders with 1000, 900 and 2000 users respectively - Taverna is used in some 350 organisations. Producing these tools necessitates foundational e-Science research in four main areas: the management of the knowledge in such environments; the production and management of the metadata, or descriptions, of the experiments and experimental holdings; the design, use and reuse of in silico experiments; and the exploitation of social networks to enhance e-Science. Explicitly engaging with users supports adoption, and it drives challenging, user-relevant research and development based on observed experience and real need. The platform grant enables the consortium to retain key staff that help sustain this world leading effort in e-Science and Open Science - they are experts in scientific workflow management, semantic technologies, intelligent middleware and social computing. Crucially it also supports our participation on the international stage, and it allows pump-priming novel and innovative research projects that are the hallmark of the consortium."
	},
	{
		"grant":72,
		"ID": "EP/G026254/1",
		"Title": "Efficient Verification of Software with Replicated Components",
		"PIID": "-202368",
		"Scheme": "First Grant Scheme",
		"StartDate": "02/02/2009",
		"EndDate": "30/10/2012",
		"Value": "425900",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-202368", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Concurrency is a model of computation that allows many units of executionto coexist. It is ubiquitous in computer science today: user processes in atime-sharing operating system execute concurrently, as do worker threads ina client-server environment. Parallel processing, once primarily ofinterest in high-performance computing, has emerged in recent years as anew way of increasing processing power, such as in multi-core concurrentsystems, even for the home personal computer.Concurrency poses new challenges for the quality assurance of software, fortwo reasons. First, concurrent programs have the potential for forms oferrors unknown in sequential computation, such as race conditions andmutual exclusion violations. Second, traditional reliability measures suchas simulation and testing fail in the presence of concurrency, due to thedifficulties of reproducing erroneous behavior. Model Checking is anautomated technique to reliably establish the correctness of software, orto reveal the existence of errors in a reproducible manner. A program isrepresented by a finite-state model, which is exhaustively searched forviolations of pre-specified properties.Exhaustive search, however, generally incurs a cost proportional to thenumber of model states that are reached during the search. This number isin turn worst-case exponential in the number of concurrent components. Thisstate space explosion problem has been a major obstacle to the widespreaduse of model checking.One avenue of our research is guided by the observation that concurrentsystems often consist of replicated components: instances of a singletemplate, generically describing the behavior of each component. Concurrentsystems of replicated components often exhibit a veryregular---symmetric---structure: their behavior is invariant underinterchanges of the components. This causes redundancy in the system modeland in the (naive) exploration of the model's state space.We propose to investigate the efficacy of symmetry reduction andparameterized verification to attack the state space explosion problem forsoftware with replicated components. Both techniques have shown to betremendously effective in principle, namely due to their potential ofreducing the size of a symmetric system by an exponential factor, or ofcollapsing the verification problem for an infinite family of systems toone for a single system or a small finite family, respectively.The applicability of these techniques to concurrent software was hampered,however, by the apparent incapability of model checking to deal withinteger variables over very large domains or even unbounded, dynamic datastructures. The situation changed dramatically with the advent of automatedabstraction-refinement techniques. Software is initially representedabstractly using coarse finite-state models, risking the possibility ofincorrect---spurious---verification results. The new paradigm came withways of detecting spuriousness, and of dealing with it by iterativelyrefining the abstract model until spurious behavior is removed.To sum up, concurrent software exhibits two sources of complexity: largevariable data domains and concurrency. Fortunately, these sources areorthogonal and can be attacked separately. This separation makes itpossible to apply symmetry reduction and parameterized techniques toconcurrent software, methods that target the concurrency aspect of statespace explosion. The ultimate goal of the proposed work is to combine thesemethods with iterative abstraction refinement to obtain verification toolsfor concurrent software that can seriously curb state space explosion atall levels."
	},
	{
		"grant":73,
		"ID": "EP/G026858/1",
		"Title": "Imbalanced Data Set Modelling and Classification for Life Threatening/ Safety Critical Applications",
		"PIID": "98457",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2012",
		"Value": "102020",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Systems Engineering",
		"OrgID": "113",
		"Investigators":[
		{"ID": "98457", "Role": "Principal Investigator"},
		{"ID": "6851", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Machine learning from imbalanced data sets is related to a broad range of very important problems in many engineering and scientific disciplines, e.g. medical diagnostics, signal detection and machine/material fault detection. Apart from the highly practical value, data learning from imbalanced data sets is also of high theoretical interest. Because the performance metrics used in conventional classifier construction may break down when applied to the imbalanced data sets, this has motivated considerable researches in machine learning communities aimed at a variety of learning methodologies for the imbalanced data setsDespite significant research  in machine learning for imbalanced data, there is still a need and/or a lack of general methodologies that are able to deliver the capability of knowledge discovery as demanded by many hugely important applications. For example, it is highly beneficial to discover new noninvasive biological markers from clinical data, which can improve early medical diagnostics results, in order to start early treatment of a cancer.  The motivation of the proposed research can be illustrated by another example. In material science, suppose that  new materials with exceptional properties, e.g. strength, are required for new mechanical structures, e. g. military vehicles. For this purpose,  a  sample of experimental trials  is performed to obtain a new material  together with the measurements of the  properties. It is highly desirable that the properties/behaviours could be discovered, by resort of data modelling using a small sample, rather than performing many more unnecessary and very expensive engineering experiments (large sample).This proposal is concerned with the development of a new modelling approach which builds upon the state-of-the-art nonlinear modelling methodologies and is specifically designed for pattern recognition using the imbalanced data sets. The objectives of the research include the modelling, classification, class probability (risk) prediction and knowledge discovery from the imbalanced data sets which are commonly found in many associated applications."
	},
	{
		"grant":74,
		"ID": "EP/G030227/1",
		"Title": "Advanced waveguide laser source development using ultrafast laser inscription",
		"PIID": "8620",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2009",
		"EndDate": "31/03/2013",
		"Value": "661205",
		"ResearchArea": "Graphene and Carbon Nanotechnology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "8620", "Role": "Principal Investigator"},
		{"ID": "46128", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Most laser systems emit a continuous wave of light, but if the laser is designed correctly it can be induced to emit very short pulses of light using a technique known as  mode-locking . There is currently a wide spread requirement for compact, mode-locked laser sources in areas ranging from bio-photonics to metrology. The aim of this proposed project is to develop such sources using two innovative technologies:  ultrafast laser inscription  and  carbon nanotubes  as described briefly below. The project will utilise the internationally leading expertise in each of these areas at Heriot Watt University and Cambridge University. If successful, the project will result in a paradigm shift in the current technology. Ultrafast lasers are lasers that emit extremely short pulses of light, routinely less than 100 fs. Due to the short durations of such pulses, the peak powers supplied by modern table top systems can often reach the GW regime. By focusing ultrafast laser pulses inside a transparent material, the structure of the material placed at the focus may be permanently altered. If the material is then translated through the focus, three-dimensional structural modifications can be inscribed in the material. The induced structural modification may manifest itself in a variety of ways, examples of which include an increased etch rate or refractive index change. Through careful control of the inscription parameters, the structural changes can be used to directly inscribe photonic components such as optical waveguides (that guide light in an analogous way to the guiding of electrical current by a metal wire) and micro-channels (that can be used to guide fluids or gases). During this project, we will utilise the unprecedented flexibility offered by ultrafast laser inscription to fabricate a number of previously impossible, or hard to fabricate elements for waveguide laser applications.Carbon nanotubes are cylindrical carbon molecules with diameters of typically only a few nanometers and lengths of up to a few cm, they are at the centre of nanotechnology research. In contrast to conventional bulk materials, the electronic and optical properties of carbon nanotubes can be controlled through their physical size and structure. If correctly fabricated carbon nanotubes are placed inside a laser, mode-locking can be induced without the need for complex electronics. A large part of the project will focus on developing carbon nanotubes with the correct properties for waveguide laser mode-locking applications, and on using ultrafast laser inscription to construct a waveguide laser element that will integrate these carbon nanotubes into the final device."
	},
	{
		"grant":75,
		"ID": "EP/G030634/1",
		"Title": "Feasibility study of plasma-assisted electroepitaxy for the growth of GaN layers and bulk crystals",
		"PIID": "-16288",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2013",
		"Value": "343689",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104",
		"Investigators":[
		{"ID": "-16288", "Role": "Principal Investigator"},
		{"ID": "8761", "Role": "Co Investigator"},
		{"ID": "5494", "Role": "Co Investigator"},
		{"ID": "107247", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "There is a high level of commercial and scientific interest in nitride semiconductors. The group III-nitrides (AlN, GaN and InN and their solid solutions) are being used for amber, green, blue and white light emitting diodes, for blue/UV laser diodes and for high-power, high-frequency and high temperature electronic devices. Group III-nitride layers for device fabrication are grown by metal-organic vapour phase epitaxy (MOVPE), hydride vapour phase epitaxy (HVPE) and molecular beam epitaxy (MBE).  One of the most severe problems hindering progress in the field of nitride technology is the rarity of suitable lattice-matched substrates onto which group III-nitride films can be grown. Group III-nitride layers are commonly grown on non-lattice matched sapphire, GaAs or SiC substrates. However, bulk GaN substrates, which are matched in lattice constant and thermal expansion properties to epitaxial nitride layers are needed for fabrication of the highest-quality GaN-based devices. Solution growth methods are normally used in the commercial production of standard III-V crystals with low dislocation density. It is well established that by using solution growth techniques, the crystallisation takes place very close to equilibrium. At present, solution growth methods are being actively explored to grow bulk GaN crystals. In order to achieve an efficient solution growth technique for GaN we need to fulfil 3 main requirements: 1) a high solubility of N in the Ga-based solution, 2) efficient transport of N and Ga to the growth interface and 3) prevent local supersaturation in the solution, which will otherwise result in spontaneous crystallisation.  High quality low dislocation density bulk wurtzite GaN substrates can be grown from liquid Ga solutions. However, the solubility of N2 in liquid Ga is very low and it is difficult to obtain reasonable growth rates and therefore large area bulk GaN crystals are still not commercially available.  Wurtzite GaN crystals have been synthesized by reacting gallium metal with atomic nitrogen produced by a microwave plasma source, which avoids the high equilibrium pressure needed for N2. Atomic nitrogen from an RF plasma source can be used to produce high concentrations of N in a Ga-based melt. Unfortunately, this high concentration of N only exists close to the surface of the metallic Ga and normally results in spontaneous crystallization of polycrystalline GaN on the surface of metallic gallium. In order to achieve an efficient epitaxial growth process one needs to develop a technique to transport the N species through the gallium melt to the growth surface and at the same time to minimize spontaneous nucleation. Liquid phase electroepitaxy (LPEE) is a crystal growth method, in which the layer growth is initiated and sustained by passing a direct electric current through the solution-substrate interface while the temperature of the overall system is maintained constant. An electric current passing through the LPEE growth cell causes four main effects: 1) electromigration of the solvents in the liquid metal solutions; 2) Peltier effect at hetero-interfaces: 3) Joule heating of the growth cell and 4) increased convection in the solution. Electromigration and Peltier cooling of the growth interface together produce the required concentration gradient to the growth interface. In this current application we are proposing to develop an entirely novel, inexpensive technique for the growth of high quality bulk GaN crystals with a low dislocation density - plasma assisted electroepitaxy (PAEE). We will combine advantages of the plasma process for producing high concentrations of N species in the Ga melt with the advantages of electroepitaxy in transferring these species from the Ga surface to the growth interface without spontaneous crystallisation on the surface or within the solution."
	},
	{
		"grant":76,
		"ID": "EP/G031576/1",
		"Title": "Real-time Numerical Optimization in Reconfigurable Hardware with Application to Model-Predictive Control",
		"PIID": "100961",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2009",
		"EndDate": "30/11/2012",
		"Value": "527277",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "100961", "Role": "Principal Investigator"},
		{"ID": "-16768", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Agility Design Solutions"},
		{"Number": "1", "Name": "European Space Agency"},
		{"Number": "2", "Name": "The Mathworks Ltd"},
		{"Number": "3", "Name": "Xilinx Corp"}
		],
		"Summary": "This proposal is concerned with the hardware acceleration of iterative numerical algorithms, with a focus on model predictive control implementations. Such model predictive controllers typically require the solution of a quadratic progamming problem every sample period. The solution of the quadratic programming problem typically requires several multidimensional Newton optimizations, each of which requires the solution of many systems of linear equations. Thus the lessons learned will be applicable to a wide class of numerical algorithms arising in practical problems within and beyond Control.The main adventurous feature of the approach from the digital electronics perspective is the potential to use Control and Systems theory to inform one of the central design problems in custom reconfigurable computing: efficient silicon utilization through appropriate finite precision number representation. In sequential (single core) computer architecture, questions of numerical precision have, by and large, been answered through the introduction of area costly high-precision IEEE compliant arithmetic units. In modern computing systems, whether FPGA-based or manycore, attention is now turning to how to make the most effective use of the silicon available for computation and, in this context, questions of numerical accuracy requirements are arising once more.The proposed approach forms a radical departure from standard industrial and academic practice in both model predictive control (MPC) and digital electronics. The main adventurous feature of the approach from the end-user perspective is the utilization of reconfigurable hardware devices, namely Field-Programmable Gate Arrays (FPGAs), to implement model predictive controllers operating at high sample rates, allowing MPC to be utilized in application areas where the computational load has been considered too great until now, such as spacecraft, aeroplanes, uninhabited autonomous vehicles, automobile control systems and gas turbines. From the theoretical perspective, the main adventure in Control is in the development of novel formulations that explcitly take advantage of parallel computational architectures.The development of a methodology to tackle this problem will involve highly novel research areas resulting from the application of control theoretic ideas to hardware development, as well as the application of hardware implementation methodologies to control system design. In particular, this proposal is the first to investigate massively parallel real-time numerical optimization on FPGAs, the first to apply control-theoretic techniques to determine appropriate number systems in custom hardware designs, and the first to study the tradeoff between circuit parallelism and numerical accuracy within a closed-loop behavioural context.As a result, this proposal directly falls within the scope of EPSRC's recently signposted  Microelectronics Grand Challenge 3 - Moore for Less."
	},
	{
		"grant":77,
		"ID": "EP/G031975/1",
		"Title": "DEVELOPING EDUCATIONAL SOFTWARE TO ASSESS IF AUTISTIC CHILDREN CAN BENEFIT FROM ACCESS TO OPEN LEARNER MODELS AND EMOTIONAL FEEDBACK ON LEARNING.",
		"PIID": "121458",
		"Scheme": "Standard Research",
		"StartDate": "31/08/2009",
		"EndDate": "31/05/2013",
		"Value": "478813",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Psychology",
		"OrgID": "7",
		"Investigators":[
		{"ID": "121458", "Role": "Principal Investigator"},
		{"ID": "29444", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal brings together research in user interface (UI) design and human computer interaction (HCI), with research in psychology, autism and learning.  It is a new collaboration specifically formed to address two novel research questions related to the development and evaluation of technologically mediated learning by children with autistic spectrum disorder (ASD).  The first question concerns whether allowing ASD children access to models of their learning held by the system (i.e. open learner models), improves their learning experience and achievements. The second question relates to whether learning in ASD children is facilitated by the presence of an interface persona displaying emotions whilst providing feedback and help on the children's learning achievements. Mathematics is the discipline chosen for investigating the learning achievements, as ASD children have strengths, and therefore confidence, in this subject. Groups of non-ASD children matched on age and ability will act as a baseline or control group. The proposed research is multidisciplinary.  From a user interface and HCI perspective there will be an emphasis on user-centred design and usability in designing the functionality, representation of, and interaction with, the open learner models (OLM) and interface personas.  From both HCI and psychology perspectives the impacts on learning for both ASD and non-ASD children will be of interest. A further novelty of the research is the intention to investigate the challenges of involving ASD children as partners in designing the learning technology they will use."
	},
	{
		"grant":78,
		"ID": "EP/G032572/1",
		"Title": "Testing Probabilistic and Stochastic Systems (ProbTest)",
		"PIID": "65573",
		"Scheme": "Standard Research",
		"StartDate": "06/03/2009",
		"EndDate": "05/03/2013",
		"Value": "72760",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Information Systems Computing and Maths",
		"OrgID": "129",
		"Investigators":[
		{"ID": "65573", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Traditionally, methods for formally specifying or modelling systems have concentrated on the representation of the functional behaviour of the systems. Such models can be used as the basis of testing that the system under test (SUT) has the required set of behaviours: its functional behaviour is correct. They are an effective way of representing the required functional properties of the system under test but do not allow us to express desired non-functional properties such as time, probabilities or resources. Several formalisms were extended in order to deal with these kind of properties. The new languages allow the explicit representation of the probability of performing a certain task, for example that if a function of the system is used it should provide the expected functionality at least 99.9% of the time. They also allow us to express the time that should be consumed by the system while performing tasks, either given by fixed amounts of time or defined in probabilistic/stochastic terms. For example, we may require that on average a result is produced within 1 second.ProbTest will consider models that include probabilities and stochastic time. Many systems have real-time constraints and thus the inclusion of time is important. Probabilities are highly relevant where resources are shared and so Quality of Service requirements can be probabilistic. In addition, many systems are probabilistic in nature due to either the use of subsystems communications over a (possibly unreliable) medium or through the system consisting of several threads or parallel components and there being different possible synchronization sequences. There are also a number of communications protocols, such as Bluetooth and Ethernet, that have probabilistic requirements and in order to reason about embedded systems, which are state-based, it is often necessary to use probabilities. Thus, there are systems with probabilistic and/or stochastic requirements and it is important to have efficient, systematic methods for testing whether they actually satisfy these requirements. ProbTest is a four year project that will support collaboration in this area between Hierons, of Brunel University, and Nunez, of Universidad Complutense de Madrid."
	},
	{
		"grant":79,
		"ID": "EP/G033056/1",
		"Title": "Theory And Applications of Induction Recursion",
		"PIID": "69411",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2009",
		"EndDate": "31/08/2012",
		"Value": "310904",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer and Information Sciences",
		"OrgID": "48",
		"Investigators":[
		{"ID": "69411", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computers are good at adding billions of numbers in microseconds. Humans, on the other hand, are good at abstract thinking. This is exemplified by the development of philosophy, literature, science and mathematics. These observations have deep consequences for programming and the design of programming languages. The overarching concern of much research in computer science is to minimize the difference between how humans conceptualise programs and how those programs are implemented in a programming language. To achieve this, we do the same thing humans have been doing for 5000 years as we try to understand the world around us. That is, we construct mathematical models --- in this case, mathematical models of computation - and then reflect that understanding of computation within the design of programming languages. Thus, there is a symbiotic relationship between mathematics, programming, and the design of programming languages, and any attempt to sever this connection will diminish each component. Recursion is one of the most fundamental mathematical concepts in computation. Its importance lies in the ability it gives us to define computational agents in terms of themselves - these could be recursive programs, recursive  data types, recursive algorithms or any of a myriad of otherstructures. The original treatments of recursion go back to the 1930s where the concept of computability was formalised via the theory ofgeneral recursive functions. It is virtually impossible to overestimate how recursion has contributed to our ability to computeand to understand the process of computation. Is it possible that there is anything fundamental left to say about recursion? We believe there is. Our central insight is this: when defining a function recursively, the inputs of the function are  usually fixed in advance. But what if they are not? What if, as we build up the function recursively, we also build up its inputs inductively? The study of functions defined in this way is called induction recursion and this proposal aims to develop the theory and applications of induction recursion.Our central ambition is to turn induction recursion, which is currently known only to a relatively small number of researchers within type theory, into a mainstream technique within the programming language community. This will require both the theoretical development of induction recursion so as to give us more ways to understand it, but also case studies and examples to make it more accessible to programmers. Fortunately this is an excellent time to do this research!  The categorical study of data types has advanced to the stage where the theoretical tools are now in place to tackle inductionrecursion. Perhaps even more fundamentally, dependently typed programming languages in the shape of Epigram and Agda have advancedto the stage where our ideas can be implemented in code and hence the benefits of induction recursion can be made directly available toprogrammers in a form they understand. We can supply them with code to play with! Indeed, we hope to go even further an explore the extent to which induction recursion can form the basis of a programming language. In summary, this proposal takes state of  the art ideas in theoretical computer science and will aim to turn them directly into state of the art techniques within programming languages. Such combinations of theory and applications going hand in hand together is often the hall mark of good science!"
	},
	{
		"grant":80,
		"ID": "EP/G033110/1",
		"Title": "Programmable Fabrics and Spatial Compilers",
		"PIID": "-44623",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2013",
		"Value": "177259",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-44623", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Advances in fabrication technology will soon provide computer architects with an almost limitless supply of transistors.  This technology scaling could permit thousands of individual processor cores to be integrated onto a single chip.  In fact, many assume that these ``manycore  architectures offer a panacea in terms of performance and scalability.  Unfortunately, roadmaps built upon such an assumption often fail to consider the fundamental shift in design trade-offs that will take place in the longer term.  It is the goal of this project to use a far more open-ended strategy to explore viable processor designs. We aim to investigate how key features of ASIC, FPGA and multicore approaches can be combined to produce the most apposite architectures. We call solutions from this region of the architectural design space  programmable fabrics . These will be targeted by novel compilers with the ability to optimise program execution under a range of physical constraints. Unlike compilers for centralised uniprocessors, we expect to incorporate features from logic synthesis and place-and-route tools. The aim of this proposal is to highlight the need to explore longer term limits to performance and to evaluate a family of potential architectural solutions."
	},
	{
		"grant":81,
		"ID": "EP/G033579/1",
		"Title": "SWAT (Semantic Web Authoring Tool)",
		"PIID": "78847",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2009",
		"EndDate": "30/11/2012",
		"Value": "640759",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing",
		"OrgID": "95",
		"Investigators":[
		{"ID": "78847", "Role": "Principal Investigator"},
		{"ID": "22733", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During the last decade the Semantic Web community has established basic standards for representing data and the conceptual systems (ontologies) through which they are defined. However, encoding information in these formalisms (OWL, RDF) remains a technically difficult task. Widespread adoption of these technologies (with their important potential benefits) would be facilitated if transparent interfaces to the technical formalisms were available.The project aims to show that metadata in OWL and RDF can be viewed and authored through computer-generated presentations in natural languages (e.g., English). The crucial step theoretically will be to develop a model for systemmatically mapping logical concepts and relations to phrase patterns in natural language. The practical challenge will be to develop a tool through which ontology developers can specify this mapping, without deploying deep knowledge of ontologies or grammars. This tool will draw on existing wide-coverage linguistic resources, so that developers can select from a range of pre-coded patterns rather than having to define new ones. If successful, the project would provide an innovative solution to an urgent and commercially relevant problem (as shown by the letters from our collaborators). The main partners are leading UK experts in the theory and practical application of ontologies (Manchester University), and the design of easily-used tools for knowledge-editing based on generated text (Open University)."
	},
	{
		"grant":82,
		"ID": "EP/G033870/1",
		"Title": "Micro-resonator Probe for THz Near-field Imaging Beyond the Diffraction Limit",
		"PIID": "-201108",
		"Scheme": "First Grant Scheme",
		"StartDate": "28/08/2009",
		"EndDate": "27/02/2013",
		"Value": "305141",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-201108", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Rutgers State University of New Jersey"}
		],
		"Summary": "Terahertz (THz) device research and studies of THz phenomena in solid state systems require detection of THz waves and signals on the scale of few microns. These measurements present a major technological problem caused by diffraction of THz waves. The diffraction limit prevents the use of the recently developed THz spectroscopic instrumentation for studies of objects smaller than approximately a wavelength. Near-field surface probing methods have shown potential solutions in overcoming the diffraction limit. However all the existing THz near-field techniques exhibit another fundamental limitation due to significant perturbations in the electric field caused by the near-field probe. The probe invasiveness and a non-uniform frequency response across the THz spectrum prevent the use of the existing near-field probes for mapping of electric field distribution in THz devices. In addition, THz near-field imaging systems with spatial resolution better than ~1/20 of a wavelength suffer from a severe reduction in sensitivity.To mitigate these problems and to allow high spatial resolution studies with THz waves we propose to develop a THz imaging and spectroscopy system with a novel near-field probe. The probe concept exploits the non-invasive nature of the electro-optic detection method and utilizes an optical micro-resonator to enhance the detection sensitivity. The proposed electro-optic micro-resonator will be integrated into a fibre-coupled near-field probe. It will allow THz wave and signal probing with a spatial resolution of ~5 microns (~1/100 of the wavelength) and it will offer full spectroscopic capabilities in the THz range (0.1-2.0  THz). The novelty of this approach is in exploiting the optical cavity resonance for electro-optic detection of THz waves by an extremely small near-field probe. The goal of this research programme is to develop and build the THz near-field probing system and apply it in device research on the sub-wavelength scale. The proposed technology will expand the spectrum of THz studies to micrometre-scale objects. It will aid in the progress of THz device research and will facilitate studies of THz phenomena in physics, materials science and other disciplines."
	},
	{
		"grant":83,
		"ID": "EP/G033935/1",
		"Title": "Recognition and Localisation of Human Actions in Image Sequences",
		"PIID": "-115131",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2009",
		"EndDate": "30/11/2012",
		"Value": "340932",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "-115131", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The explosion in the amount of generated and distributed digital visual data that we nowadays witness can only be paralleled to the similar explosion in the amount of textual data that has been witnessed the decade before. However, while retrieval based on textual information made great progress and resulted in commercially usable search engines (e.g. Google, Yahoo), vision-based retrieval of multimedia material remains an open research question. As the amount of produced and distributed videos increases at an unprecedented pace, the significance of having efficient methods for content-based indexing in terms of the depicted actions can hardly be overestimated. In particular in the domain of analysis of human motion progress is expected to boost applications in human computer interaction, health care, surveillance, computer animation and games, and multimedia retrieval. However, mapping low level visual descriptors to high level action/object models  is open problem and the analysis faces major challenges to the degree that the analysed image sequence exhibits large variability in appearance and the spatiotemporal structure of the actions, occlusions, cluttered backgrounds and large motions. In addition learning structure and appearance models is hindered by the fact that segmentation and annotation for the creation of training datasets are onerous tasks. For these reasons, there is a great incentive for the development of recognition and localisation methods that can either learn from few annotated examples or in a way that minimizes the amount of required manual segmentation and annotation.This project will build on recent development in Computer Vision and Pattern Recognition in order to develop methods for recognition and localisation of human and animal action categories in image sequences. Once trained, the methods should be able to detect and localise in a previously unknown image sequence, all the actions that belong to one of the known categories. The methods will allow learning the models in an incremental way starting from few examples and will allow computer assisted manual interaction using appropriate interfaces in order to facilitate model refinement. The methodologies will allow training the models in image sequences in which there is significant background clutter, that is in the presence of multiple objects/actions in the scene and moving cameras. No prior knowledge of the anatomy of the human body is a-priori considered, and therefore the models will be able to identify a large class of action categories, including facial/hand/body actions, animal motion, as well as interaction between humans and objects in their environment (such as drinking a glass of water)."
	},
	{
		"grant":84,
		"ID": "EP/G034109/1",
		"Title": "Reusability and Dependent Types",
		"PIID": "98932",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2009",
		"EndDate": "28/02/2013",
		"Value": "244671",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "98932", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Robin Milner coined the slogan  well typed programs cannot gowrong , advertising the strength of typed functional languages like MLand Haskell in using types to catch runtime errors. Nowadays, we canand want to go further: dependently typed programming exploits thepower of very expressive type systems to deliver stronger guaranteesbut also additional support for software development, using types toguide the development process. This is witnessed by a recent surge oflanguage proposals with the goal to harness the power of dependenttypes, e.g.  Haskell with GADTs, Agda, Coq, Omega, Concoqtion, Guru,Ynot, Epigram and so on.However, expressive type systems have their price: more specific typesfrequently reduce the reusability of code, whose too-specificimplementation type may not fit its current application.  Thisphenomenon already shows up in the traditional Hindley-Milner styletype system of ML and Haskell; it becomes even more prevalent in adependently typed setting. Luckily, all is not lost: dependent typesare expressive enough that they can talk about themselvesreflectively, making meta-programming one of its potential killerapplications with the potential of combining expressive types andreusable software components.Based on and inspired by recent research at Nottingham on dependentlytyped programming (EPSRC EP/C512022/1) and container types (EPSRCEP/C511964/2) and at Oxford on datatype-generic programming (EPSRCGR/S27078/01, EP/E02128X/1) we plan to explore the potential ofdependent types to deliver reusable and reliable softwarecomponents. To achieve this, we intend to explore two alternativeroads - reusability by structure and reusability by design - andexpress both within a dependently typed framework. Our programme is tobuild new tools extending the Epigram 2 framework, investigate theunderlying theory using container types, and most importantlyestablish novel programming patterns and libraries.  We seek fundingfor an RA at Nottingham (Peter Morris, whose PhD laid much of thegroundwork for this proposal), and two doctoral students (one each atOxford and Strathclyde), together with appropriate support forequipment, coordination, travel, and dissimination (i.e. a workshopand a summer school)"
	},
	{
		"grant":85,
		"ID": "EP/G035202/1",
		"Title": "Terahertz acoustic laser (saser) devices: fabrication and characterisation",
		"PIID": "8761",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2014",
		"Value": "621760",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104",
		"Investigators":[
		{"ID": "8761", "Role": "Principal Investigator"},
		{"ID": "5494", "Role": "Co Investigator"},
		{"ID": "-16288", "Role": "Co Investigator"},
		{"ID": "107247", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "V E Lashkar'ov Instititute of"}
		],
		"Summary": "SASER is the acronym for Sound Amplification by Stimulated Emission of Radiation and is the acoustic analogue of the optical laser [http://en.wikipedia.org/wiki/Sound_Amplification_by_Stimulated_Emission_of_Radiation]. A terahertz (THz) saser device would produce an intense beam of coherent acoustic waves with nanometre wavelengths. As well as being a subject of pure scientific curiosity, the acoustic beams produced by saser could have a number of scientific and technological applications: e.g. probing and imaging of nanometre scale objects, and conversion to THz electromagnetic waves, which may be used for medical imaging and security screening.Recently, we have demonstrated a device which displays some of the key characteristics expected of a saser. The work was published in a scientific journal and subsequently reported in a number of academic and popular science magazines, including the science and technology pages of The Economist [10-16 June 2006, p96]. The device was based on a semiconductor superlattice: a man-made nanostructure consisting of many (typically 40 or 50) alternating layers of two different semiconductor materials, in this particular case Gallium Arsenide and Aluminium Arsenide, each a few nanometres thick. In the superlattice, the conditions for phonon amplification can be achieved when electrons are made to travel vertically through the stack of layers. The electrons  hop  between neighbouring Gallium Arsenide layers and, to conserve energy, emit a phonon (quantum of sound) as they go. These phonons can stimulate further electron hops and emission of phonons giving rise to phonon amplification. In addition to acoustic gain, a saser requires an acoustic cavity to confine the phonons so that they are available to take part in further stimulated emission processes. This is analogous to the optical cavity formed between the two mirrors of a laser. Superlattices can be used as phonon mirrors: owing to the differences of the speed of sound and density between the two materials making up the superlattice, phonons are partly reflected and partly transmitted at each interface. Constructive interference of all the reflections, which occurs when the sound wavelength matches the thickness of a single pair of layers, leads to a strong reflection and confinement of the phonons.In this project, we plan to carry out a detailed investigation of the physics of the separate elements of a THz saser device based on semiconductor superlattices. These include: the gain medium and the process of phonon amplification within it; the pumping schemes, both electrical and optical, for achieving the necessary population inversion; and the acoustic mirrors and cavities for confining phonons. The main goal of the work is to develop a milliwatt per square centimetre class saser device emitting coherent phonons in the range 0.5 - 1 THz and to characterise the saser sound emitted."
	},
	{
		"grant":86,
		"ID": "EP/G036136/1",
		"Title": "Numerical Algorithms and Intelligent Software for the Evolving HPC Platform",
		"PIID": "117266",
		"Scheme": "Science and Innovation Awards",
		"StartDate": "01/08/2009",
		"EndDate": "31/07/2014",
		"Value": "4550814",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Mathematics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "117266", "Role": "Principal Investigator"},
		{"ID": "-216948", "Role": "Principal Investigator"},
		{"ID": "34298", "Role": "Co Investigator"},
		{"ID": "79744", "Role": "Co Investigator"},
		{"ID": "21508", "Role": "Co Investigator"},
		{"ID": "34378", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Orange France Telecom"},
		{"Number": "1", "Name": "SAS Institute Inc"}
		],
		"Summary": "Advances in computing power have broadened the spectrum of applications amenable to computational treatment, but software improvements must keep pace with advances in computing technology if new hardware investment is to be fully exploited for the benefit of society. Numerical analysis is a traditional strength of UK mathematics, but it must establish new means of collaboration with computer scientists to be relevant for the fast changing platforms of high performance computing. In this well-supported and timely initiative, numerical analysts at Edinburgh, Heriot-Watt and Strathclyde Universities will work together with compiler experts from Edinburgh Informatics and specialists in parallel computing from the Edinburgh Parallel Computing Centre (EPCC) to improve the software development paradigm for implementation of numerical algorithms on diverse and evolving multiprocessor systems. By bringing mathematicians and computer scientists into close collaboration with HPC specialists, this initiative will address key issues raised in the international reviews of UK mathematics and high performance computing. Additional strategic appointments will be made by the universities, providing a sustainable, long-term commitment. Advanced numerical algorithms will be developed for state-of-the-art applications, such as high order adaptive finite elements for solid and fluid mechanics, numerical optimization, multi-scale methods, and new parallel methods for molecular simulation and data analysis. Algorithms will be coded using better systems of markup and annotation, and new compilation techniques will be introduced by the computer scientists and implemented in collaboration with researchers at EPCC. This paradigm shifts the details of implementation to compilers, but compilers informed by algorithm developers via annotation. The methods developed will have clear potential to impact the key themes of the EPSRC delivery plan, including energy, health sciences, nanoscience, and the digital economy. To strengthen the uptake of new methodology among the research base, algorithms will be tested and their performance evaluated in collaboration with applications scientists and engineers. This proposal includes knowledge exchange partnerships with major computing companies (HP, IBM, SGI) as well as industrial users of HPC algorithms (Schlumberger, Orange/France Telecom, SAS), opening new pathways for effective utilisation of new software techniques. Connections to national laboratories such as Daresbury and Rutherford Appleton are also planned. The project is further enhanced through funded connections with Cambridge University, the University of Warwick, and the Wales Institute for Mathematical and Computational Science."
	},
	{
		"grant":87,
		"ID": "EP/G036454/1",
		"Title": "Integrated Scheduling for Wireless Mesh Networks",
		"PIID": "74374",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2009",
		"EndDate": "31/10/2012",
		"Value": "244232",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Actuarial Science and Insurance",
		"OrgID": "83",
		"Investigators":[
		{"ID": "74374", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "British Telecommunications Plc"}
		],
		"Summary": "Wireless mesh networks (WMN) represent a new networking technology involving wireless devices that are typically fixed at buildings and other infrastructure. These devices act as access points for wireless services such as the Internet. Importantly, the access points may directly connect to each other and forward data to a destination. This is typically an Internet gateway where communication is transferred from wireless to wires/cables. The relaying of data in a WMN causes problems for maintaining quality of service. It is important that data is scheduled sequentially for transfer between pairs of sending and receiving devices because processing within a device cannot occur in parallel. Consequently scheduling ensures that collisions between transmissions do not occur. It also allows data to be routed along paths so that objectives such as latency and delay of data are minimized while fairness between users is maximized. Our contribution will include eliciting the complexities of the underlying communication dynamics in mathematical terms. Collaboration with our project partner (BT plc) will ensure that all relevant engineering issues are incorporated. The research project specifically looks at the problem of creating schedules so that objectives are resolved. Two types of schedules are addressed: those for the user who wishes to transfer data to-and-from a particular access point and those needed to relay data to other access points in the WMN. These scheduling problems are computationally complex and require research based on mathematics and computer science. This will determine the existence of such schedules and their creation using advanced computational methods. The outcome of this research is of particular interest to our project partner who will examine the engineering implications of using the techniques developed in this project for future WMN deployments such as  Wireless Cities  initiatives."
	},
	{
		"grant":88,
		"ID": "EP/G036659/1",
		"Title": "High power mm and sub-mm wave amplifiers for high frequency ESR/DNP, high resolution radar and remote sensing",
		"PIID": "12400",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2009",
		"EndDate": "31/03/2013",
		"Value": "771109",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "12400", "Role": "Principal Investigator"},
		{"ID": "68356", "Role": "Co Investigator"},
		{"ID": "58233", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Microwave International Ltd"},
		{"Number": "1", "Name": "University of St Andrews"}
		],
		"Summary": "In a gyrotron, electrons gyrating in a magnetic field are coupled to electromagnetic radiation in such a manner that the radiation is amplified by extracting the electron kinetic energy. We will investigate a novel concept which uses a helical corrugation on the inside surface of a 'cylindrical' waveguide to radically modify the wave dispersion giving eigenmodes with finite, constant group velocity in the region of near infinite phase velocity. This novel dispersion opens up for the first time the potential for a high power (5kW), broadband (10%), high gain >40dB, efficient (30%) gyrotron amplifier in the 90GHz to 100GHz frequency range and above. We have performed a preliminary experiment at X-band (8GHz to 10GHz) frequencies and will build on our lead to create an amplifier in the W-band (90GHz to 100GHz) frequency band based on the best understanding of this new concept and perform precision measurements of its gain, bandwidth, efficiency and stability against oscillations. New theory and computational models benchmarked against W-band experimental data will be used to demonstrate the potential for this novel amplifier to generate high frequency (360GHz to 400GHz and 460GHz to 500GHz), high power (~0.5kW), broadband (10%), pulsed and continuous coherent radiation crucially needed by the many known applications."
	},
	{
		"grant":89,
		"ID": "EP/G037590/1",
		"Title": "Bayesian Inference for Multi-object Tracking with Application to Single Molecule Fluorescence Microscopy",
		"PIID": "-170360",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2009",
		"EndDate": "28/02/2013",
		"Value": "290844",
		"ResearchArea": "Analytical Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-170360", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal concerns the Bayesian analysis of data arising from multi-object tracking problems.As a specific example of object tracking, consider a surveillance system comprised of a network of cameras. The aim is to detect new objects that enter their field of view, classify them (pedestrians or vehicles, in the latter case, the type of vehicle), follow their trajectories and ultimately infer their intent.With advances in computing, networking and sensor technologies, surveillance systems are becoming more pervasive and sophisticated.A recent example is the Cambridge Transport Information Monitoring Environment which aims to monitor and improve the efficiency of the public transport infrastructure in Cambridge, e.g. by predicting the onset of congestion on roads and reschedule services as necessary.If current trends persist, it is likely that the amount of such data that is collected will only increase in years to come and developing efficient statistical methods for analysing it will continue to be an important challenge.This project aims to develop a flexible and robust methodology for Bayesian inferencefor multi-object tracking to meet the demands of existing and theever-increasing complexity of new applications.It will consider the computational and statisticalaspects of the problem and the developed methodology will be applied primarily to a new application area, namely Single Molecule Fluo-rescence Microscopy (SMFM).SMFM is an emerging area of study in Biology which is contributing tothe quantitative understanding of the fundamental processes in living cells.Experimentation techniques are advanced but the statisticalanalysis of the data, which is needed to support the scientific investigations, has not been addressed. The multi-object tracking framework is ideally suited to this application and our developed computational methodology will be specialized for it.In the methodological work, we will consider issues of generic and practical importance. Currently,there are three main challenges which will be addressed in this project. The first is computing the complex posterior probability distribution from which all inference is to be based.Getting the  right answer  to the practitioner's problem rests on a comprehensive solution for this problemwhich is presently unavailable. This proposal aims systematically address this using Monte Carlo as a primarymeans for computation.The second is calibrating the statistical models in multi-object tracking which are complex and have many tunable parameters.This problem is important as analysis based on models that are not properly calibrated will yieldincorrect results but unfortunately has received very little attention.The third is model assessment, which involves determining whether or not a proposed model adequately fits the data, and determining whether or not different models would be more suitable.In multi-object tracking, the data structures and the models are complex and routine methods for Bayesian model assessment cannot be immediately applied. A dedicated set of diagnostic tools need to be developed."
	},
	{
		"grant":90,
		"ID": "EP/G039070/2",
		"Title": "Random structures, spin glasses and efficient algorithms",
		"PIID": "-209266",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/01/2010",
		"EndDate": "31/10/2012",
		"Value": "295184",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Mathematics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-209266", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "An Algorithm is a systematic procedure for solving a computational problem that can be implemented on a computer. An example is the Gaussian elimination method for solving a system of linear equations. The running time of an algorithm is the number of elementary steps (e.g., addition, modification of a symbol in a string, etc.) that the algorithm performs. Of course, the running time depends on the size of the input. For example, in the case of Gaussian elimination the size of the input is the number of symbols needed to write down (or enter) the linear system of equations. Denote this quantity by m. Then the running time of Gaussian elimination is bounded by m^3. Generally  an algorithm is considered Efficient if for every possible input its running time is bounded by a polynomial in the size of that input. (Hence, Gaussian elimination is efficient.)In spite of intensive research since the early days of computing, there is a broad class of computational problems for which no efficient algorithms are known. In terms of complexity theory, most of these problems can be classified as  NP-hard . One example is the Boolean Satisfiability problem (SAT). In this problem the input is a Boolean formula, and the objective is to find an assignment to the Boolean variables that satisfies the entire formula (if such a satisfying assignment exists).Although the SAT problem is NP-hard, it occurs as a sub-problem in numberless real-world applications. In fact, SAT is of similarly eminent importance in Computer Science as solving polynomial equations is in Algebra. Therefore, an immense amount of research deals with heuristic algorithms for SAT. The goal of this line of research is to devise algorithms that can efficiently solve as general types of SAT inputs as possible (although none of these methods solves all possible inputs efficiently).Despite this bulk of work, it remains extremely simple to generate empirically  hard  problem instances that elude all of the known heuristic algorithms. The easiest way to do so is by drawing a SAT formula at random (from a suitable but very simple probability distribution). Indeed, random input instances were considered prime examples of hard inputs to such an extent that it was proposed to exploit their hardness in cryptographic applications. Random SAT formulas also occur prominently in the seminal work on Algorithms and Complexity from the 1970s, where their empirical hardness was reckoned  most vexing . However, it remained unknown why these types of instances eluded all known algorithms (let alone how else to cope with these inputs).Therefore, it came as a surprise when statistical physicists reported that a new algorithm called Survey Propagation ( SP ) experimentally solves these  hard  SAT inputs efficiently. Indeed, a naive implementation of SP solves within seconds sample instances with a million of variables, while even the most advanced previous SAT solvers struggle to solve inputs with a few hundred variables. SP comes with a sophisticated but mathematically non-rigorous analysis based on ideas from spin glass theory. This analysis suggests why all prior algorithms perform so badly. Its key feature is that it links the difficulty of solving a SAT input to  geometric  properties of the set of solutions.Though the physics methods have inspired the SP algorithm, they do not provide a satisfactory explanation for the success (or the limitations) of SP. Therefore, the goal of this project is to study these new ideas from spin glass theory from a Computer Science perspective via mathematically rigorous methods.  On the one hand, we are going to provide a rigorous analysis of SP to classify what types of inputs it can solve. On the other hand, we intend to study the behaviour of algorithms from the point of view of the  solution space geometry ; this perspective has not been studied systematically in Algorithms and Complexity before."
	},
	{
		"grant":91,
		"ID": "EP/G041296/1",
		"Title": "Coalgebraic Logic: Expanding the Scope",
		"PIID": "-25558",
		"Scheme": "Standard Research",
		"StartDate": "06/09/2009",
		"EndDate": "05/01/2013",
		"Value": "361165",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "66",
		"Investigators":[
		{"ID": "-25558", "Role": "Principal Investigator"},
		{"ID": "48213", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "COALGEBRAIC LOGICLogic plays a fundamental role in Computer Science. At the most basiclevel, Boolean logic is used to design the circuits we use every day inour computers. At the higher end, the tasks that computers perform need toconform to specifications expressed in logics suitable for programmers,analysts or even other computational devices.Such specification logics have to be able to express many differentconcepts such as time, knowledge, space, mobility, communication,probability, conditionals etc. Bespoke logics for each of these conceptsexist and are studied under the umbrella of Modal Logic.In any substantial application of Modal Logic to the specification ofa system, the need to combine different logics will arise, each logicaccounting for, eg, one of the aspects mentioned above. The need thenarises to deal with these logics in a uniform and modular way.Not all of these logics have a standard Kripke semantics, but in allcases, the semantics can be considered to be coalgebraic. Coalgebrasgeneralise the standard Kripke semantics of modal logic to encompassnotions such as neighbourhood frames, Markov chains, topologicalspaces, etc.Moreover, Coalgebra is a concept from Category Theory. Category Theoryis an area of mathematics which describes mathematical constructionsin abstract terms that make these constructions available to manydifferent areas of mathematics, logic, and computer science. Inparticular, the category theoretic nature of Coalgebras allows us totackle the modularity problem using category theoreticconstructions. One of the benefits of category theory is that theseconstructions, because of their generality, apply to specificationlanguages and to their semantic models.To summarise, Coalgebraic Logic combines Modal Logic withCoalgebra. This generalises modal logics from Kripke frames tocoalgebras and makes category theoretic methods and constructionsavailable in Modal Logic.EXPANDING THE SCOPECoalgebraic Logic can be traced back to 1997 when the first draft ofMoss's paper with the same title was circulated. Since then, it hasbeen developed by a number of researchers. Just now, Coalgebraic Logicis about to establish itself as an own area. Whereas much of thecurrent work in Coalgebraic Logic aims at exploiting the currentachievements towards more applications, this project starts from thefollowing two observations:First, Coalgebraic logic did not yet make use of many of the importantdevelopments that have taken place in Modal Logic. Two of thesedevelopments are:1) the relationship between Modal Logic and First-Order Logic and2) the uniform treatment of classes of modal logics.Second, there exist many parallel developments in Modal Logic andDomain Theory. Some of the relationships have only recently becomeclear, through the connection of both areas with Coalgebra. Wetherefore plan to3) generalise methods from Modal Logic so that they can be applied tothe logics arising in Domain Theory (this will include the work doneunder 1 and 2 above)"
	},
	{
		"grant":92,
		"ID": "EP/G042322/2",
		"Title": "Inference Mechanisms for a Separation and Numerical Domain",
		"PIID": "-113600",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2010",
		"EndDate": "30/09/2013",
		"Value": "332629",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "94",
		"Investigators":[
		{"ID": "-113600", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proliferation of software across all aspects of our life means that software failure can have significant economic and social impact. It is therefore highly desirable to be able to develop software that is formally verified as correct with respect to its expected specification. This has also been identified as a key objective in one of the UK Grand Challenges (GC6). Although research on formal verification has a long history, dating back to the 1960's, it remains a challenging problem to automatically verify programs written in mainstream imperative languages such as C, C++, C# and Java. This is in part due to the prolific use of (recursive) shared mutable data structures which are difficult to keep track of statically and in a precise and concise way.The emergence of separation logic promotes scalable reasoning via explicit separation of structural properties over the memory heap where recursive data structures are dynamically allocated. Using separation logic, progress has recently been made on automated verification for pointer safety in the separation/shape domain. To verify the more general memory safety and functional correctness, it will require the combination of both separation (structural) and numerical (e.g. size) information. Therefore, advanced analysis and verification techniques are needed in the combined separation and numerical domain  to verify memory safety and functional correctness. Nevertheless, this remains a clear challenge for program analysis research.As a first step to tackle the challenge, Our recent development on program verification using a combined separation and numerical domain also allows user-specified inductive predicates to appear in program specifications for better expressivity. Based on this specification mechanism, a verification system called HIP/SLEEK has been built to conduct the automated verification and proof search. Our experimental results have confirmed the viability of this approach. One issue with the current system is that it is a liability for the users to supply all loop invariants and method pre/post-conditions prior to the verification. This can be very demanding and challenging for the users.As the second phase towards tackling the challenge, we propose to develop advanced inference mechanisms in the combined separation and numerical domain with user-defined predicates so that loop invariants and method pre/post-conditions can be automatically synthesised, where possible. Achieving this goal means that a much higher level of automation will be achieved, therefore a significant advance will be made in automated verification on memory safety and functional correctness.A key objective in the proposed research is to find a systematic approach to abstraction construction in the combined domain, so that appropriate abstractions can be employed by the inference process. Abstractions are required in the analysis and verification  for various reasons, such as termination and scalability. Appropriate abstraction mechanisms are crucial in maintaining a desirable scalability/precision trade-off. Apart from the abstraction mechanisms, we  also intend to design analysis algorithms for loop invariant synthesis, method post-condition inference and method pre-condition discovery for the combined domain with arbitrary user-defined predicates. We will build a tool to implement these analyses and apply it to sizeable benchmark programs. As a challenging example, we will apply our tool for the verification of memory safety of a Linux kernel. Such a sizeable program can well be used to test the limit of our inference mechanisms. We believe our research outcomes will further improve the level of automation, and therefore significantly extend the viability and applicability of automated verification on memory safety as well as functional correctness for substantial imperative programs."
	},
	{
		"grant":93,
		"ID": "EP/G043434/1",
		"Title": "Algorithmic Aspects of Graph Coloring",
		"PIID": "-26269",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/10/2013",
		"Value": "437514",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-26269", "Role": "Principal Investigator"},
		{"ID": "-26259", "Role": "Co Investigator"},
		{"ID": "-403442", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We consider a variety of practical situations in which attributes (wavelengths, frequencies, time slots, machines) have to be allocated to conflicting objects (optical data streams, transmitters, traffic streams, jobs) in such a way that no pair of conflicting objects receives the same attribute. We model such situations as graph coloring problems. A graph is given by a set of vertices that represent the objects and a set of unordered pairs of vertices, called edges, representing the conflicts between pairs of objects. Graph coloring involves the labeling of the vertices of some given graph by integers called colors such that no two adjacent vertices receive the same color. In many applications the objective is to minimize the number of colors. Graph coloring has been a popular research topic since its introduction as a map coloring problem more than 150 years ago. Some reasons for this are its appealingly simple definition, its large variety of open problems, and its many application areas. Whenever conflicting situations between pairs of objects can be modeled by graphs, and one is looking for a partition of the set of objects in subsets of mutually non-conflicting objects, this can be viewed as a graph coloring problem. This holds for classical settings such as neighboring countries (map coloring) or interfering jobs on machines (job scheduling), as well as for more recent settings like colliding data streams in optical networks (wavelength assignment), colliding traffic streams (time slot allocation) or interfering transmitters and receivers for broadcasting, mobile phones and sensors (frequency assignment), to name just a few. Note that even the nowadays so immensely popular pass-time of Sudokus comes down to coloring a (partially precolored) graph on 81 vertices (representing the 81 squares of the Sudoku) with 9 colors (the integers 1 to 9).In the classical setting the coloring is done off-line in the sense that the whole graph is known and it does not change over time. Many variants on this simple off-line graph coloring concept have been defined and studied, mainly due to additional restrictions on the coloring. We illustrate this by considering the general framework for coloring problems related to frequency assignment. In this application area graphs are used to model the topology and mutual interference between transmitters (receivers, base stations): the vertices of the graph represent the transmitters; two vertices are adjacent in the graph if the corresponding transmitters are so close (or so strong) that they are likely to interfere if they broadcast on the same or `similar' frequency channels. The problem in practice is to assign the frequency channels to the transmitters in such a way that interference is kept at an `acceptable level'. In many technological applications off-line coloring is not a suitable concept because complete information on the graph one has to color is not known beforehand, e.g. if jobs come in one-by-one and have to be scheduled on machines right away and rescheduling is not possible. In this case one has to consider another variant of coloring, namely on-line graph coloring. In this setting the graph is presented vertex by vertex, and a vertex must irrevocably be assigned a color as it comes in, i.e. the choice of color is only based on the knowledge of the subgraph that has been revealed so far.  In general, minimizing the number of colors is an NP-hard problem (it is even more problematic in the on-line setting) . This means that most likely there is no polynomial time ( fast ) algorithm for this problem (an algorithm can be seen as a set of instructions for solving a problem).  However, coloring problems occuring in specific situations with extra restrictions might have a different time complexity. Therefore, we try to design and analyse algorithms that solve graph coloring problems both in the on- and off-line setting for several variants as described in our proposal."
	},
	{
		"grant":94,
		"ID": "EP/G043507/1",
		"Title": "Interactions on the Move: Understanding Strategy Adaptation in Dynamic Multitask Environments",
		"PIID": "-99600",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/04/2009",
		"EndDate": "30/09/2012",
		"Value": "210681",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "UCL Interaction Centre",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-99600", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "With computers having been untethered from the relative safety of the desktop there comes a growing need to understand the implications of interface design for how people interact with information communication technologies on the move. Nowhere is this need greater than in situations where people interact with technology systems in safety critical environments, such as when driving a car. In many such multitasking situations, people can often only actively attend to a single task at a time because of competition for limited attentional resources between tasks. At the same time many of our interactions with technology systems tend to be shaped by prior knowledge of how to perform routine procedural tasks on that device. It is therefore not clear to what extent decisions about how to interleave attention between tasks is constrained by this prior experience of using a device. If people do not adjust their interaction style to the demands of the task environment this could be potentially dangerous. There are a number of accounts for how people might choose to interleave resources between tasks. One possibility is that task interleaving is constrained to natural break points in the execution of a task. For example, consider a driver dialling a telephone number. In this situation, the driver might choose to enter only the area-code part of the telephone number (or indeed select the 'Address Book' option from an interactive menu), and then return attention to monitoring the road ahead before completing another small step of the secondary task. In this way, natural break points in the representational structure of the task act as a cue to switch from one task to another. Alternatively, drivers might simply set a limit (or threshold) on the amount of time they are prepared to look away from the road and complete as much of the secondary task as possible within this window of opportunity. A further possibility is that task interleaving strategies are selected that optimally trade the time required to complete the secondary task against any additional time taken to switch to the primary driving task in order to maintain a stable lane position while dialling.This research proposal sets out a series of planned experiments that will be conducted to investigate how people allocate resources between multiple ongoing tasks while driving. Experiments will be conducted in a desktop driving simulator using specially instrumented devices for secondary task interactions. The experiments will be informed by various computational accounts of how people might choose to schedule resources between tasks, and will investigate the consequences of manipulating the representational structure of secondary in-car tasks and features of the functional task environment on performance and strategy adaptation. In tandem with the running of these experiments, modelling will be conducted that will implement these various computational accounts of human multitask scheduling, deriving key quantitative performance predictions for each. This modelling work will be aimed at determining which account provides the best characterisation of human behaviour, and in doing so, will set the foundation for future work directed towards developing design tools for rapidly predicting the efficiency of design alternatives for supporting the multitasking user on the move.This programme of research will lead to greater understanding of human behaviour in complex multitasking environments and the knowledge gained will be of potential value to the designers of mobile interactive systems. The empirical data will give insights into how interfaces for in-car devices might be redesigned to support users' needs in a safe and efficient manner. These conclusions will be of value for understanding behaviour in a variety of contexts where people must allocate attention between multiple concurrent task while monitoring safety critical systems."
	},
	{
		"grant":95,
		"ID": "EP/G04354X/1",
		"Title": "The Birth, Life and Death of Semantic Mutants",
		"PIID": "65573",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2009",
		"EndDate": "31/05/2013",
		"Value": "372405",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Information Systems Computing and Maths",
		"OrgID": "129",
		"Investigators":[
		{"ID": "65573", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Traditional Mutation Testing produces test cases that distinguish between some description N and variants of it. Each variant is produced by applying a mutation operator to N. A test set that is good at distinguishing N from variants of N is likely to be good at finding faults that are similar to applications of the mutation operators. Mutation testing was originally applied to programs but more recently it has been applied to other forms of descriptions such as specifications. Mutants are produced through the application of mutation operators, each of which may be applied to a relevant point in a program in order to produce a mutant. The mutation operators carry out small syntactic changes. For example, + might be replaced by -, > might be replaced by >=, a variable in an expression may be replaced by a constant, or part of an expression may be deleted. The mutation operators are designed to represent syntactically small errors. Typically, mutants are used to either judge the adequacy of a test set (does it distinguish between N and its mutants?) and also to drive test generation (we want a test set that distinguishes between N and its mutants).Traditional mutation testing produces mutants that represent small  slips  or mistakes in programming and thus represent a class of faults. A mutant program differs from the program under test by a small syntactic change (e.g. a  /  replaces a  * ). However, real developers will also suffer from misunderstandings, especially when moving between description notations. They misapprehend the semantics of the description before them. They may, for example, import their understanding from a previously used programming language, or else from an understanding of how a particular tool interprets the notation. We believe that a semantically oriented mutation testing approach may assist in the discovery of such problems. We seek to show that a semantically oriented mutation testing approach is feasible and can find faults not found by traditional syntactic mutation (and likely, by other popular testing strategies).Misunderstanding the semantics of descriptive notations is a common source of problems in software development. We believe that these misunderstandings can be represented as semantic mutants over descriptions and that test data produced to kill semantic mutants is effective at finding faults caused by such misunderstandings: It will often find faults that are typically missed by test sets produced by extant testing strategies (and in particular, by test sets that are produced to kill traditional syntactic mutants). We also believe that he production of semantic mutants and the generation of test data to kill them can be automated."
	},
	{
		"grant":96,
		"ID": "EP/G044163/1",
		"Title": "Nonlinear photonics in silicon-on-insulator nanostructures",
		"PIID": "76432",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2009",
		"EndDate": "28/02/2013",
		"Value": "462262",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "7",
		"Investigators":[
		{"ID": "76432", "Role": "Principal Investigator"},
		{"ID": "45280", "Role": "Co Investigator"},
		{"ID": "82897", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Silicon based nano-photonics is becoming a prominent contender in the race for effective all-optical information processing - and is simultaneously becoming a fascinating arena for fundamental research. Integration of optical devices into microelectronic chips is now not only discussed by academic researchers, but is also included in the business plans of microelectronics giants such as Intel and IBM. One of the first practical applications of on-chip nanophotonics is likely to be compact optical processing of multifrequency data streams. Nano-sized silicon waveguides (photonic wires) and resonators offer a very attractive way of realizing photonic components on a chip. This is due to the large index contrast between silicon and air, so that light at a wavelength of 1550nm can be tightly confined for waveguide widths as small as 500 nm. Another widely-recorgnised advantage is the possibility of using well established and wide-spread complementary metal-oxide-semiconductor (CMOS) technology. The presence of a substantial ultrafast Kerr nonlinearity in Silicon nano-structures potentially allows devices to perform at the THz rates that will be required in near-future high-performance sub-systems. Nonlinearity and dispersion control are the key properties needed to develop all-optical processing devices such as modulators, switches, delay lines and amplifiers. They are also the key parameters to be controlled if we are to understand and explore the fundamental optical physics in these structures. The interplay between dispersion and nonlinearity leads to such effects as soliton formation and modulation instability, which will be essential for temporal control and spectral modification. One of the dreams of the optical soliton community has been a three dimensional photonic chip made of a nonlinear material where all the routing is done by means of the spatial solitons, which then serve as an instantly reconfigurable and flexible network of waveguides for transmission and processing of data by means of temporal solitons. The soliton effects in planar silicon chips proposed here are possibly as close as we can hope to get to this dream.      The overall aims of the research programme are: to fabricate a range of silicon-on-insulator structures for observation of  spatiotemporal solitons, frequency conversion, and spectral, temporal and spatial shaping of femtosecond pulses;bistability effects in cavity arrays; experimentally observe and model the above effects, develop their physical understanding; use the unique properties of  silicon to observe new optical phenomena; ensure further scientific progress in the area of nonlinear  nano-photonics."
	},
	{
		"grant":97,
		"ID": "EP/G049165/1",
		"Title": "XML with Incomplete Information: Representation, Querying, and Applications",
		"PIID": "-117299",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2009",
		"EndDate": "30/11/2013",
		"Value": "565505",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-117299", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Data on the Web - particularly XML data - is often incomplete andinconsistent, due to such factors as the lack of centralisation andcontrol over data quality.  While the transfer and extension ofrelational tools to deal with XML data has been a central theme indata management research over the past decade, the standard databasetoolbox offers us little in terms of handling ofincompleteness. Indeed, it is one of the most notoriouslyunderdeveloped and most often criticised aspects of relationaldatabases.  In addition, the flexibility of XML leads to many ways inwhich incompleteness of data can be accommodated, in addition to thestandard relational null values.There has not yet been any detailed study of incompleteness in XML.Our main goal is to conduct such a systematic study, and develop itsapplications in the area that underlies data management tasks on theWeb -- the use of data across multiple independent applications.We shall investigate models of XML with incomplete information andalgorithmic techniques for querying such data, paying particularattention to the correctness/complexity tradeoffs and to the practicalityof algorithmic tools. We shall investigate the fundamental role ofincompleteness in applications that involve the movement of XML data,such as integration of data from various sources or moving databetween peers according to mappings between their schemas. We shalldevelop a specification and algorithmic toolbox for dealing withincomplete information as it arises in such applications."
	},
	{
		"grant":98,
		"ID": "EP/G049416/2",
		"Title": "New directions in quantum algorithms",
		"PIID": "-100599",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/08/2010",
		"EndDate": "30/09/2012",
		"Value": "163447",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Applied Maths and Theoretical Physics",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-100599", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Quantum computation is a new model of computing based on the principles of quantum mechanics. Excitingly, it offers the prospect of obtaining faster algorithms for certain problems than are possible in classical (ie. non-quantum) computation.As an example, it is believed that there exists no efficient classical algorithm for the task of decomposing a large composite number into its prime factors. This problem is important in cryptography. However, there does exist an efficient quantum algorithm for this task (known as Shor's algorithm). The problem appears to contain some structure that quantum computation can use in a way that classical computation cannot. Another important quantum algorithm is known as Grover's algorithm; surprisingly, using this algorithm a quantum computer can achieve a speed-up over classical computers in the basic task of searching an unsorted list.The aim of this research is to develop new quantum algorithms based on different principles to these two algorithms, and conversely to improve our understanding of the limitations of quantum computing. Specific goals of the project are:1. To obtain new quantum speed-ups by extending classical heuristics to quantum algorithms. The vast majority of research in quantum computing has considered worst-case measures of complexity. This project aims to develop quantum algorithms that outperform classical algorithms on average. This should vastly increase the range of problems to which quantum computers can be usefully applied.2. To initiate a quantum theory of inapproximability of optimisation problems. It is known that it is hard for standard computers to approximate the answer to certain optimisation problems. This project will take the first steps in translating this concept to quantum computation, and will thus prove limitations on the power of quantum computers.3. To produce the first efficient quantum data structures. This project will investigate the question of whether quantum states can be used as data structures to achieve a reduction in space compared to classical data structures.Large-scale quantum computers are yet to be built. However, a better understanding of the potential of these machines could both greatly accelerate their development, and give additional reasons for attempting to build them in the first place. This research aims to help produce such an understanding."
	},
	{
		"grant":99,
		"ID": "EP/G050112/2",
		"Title": "Trust metrics for SPKI/SDSI",
		"PIID": "-214796",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/12/2010",
		"EndDate": "30/11/2012",
		"Value": "156673",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-214796", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "I propose to extend the SPKI/SDSI authorization protocol to allow for quantitative trust specification. SPKI/SDSI is a distributed certificate system that enhances Public Key Infrastructure (PKI) and allows for issuing authorization certificates granting permissions to access selected parts of privileged data not only to single principals, but also to user defined groups. The fact that the protocol is decentralized and there is no designated entity that verifies the identity of the users of the system makes the trustfulness vary significantly from one user to another. In order to tackle this problem in decentralized PKI systems many trust metrics were created for computing how much one can trust a given user. I would like to apply two of these metrics in the SPKI/SDSI setting. In order to do that I will introduce and study several new models that are based on Labeled Pushdown Graphs, which are graphs generated by pushdown systems with labeled transitions. A robust extension of SPKI/SDSI with quantitative trust management will enhance its capabilities and potentially foster its usage in real-life computer systems."
	},
	{
		"grant":100,
		"ID": "EP/G050821/1",
		"Title": "Probabilistic Auditory Scene Analysis",
		"PIID": "-215861",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "04/01/2010",
		"EndDate": "03/01/2013",
		"Value": "232105",
		"ResearchArea": "Speech Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-215861", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "New York University"},
		{"Number": "1", "Name": "University College London"}
		],
		"Summary": "Auditory environments are typically very complicated. For example, thecocktail party comprises many sources; the chinking of glasses; thechattering of the many guests; the sound of backgroundmusic. Nevertheless, our auditory system can make sense of such ascene; it can work out how many acoustic sources there are anddetermine the individual contributions to the scene fromeach. Remarkably, it can do this using the information from a singlemicrophone.  A major goal of auditory neuroscience is to understandhow the auditory system achieves this feat.Broadly speaking, it is thought that there are three stages toauditory scene analysis. The first stage is well understoodphysiologically and that is to convert the incoming sound into atime-frequency representation. This reveals the local energy in afrequency band at a particular time. In the second stage,psychophysical evidence suggests that primitive grouping principlesare used to group local regions of spectral-temporal energy arisingfrom a common source. By using simple stimuli - like tones and noise -a long list of primitive grouping principles have been elucidated. Forexample, the principle of good continuation identifies smoothlyvarying features with a single source and abrupt changes as asignature of separate sources.  In the final stage of auditory sceneanalysis, called schema-based grouping, higher level knowledge, likethe structure of music or speech, is used to bind the groups ofspectral-temporal energy into streams so that there is one stream foreach source.There are many outstanding questions with this framework.  Oneimportant open question is the role that auditory cortex plays inauditory scene analysis as it is not well established.  Anotherconcerns the generality and completeness of the established list ofprimitive grouping rules. For although the principles successfullycharacterise perception of simple sounds it is unclear how successfuland relevant the description is for natural sounds. This project aims to resolve these questions though modelling work,psychophysics experiments and neural recording experiments. The newidea is to view the primitive grouping principles as arising frominference in a latent variable model of auditory scenes. A latentvariable model is a description of how an auditory scene, like thatencountered at a coctail party, is composed of latent auditorysources, like the chinking glasses and chattering guests. It alsoincludes a description of the statistics of these sources, like thefact that the chinking glasses tend to be isolated, high frequencyevents whist the chattering rather more constant and lower infrequency. The idea is that the brain is trying to infer these latentsources using prior knowledge of their statistics. New tools ofprobabilistic inference can make these intuitions concrete.This new perspective, called probabilistic scene analysis, has twomain advantages; one practical and one theoretical. The practicaladvantage is that a statistical characterisation of sounds can be usedto produce stimuli with complicated, but controlled structure, for usein experiments.  The theoretical benefit is that the list of primitivegrouping rules, and the manner in which they trade off, are nowderived from the statistics of sounds; Heuristic implementation is nolonger required. This enables us to predict the results of theexperiments.  In particular, the psychophysics experiments are aimedat resolving both how auditory grouping operates in synthetic auditorytextures (e.g. rain, wind, water etc.) and whether this is consistentwith the probabilistic account.  Furthermore, the neural recordingexperiments will investigate the role of auditory cortex in auditoryscene analysis, and the hypothesis that it is representing high levelstatistics of sounds like slowly varying modulatory components."
	},
	{
		"grant":101,
		"ID": "EP/G054304/1",
		"Title": "Quality of Service Provision for Grid Applications via Intelligent Scheduling",
		"PIID": "99595",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2013",
		"Value": "227069",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "99595", "Role": "Principal Investigator"},
		{"ID": "84803", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Grid computing can be defined as coordinated resource sharing and problem solving in dynamic, multi-institutional collaborations. The success of a Grid infrastructure is based on a number of fundamental requirements, including the ability to provide dynamic and efficient services. Underpinning such a system is the need to ensure that the Grid infrastructure is delivering the required Quality of Service to its users. Quality of Service (QoS) is the ability of an application to have some level of assurance that users' requirements can be satisfied. It can be seen as an agreement between a user and a resource provider to execute an application within a guaranteed time frame at a pre-specified cost. As a rule, the higher the cost paid by the user, the smaller the execution time a resource provider can ensure. The novel contribution of the proposed project is to produce a new type of Grid resource broker with an advanced scheduling component aimed at optimising both resource usage costs and applications' execution times to enforce QoS. It should combine the features of two types of brokers: system-centric and user-centric providing a transparent means of meeting users' requirements and at the same time optimising the usage of Grid resources on the provider's end. This proposal is timely in that it addresses the need for continued development of infrastructure support for Grid computing. It responds to the increased attention of the Grid community to QoS provision and higher expectations of Grid users to receive adequate services at an agreed price payable for the agreed execution time. Our research will take advantage of the achievements in the classical scheduling theory and the newly emerged Grid scheduling research and will advance the frontiers of both areas. Grid applications give rise to new enhanced scheduling models. These enhanced models generally cannot be handled by the existing scheduling techniques developed mainly for manufacturing applications. They are characterised by complex additional constraints including those related to data storage and data transfer, co-ordinating the execution of linked tasks and arranging the required data interchange. Further challenges are related to the dynamic nature of Grid systems with the changing availability and quality of resources. The new aspects of QoS provision introduce additional complexity to scheduling. The project will draw on expertise of two established research groups at the University of Leeds: Algorithms and Complexity Group and Collaborative Architectures and Performance Group. The Algorithms and Complexity Group performs multidisciplinary research in algorithms, combinatorics and optimisation. Inter alia, the group develops and analyses advanced mathematical techniques for solving complex optimisation problems including those related to the areas of scheduling and optimal resource allocation. Research of the Collaborative Architectures and Performance Group focuses on Intelligent Infrastructures for large-scale applications. In particular, research of the group brings together e-science, Grid and adaptive computing systems research."
	},
	{
		"grant":102,
		"ID": "EP/G055114/1",
		"Title": "Constraint Satisfaction for Configuration: Logical Fundamentals,Algorithms, and Complexity",
		"PIID": "-106108",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2009",
		"EndDate": "31/10/2013",
		"Value": "483829",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-106108", "Role": "Principal Investigator"},
		{"ID": "8225", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The project deals with configuration problems. Flexibility and efficiency in the customization of products and services --rather than series production-- has become a key factor of competitiveness in the post-industrial economy. Configuration development is an excellent application area for artificial intelligence methods and for constraint satisfaction, in particular.Developing a product configuration system is a challenging task in many ways. Product configuration tools should be designed to encode the complex knowledge from domain experts, such as the characteristics of the different components involved in thecustomization and the restriction on how these components can be combined with each other. However, this might be very difficult in general, because customization is a generative process, where both the number of the involved components and the types of components themselves may be unknown at the beginning.The second challenge in building automatic configurators concerns the efficiency of the algorithms supporting the customization. In fact, product configuration in real scenarios is likely to involve several components and hundreds of associated variables, whosevalues have to be dynamically determined based on the customer's needs. For instance, telephone switching systems often consisting of several hundreds of racks, thousands of frames, and dozens of thousands modules. Given the huge size of the problems to betreated, a major requirement is to integrate efficient algorithms to both building the configuration that best matches with the customer's desires and checking whether a given configuration satisfies the technological requirements from the industry.In order to achieve significant progress, we will first study existing approaches and compare them formally and request feedback from the industrial advisory board. We propose to investigate a formalism suited to the cope with the above mentioned challenges, called the extensible constraint satisfaction problem (ECSP). Wewill study the expressive power and the complexity of decision and computational problems related to this formalism. We also propose to investigate the complexity issues in the presence of value-generating constraints, which is a well-known type ofconstraints used in database theory, but has not been investigated in the context of CSPs so far. Once the framework for extensible CSPs has been layed out, our plan is to investigate decomposition techniques, to find tractable subclasses of ECSPs. Finally, we will implement and test a configurator system, based on our framework,and using our decomposition algorithms.  The project is organised into four main work packages. WP1 systematically studies the relevant problems to configuration, both by formally comparing existing approaches in the literature and by receiving feedback from the industrial advisory board.  WP2 focuseson the extensible constraint satisfaction problems (ECSPs).  Particular focus will be given to complexity analysis of the relevant decision and computational problems.  WP3 consists of a comprehensive study of decomposition methods suitable to ECSPs toidentify tractable subclasses. In WP4, we will implement and test a proof-of-concept prototype of configuration system, based on ECSP and on the decomposition methods developed in WP3.The scientific project staff will consist of one post-doc, and one doctoral student. The student is expected to intensively co-operate with the post-doc.We plan to publish the results in top artificial intelligence journals and at leading international conferences."
	},
	{
		"grant":103,
		"ID": "EP/G05536X/1",
		"Title": "All-Semiconductor Integrated Terahertz Time Domain Spectrometer",
		"PIID": "-149095",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2010",
		"EndDate": "31/12/2013",
		"Value": "270090",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-149095", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The terahertz (THz) part of the EM spectrum stands between infrared and microwaves. Therefore, THz technology stands in the interface of optics and electronics; until recently, both electronics and optics technological approaches failed to produce a practical THz source or detector. THz sources and detectors were prohibitingly expensive and as a consequence were used only in astronomy and physics research. This is the reason why traditionally this spectral region was called the THz gap. Although there was a lot of interest for medical diagnosis and material spectroscopy because this region is very rich in absorptions, practical applications were not possible. However, THz science expanded during the 1990s with the invention of ultrafast laser systems that can be used to generate and measure THz fields. The instrument that redefined THz science is called Terahertz time domain spectrometer (THz-TDS). At present, companies such as Teraview Ltd. target the development of imaging solutions and produce spectrometers with advanced capabilities in drug research and medical diagnosis but also with high cost and size. It is obvious though that research in the area should be sponsored to expand outside this approach of complicated spectroscopic imaging machines to a more flexible compact solution that will open even more application possibilities. THz spectrometers need further development to unleash their full market potential; mainly, development to bring the cost and size down. Furthermore THz radiation is strongly absorbed by water and this makes it impossible to use THz to penetrate the human body or make measurements of liquids; this limitation should be overcome. Nevertheless, THz cancer detection is specifically based on the sensitivity of THz on water content; a THz spectrometer can detect the higher water concentration of cancerous cells. Scientists are addressing these problems daily, doing research for smaller size spectrometers and liquid measurement ability; it is generally accepted that there is a place for Terahertz technology in the near-future of sensing for medicine, biology and homeland security.The research proposed here is crucial to further expand terahertz technology and it results to a device that can be directly used in the industry with solid and liquid sample capabilities. It is a highly interdisciplinary attempt to give a new route for the use of this technology in chemistry, medicine and biology. The proposed research is for an integrated THz time domain spectrometer in an all-semiconductor configuration. It aims to the development of a terahertz spectrometer with reduced cost and size by using compact semiconductor pump laser sources and integrated optics. Furthermore, it overcomes the water absorption problem because it uses very small water samples to keep absorption values low. This research is timely because at the moment terahertz industry is coming to maturity commercially around the world as it is slowly adopted in the pharmaceutical industries and is tested also for medical diagnosis and security controls."
	},
	{
		"grant":104,
		"ID": "EP/G055548/1",
		"Title": "TEMPO: Time Driven Modelling and Resource Management of Real-Time Systems on Multiprocessor Systems-on-Chip",
		"PIID": "48487",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/03/2013",
		"Value": "622511",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "48487", "Role": "Principal Investigator"},
		{"ID": "2307", "Role": "Co Investigator"},
		{"ID": "-249197", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Embedded real-time systems (ERTS) implemented on Multiprocessor System-on-Chip (MPSoC) platforms present fundamental challenges to the offline modeling of system behaviour, offline prediction of system temporal and resource usage at run-time, and run-time resource management. MPSoC platforms offer the prospect of immense computational power via parallelism, but this very parallelism makes them difficult to model. Hence from a real-time systems perspective, it is extremely difficult to predict offline the run-time behaviour and performance of ERTS implemented upon MPSoC platforms. We note that such predictions are a fundamental requirement of real-time systems. Understanding and predicting system performance from a time and resource usage perspective requires that sufficient aspects of the application and system software, resource management and MPSoC platform must be accurately modeled to establish the complex inter-dependencies between application software, system software and the MPSoC platform. Without this understanding, it is difficult to verify offline that a system will meet its timing requirements and resource usage constraints (eg. memory). The research challenge is exacerbated by observing that such systems are essentially dynamic: application configuration (or mode) changes, movement of application software between processing elements, changes in available resource (eg. power), changes in the configuration of the hardware platform itself (ie. functionally dynamically reconfigurable).This project directly addresses the challenge of understanding the behaviour of dynamic ERTS implemented on MPSoC platforms by advocating an approach that puts time (rather than structure or functional behaviour) at the centre of the modeling method, run-time resource management and offline verification approach. The intuition behind adopting a time-centric view of complex parallel systems is the observation that conventional functional or structural approaches inevitably hide or obfuscate pertinent non-functional system timing behaviour. We believe that by modeling the system from the perspective of time, we can model and relate all behaviours at differing time granularities (from application to hardware), so enabling accurate offline analysis and prediction of timing properties - essential for real-time systems.The project will focus upon three specific areas of the challenge: modeling, resource management and verification. We will develop a resource oriented time banded framework to allow the system to be modeled. This will support and integrate with resource management policies and mechanisms. The latter will be based upon hierarchical contracts, taking a resource virtualisation approach expanded to include multiprocessors, memory hierarchies, on-chip networks, energy and space. Verification will be achieved by developing appropriate scheduling and allocation algorithms for the resource contracts, incorporating both rigorous analysis (ie. schedulability analysis) and  simulation. Importantly, the project will utilise an integrated approach across modeling, resource management and verification at each level of timing granularity."
	},
	{
		"grant":105,
		"ID": "EP/G055890/1",
		"Title": "Low Power Body Worn Antenna Systems",
		"PIID": "55374",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2013",
		"Value": "488378",
		"ResearchArea": "Clinical Technologies (excluding imaging)",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering & Digital Arts",
		"OrgID": "27",
		"Investigators":[
		{"ID": "55374", "Role": "Principal Investigator"},
		{"ID": "106743", "Role": "Co Investigator"},
		{"ID": "14785", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Great Ormond Street Hospital"},
		{"Number": "1", "Name": "W L Gore & Associates Ltd"}
		],
		"Summary": "Future tele-medicine and communications systems will increasingly be worn on the human body with wireless links to external systems.   Radio channels will propagate around the body and it is very important to be able to understand and characterize exactly how this energy is distributed about the surface of a human subject.  There remains a concern among the public and users in general that exposure to radio frequency (RF) energy should be minimized to avoid possible health issues and reducing transmit powers will also improve battery life.  Research has been done to assess how electromagnetic waves are guided by animated human models wearing radio devices, but this work does not include the effect of gaps caused by loose fitting clothing and how this causes the radio device to tilt with time.   The position of a radio antenna will have a significant effect on how much energy is directed towards the surface of the skin and therefore on the wireless channel.  This project is a collaboration between two established UK centres of wearable antenna and flexible screening research.  The project will investigate new techniques for integrating antenna systems and screens into clothing for worn applications.  The novelty of the proposal is to reduce RF powers by utilizing diversity between disguised and low profile antennas realizing the potential for low interference and improved reliability.  New design philosophies will be created for worn wireless systems where movement data and worn antenna tilting will be captured from real humans and used to find propagation paths around the body while switchable periodic screens will select between magnetic reflection and surface guided modes.  Antennas for medical applications mounted on disposable paper clothing will be explored as well as auto-tuning of antennas on different body types using varactor diodes, liquid crystal mixtures and MEMs switches.  The outcomes of this research will well place the UK as a leader in, and an exploiter of, future wireless systems such as tele-medicine and pervasive computing."
	},
	{
		"grant":106,
		"ID": "EP/G056447/1",
		"Title": "From Frequent Itemsets to Informative Patterns: Theory and Applications",
		"PIID": "-124229",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2013",
		"Value": "292231",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Mathematics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-124229", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Unilever Corporate Research"},
		{"Number": "1", "Name": "University of Leuven"}
		],
		"Summary": "This project will address a problem that has recently been highlighted as a major research challenge in data mining: the fact that the output of many data mining algorithms is typically (and with the present technology often unavoidably) too large for human understanding and interpretation, and therefore of little use for many practical purposes. This is the case in particular for the output of the important and broad class of frequent pattern mining algorithms, which are in the focal point of this proposal. (Such patterns can be subsets in sets, subgraphs in graphs, or subsequences, substrings, or approximate substrings in strings, and much more.)To address this challenge, we will develop new data mining approaches for effective database summarization and understanding. We will achieve this by combining, integrating, and developing state-of-the art ideas from data mining, statistical modeling, and optimization theory.The solution of this problem is likely to boost the impact of all frequent pattern mining techniques, of which the development required a considerable community effort, but of which the uptake in applied research and industry has remained limited so far. Therefore, this project may have a significantly non-linear effect on the research community: it has the potential to unleash a large amount of data mining expertise for practical application.To underline the impact of our theoretical results, and to ensure that they are used in practice, in a second part of this project we will be the first ones to apply our results to a number of case studies. We have intentionally chosen these case studies from three diverse domains: bioinformatics, text mining, and marketing. Each of these domains has a large user base, all of whom will become beneficiaries of this project. By developing these applications, we will be able to demonstrate the use of our newly developed methods to these user groups. The bioinformatics application we will tackle will be the search for transcriptional modules of genes, which are regulated by the same regulatory program under certain conditions. The data driven nature of bioinformatics makes data mining approaches very well suited, and our new methodologies will ensure their successful application with a minimal amount of expert knowledge required. Note that this is just one example of a bioinformatics application -- frequent pattern mining (subsets, subgraphs, subsequences, substrings...) is of use in many other branches of bioinformatics.In the text mining application we will search for interesting sets of words, sequences of words, or approximate sequences (strings) of words, occurring in a corpus of text documents. These text documents will be sets of news articles over a certain time span. We should point out that we are currently already gathering thousands of news articles every day for subsequent text mining analyses, and this in the context of another project being developed together with Prof. Nello Cristianini in the same department. The integration of this result with that project will provide our new methodology with a unique showcase demonstration.For the marketing application, the most obvious application would be the search for items in a supermarket store that are often sold together. Marketers are interested in this information, for example to allow them to optimize their promotion strategies. For example, they may choose to reduce the price of one product, and increase the margin on other products that are strongly associated with the former. Our collaboration with Unilever, who have such retail transaction data available for use in this project, will enable us to successfully complete this application."
	},
	{
		"grant":107,
		"ID": "EP/G059268/1",
		"Title": "Femtosecond semiconductor lasers",
		"PIID": "16022",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2012",
		"Value": "335713",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117",
		"Investigators":[
		{"ID": "16022", "Role": "Principal Investigator"},
		{"ID": "-120856", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Fianium Ltd"}
		],
		"Summary": "The aim of this proposal is to demonstrate for the first time a semiconductor laser emitting transform-limited optical pulses of less than 200 fs duration in a diffraction-limited beam.  This achievement will open the way for the development of truly compact ultrafast optical systems.  Our device is a surface-emitting laser, optically pumped using the cheap and rugged technology developed for diode-pumped solid state lasers, with perfect beam quality enforced by an extended cavity.  It emits a periodic train of ultrashort pulses at a repetition rate of a few GHz using the optical Stark effect passive mode-locking technique introduced by the Southampton group.  Recent proof-of-principle experiments have shown that these lasers can generate stable 260-fs pulse trains.  We have shown, moreover, by modelling and by experiment, that the optical Stark mechanism can shorten pulses down to durations around 70 fs, comparable with the quantum well carrier-carrier scattering time.  Our proposal is to build on these world-leading results with a systematic exploration of the physics of lasers operating in this regime.  The key is to grow quantum well gain and saturable absorber mirror structures in which dispersion, filtering and the placing of the quantum wells under the laser mode are controlled to tight tolerances.  We shall achieve this using molecular beam epitaxy to realise structure designs that are developed with the aid of rigorous numerical modelling of the optical Stark pulse-forming mechanism.  We shall also use femtosecond pump and probe spectroscopy to determine the dynamical behaviour of our structures in this regime directly.  For these pioneering studies, the compressively-strained InGaAs/GaAs quantum well system operating around 1 micron is most suitable; and this is where we shall work; however, the devices that we develop can in principle in future be realised in other material systems in different wavelength regions.  We shall also make a first study of incorporating quantum dot gain and absorber material into optical Stark mode-locked lasers, aiming to exploit the intrinsically fast carrier dynamics of these structures.  In summary, this proposal aims to shrink femtosecond technology from shoebox-size to credit-card size, and in the process explore a regime of ultrafast semiconductor dynamics that has never before now been exploited to produce light pulses."
	},
	{
		"grant":108,
		"ID": "EP/G060347/1",
		"Title": "Dialectical Argumentation Machines",
		"PIID": "71452",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/03/2013",
		"Value": "561059",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing",
		"OrgID": "37",
		"Investigators":[
		{"ID": "71452", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Humans use argument to express disagreement, to reach consensus and to both formulate and convey reasoning. The theory of argument has found wide application in artificial intelligence, providing mathematical structures for automated reasoning, communication protocols for distributed processing and linguistic models for natural language processing. A key stumbling block, however, has been joining together models that focus on abstract, mathematical relationships with those that focus on concrete, linguistic relationships. The first objective of this project is to develop for the first time a theoretical account that connects static,  monologic argumentwith dynamic, multi-person,  dialogic  argument and ties together abstract, mathematical models with concrete, linguistic representations.Furthermore,models of argument have been predominantly confined to the lab. Our goal is to translate the research advances into high profile, large scale deployments using partners with enormous user bases. Prototype systems in this area have been sufficient to demonstrate the unique advantages of practical argumentation systems to potential users of this research such as those within the broadcasting domain. There is a demonstrated public demand for argument-based exploration of current issues with complex scientific and ethical dimensions, demonstrated, for example, by the longevity and success of high profile programming featuring topical issues discussed in a stylised argumentative debate format. The second objective of this project is to develop the theory into implemented components that can form a foundation for application development to support actual programmes with prototype testing Unique advantages afforded by the technology will allow users to interact with  the programme material as if they were themselves contributors, allowing arguments to be probed, tested and extended, and the distinction between in-programme and post-programme content to be blurred. The interaction metaphor shifts from 'message-then-next-message' to 'question-answer-riposte-challenge...'.The rich structure is natural for users, and provides rich metadata for programme-makers. Finally, in 2007 an exciting vision of the  world-wide argumentation web (WWAW) was laid out, in which systems such as those constructed to work alongside practical prototypes could interact, both with each other and with other debate and argumentation systems, both populist and academic.Argument fragments, expressed as resources on the Semantic Web, can cross-refer, allowing different debating systems to navigate the WWAW according to various rules of dialogue captured by dialectical games. To bring this vision of the WWAW into reality, the third and final objective of the project is to allow execution of arbitrary dialogue games on a platform that provides interfaces for human players, and both interfaces and control for computer players of dialogue games. In this way, we want to harness the enormous channel to market and the high-profile reference case that is offered by collaboration within broadcasting. At the same time, the project will be developing platform technology that can support exploitation in other areas. During the project, we will work with the Scottish Mediation Network in the context of mediation tools, with the Ontario courts in the context of judicial summaries, and with the Universities of Lugano and Groningen in the context of legal education to identify exploitation routes for the technology."
	},
	{
		"grant":109,
		"ID": "EP/G060363/1",
		"Title": "NANOSTRUCTURED PHOTONIC METAMATERIALS",
		"PIID": "17757",
		"Scheme": "Programme Grants",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2015",
		"Value": "5043275",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "17757", "Role": "Principal Investigator"},
		{"ID": "-228147", "Role": "Co Investigator"},
		{"ID": "497", "Role": "Co Investigator"},
		{"ID": "4633", "Role": "Co Investigator"},
		{"ID": "73950", "Role": "Co Investigator"},
		{"ID": "-172590", "Role": "Co Investigator"},
		{"ID": "4037", "Role": "Co Investigator"},
		{"ID": "40395", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Over the last twenty years photonics, the science of light, has played a key role in creating the world as we know it. Today it is impossible to imagine modern society without internet and mobile telephony made possible by the implementation of optical fibre networks, CD's and DVD's underpinned by the development of  lasers, modern image display technologies, and laser-assisted manufacturing.We believe that the next photonic revolution will continue to grow, explosively fuelled by a new dependence on a radically different type of photonic materials called metamaterials. Metamaterials are artificial electromagnetic media with unusual and useful functionalities achieved by structuring on a sub-wavelength scale. Nanotechnology-enabled materials are now universally seen as the direction where the global economy will grow strongly in the 21st century. The proposed Programme is at the core of this global movement and focuses on an area of particular interest to the UK - nanophotonics and metamaterials. Our vision for this Programme is to develop a new generation of revolutionary switchable and active nanostructured photonic media thus providing groundbreaking solutions for telecoms, energy, light generation, imaging, lithography, data storage, sensing, and security and defence applications. The Programme will mobilize and focus all of the resources and interdisciplinary expertise available at the University of Southampton and with our collaboration partners in the UK and around the world, to create a world-leading centre of research on Nanostructured Photonic Metamaterials. The elements of adventure and key research challenges in this project can be summarized as follows: we aim to develop photonic media allowing for ultra-high-density integration, the lowest possible energy levels and the highest speeds of optical switching. This will be achieved by advancing the physics of the control, guiding and amplification of light in nanostructures and by developing new nanofabrication techniques and methods of hybridization and integration into the waveguide and fiber environment of different novel metamaterial structures.The main methodological paradigm for the Programme is to achieve new functionalities by developing hybrid photonic metamaterials. The Programme will consist of strongly interlinked projects on fabricating hybrid metamaterials, metamaterials as a platform for photonic devices and fundamental physical experiments, controllable, switchable and active hybrid metamaterials, and developing new ideas emerging from theoretical analysis. Essential to the project will be the new world-leading 105M cleanroom and laboratory Mountbatten complex at the University of Southampton. This proposal is submitted on behalf of an internationally leading team with a formidable research track record that within the last 10 years has led and participated in research projects with funding exceeding 34 millions, published 463 journal research papers and given more than 200 invited talks at major international meetings. The research will be developed in collaboration with key international research groups and industrial laboratories and in this way form a  Global Laboratory  for the project.This high-risk/high-reward Programme will be run by a strong Director-led management team which will benefit from advice from an independent Project Mentor and Advisory Board. Strategic decisions will be made using the  search-and-focus  approach involving regular critical reviews of the research programme under an  active resources and risk management scheme  allowing for the redistribution of resources and usage of reserves where they are most needed and to quickly foster new research directions."
	},
	{
		"grant":110,
		"ID": "EP/G060525/2",
		"Title": "CREST: Centre for Research on Evolution, Search and Testing, Platform Grant.",
		"PIID": "47895",
		"Scheme": "Platform Grants",
		"StartDate": "01/08/2010",
		"EndDate": "31/08/2014",
		"Value": "925401",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "47895", "Role": "Principal Investigator"},
		{"ID": "93911", "Role": "Co Investigator"},
		{"ID": "-207482", "Role": "Co Investigator"},
		{"ID": "116951", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "CREST's approach to Software Engineering places  search--based optimisation at its heart. The CREST philosophy is that software engineering can be thought of as a search for good quality solutions within a potentially enormous search space, while balancing many different competing, sometimes conflicting, objectives. We believe that this philosophy is important in all engineering disciplines, but it is in Software Engineering where it finds its greatest potential. The materials and artifacts used in software engineering are very different; they are pure processes and information, essentially without physical manifestation. This makes the search--based optimisation agenda particularly apposite and offers the significant advantage of automation. The work supported by this platform proposal will develop, extend and disseminate this agenda of Automated Optimisation in Software Engineering."
	},
	{
		"grant":111,
		"ID": "EP/G060649/1",
		"Title": "Soft NanoPhotonics Programme Grant (sNaP)",
		"PIID": "47424",
		"Scheme": "Programme Grants",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2014",
		"Value": "3510871",
		"ResearchArea": "Photonic Materials and Metamaterials",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "24",
		"Investigators":[
		{"ID": "47424", "Role": "Principal Investigator"},
		{"ID": "76573", "Role": "Co Investigator"},
		{"ID": "-184270", "Role": "Co Investigator"},
		{"ID": "40210", "Role": "Co Investigator"},
		{"ID": "117202", "Role": "Co Investigator"},
		{"ID": "-219248", "Role": "Co Investigator"},
		{"ID": "9933", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "De La Rue"},
		{"Number": "1", "Name": "Kodak Ltd"},
		{"Number": "2", "Name": "Nokia Research Centre"},
		{"Number": "3", "Name": "Renishaw Diagnostics Ltd"}
		],
		"Summary": "Visible light can be made to interact with new solids in unusual and profoundly different ways to normal if the solids are built from tiny components assembled together in intricately ordered structures. This hugely expanding research area is motivated by many potential benefits (which are part of our research programme) including enhanced solar cells which are thin, flexible and cheap, or surfaces which help to identify in detail any molecules travelling over them. This combination of light and nanoscale matter is termed NanoPhotonics.Until now, most research on NanoPhotonics has concentrated on the extremely difficult challenge of carving up metals and insulators into small chunks which are arranged in patterns on the nanometre scale. Much of the effort uses traditional fabrication methods, most of which borrow techniques from those used in building the mass-market electronics we all use, which is based on perfectly flat slabs of silicon. Such fabrication is not well suited to three-dimensional architectures of the sizes and materials needed for NanoPhotonics applications, and particularly not if large-scale mass-production of materials is required.Our aim in this programme is to bring together a number of specialists who have unique expertise in manipulating and constructing nanostructures out of soft materials, often organic or plastic, to make Soft NanoPhotonics devices which can be cheap, and flexible. In the natural world, many intricate architectures are designed for optical effects and we are learning from them some of their tricks, such as irridescent petal colours for bee attraction, or scattering particular colours of light from butterfly wings to scare predators. Here we need to put together metal and organics into sophisticated structures which give novel and unusual optical properties for a whole variety of applications.There are a number of significant advantages from our approach. Harnessing self-assembly of components is possible where the structures  just make themselves , sometimes with a little prodding by setting up the right environment. We can also make large scale manufacturing possible using our approach (and have considerable experience of this), which leads to low costs for production. Also this approach allows us to make structures which are completely impossible using normal techniques, with smaller nanoscale features and highly-interconnected 3D architectures. Our structures can be made flexible, and we can also exploit the plastics to create devices whose properties can be tuned, for instance by changing the colour of a fibre when an electrical voltage is applied, or they are stretched or exposed to a chemical. More novel ideas such as electromagnetic cloaking (stretching light to pass around an object which thus remains invisible) are also only realistic using the sort of 3D materials we propose.The aim of this grant is bring together a set of leading researchers with the clear challenge to combine our expertise to create a world-leading centre in Soft NanoPhotonics. This area is only just emerging, and we retain an internationally-competitive edge which will allow us to open up a wide range of both science and application. The flexibility inherent in this progamme grant would allow us to continue the rapid pace of our research, responding to the new opportunities emerging in this rapidly progressing field."
	},
	{
		"grant":112,
		"ID": "EP/G061688/1",
		"Title": "Bioplatform Grant Renewal: Next Generation Biophotonics",
		"PIID": "45650",
		"Scheme": "Platform Grants",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2014",
		"Value": "1116536",
		"ResearchArea": "Clinical Technologies (excluding imaging)",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "45650", "Role": "Principal Investigator"},
		{"ID": "43826", "Role": "Co Investigator"},
		{"ID": "116335", "Role": "Co Investigator"},
		{"ID": "116330", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Light is quite astounding: our understanding of light and its applications has grown immensely, particularly following the invention of the laser nearly fifty years ago. Major strides are now being made to use light in modern biomedical applications and healthcare. This area may be termed  Biophotonics  and encompasses all interactions of light with biological specimens. The aim of this grant is to capitalise upon the major internationally recognised success of the St Andrews team in this area. Our previous Platform grant allowed us to establish major new strands of research in this topic area, that have reached international status. This includes the areas of targeted delivery with light of biological material (photoporation), Raman (taking an  optical fingerprint  of cells for diagnosis of cell abnormality) and photodynamic therapy (using light for cancer treatment) amongst others. The aim of our new grant is to sustain and enhance our activities that ensure our cohesive working practices, as well as pump-priming key feasibility studies in the areas of single molecules, cell biology and clinical applications. The emphasis is to move towards real world applications and runs in parallel with the advent of a new Medical School at St Andrews, as well as several current and upcoming appointments in Biophotonics. In tandem, we are very keen to retain, develop and promote the careers of very promising young PDRAs in this exciting field, as well as gain international visibility by fostering new projects through visits and pump-prime studies. Thus the large bulk of funding will cover salaries for young people to carry out the feasibility studies. The success of the new grant will be measured through the career progression of these people, the generation of new  responsive mode  grants, and dissemination through journal and conference papers with joint authorship between the Schools at St Andrews but also with international colleagues as appropriate."
	},
	{
		"grant":113,
		"ID": "EP/G062331/1",
		"Title": "Electrical identification of single dopant atoms",
		"PIID": "-130158",
		"Scheme": "Standard Research",
		"StartDate": "05/10/2009",
		"EndDate": "04/10/2012",
		"Value": "327447",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-130158", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Walter Schottky Institute"}
		],
		"Summary": "Is it possible to design a solid-state electronic device with functionality based on the electronic occupancy, orbital-state or spin-state of a single atom? A positive answer could enable the ultimate miniaturisation of semiconductor devices. With this question in mind we will identify individual dopant atoms (intentionally added impurities) in silicon nanostructures and then determine the lifetime of their quantum mechanical spin states.Our main experimental technique to detect dopant atoms is ultra-sensitive charge detection using the single electron transistor. This will enable us to determine whether an electron resides on a randomly positioned dopant, or if the dopant is in its ionised state. Using radio-frequency techniques we will be able to measure this occupancy in a millionth of a second.  On its own, this would only tell us that a dopant atom (or charge trap) is present but nothing of its identity. The key is to combine our charge detection technique with a means of spectroscopy. Electron spin resonance is a suitable technique, capable of identifying the unique spin environment of each species of impurity atom. To aid us we will collaborate with an expert in electron spin resonance, Prof. Martin Brandt at Walter Schottky Institute.Once we have identified a dopant atom we will use electron spin resonance not as a spectroscopy technique but to control its electron spin state. A similar technique has already been used in the case of electrons bound in quantum dots - devices often known as 'artificial atoms'. In this way we will be able to measure the quantum mechanical spin lifetimes of a single electron in silicon. Electron spins in silicon are known, from ensemble measurements, to be long-lived when compared to most other materials. Due to this longevity they are excellent candidates to be qubits - the building blocks of a quantum mechanical computer."
	},
	{
		"grant":114,
		"ID": "EP/G062404/1",
		"Title": "Engineering polariton non-linearity in organic and hybrid-semiconductor microcavities",
		"PIID": "52001",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2010",
		"EndDate": "30/06/2013",
		"Value": "310062",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "116",
		"Investigators":[
		{"ID": "52001", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cambridge Display Technology Ltd"}
		],
		"Summary": "Strongly-coupled microcavities are a fascinating system for the exploration of the fundamental physics of the interactions between light and matter. Under such circumstances, the emissive states in such microcavities are termed 'polaritons', and can be described as an admixture between an exciton and a confined cavity photon. The optical properties of polaritons can be very different from their constituent parts (excitons and cavity photons), and thus there is a significant opportunity to explore new fundamental processes, and develop new types of devices that may find applications as low-threshold lasers, optical-amplifiers and high-speed optical switches. At present, the majority of work done on the strong-coupling regime in microcavities has centred on structures that contain inorganic semiconductors (either III-V, II-VI or GaN based materials). We have however pioneered the study of strong-coupled microcavities containing organic (carbon-based) semiconductors, which are anticipated permit new effects to be engineered. Despite the importance of organic-semiconductors in a range of optoelectronic devices (LEDs, photovoltaics, FETs, lasers etc) relatively little is understood regarding the microscopic processes that occur in strongly-coupled organic microcavities.Development of a basic understanding of non-linear processes and properties of organic-semiconductors in strongly-coupled  microcavities will thus be a key area that we will address in this project. Key components of the research include studies the interactions between organic-polaritons and vibrational modes of the molecular semiconductor and the generation of organic exciton-polaritons at high density following electrical injection of carriers. We will also explore the fabrication and optical properties of 'hybrid-semiconductor' microcavities and devices (containing organic and inorganic semiconductors), and will study optically-driven energy-transfer between the different types of excitation using both linear and ultra-fast measurements. We are confident that our work will provide new fundamental insights into the optical properties of organic-polaritons (including relaxation and condensation), the transfer of excitations between different semiconductor materials via a cavity photon over large distances (> 100 nm) and the generation of new electrically-driven polariton devices. We believe that we are in an excellent position to undertake such an ambitious programme of research due to our world-leading expertise in strongly coupled organic semiconductor microcavities (Sheffield), and two-colour ultra-fast spectroscopy of microcavities (Southampton)."
	},
	{
		"grant":115,
		"ID": "EP/G062501/1",
		"Title": "A New Method for Antenna Efficiency Measurements",
		"PIID": "41844",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2012",
		"Value": "120365",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68",
		"Investigators":[
		{"ID": "41844", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "National Physical Laboratory NPL"},
		{"Number": "1", "Name": "SATIMO"}
		],
		"Summary": "The antenna, as an essential device for radio systems, has a number of important parameters to characterise its performance. One of them is the antenna radiation efficiency (i.e. antenna efficiency) which is the ratio of the radiated power to the input power. This parameter is very useful to calculate the radiated power once the input power is known. Since the theoretical value for this parameter is normally not reliable due to the complexity of the antenna system (including materials), measured antenna efficiency is often required for system design (link budget) and performance evaluation. The traditional methods to obtain this parameter are 1). radiation pattern integration method which is time consuming and not accurate; 2). Wheeler cap method which is accurate and cheap but only suitable for electrically small antennas; 3). the Q factor method which is also just suitable for electrically small antennas. The 1st method was normally used when the operating frequency is relatively high while the other two methods were employed when the operating frequency is relatively low. More recently the reverberation chamber was attempted as a new facility for measuring the antenna efficiency, but this method requires a reference antenna with a known efficiency and the accuracy is yet to be proven. As the trend of radio systems moves towards broader bandwidth and higher frequencies, the traditional methods are no longer adequate, for example, the Wheeler cap method cannot be employed to measure UWB antennas due to the ultra-wide bandwidth nature. There is an urgent need for a better method which can be used to measure these new antennas efficiently and accurately. This project consists of both theoretical and experimental work and is aimed at developing a new antenna efficiency measurement method which is based on the Wheeler cap idea, i.e. utilising a conducting cavity. However, this cavity does not need to be electrically small and the resonant modes inside the cavity can be as many as that in a reverberation chamber (which can be viewed as an over mode cavity with rotating stirrers). In the new method, the antenna under test (AUT) is placed inside the cavity at a number of strategic positions to remove the effects of the resonance and the complex reflection coefficient S11 of the antenna is measured. The antenna efficiency can then be obtained using these measured S11. A calibration and data interpretation software are required. Unlike any existing methods, the loss of the cavity, which could be significant in some cases, will be taken into account. Thus the proposed method should be more accurate than current methods and it should also be cheap and convenient as the Wheeler cap method. Furthermore, It will be suitable for various antennas over a large range of frequency. Thus this innovative method can be used to measure small and large, narrow and wide band antennas, meet the demand from the industry and provide an alternative method to the measurement and wireless community."
	},
	{
		"grant":116,
		"ID": "EP/G064504/1",
		"Title": "Aperiodic Lattices for Photonic Engineering of Terahertz Quantum Cascade Lasers",
		"PIID": "-147964",
		"Scheme": "First Grant Scheme",
		"StartDate": "05/10/2009",
		"EndDate": "04/12/2012",
		"Value": "389690",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "-147964", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Optical wavelength-scale photonic lattices (i.e. lattices formed from a spatially-varying refractive index) offer a very powerful mechanism to define and modify the photon resonance characteristics in a range of optical devices. Although highly successful up to now this approach has only been based on periodic lattices (with only a single, underlying spatial frequency, and associated with only a single colour of light) and so does not provide the ability to control the colour of the confined photons. In this proposal, we describe the use of aperiodic lattices (ALs) to provide significantly advanced spectral functionalities, e.g. to give just one example, multi-coloured lasing at user-defined frequencies. In this proposal, we aim to apply AL concepts to a terahertz (THz) quantum cascade laser (QCL) so as to enable tunability. This proposal is highly timely, since it takes advantage of recent important advances in THz QCL fabrication technologies and combines them with new theoretical insights into the properties of aperiodic structures. In particular, this proposal proposes the first ever active photonics device exhibiting an AL integrated into its opto-electronic structure.Recent years has seen THz technology (frequency: 1-10 THz, wavelength: 30-300 micron) the focus of much attention owing to its important impact in a wide range of commercial and security applications, for example, in medicine, microelectronics, security imaging, biotoxin detection, agriculture, gas sensing and environmental monitoring, and forensic science, etc. The development of the THz QCL has been a key development in the burgeoning of these technology areas. However, lack of THz QCL tunability has also acted as a major constraint. Hence, the demonstration of a compact, coherent, tunable THz QCL arising from this proposal will act as a significant enabler in the advance of THz photonics.The THz QCL employs sophisticated techniques for the control of electron propagation, with an  active region  comprising a repeated superlattice of only a few atoms thick of one semiconductor material, interleaved with similarly thin barrier layers of another material. In these semiconductor nanostructures, the energy bands split into subbands and minibands, with energy separations of several tens to a few hundreds of millielectronvolts, which determine electronic transport and also enable new optical transitions. When a bias voltage is applied across the material, a periodic cascade of such intersubband transitions is established. The population inversion necessary for lasing is then achieved through electrical injection. Adjusting the specific sequence of quantum wells and barriers to form an electronic AL allows both the electronic and optical properties of the THz QCL to be tailored at will. A photonic AL, on the other hand, provides arbitrary filter responses in a user-defined way, for example, to provide high transmission and high-resolution output at single or multiple wavelengths. The novel filtering functionality available from the photonic AL in conjunction with the gain spectrum available from the electronic AL provides far greater control of single or multiple laser wavelengths than with conventional methods. A combined approach to integrate both electronic and photonic ALs within a single THz QCL device is therefore another important aspect of this proposal. Such integration gives photons a strongly enhanced interaction time with the host material, and creates significant opto-electronic nonlinear and quantum effects."
	},
	{
		"grant":117,
		"ID": "EP/G064679/1",
		"Title": "Advances in Sublinear Algorithms",
		"PIID": "-161185",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2009",
		"EndDate": "28/02/2013",
		"Value": "296846",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-161185", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "With the growing significance of ICT technologies and with the steady increase in computational power, the need for new, computational techniques to process efficiently massive datasets is widely acknowledged by the ICT industry and the Computer Science community. From the computational viewpoint, the modern approach to massive datasets requires the use of sublinear algorithms, that is, algorithms that use resources (time and space) significantly less than the input size. Constructing sublinear time algorithms may seem to be an impossible task since it assumes that one can read only a small fraction of the input. However, in recent years, we have seen developments of sublinear time algorithms for combinatorial and optimisation problems arising in such diverse areas as graph theory, geometry, algebraic computations, and computer graphics. The main objective of this proposal is exploit the cutting edge expertise of the PI in the area of randomised algorithms and sublinear algorithms to develop new algorithmic techniques for the analysis of massive datasets by making advances in the broadly understood area of sublinear algorithms for combinatorial problems.In this project, we intend to make a progress in our understanding of sublinear-time algorithms and enlarge the class of problems for which sublinear-time are known, as well as, to characterise problems for which sublinear-time algorithms are impossible to exist. The main objective of this proposal is to develop algorithmic technology for the analysis of massive data sets in the context of two central models: sublinear-time approximation algorithms and property testing algorithms. Our main focus is on graph problems, though also some related combinatorial problems will be considered."
	},
	{
		"grant":118,
		"ID": "EP/G066159/1",
		"Title": "Digital coherent receivers for 100GbE optical transmission",
		"PIID": "126618",
		"Scheme": "First Grant Scheme",
		"StartDate": "28/10/2009",
		"EndDate": "27/04/2013",
		"Value": "297312",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "126618", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Bookham Technology Plc"}
		],
		"Summary": "The optical fibre core network underpins the internet and the digital economy, with the present capacity of today's core networks being limited to ~ 1Tbit/s per fibre. While in current networks, the limited broadband data rates afforded by the copper based access network prevents the optical core network from being stretched to capacity, as optical fibre permeates the access network, the bottleneck will move from the access network to the core network. To overcome these limitations and to maximise the opportunities afforded by a fibre optic access network will require the capacity of the installed core network to be increased, either by increasing the number of wavelengths used or by increasing the data rate per wavelength. The proposed research aims to combine both techniques simultaneously - transmitting 100 gigabit Ethernet (GbE) on each wavelength, while employing wavelength division multiplexing (WDM) to increase the capacity of the core network to beyond 10Tbit/s.Using conventional intensity modulation schemes, much of the installed fibre base is unable to support data rates faster than 10Gbit/s due to imperfections in the installed fibre which causes pulse spreading. Current research at UCL, led by the principal investigator (PI), has recently experimentally demonstrated the potential of digital signal processing (DSP) combined with coherent detection of spectrally efficient modulation formats to overcome these limitations for 40Gbit/s transmission systems, with the same principles being equally applicable to 100GbE systems. In a digital coherent receiver the four components of the optical field, the in-phase and quadrature components of the two polarisations, are mapped into the electrical domain. This allows digital compensation of transmission impairments and the use of spectrally efficient four-dimensional modulation formats. Given the huge investment which has been made into installing the fibre base infrastructure, the ultimate aim of the research is to determine how this four-dimensional modulation space can be used in conjunction with DSP to maximise the capacity of the installed fibre.The proposed research combines fundamental theoretical research with a determinedly experimental research program into the nonlinear transmission of four-dimensional modulation formats at 100Gbit/s+ and beyond. The initial workpackage will investigate both experimentally and theoretically quadrature amplitude modulation, in combination with polarisation division multiplexing as a four dimensional modulation scheme for 100GbE transmission systems. Within this first workpackage, the system under investigation will be receiver centric, such that all of the DSP, both linear and nonlinear, is based at the receiver. In the second workpackage this assumption will be relaxed and combined transmitter and receiver DSP will be investigated, both experimentally and through simulation. The third and final workpackage which is a theoretical study, will draw on the conclusions of the previous workpackages, and will aim to answer the question  Given the optical fibre is dispersive and nonlinear, what is the optimal modulation scheme which enables the capacity of the core network to be maximised assuming we are able to employ appropriate digital signal processing?"
	},
	{
		"grant":119,
		"ID": "EP/G066361/1",
		"Title": "Reliable cell design methods for variable processes (RelCel)",
		"PIID": "22017",
		"Scheme": "Standard Research",
		"StartDate": "15/09/2009",
		"EndDate": "14/09/2012",
		"Value": "248293",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "22017", "Role": "Principal Investigator"},
		{"ID": "13747", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Silistix Ltd"}
		],
		"Summary": "As the fabrication dimensions of devices have reduced, the variability of their electrical parameters has increased.  This effect can lead to circuit failure, particularly for low-power  circuits designed in very small geometry processes.  This problem may result in the potential of processes with 45 nm, 22 nm Leff and below not being fully realised.  Current cell designs, such as those for SRAM cells have failure rates for the random fluctuations envisaged at 22nm that make large SRAM arrays unviable, and other cells have timing variations which force the clock rate to be unacceptably slow.  This three-year proposal aims to develop techniques to develop robust cell circuits, and to develop design tools and techniques for circuits which will deliver the reliability and performance necessary in cell libraries for future multi-billion-transistor SoC.The main goals of the project are: (1) Evaluate circuit design tools for cell design in the face of severe and random process fluctuations,  (2) Show how these tools can be improved to assist circuit designers achieve higher productivity, (3) Design more robust integrated circuits capable of good performance in nanometre technologies, with low supply voltages and large parameter fluctuations, (4) Derive general principles for robust cell design in nanometre processes, and to evaluate them in a realistic demonstrator"
	},
	{
		"grant":120,
		"ID": "EP/G066604/1",
		"Title": "Approximation and mixing times in the ferromagnetic Potts model",
		"PIID": "-105665",
		"Scheme": "First Grant Scheme",
		"StartDate": "11/01/2010",
		"EndDate": "01/04/2013",
		"Value": "250380",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-105665", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Potts model was introduced in 1952 as a model of magnetism.  The Potts model has been extensively studied not only in statistical physics, but also in computer science, mathematics and further afield. In physics the main interest is in studying phase transitions and modelling the evolution of non-equilibrium particle systems. In computer science, the Potts model is a test-bed for approximation algorithms and techniques. It has also been heavily studied in the areas of discrete mathematics and graph theory, through an equivalence to the Tutte polynomial of a graph, and thereby links to the chromatic polynomial and many other graph invariants. The Potts model and its extensions have also appeared many times in the social sciences, for example in modelling financial markets and modelling voter interaction in social networks.In simple terms, a magnet is regarded as a large number of atoms arranged in a grid. These atoms oscillate randomly and are more likely to align themselves with their immediate neighbours than to orientate themselves differently. Under some circumstances all the atoms quickly become aligned uniformly. Under other circumstances a mixed state, in which blocks of atoms are orientated differently, persists for much longer. We are interested in exactly what aspects of the circumstances are key to determining which behaviour occurs. This project is concerned with understanding  the speed of convergence of alignment to a steady state, and with computing the probability of a given configuration arising. The latter problem is hard, in a rigorous sense, and so the focus of effort is on approximation methods. We will develop approximation algorithms for this problem and study when approximations are or are not possible under standard complexity theoretic assumptions."
	},
	{
		"grant":121,
		"ID": "EP/G068917/1",
		"Title": "Categorical Foundations for Indexed Programming",
		"PIID": "-219232",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2012",
		"Value": "281879",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer and Information Sciences",
		"OrgID": "48",
		"Investigators":[
		{"ID": "-219232", "Role": "Principal Investigator"},
		{"ID": "69411", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Type systems have become an integral part of programming language design and implementation, leading to fundamental contributions in such areas as security, databases, and concurrent and distributed programming. Type systems provide information that can be used for such tasks as error detection, program optimisation, and memory management. A type system governs a programming language's classification of values and expressions into data types, which determines how data of those types can be manipulated and how those data can interact. Originally, type systems contained only simple data types like Int and Char for classifying basic data such as integers and characters. However, advanced type systems support sophisticated types that can guarantee not only that well-typed programs are free of simple errors, such as trying to add non-numeric expressions, but also that they preserve invariants, do not violate space or other constraints, can be optimised in principled ways, or satisfy other sophisticated correctness properties. Much of the effort in type systems research is aimed at closing the semantic gap between what programmers know about computational entities and what types can express about them.One of the most promising approaches to increasing the expressiveness and reasoning power of type systems is to allow types to be indexed by other information. For example, rather than simply having a type List denoting the list data type, list types can be indexed by additional information that can be used to detect more errors, perform more optimisations, and provide greater guarantees of correctness than would otherwise be possible. The extra information in the indices thus helps close the aforementioned semantic gap.Indexed programming is the practice of programming with indexed types.  Perhaps the most common form of indexing is indexing types by other types. To model lists of integers or lists of booleans or lists of lists of data of type t, for example, type-indexed types such as List Int or List Bool or List (List t) can be used. More recently, programming languages have been developed which allow types to be indexed not just by other types, but also by terms. For example, to model lists of length 3 or lists that a particular proof p proves are sorted, term-indexed types such as List 3 or List p can be used. The practice of indexed programming has now advanced to the stage where principled foundations are required to take the subject further. The aim of the proposed research is to develop a categorical foundation of indexed programming. That is, it aims to improve the understanding of, and the ability to solve specific problems involving traditional term-indexed and type-indexed types, and to provide a theory of indexed programming which is general enough to both describe type- and term-indexed programming and prescribe approaches to programming with more general forms of indexing. This will be achieved by considering specific problems in indexed programming (WP1 through WP4), as well as by developing a foundation for indexed programming based upon the categorical notion of a fibration (WP5). The application of categorical methods to functional programming has proven to be hugely successful over the years, so there is good reason to believe that our categorical methodology is appropriate for the proposed research."
	},
	{
		"grant":122,
		"ID": "EP/G069239/1",
		"Title": "Efficient Decentralised Approaches in Algorithmic Game Theory",
		"PIID": "59559",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/12/2012",
		"Value": "398279",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "59559", "Role": "Principal Investigator"},
		{"ID": "49970", "Role": "Co Investigator"},
		{"ID": "-194260", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Game theory is the mathematical analysis of competitive (more generally, non-cooperative) situations, and is frequently studied in the context of mathematics and economics. A  solution concept  (such as Nash equilibrium, or correlated equilibrium) is a mathematical description of the outcome of a game -- the set of choices made by all players, or participants. Many algorithms have been proposed (and implemented as programs) to compute these solutions.Work in Computer Science has attracted considerable interest from the Game Theory community. Computer scientists have developed analytical tools to understand the speed and efficiency of a computational process, and the inherent difficulty of computational problems. This provides new criteria for judging alternative solution concepts in Game Theory, and the algorithms that have been proposed to find them. The field of Distributed Computation has provided useful mathematical models of the interaction amongst the players in a game. This cross-fertilisation is very much a two-way process, since at the same time, computer scientists have applied game-theoretic concepts to model large-scale interactions on the Internet, electronic marketplaces, and interactions amongst computational agents (in the context of AI).So far, many of the algorithms that have been proposed to compute the outcomes of games, have the drawback that the algorithm essentially coordinates the behaviour of the players in a centralised manner.  Decentralisation  refers to the development of algorithms that are used by multiple players (or agents) in the absence of any central control. The development of these algorithms requires models of precisely how the players interact (and various models have been proposed for this purpose). This research agenda gives rise to a large number of important and interesting problems, which essentially ask: What assumptions have to be made about how players interact, in order for solutions to be found rapidly? This research aims to develop novel algorithms that find solutions efficiently, and in a decentralised manner. We also seek a better understanding of the general limitations on what can be achieved by provably efficient algorithms (characterised mathematically as algorithms that run in polynomial time).Generally, we are interested in solutions that can be found by algorithms that are both efficient and decentralised. Solutions that can be found by such algorithms are arguably more realistic than other solution concepts. The research outlined in this proposal will apply to games that model large-scale interactions amongst many players, as well as standard normal-form games."
	},
	{
		"grant":123,
		"ID": "EP/G069557/1",
		"Title": "FRESNEL: FedeRatEd Secure sensor NEtwork Laboratory",
		"PIID": "93129",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2012",
		"Value": "610072",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "93129", "Role": "Principal Investigator"},
		{"ID": "3618", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "British Telecommunications Plc"},
		{"Number": "1", "Name": "Microsoft Research Ltd"},
		{"Number": "2", "Name": "Sensors and Instrumentation KTN"},
		{"Number": "3", "Name": "Symbian Software Ltd"},
		{"Number": "4", "Name": "TRW Conekt"}
		],
		"Summary": "Wireless sensor networks are more and more seen as a solution to large-scale tracking and monitoring applications. The deployment and management of these networks, however, is handled by a central controlling entity and the sensor network is often dedicated to a single application. We argue that this is due to the fact that we do not yet have the means to deal with a secure multi-purpose federated sensor network, running different applications in parallel and able to reconfigure dynamically to run others.The communication paradigms which have been usually devised for small and single owner sensor networks simply do not have the right scalability, security, reconfigurability characteristics required for this environment.With FRESNEL we aim to build a large scale federated sensor network framework with multiple applications sharing the same resources. We want to guarantee a reliable intra-application communication as well as a scalable and distributed management infrastructure. Orthogonally, privacy and application security should also be maintained.We evaluate our proposal though a large scale federation of sensor networks over the Cambridge campus. The sensors monitor different aspects (temperature, pollution, movement, etc) and the network will be running various applications belonging to different authorities in the city."
	},
	{
		"grant":124,
		"ID": "EP/G069727/1",
		"Title": "Model Checking Timed Systems with Restricted Resources: Algorithms and Complexity",
		"PIID": "-145660",
		"Scheme": "First Grant Scheme",
		"StartDate": "10/01/2010",
		"EndDate": "09/09/2013",
		"Value": "212217",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-145660", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computer software and hardware systems are among the most complexartifacts created by humans, thus it is not surprising that they often suffercostly or catastrophic failures due to errors in design.  In 2002 a study by the US National Institute of Standard and Technology estimated that software failures alone cost the US economy 60 billion dollars per year.  Against this background it is increasingly recognized that model checking---an approach to formally verifying the correctness of software and hardware systems---has an important role to play in meeting the challenge of producing correctly functioning systems.  Intel, Lucent, Microsoft, Motorola, and NASA, among many others, already use model checking as part of their quality assurance process.  In a nutshell, model checking involves constructing a mathematicalmodel of a given system and then checking, automatically orsemi-automatically, that the model meets a given formal specification.One of the main challenges of this task is the so-called stateexplosion problem.  For example, a 10 mega-byte cache has10^(20,000,000) states.  The challenge presented by the stateexplosion problem has spurred the development of a rich body oftechniques, incorporating ideas from automata theory, artificialintelligence, combinatorial optimization, game theory, graph theoryand mathematical logic.  In 2007 Clarke, Emerson and Sifakis wereawarded a Turing award (the Computer Science equivalent of a Nobleprize) for their pioneering work in model checking.In this project we are concerned in particular with real-time systems,such as hardware, controllers and embedded systems.  The correctnessor acceptability of such systems can depend on real-time constraints,e.g., the response time of an anti-lock braking system or the latencyin video transmission.  The state explosion problem is particularlyacute for real-time systems--indeed they are essentiallyinfinite-state systems.  As a consequence, in real-time model checkingone must take great care in designing the modelling and specificationformalisms.  Apparently minor variations in these can lead to drasticchanges in the tractability of model checking.The aim of this project is to identify modelling and specification formalisms that can express the type of system requirements described above, that also permit model checking algorithms that have reasonable complexity.  An important outcome of this project will be algorithms and tools for modelchecking real-time systems.  Such algorithms will employ novel combinatorial and automata-theoretic ideas, and will use symbolic techniques to permit exhaustive search of infinite state spaces.  Another outcome of this project will be to enhance understanding of the use of temporal logics for reasoning about real-time behaviours, building on the highly successful use of temporal logics for discrete-time systems."
	},
	{
		"grant":125,
		"ID": "EP/G069840/1",
		"Title": "Scaling up Statistical Spoken Dialogue Systems for real user goals using automatic belief state compression",
		"PIID": "54937",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/12/2012",
		"Value": "297343",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40",
		"Investigators":[
		{"ID": "54937", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Spoken dialogue systems (SDS) are increasingly being deployed in avariety of commercial applications ranging from traditional CallCentre automation (e.g. travel information) to new ``troubleshooting''or customer self-service lines (e.g. help fixing broken internetconnections).SDS are notoriously fragile (especially to speech recognition errors),do not offer natural ease of use, and do not adapt to differentusers. One of the main problems for SDS is to maintain an accurateview of the user's goals in the conversation (e.g.  find a good indianrestaurant nearby, or repair a broadband connection) underuncertainty, and thereby to compute the optimal next system dialogueaction (e.g. offer a restaurant, ask for clarification).  Recentresearch in statistical spoken dialogue systems (SSDS) hassuccessfully addressed aspects of these problems but, we shall show,it is currently hamstrung by an impoverished representation of usergoals, which has been adopted to enable tractable learning withstandard techniques.In the field as a whole, currently only small and unrealistic dialogueproblems (usually less than 100 searchable entities) are tackled withstatistical learning methods, for reasons of computationaltractability.In addition, current user goal state approximations in SSDS make itimpossible to represent some plausible user goals, e.g. someone whowants to know about nearby cheap restaurants and high-quality onesfurther away.  This renders dialogue management sub-optimal and makesit impossible to deal adequately with the following types of userutterance: ``I'm looking for french or italian food'' and ``NotItalian, unless it's expensive''. User utterances with negations anddisjunctions of various sorts are very natural, and exploit the fullpower of natural language input, but current SSDS are unable toprocess them adequately.  Moreover, much work in dialogue systemevaluation shows that real user goals are generally sets of items withdifferent features, rather than a single item. People like to explorepossible trade offs between features of items.Our main proposal is therefore to:    a) develop realistic large-scale SSDS with an accurate, extended    representation of user goals, and    b) to use new Automatic Belief Compression (ABC) techniques to    plan over the large state spaces thus generated.Techniques such as Value-Directed Compression demonstrate thatcompressible structure can be found automatically in the SSDS domain(for example compressing a test problem of 433 states to 31 basisfunctions).These techniques have their roots in methods for handling the largestate spaces required for robust robot navigation in realenvironments, and may lead to breakthroughs in the development ofrobust, efficient, and natural human-computer dialogue systems, withthe potential to radically improve the state-of-the-art in dialoguemanagement."
	},
	{
		"grant":126,
		"ID": "EP/G069875/1",
		"Title": "Formal Methods and Cryptography: The Next Generation of Abstractions (CryptoForma)",
		"PIID": "56494",
		"Scheme": "Network",
		"StartDate": "01/05/2009",
		"EndDate": "31/10/2012",
		"Value": "73820",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "27",
		"Investigators":[
		{"ID": "56494", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hewlett Packard Ltd"},
		{"Number": "1", "Name": "Microsoft Research Ltd"}
		],
		"Summary": "The purpose of CryptoForma is to build an expanding network in computer science and mathematics to support the development of formal notations, methods and techniques for modelling and analysing modern cryptographic protocols. This work increases security and confidence in such protocols and their applications (e.g. in e-commerce and voting), to the benefit of protocol designers, businesses, governments, and application users.The United Kingdom has traditionally been a world leader in research into, and applications of, formal methods. Work in the UK on the foundations of languages and notations such as CSP, pi-calculus, and Z have led to their widespread use in critical systems development in industry.The UK is also well known to contain leading experts in cryptography. In the last two decades we have done important work in stream cipher design, symmetric ciphers (differential and algebraic cryptanalysis), elliptic curve cryptography, identity based cryptography and in protocols (e.g. in the mobile phone industry).In the 1990s in the UK, formal methods were successfully applied to the modelling and verification of security protocols at a high level of abstraction. However, modern cryptographic protocols contain probabilistic and complexity theoretic aspects which require a different set of abstractions. Several approaches for dealing with this have appeared since, including: automated proof-checking; compositional techniques; higher level proof structures; abstractions of computational models; and specialised logics.The network aims to bring together research groups working in the UK in these areas, starting with 7 sites and expanding rapidly from that.It will allow a systematic and effective cross-fertilisation between the differing strands of work. The consortium contains mathematicians and computer scientists, experts on cryptography, on formal methods, and on their interconnection, and developers of practical cryptographic protocols both from academia and from industry. A group with such a very broad spectrum of expertise will enable both(a) informing/strengthening practical developments by solid mathematical analysis, and (b) motivating foundational analysis by practice-based needs and requirements.To do so the network will organise meetings around both fundamental and more directly applicable issues, such as:- adequate abstractions of cryptographic primitives;- specialised specification notations with notions of probability, timing, and complexity;- abstract concepts and logics that allow the expression of security properties and reasoning about them;- practical protocols, e.g. e-voting, trust management, and those involving zero-knowledge proofs and commitments which formal methods cannot currently deal with.With these interests and questions in mind, the overall aims of CryptoForma are to:- bring together academics and industrialists interested in the application of formal methods to the specification, development and verification of cryptographic protocols;- stimulate collaboration between individuals and groups in order to tackle the questions highlighted above;- disseminate problems and results to researchers and practitioners in the field and to the wider communities in cryptography and formal methods.To achieve this research meetings and workshops are planned which will foster collaboration and disseminate key results.This proposal is timely and important because- society has recently experienced a reduced confidence in protection of electronic data, and in applications such ase-commerce and e-voting;- a critical mass of researchers in this area exists in the UK but this is not being exploited;- the subject is high on the agenda of research institutions, industries and academics world-wide. This is evidenced by the letters of support and the enthusiasm for the preliminary meeting of the network in January 2009 where all partners and 10 others will meet."
	},
	{
		"grant":127,
		"ID": "EP/H000488/1",
		"Title": "Spin Ping Pong - Towards a Quantum Dot spin QuBit",
		"PIID": "11377",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2013",
		"Value": "901314",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "11377", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal aims to fabricate a solid state qubit, the quantum computing analog of a transistor. The method employs epitaxially grown quantum dot bilayers (artificial molecules). These can be doped to accommodate one or more electrons which will have a particular spin (up or down). By using an applied electric field the electron can be forced to reside in one dot or the other. This is the basis of a  Spin Ping-Pong  device. Using circularly polarised light pulses each dot can be addressed individually and spin information written to it. The spin information can be read using linearly polarised light pulses. Doping with two electrons and using the applied electric field to force an electron into each dot allows the spins to interact and perform quantum computation. Such a device with the promise of scalability will be a major step forward in the field of quantum information."
	},
	{
		"grant":128,
		"ID": "EP/H000666/1",
		"Title": "Submodular optimization, lattice theory and maximum constraint satisfaction problems",
		"PIID": "83531",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2010",
		"EndDate": "31/03/2014",
		"Value": "297257",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "83531", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Sub- and supermodular functions are special functions defined on the powerset of a set. Such functions are a key concept in combinatorial optimization, and they have numerous applications elsewhere. The problem, SFM, of minimizing a given submodular function is one of the most important tractable optimization problems. Our first goal is to investigate algorithmic aspects of the SFM generalized to arbitrary finite lattices rather than just families of subsets (thus representing order different from that of inclusion). In our setting, the classical SFM would correspond to the simplest non-trivial case of the two-element lattice. We intend to find a new wide natural class of tractable optimization problems.Recently, a strong connection was discovered between the properties of sub- and supermodularity on lattices and tractability of the so-called maximum constraint satisfaction problems (Max CSP), which are very actively studied problems in computer science and artificial intelligence. In a Max CSP, one is given a collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to find an assignment of values from a fixed domain to the variables with a maximum number (or total weight) of satisfied constraints. We intend to investigate the full extent of this connection. We will also consider an extension of the Max CSP framework to valued, or soft, constraints that deal with desirability rather than just feasibility, and hence define a more general optimization problem. Thus, our second goal is to understand the reasons for tractability within a wide class of (generally hard) combinatorial optimization problems."
	},
	{
		"grant":129,
		"ID": "EP/H000968/1",
		"Title": "Towards More Effective Computational Search",
		"PIID": "54639",
		"Scheme": "Platform Grants",
		"StartDate": "22/02/2010",
		"EndDate": "21/02/2015",
		"Value": "1010976",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "36435", "Role": "Principal Investigator"},
		{"ID": "68066", "Role": "Principal Investigator"},
		{"ID": "54639", "Role": "Principal Investigator"},
		{"ID": "110493", "Role": "Co Investigator"},
		{"ID": "-15303", "Role": "Co Investigator"},
		{"ID": "76112", "Role": "Co Investigator"},
		{"ID": "121261", "Role": "Co Investigator"},
		{"ID": "-216990", "Role": "Co Investigator"},
		{"ID": "68066", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The ASAP group has set the international research agenda in exploring the development of computational systems that can automatically build decision support systems. The group addresses a broad range of scientifically challenging problems, many of which are drawn from the real world where the complexities of the problem have not been abstracted away in order to make the problem easier to model/solve.The group's key research goals include:- Automating the Heuristic Design Process: We lead the international community in hyper-heuristics (heuristics to choose heuristics) research, with the aim being to investigate the extent to which we can replace human decision making by computer algorithms.- Closing the gap between industrial and real world issues and academic practice: We aim to explore dynamic and complex computational modeling and intelligent decision support within the context of real world problems such as aircraft scheduling, timetabling, manufacturing, bioinformatics, production scheduling and healthcare rostering. ASAP aims to establish new decision support methodologies that explore the use of automated search methodologies and the complexity that they are able to handle.- Closing the gap between theoretical understanding and the construction of search methodologies:  We aim to theoretically analyse complex real world scenarios with a view to deepening our understanding of search methodology development.  The state of the art in theoretical study in this field tends to deal with models that are too simple to be placed into real world practice.  We aim to study the theory of real world applications.Our core research on modeling and search methodologies has redefined the inter-disciplinary interface between Computer Science and Operational Research, while our grounding in diverse applications involves dialogue with many other disciplines spanning biomedical science (new computational methodologies in bioinformatics, systems and synthetic biology as well as in nanoscience) through to the built environment (search methodologies for office space allocation).  In this renewal proposal to our current Platform award, we are requesting support for 132 months of research assistant funding (at varying levels of seniority), over a five year period. This would enable us to conduct (and continue) a programme of transformative and innovative research that is not only high risk and high return, but which also has a clear multi-disciplinary and industrial focus.A Platform award would enable the ASAP group to retain key personnel at the interface of Computer Science and Operational Research.  The potential benefits in providing the grounding for tomorrow's decision support systems could be far reaching in laying the foundations for more efficient, effective, cheaper, easier-to-implement and easier-to-use systems across many industries and sectors."
	},
	{
		"grant":130,
		"ID": "EP/H003304/1",
		"Title": "GNSS scintillation: detection, forecasting and mitigation",
		"PIID": "77266",
		"Scheme": "Standard Research",
		"StartDate": "15/03/2010",
		"EndDate": "14/03/2014",
		"Value": "434107",
		"ResearchArea": "Mobile Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "7",
		"Investigators":[
		{"ID": "77266", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Chronos Technology Ltd"},
		{"Number": "1", "Name": "Fugro Intersite"},
		{"Number": "2", "Name": "Septentrio"},
		{"Number": "3", "Name": "SPIRENT COMMUNICATIONS"}
		],
		"Summary": "Although GNSS systems now underpin a significant part of modern infrastructure, such as financial markets, telecoms, power generation and distribution as well as transport and emergency services, they suffer from a number of known vulnerabilities. One such shortcoming relates to an ionospheric disturbance known as scintillation. The phenomenon of scintillation is familiar to most people through the twinkling of star light as it crosses the atmosphere.  Ionospheric scintillation causes amplitude and phase variations on signals from GNSS satellites when they cross the ionised upper atmosphere (the ionosphere). Currently, GNSS receivers are not robust against radio scintillation; effects range from degradation of positioning accuracy to the complete loss of signal tracking. During scintillation events, required levels of accuracy and continuity, as well as availability, may not be met, thus compromising commercial operations, such as maritime navigation, geophysical exploration and airplane navigation during airport precision approach. The project will quantify the problem of ionospheric scintillation over the forthcoming solar maximum (2010-2013) and develop algorithms to reduce the impact on the users. The research will lead to improved GNSS receiver design that will enable robust performance of receivers that are compromised by effects of the natural environment through ionospheric scintillation."
	},
	{
		"grant":131,
		"ID": "EP/H004092/1",
		"Title": "A Constraint Solver Synthesiser",
		"PIID": "107856",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2014",
		"Value": "929076",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "119",
		"Investigators":[
		{"ID": "107856", "Role": "Principal Investigator"},
		{"ID": "33069", "Role": "Co Investigator"},
		{"ID": "75756", "Role": "Co Investigator"},
		{"ID": "68891", "Role": "Co Investigator"},
		{"ID": "90489", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Constraints are a natural, powerful means of representing and reasoning about combinatorial problems that impact all of our lives. For example, in the production of a university timetable many constraints occur, such as: the maths lecture theatre has a capacity of 100 students; art history lectures require a venue with a slide projector; no student can attend two lectures at once. Constraint solving offers a means by which solutions to such problems can be found automatically. Its simplicity and generality are fundamental to its successful application in a wide variety of disciplines, such as: scheduling; industrial design; aviation; banking; combinatorial mathematics; and the petrochemical and steel industries, to name but a few examples.Currently, applying constraint technology to a large, complex problem requires significant manual tuning by an expert. Such experts are rare. The central aim of this project is to improve dramatically the scalability of constraint technology, while simultaneously removing its reliance on manual tuning by an expert.  We propose a novel, elegant means to achieve this: a constraint solver synthesiser, which generates a constraint solver specialised to a given problem. Synthesising a constraint solver tailored to the needs of an individual problem is a groundbreaking direction for constraints research, which has focused on the incremental improvement of general-purpose solvers. Synthesising a solver from scratch has two key benefits, both of which will have a major impact. First, it will enable a fine-grained optimisation not possible for a general solver, allowing the solution of much larger, more difficult problems. Second, it will open up many exciting research possibilities. There are many techniques in the literature that, although effective in a limited number of cases, are not suitable for general use. Hence, they are omitted from current general solvers and remain relatively undeveloped. The synthesiser will, however, select such techniques as they are appropriate for an input problem, creating novel combinations to produce powerful new solvers. The result will be a dramatic increase in the number of practical problems solvable without the input of a constraints expert."
	},
	{
		"grant":132,
		"ID": "EP/H004289/1",
		"Title": "EXTRAMS: Exploiting Traditional Map Signage with Mobile Devices",
		"PIID": "90168",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/12/2012",
		"Value": "83549",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "90168", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "There is an interesting contrast between mobile maps, e.g. Google maps, Nokia maps, etc. and traditional You-Are-Here (YAH) maps - the kind that appear situated at particular places, e.g. a trailhead for walkers, a town centre or a university campus and explicitly indicate the position of the map reader. One difference between the two is that YAH maps are very much designed for the anticipated task that is likely to occur at the place where they are situated. For example, the map at a trailhead will typically not be simply a reproduction of an Ordinance Survey map of the local area but a representation that increases the salience of the trails and landmarks that may be more relevant to the walker. The YAH map is still likely to be 'Northed' but may be more sketch-like or 'low fidelity' in its design and certain liberties may have been taken with the map's scale. Indeed, even the choice of what area to cover on the YAH map is something that will have been considered by its designer based on the anticipated requirements of the map reader at the site of the map's location.It is not difficult to imagine scenarios in which a hiker arrives at a YAH map at a trailhead, and attempts to make a mental sketch of the map or even a physical sketch. Of course, the walker may or may not be carrying the relevant OS map (and one would certainly hope so if a serious walk was being considered) or have access to a separate GPS device or even a mobile maps application running on her personal GPS-enabled mobile phone. But the image on the YAH map will still be a useful one for the walker to carry away to assist her navigation.One way in which the hiker can 'walk away' with this YAH map is to 'capture' the YAH map by taking a photo of it with her camera phone and then refer to this image en route, the map image effectively remaining as a static artefact. Another approach, however, would be to utilise the context of the capture (i.e. the latitude and longitude coordinates of the location of the YAH map obtained from the mobile phone's built-in GPS) in order to transform the image into a georeferened image. This would effectively produce a mobile map but one based on the YAH map design and so perhaps more useful for the hiker's task at hand than the generic map that would appear if the hiker performed a download using Google or Nokia maps.The central aim of this research is to explore this new paradigm, to understand the way in which mobile phones can provide a useful and usable facility by supporting the capture, georeferencing and display of traditional YAH map signage and the Human-Computer Interaction (HCI) issues associated with this interaction."
	},
	{
		"grant":133,
		"ID": "EP/H004602/1",
		"Title": "Ultra energy efficient III-nitride/polymer hybrid white LEDs using nanotechnology",
		"PIID": "111741",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "31/03/2014",
		"Value": "389462",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "111741", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The last decade has seen dramatic advances in the development of III-nitride light emitters, whose emergence is significantly changing many aspects of our lives. Materials from the III-nitride family can emit light over the complete visible spectrum and a major part of the ultraviolet (UV) and are ideally suited for white light sources. Developments in solid-state lighting are occurring at pace and will lead to near-ultimate lighting sources, likely to be based on III-nitride materials. This will result in a fundamental change in the concept of illumination and has the potential to lead to massive savings in energy, estimated to be equivalent to $112 billion by the year 2020. Such increases in efficiency are increasingly important due to the growing world-wide energy-crisis and threat of global warming.Currently, there are three main approaches for the fabrication of white light emitting diodes (LEDs) needed for solid-state lighting: (1) a package of three LED chips each emitting at a different wavelength (red, green and blue, respectively); (2) a combination of a blue (460 nm) LED with a yellow phosphor pumped by blue light from the LED; (3) a single chip emitting UV light which is absorbed by three phosphors (red, green and blue) and reemitted as a broad spectrum of white light. The first method is ideal for achieving a true white light source, but it is extremely difficult to balance the electro-luminescence intensities of these different colours and there exist a number of fundamental challenges related to the different requirements of the individual LEDs. The performance of current UV-LEDs is far below blue-LEDs and presents a major limitation to the third route. As a result the  blue LED+ phosphor  approach is maintaining its strong lead for the fabrication of white LEDs with several commercial successes. However, the most promising commercially available white LEDs are based on blue epiwafers with the highest crystal quality, which are thus extremely expensive. This raises the price and thus limits their applications in general illumination. Further development of the technology is also still faced with problems, which are the driving force behind the new type of LED proposed here. We aim to develop a hybrid nanotechnology delivering a new type of ultra high energy efficient white-LED without need for the premium price blue epiwafers. The technologies to be hybridised are arrays of semiconductor nano-rods, with dimensions on the scale of 100s of nanometres, metal nano-particles and polymers. Building on our work demonstrating efficient light emission from conjugated polymers we will optimise these materials for converting blue light to yellow. The new polymers will be used to fill the spaces between nanorods prepared in GaN-based LED structures, maximising the contact between the blue light source and the yellow emitter. Blending silver nano-particles into the polymer will be used to further improve optical performance as a result of a coupling effect between the metal and semiconductor. Relatively thick capping layers usually limit the strength of this effect but the close contact between the polymer/metal blend and the side-walls of the nano-rods enables us to get close to the full benefit.This work will combine the existing strengths at the Sheffield in III-nitride device fabrication and characterisation of III-nitride emitters with those at Strathclyde in polymer chemistry, fundamental optical studies of III-nitrides and characterization of nanostructures, including metal nanoparticles and semiconductors. This combined effort aims to achieve an improved understanding of the fundamental issues in the optical emission processes mentioned above and to optimise fabrication processes of hybrid III-nitride/polymer white LEDs. It will lead to the demonstration of next generation white-LEDs suitable for replacement of conventional light sources in terms of cost and luminous efficacy."
	},
	{
		"grant":134,
		"ID": "EP/H004793/1",
		"Title": "Cross Layer Techniques for Intrusion Tolerant Networks",
		"PIID": "36060",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2012",
		"Value": "204936",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10",
		"Investigators":[
		{"ID": "36060", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Rohde and Schwarz Ltd"},
		{"Number": "1", "Name": "SAP AG"},
		{"Number": "2", "Name": "Traffic Observation & Management TOM"}
		],
		"Summary": "The proliferation of wireless networks over the past decade has made security a major concern for these networks and the applications that have to use them. Wireless networks have fundamental characteristics that make them significantly different from traditional wired networks, particularly with regard to security and reliability. Moreover there is an increasing trend for access to services residing on fixed (e.g. enterprise) networks via wireless access. Therefore the design of secure and reliable wireless networks presents a major challenge to the designers of next generation networks with general public wireless access. It is also expected that many future networks will have to live under the threat of attacks as a matter of course. Current research attempts to secure networks against all types of attack, at all times and generally irrespective of the cost to the performance of the network. This research proposal aims to investigate a new type of integrated, flexible, and intelligent security architecture for providing tolerance to intrusion attacks against next generation networks with wireless access. Thus our goal is not to prevent intrusions but to enable network architectures to withstand them. Central to the work will be the design of a distributed Intrusion tolerance system that is based on a cross layer detection and mitigation approach. As such, intrusion detection and mitigation will be integrated within the layered architecture of the network so that the network has an intelligent view of the overall level of threat(s) posed at any time throughout the network. This approach brings a number of significant advantages over existing intrusion detection systems (IDS) particularly when applied to wireless access networks that have to withstand some level of attacks over prolonged periods."
	},
	{
		"grant":135,
		"ID": "EP/H004963/1",
		"Title": "NOVEL METHOD FOR OPTICAL COHERENCE TOMOGRAPHY AND MULTIPLEXED SENSING",
		"PIID": "75997",
		"Scheme": "Standard Research",
		"StartDate": "15/11/2009",
		"EndDate": "14/11/2012",
		"Value": "394158",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Physical Sciences",
		"OrgID": "27",
		"Investigators":[
		{"ID": "75997", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Optical coherence tomography (OCT) allows high depth resolution imaging and has received considerable interest in the last 5 years, which resulted in a 100 times increase in the speed of operation and extension to a large variety of imaging problems. There are two main types of OCT: time domain (TD) and spectral domain (SD). The recent progress in speed has been achieved by developers working in the SD-OCT field. However, SD-OCT has several disadvantages, non present in TD-OCT. The research in SD-OCT has also stimulated the development of novel fast tunable lasers, the so called swept sources (SS). Tremendous effort has been put in understanding and controlling the phenomena in closed optical loops containing optical amplifiers for optical sources to be used in SS-OCT. The present proposal puts forward a novel type of OCT and a novel class of optical interferometers. This is driven by the needs to address the limitations in terms of speed of the TD-OCT method, by the potential of the en-face OCT, not investigated so far, in producing 3D OCT exploration and by the limitations in terms of range and dynamic focus of the SD-OCT methods as well as dispersion compensation of all OCT methods. The novel method and devices proposed are inspired by the progress in swept sources and SS-OCT. A new class of OCT systems is researched, as a marriage between the TD-OCT and SD-OCT methods. The new OCT method eliminates or ameliorate the disadvantages of the SD-OCT method. The novel method presents the generality of being compatible with both TD-OCT and SD-OCT. The applicant has demonstrated the principle of operation which guarantees the success of research. However, the marriage between the SD-OCT and TD-OCT to be researched here promises much more than what the demonstrator set-up has shown so far, therefore research support is sought to explore the multiple avenues of the novel method. This may revolutionise the field of OCT and open applications not currently possible with the present OCT technology, such as SD-OCT of objects extended in depth by more than 1 cm (eye), or en-face imaging of moving organs or tissue (of the whole retina volume in fractions of a second, with better sampling in all directions than achievable with the most advanced SD-OCT set-up). The method to be researched will allow combination of scanning regimes and modes of operation to achieve versatile functionality in measurements, in the 3D imaging of moving tissue such as the eye, heart, or moving embryos or functional/low noise imaging by making use of angular compounding or polarisation. Novel directions are opened in the instantaneous elimination of the movement effects of the tissue when determining the liquid (blood) flow profile in a vessel, in tracking the axial position of objects (cornea or retina), automatic dispersion compensation as well as improvement in the synchronism between the coherence gate and the focus in axial scanning (a problem for both types of SD-OCT methods). The method proposed is also applicable in multiplexing of sensors. Simultaneous measurements over multiple path lengths become feasible. Three proof of concept demonstrators will be assembled, for multiple imaging at different depths using en-face TD-OCT, fast and long range tracking and swept source systems for long axial scanning."
	},
	{
		"grant":136,
		"ID": "EP/H005412/1",
		"Title": "High-power planar waveguide visible-lasers.",
		"PIID": "110324",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2010",
		"EndDate": "31/12/2013",
		"Value": "427649",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "110324", "Role": "Principal Investigator"},
		{"ID": "59852", "Role": "Co Investigator"},
		{"ID": "59078", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Laser Quantum Ltd"}
		],
		"Summary": "Lasers have become ubiquitous in our daily lives, underpinning much of our technology dependent society. No longer  a solution looking for a problem ; lasers are a tool in their own right. Many different solid-state laser configurations have evolved since the inception of the laser, the majority are based on rare-earth (RE) ions and produce coherent radiation in the near-IR. In some cases, multi-kW power levels that can cut through centimetres of steel in seconds are available. The light emission from these lasers starts from low-lying meta-stable excited states of the RE ions; however, many applications exist, such as display technologies, diagnostic tools at the life-science interface, or capitalising on the transmission window of seawater for sub maritime sensing and communications, which require higher-energy photons, that is, visible light. Such wavelengths can be obtained through non-linear frequency upconversion of IR lasers or directly through stimulated emission originating from higher-lying RE-ion excited-states. Solid-state visible lasers based on emission from these higher-lying excited states are still a research laboratory phenomenon, typically with low powers and low efficiencies and a limited range of wavelengths. The principle technical challenge restraining these devices is an effective pumping cycle that avoids loss generating mechanisms such as parasitic upconversion and photo-darkening, which, for example, have so far foiled the commercialisation of upconversion-pumped visible fibre lasers. We present a new approach for generating visible output from RE ions that will expand the toolkit of the laser user. Drawing upon our pioneering research and expertise with recent and critical technological advancements, namely ultra-low-loss rare-earth-doped crystalline waveguides and narrow-linewidth high-power diode-laser pump sources, we can now for the first time generate the right pump excitation parameters that will enable efficient high-power operation of this laser architecture. Our vision is to simplify the solid-state visible laser to just a single oscillator, while simultaneously broadening its capabilities through exploiting the rich spectroscopic properties of the higher-lying excited-states for generating different colours, and, their unique energy storage capacity. A power-scalable architecture will be realised, demonstrating applicability to high-power operation in both continuous wave (cw) and pulsed modes of operation, with unrivalled characteristics compared to alternative solid-state systems. Successful demonstration of the milestones in this project will place the UK at the forefront of international research in the field of high-power visible solid-state lasers, with an excellent opportunity to commercialise the technology through our industrial partners."
	},
	{
		"grant":137,
		"ID": "EP/H005587/1",
		"Title": "Efficient Photonic Devices for Near- and Mid-Infrared Applications",
		"PIID": "103654",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/02/2010",
		"EndDate": "31/01/2015",
		"Value": "1003323",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "ATI Physics",
		"OrgID": "51",
		"Investigators":[
		{"ID": "103654", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Nanyang Technological University"},
		{"Number": "1", "Name": "Philipps University of Marburg"},
		{"Number": "2", "Name": "University of Victoria"}
		],
		"Summary": "This project aims to address many issues of growing importance in today's world. We are all becoming increasingly technology-dependent, whether for entertainment, critical areas, e.g. healthcare and perhaps most notably for communication. All of these technologies require energy and as our appetite for higher performance, faster and better technology increases, the demand on natural resources increases correspondingly. Photonics (the use and manipulation of light) is perhaps one of the most widely used technologies, whether it be for sending information at high speeds across the internet, for reading/writing data onto DVDs, laser surgery and so on. Photonic components (lasers, light emitting diodes etc.) are the fundamental building blocks of this technology and are produced in their billions annually (with revenues in the multi $1Bs).  In spite of the widespread use of these devices, their efficiency is often relatively low, and compounded by a strong temperature sensitivity, particularly for devices operating in the near- and mid-infrared regions of the electromagnetic spectrum. This has largely held back the widespread deployment of mid-infrared lasers, for example in environmental and medical sensing (many gases are absorbed at these wavelengths) and other forms of  free-space  optical communication. In the near-infrared, telecommunications lasers operating in the optical fibre optimum transmission  window  at 1.55um are both inefficient and temperature sensitive. As a result, these devices require additional control electronics which consume significantly more power than the lasers themselves! Typically, more than 90% of the energy is such a system is wasted as heat.This proposal aims to tackle these issues in a coordinated manner since the core issues influencing near- and mid-infrared emitters is the same. The approach of this project is two-fold: (a) to work to develop a better understanding of the physical processes which give rise to poor efficiencies and to work in collaboration with other leading international groups towards developing new semiconductor materials systems which the PI has predicted will strongly suppress such processes (e.g. narrow band gap quantum dot systems and relatively unexplored semiconductor alloys, such as (In)GaAsBi) and (b) to develop novel materials such as  dilute nitride phosphides  to embed photonic components directly in electronic circuits, which are primarily silicon based. Routing data optically in such circuits could significantly reduce power (heat) dissipation in computers. Together, these approaches offer the potential to provide both large energy savings due to the use of better materials, and cost savings in manufacture, due to integration.The materials and devices in this project will be obtained from leading semiconductor growth groups in North America, Europe and Asia. At Surrey, the PI has established unique experimental techniques (e.g. low temperature and high pressure systems) to probe the physical properties of photonic materials and devices and will use these to determine both the basic materials parameters and the influence these have on device performance.  The fellowship will allow the PI an excellent opportunity to lead a significant effort working together with a strong international team to investigate the fundamental physical characteristics of new materials with the aim of developing high efficiency improved photonic technology for widespread applications of importance to UK industry."
	},
	{
		"grant":138,
		"ID": "EP/H005617/1",
		"Title": "Quantitative Verification: From Model Checking to Model Measuring",
		"PIID": "-114981",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2009",
		"EndDate": "30/09/2014",
		"Value": "1018521",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-114981", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "AIRBUS OPERATIONS LIMITED"},
		{"Number": "1", "Name": "NASA Jet Propulsion Laboratory"},
		{"Number": "2", "Name": "Tracetronic"}
		],
		"Summary": "Enabling engineers, programmers, and researchers to automaticallyverify the correctness of the computer systems that they design is oneof the Grand Challenges of computing research. The scientific andeconomic importance of this goal has long been recognised, yet despitesubstantial progress in basic and applied research over the last fewdecades, much work remains ahead of us. The aim of this project is toaddress the issue of quantitative verification and analysis ofreal-time and probabilistic systems, from the development of novel andfundamental algorithms all the way to the design and implementation oftools.The software research outcomes will be evaluated in collaboration withmy industrial partners, which include Airbus UK, NASA JPL, andTraceTronic, an automotive engineering firm. We will apply theverification technology to case studies derived from actual products,which is an essential part of the validation process of this researchenterprise.Ultimately, the principles and software tools arising from thisresearch will enable software firm to enhance the quality of theirsafety-critical products, potentially averting loss of life and/ormajor financial disasters."
	},
	{
		"grant":139,
		"ID": "EP/H005633/1",
		"Title": "Semantic Foundations for Real-World Systems",
		"PIID": "61326",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2014",
		"Value": "1523815",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "61326", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "ARM Ltd"},
		{"Number": "1", "Name": "IBM"},
		{"Number": "2", "Name": "Microsoft Research Ltd"},
		{"Number": "3", "Name": "Oswego State University of New York"},
		{"Number": "4", "Name": "Sun Microsystems Inc"}
		],
		"Summary": "Computer systems have been pervasive for many years, but despite this, and despite the huge resources devoted to their construction, they are still typically insecure, prone to failure, and hard to use.  Major failures are commonplace, in sharp contrast with the products of other engineering industries, and dealing with them, and with the day-to-day lesser flaws, has huge economic and social costs.  The core technical difficulty is system complexity: the range of behavior, the large scale, and the legacy of old design choices combine to make it hard to understand these systems well enough to engineer them well.  My main research goal is to develop intellectual tools that suffice for solid system-building, analogous to the applied mathematics of more traditional engineering disciplines.  This must be grounded on real systems - it cannot be done in theoretical isolation.  My approach, as documented in the Track Record, is to focus on the key articulation points in the hierarchy of abstractions used to build systems: programming languages, processor instruction sets, network protocols, and so forth.  These are relatively stable points in a rapidly changing environment, are critical to all system development, and are small enough that a modest team can address them.  Each demands different research: new language constructs, new specification, reasoning, and testing techniques, and so forth.  In this Fellowship I will pursue this approach, focussing on the problems in building computer systems above the intricate relaxed memory models of modern multiprocessors.  Multiprocessor systems are now the norm (as further speed-up of sequential processors has recently become impractical), but programming them is very challenging.  A key difficulty is that these systems do not provide a sequentially consistent memory, in which events appear to occur in a single global time order, but instead permit subtle reorderings, rendering intuitive global-time reasoning unsound.  Much previous work across a range of Computer Science, in programming languages, program logics, concurrency theory, model checking, and so on, makes the now-unrealistic assumption of sequential consistency, and it must now be revisited in this more complex setting.I will develop precise mathematical models of the behavior of real-world multiprocessors that take such reorderings into account, and develop semantics and reasoning techniques above them.  Using those, I will consider the verification of high-performance concurrent algorithms (as used in operating system and hypervisor kernels), the design of higher-level languages, and verified compilation of those languages to real machines.  This will enable future applications to be developed above a high-confidence and high-performance substrate.  It should also have a broader beneficial effect on research in Computer Science, drawing together mathematically well-founded theory and systems-building practice."
	},
	{
		"grant":140,
		"ID": "EP/H006419/1",
		"Title": "Coupling of single quantum dots to two-dimensional systems",
		"PIID": "-124398",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2009",
		"EndDate": "28/02/2013",
		"Value": "294745",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "63",
		"Investigators":[
		{"ID": "-124398", "Role": "Principal Investigator"},
		{"ID": "-173229", "Role": "Co Investigator"},
		{"ID": "111427", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Berlin University of Technology"},
		{"Number": "1", "Name": "Eindhoven University of Technology"},
		{"Number": "2", "Name": "University of Duisburg-Essen"}
		],
		"Summary": "The interaction between nano-objects of different dimensionality, e.g. electrostatic Coulomb-coupling of a zero-dimensional quantum dot (QD) to a two-dimensional (2D) system is of fundamental interest and of great relevance for charge-based memories. This interaction between a single QD and a 2D system shall be studied here. Innovative use of the complementary expertise of the partners will combine, for the first time, Sb-based QDs with a split-gate structure, which will allow the precise control of the charge-state of a single QD. Sb-based QDs have strong hole confinement yielding a potential retention time of many years at room temperature, enabling the analysis of the influence of charged QDs on a 2D system up to 300 K. In the mid-long term perspective, the results could be important for future generations of memories: knowledge of the interaction of a 2D system with a single QD might allow us to reach the ultimate limits of charged-based memories (e.g. Flash)."
	},
	{
		"grant":141,
		"ID": "EP/H006583/1",
		"Title": "Interfacing Functional Nanocomposites for Non-Volatile Memory Devices",
		"PIID": "125902",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "30/06/2013",
		"Value": "230474",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Comput Engin and Physical Sci",
		"OrgID": "112",
		"Investigators":[
		{"ID": "125902", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hebrew University of Jerusalem"},
		{"Number": "1", "Name": "Leiden Institute of Chemsitry"},
		{"Number": "2", "Name": "Technion Israel Institue of Technology"},
		{"Number": "3", "Name": "University of Bayreuth"}
		],
		"Summary": "The continuous demand for device miniaturization poses technological and economic barriers that cannot be answered by current fabrication techniques. This proposal is aimed at the development of a simple technique for the fabrication of crossbar electrode arrays for non-volatile memory devices based on a modulated block copolymer/nanoparticle (BCP/NP) assembly approach, where the ability to control the interfacial interactions between the NPs and the BCP domains under an electric field is crucial for obtaining the desired structure. Through a tight collaboration between experimental chemists, theoreticians, and an electrical engineer we intend to unravel the fundamental behavior ofBCP/NP assembly under the influence of a directing electric field, and then to utilize the structures formed for the creation of an ultrahigh-density, multi-component memory device."
	},
	{
		"grant":142,
		"ID": "EP/H006680/1",
		"Title": "SEmicoNducting SupramOlecular nanoscale wiRes and Field-Effect TransistorS (SENSORS)",
		"PIID": "45744",
		"Scheme": "Standard Research",
		"StartDate": "10/11/2009",
		"EndDate": "09/11/2012",
		"Value": "256407",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "London Centre for Nanotechnology",
		"OrgID": "81",
		"Investigators":[
		{"ID": "45744", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Louis Pasteur University (Strasbourg I)"},
		{"Number": "1", "Name": "Max Planck"}
		],
		"Summary": "SENSORS aims to generate new knowledge to underpin the development of new interfacing protocols for nanoscale organic logics. SENSORS relies on supramolecular approach to the design, synthesis and use of solution-processable Supramolecularly Engineered Nanostructured Materials (SENMs) with electronic function, to seek new solutions for solving key interfacing issues of wires and transistors prototypes (molecular-/meso-scale)."
	},
	{
		"grant":143,
		"ID": "EP/H00680X/1",
		"Title": "Lasing of Erbium in Crystalline Silicon Photonic Nanostructures - LECSIN",
		"PIID": "-190287",
		"Scheme": "Standard Research",
		"StartDate": "25/02/2010",
		"EndDate": "24/02/2013",
		"Value": "321399",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "-190287", "Role": "Principal Investigator"},
		{"ID": "41641", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "CNRS Grenoble"},
		{"Number": "1", "Name": "University of Catania"},
		{"Number": "2", "Name": "University of Pavia"}
		],
		"Summary": "This project focuses on the control of radiative emission of  Erbium ions in  photonic crystal nanostructures made of  crystalline Silicon, with the goal of achieving  laser emission around 1.54 micron wavelength. To this purpose, photonic crystal waveguides and nanocavities will be fabricated in Si membranes containing Erbium ions. Photonic structures will be designed such that the high-Q cavity modes be resonant with the narrow lines corresponding to Er emission, in order to tailor the radiative dynamics and to enhance optical gain. Micro-photoluminescence and pump-probe experiments under suitable pumping conditions will probe the radiative emission of Er, to achieve net gain and lasing threshold. Theoretical studies of Er emission coupled to nanocavity modes will allow exploring cavity quantum electrodynamics effects. The proposal builds on a new partnership involving a number of  Young Researchers by leading groups with complementary expertise in Silicon photonics, nanotechnology, nano-photonics, and quantum optics."
	},
	{
		"grant":144,
		"ID": "EP/H006869/2",
		"Title": "Active Plasmonics and Lossless Metamaterials",
		"PIID": "115433",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2011",
		"EndDate": "30/05/2013",
		"Value": "273022",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "115433", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "CNIT"},
		{"Number": "1", "Name": "Tampere University of Technology"}
		],
		"Summary": "Metal surfaces can support so called surface plasmons, density waves of free electrons. These plasmon waves can interact with light, opening the way to a novel area of optics, namely plasmonics. When the metal surface is nanostructured, a possibility for true nanoscale optics emerges. In this work we aim to alleviate or even remove the unavoidable absorption losses caused by the metal by amplifying the plasmon waves with semiconductor quantum wells and dots, thus demonstrating low-loss plasmonic components. They will be designed by novel electromagnetic simulation methods developed during the project, running on a supercomputer cluster. We will also use this approach to design and fabricate novel wide-band low-loss or even lossless metamaterials, highly promising structures with a negative refractive index that can for example slow or even stop incoming light pulses."
	},
	{
		"grant":145,
		"ID": "EP/H008373/2",
		"Title": "Resource Reasoning",
		"PIID": "36580",
		"Scheme": "Programme Grants",
		"StartDate": "01/03/2012",
		"EndDate": "31/12/2015",
		"Value": "2229287",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "36580", "Role": "Principal Investigator"},
		{"ID": "45736", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Resource has always been a central concern in computer systems.  In classical operating systems for example, processes access system resources such as memory, locks, files, processor time, and network bandwidth; correct resource usage is essential for the overall working of the system.  In the World Wide Web, the whole structure can be regarded as a giant, dynamic resource net, with its Uniform ResourceIdentifiers referring to located data and code.  Generally, for all kinds of computer system, it is second nature for a programmer to think carefully about the way the system handles resource as part of the programming task in hand. The point of view of this research is that there can be a novel kind of mathematical theory, which we call resource reasoning, which goes hand in hand with programmers' informal thinking about resources.  The first stage of such a theory is represented by Separation Logic and its relatives, developments which are suggestive of much more.  Our hypothesis is that, just as `resource-oriented thinking' occupies a central position in pre-formal system design, resource is also central to the formal analysis and verification of programs across the modern-day and future computing infrastructure. The challenge is to develop an elegant theory (or theories) of resource reasoning, that both meshes with programmers' and system designers' informal intuitions, and leads to tractable analysis and verification techniques for a range of problem domains such as classic operating systems, the continually-evolving web software, concurrent programming, and even hardware synthesis.The project will provide new theoretical concepts and new prototype tools that will eventually help to make computers that crash less frequently, mishandle our data less frequently, and generally  have improved reliability."
	},
	{
		"grant":146,
		"ID": "EP/H009744/1",
		"Title": "ESPRIT with Pervasive Sensing (Programme Grant)",
		"PIID": "68959",
		"Scheme": "Programme Grants",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2015",
		"Value": "6119249",
		"ResearchArea": "Assistive Technology, Rehabilitation and Musculoskeletal Biomechanics",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Biomedical Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "68959", "Role": "Principal Investigator"},
		{"ID": "3262", "Role": "Co Investigator"},
		{"ID": "39084", "Role": "Co Investigator"},
		{"ID": "-232654", "Role": "Co Investigator"},
		{"ID": "28325", "Role": "Co Investigator"},
		{"ID": "81375", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Age UK"},
		{"Number": "1", "Name": "Assoc. of British Healthcare Industries"},
		{"Number": "2", "Name": "BAE Systems"},
		{"Number": "3", "Name": "British Olympic Medical Centre"},
		{"Number": "4", "Name": "Defence Science & Tech Lab DSTL"},
		{"Number": "5", "Name": "IMEC - REALITY"},
		{"Number": "6", "Name": "LGC Limited"},
		{"Number": "7", "Name": "Livework Studio Ltd"},
		{"Number": "8", "Name": "National Physical Laboratory NPL"},
		{"Number": "9", "Name": "Openreach BT"},
		{"Number": "10", "Name": "UK Sport"}
		],
		"Summary": "Elite athletes walk a fine line between performance success and failure. Although regarded by the public as examples of ultimate fitness, in reality they often exhibit vital signs bordering on clinical pathology. Their physiological parameters challenge our notions of what we consider clinically normal, for, as individuals, athletes represent a unique model of human stress adaptation and often, sadly, mal-adaptation. Understanding this human variance may assist ultimately in understanding aspects of well being in the population at large, in the work place and during healthy exercise, as well as when undergoing lifestyle changes to overcome disease, age-related changes and chronic stress.To maximise the potential of GB athletes and support the quest for gold at future World Championships, Summer and Winter Olympic and Paralympic Games, the UK's sports governing bodies and the UK sports governing bodies and research councils have identified the opportunity for engineering and physical science disciplines to support and interact with the sports community during training. Not only will this secure competitive advantage for UK athletes, it will also, of more general application, contribute understanding of the biology of athletic performance to gain insights which will improve the health and wellbeing of the population at large.The vision of ESPRIT is to position UK at the forefront of pervasive sensing in elite sports and promote its wider application in public life-long health, wellbeing and healthcare, whilst also addressing the EPSRC's key criteria for UK science and engineering research. The proposed programme represents a unique synergy of leading UK research in body sensor networks (BSN), biosensor design, sports performance monitoring and equipment design. The provision of  ubiquitous  and  pervasive  monitoring of physical, physiological, and biochemical parameters in any environment and without activity restriction and behaviour modification is the primary motivation of BSN research. This has become a reality with the recent advances in sensor design, MEMS integration, and ultra-low power micro-processor and wireless technologies. Since its inception, BSN has advanced very rapidly internationally. The proposing team has already contributed to a range of novel, low cost, miniaturised wireless devices and prototypes for sports and healthcare."
	},
	{
		"grant":147,
		"ID": "EP/H009817/1",
		"Title": "Silicon emission technologies based on nanocrystals",
		"PIID": "45348",
		"Scheme": "Standard Research",
		"StartDate": "16/11/2009",
		"EndDate": "01/04/2014",
		"Value": "608548",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "45348", "Role": "Principal Investigator"},
		{"ID": "115824", "Role": "Co Investigator"},
		{"ID": "29195", "Role": "Co Investigator"},
		{"ID": "6674", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "McMaster University"},
		{"Number": "1", "Name": "QinetiQ Ltd"}
		],
		"Summary": "The global semiconductor market has a value of around $1trillion, over 90% of which is silicon based. In many senses silicon has driven the growth in the world economy for the last 40 years and has had an unparalleled cultural impact. Given the current level of commitment to silicon fabrication and its integration with other systems in terms of intellectual investment and foundry cost this is unlikely to change for the foreseeable future. Silicon is used in almost all electronic circuitry. However, there is one area of electronics that, at the moment, silicon cannnot be used to fill; that is in the emission of light. Silicon cannot normally emit light, but nearly all telecommunications and internet data transfer is currently done using light transmitted down fibre optics. So in everyones home signals are encoded by silicon and transmitted down wires to a station where other (expensive) components combine these signals and send light down fibres. If cheap silicon light emitters were available, the fibre optics could be brought into everyones homes and the data rate into and out of our homes would increase enormously. Also the connection between chips on circuit boards and even within chips could be performed using light instead of electricity. The applicants intend to form a consortium in the UK and to collaborate with international research groups to make silicon emit light using tiny clumps of silicon, called nanocrystals;. These nanocrystals can emit light in the visible and can be made to emit in the infrared by adding erbium atoms to them. A number of techniques available in Manchester, London and Guildford will be applied to such silicon chips to understand the light emission and to try to make silicon chips that emit light when electricity is passed through them. This will create a versatile silicon optical platform with applications in telecommunications, solar energy and secure communications. This technology would be commercialised by the applicants using a high tech start-up commpany."
	},
	{
		"grant":148,
		"ID": "EP/H010432/1",
		"Title": "Evolutionary Optimisation of Self Assembling Nano-Designs (ExIStENcE)",
		"PIID": "110493",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2010",
		"EndDate": "30/04/2013",
		"Value": "945423",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "110493", "Role": "Principal Investigator"},
		{"ID": "1313", "Role": "Co Investigator"},
		{"ID": "54286", "Role": "Co Investigator"},
		{"ID": "46021", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The primary objective of this proposal is the development of novel evolutionary algorithms (EAs) and protocols, based on deeper principles than currently available, for the optimisation, design and exploitation of molecular self-assembly. Evolutionary algorithms are nowadays well established techniques that have shown their worth on a large variety of applications that range from timetabling and scheduling problems to robotics and space antenna design. Surprisingly, EAs have not yet been systematically analised in the context of molecular tile design. At the core of our approach lies the assumption that self-assembly can be understood as an information-driven process and hence be better exploited by directly linking it to computational phenomena. Taken as an operational hypothesis, which our research programme will analyse both theoretically and experimentally, this assumption implies that with suitable tools, desired emergent phenomena could in principle be programmed into self-assembling nanosystems. Our experimental target will be based around molecular tiles as these have been shown to be computationally complete [1,2,3]. Hence, they can potentially be programmed to perform any set of discrete information processing steps which in turn could induce a specific emergent pattern of complex behaviour. This project will seek to automate the process of programming molecular tiles using evolutionary algorithms. In an interview for Thomson Scientific's  Science Watch  newsletter [quoted in J.A. Pelesko, Self Assembly, The Science of Things that Put Themselves Together,  Chapman & Hall/CRC, 2007], G. Whitesides, one of the most prolific and highly cited chemists in the world, noted that the holy grail of his research was  To be able to make complex systems, either structurally or functionally, by self-assembly...We would like to develop a synthesis technology that would enable the making of nanometer-scale... structures on surfaces with arbitrary chosen properties . Whitesides' challenge is at the heart of our research programme. We will seeks to leverage state-of-the-art research in Computer Science and Nanoscience to meet this challenge."
	},
	{
		"grant":149,
		"ID": "EP/H011536/1",
		"Title": "Cooperative Localisation: Distributed Optimisation with Hypothesis Testing",
		"PIID": "-53274",
		"Scheme": "Standard Research",
		"StartDate": "31/08/2010",
		"EndDate": "30/08/2013",
		"Value": "117268",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Adastral Park Campus",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-53274", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Critical locational information has long been used in a variety of military settings. In many of these settings, it would be advantageous if information concerning the spatial locations of particular entities and/or events could be conveyed. Localisation requirement has also become more pervasive and has found many strategic applications to the ground forces. For instance, the troops' safety can be directly linked to the degree of location-awareness of the troops and their enemies in a battlefield. For non-military applications, the first need to track a mobile user appeared as an essential public safety feature from the order issued by the Federal Communications Commission (FCC) in the US in 1996, which mandated all wireless service providers to deliver accurate location of an emergency 911 (E-911) caller to public safety answering points. Since then, various location-based services (LBSs) have emerged in the market, from identifying the whereabouts of a friend or employee to personalised LBS such as discovering the nearest cash machine. It is estimated that LBSs will generate annual revenues of the order of 10 billions worldwide.Providing a useful localisation will require, in some cases, metre-perfect resolution to be achieved over air. Yet, the fundamental physical challenges such as channel fading, low signal-to-noise ratios (SNRs), multiuser interference, and multipath conditions have put obstacles on meeting the objective. Our vision of next-generation (xG) localisation systems will be the provision of dynamic, distributed, robust and high-resolution LBSs. The realisation of these xG LBSs will require advanced signal processing and intelligence at the nodes to resolve the problem of interference and to detect the presence of a direct line-of-sight (LoS) for reliable ranging and localisation. Advanced multi-antenna technologies such as multiple-input multiple-output (MIMO) are expected to be adopted at mobile terminals for providing enhanced locational information as well as mitigating the multipath and multiuser interference.To achieve the needed LBSs, this project proposes to investigate the use of mobile user cooperation for localisation. The novelty of user or node cooperation lies in that nodes can work collaboratively by proper relaying to mitigate the multipath interference that can help identify the LoS for ranging in the presence of delay paths. The cooperation can, more importantly, exchange locational information from one node to another so that location ambiguity due to the lack of LoS signal paths could be removed and higher resolution can also be achieved. Another novelty of this proposal is the use of hypothesis testing based machine learning for the detection of LoS, which will be integrated with the cooperative signal processing for wireless localisation. This exceptionally challenging objective also has the potential to redefine the architecture of wireless networks, provide a novel system solution for organising the access of users to the system resources in this cooperative and self-regulating architecture, and revolutionise key areas of the 21st century ICT.Of particular relevance to the interests of DSTL, the areas that this project covers include:(1) Broadband signal separation - classification of LoS and non-LoS signals;(2) Detection - enhanced reception in multiuser and multipath interference channels using cooperative signal processing;(3) High-resolution localisation - determining the locations of mobile users in wireless environments;(4) Multipath mitigation - suppressing multipath interference using cooperative signal processing.The final outcomes of this project will not only elucidate the benefits of a wireless positioning problem using mobile node cooperation but also offer distributed algorithms for realising high-resolution localisation as well as address some key problems in communications systems."
	},
	{
		"grant":150,
		"ID": "EP/H01182X/1",
		"Title": "SiGe based Optoelectronics",
		"PIID": "115824",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "31/03/2014",
		"Value": "130219",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "115824", "Role": "Principal Investigator"},
		{"ID": "12223", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The field of this proposal is that of SiGe based optoelectronic devices. Though Si and Ge are now and will remain in the immediate future the most used materials for electronic devices, group IV optical emitters that can rival those made of III-V semiconductors have yet to be realised as commercial devices. If this was achieved it would represent a giant leap forward in technology, as such devices could be easily interfaced with the existing CMOS technology. This would mean the integration on the same chip of both the optical emitter and the logic circuit that drives the device. Undoubtedly a novel development that promises a high return from the global economy.It is worth mentioning that this work fits in the themes of the EPSRC ICT programme  Grand challenges in Silicon Technology , in particular GC1  Novel devices and processes using silicon-based technologies , GC2  Modelling and simulation for silicon-based technologies  and GC3  Characterisation for silicon-based technologies .This proposal, designed around a research project for a PhD student, describes a combined experimental and modelling approach to designing novel SiGe structures for optoelectronic applications. This work will be performed in collaboration with the University of Roma 3 in Italy, where test structures will be prepared, and a scientific software company, Accelrys Ltd, which is interested in the commercial exploitation of the software developed for this project. The work in Manchester will be predominantly simulations with some assessment work of the semiconductor materials and structures produced in Rome in order to test the viability of the modelling work.The novel modelling tools developed by the applicants will provide an essential tool to solving a wide range of problems in Si optoelectronics that have so far hampered worldwide efforts to transforming what appears to be fundamentally possible into a real device. Silicon, Germanium and their alloys are well known indirect bandgap materials. Computer design of strain engineering of SiGe layers will make possible to understand the paradigm behind the generatation of direct bandgap in real devices.  Despite recent efforts Si integrated heteropolar optical devices in the near infrared have not yet been experimentally successfull.  In first instance we intend to understand if the limitations are purely practical or fundamental.More important, and a crucial aspect of this proposal, is that a detailed knowledge of the band alignment at the heterointerface will contribute to the current global effort in generating unipolar devices such as Quantum Cascade Lasers operating in the THz regionThe significance of this work is duplex: On one hand the modelling techniques developed will represent a major step towards predictive modelling of large scale simulations of Semiconductors, with implications extending to compound semiconductors and nanostructures. In view of which we have established the commercial collaboration with Accelrys Ltd. On the other hand the impact of the modelling techniques on the current experimental effort could ultimately provide the key to revolutionary SiGe based laser emitters."
	},
	{
		"grant":151,
		"ID": "EP/H011919/1",
		"Title": "Distributed and Iterative Processing for Wireless Sensor Networks with Multiple Local Fusion Centres",
		"PIID": "-153897",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2010",
		"EndDate": "31/10/2013",
		"Value": "117504",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "125",
		"Investigators":[
		{"ID": "-153897", "Role": "Principal Investigator"},
		{"ID": "-81303", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Wireless sensor networks (WSNs) are becoming popular as wireless sensor nodes are matured with advanced sensor and wireless communication technologies. In general, sensor nodes are capable of sensing, computing, and communicating. Therefore sensor networks are widely used in military battlefield surveillance. However, at the same time, as most sensor nodes are battery-powered, some functions may not be fully utilized and limited by certain constraints. To overcome power constraints of sensor nodes, the use of multiple local fusion centers (LFCs) could be effective, which allows building the WSN in a layered manner. A LFC will collect information from closely located sensor nodes. Thus, sensor nodes can save power. To share measured information for better estimation, the LFCs in a WSN are connected through a certain wireless mesh network. In this proposed project, we aim at developing i) distributed estimation methods at a local fusion; ii) iterative cooperation communication mechanisms between LFCs; and iii) performance analysis to optimize the performance with constraints."
	},
	{
		"grant":152,
		"ID": "EP/H012354/1",
		"Title": "Target detetction in Clutter for sonar imagery",
		"PIID": "101189",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2009",
		"EndDate": "31/01/2013",
		"Value": "120989",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "101189", "Role": "Principal Investigator"},
		{"ID": "59531", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Systems Engineering Assessment Ltd (SEA)"}
		],
		"Summary": "This proposal aims at studying new techniques for detection and classification of targets underwater using 3D and texture analysis. On simple seabed types such as flat sand, it is very easy to detect and classify targets. It becomes much more difficult when the seabed is either highly cluttered with rocky or coral structures, marine life such as seaweed or is of a complex nature (large rocky outcrops and sand dunes). In those areas, classical target detection and classification techniques fails as they tend to concentrate on the shape of the target, classically recovered using shadow analysis (the acoustic shadow is casted by the target on the seabed). On the other hand, the analysis of the target echo is difficult for classical high resolution sonars as they are susceptible to speckle noise and in general not resolved enough for classification. Detection and classification in such challenging scenarios can be improved by detectiing the targets as an outlier in the current texture field. This can be done using 2D or 3D texture measures but as most strong textures are due to the 3D nature of the seabed, we believe that 3D texture analysis will be more effective and therefore propose to focus on these. Classification can be addressed with the development of new higher resolution sonars (SAS) and new 3D sonars (Interferometric SAS / Side Scan). As resolution increases, the structure of the echo will become more apparent and techniques developed in the machine vision and pattern recognition communities can be used. This is the secondary objective of this proposal."
	},
	{
		"grant":153,
		"ID": "EP/H012370/1",
		"Title": "SAR processing with zeros",
		"PIID": "34198",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2010",
		"EndDate": "31/08/2013",
		"Value": "106109",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "41",
		"Investigators":[
		{"ID": "34198", "Role": "Principal Investigator"},
		{"ID": "11300", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Synthetic Aperture Radar (SAR) provides the military with an extremely valuable means of remote imaging and plays an important role in target detection. SAR works by measuring the electromagnetic signal reflections from the ground. Processing the raw received data to generate the image is usually performed using linear frequency domain techniqes such as the Polar Format Algorithm. However when there is missing data, or when gaps in the data (either spatially or spectrally) are introduced the performance of such estimators deteriorates considerably. The resulting images exhibit distortion and aliasing artefacts.Recently a new theory for signal reconstruction, called compressed sensing, has emerged. It explores the extent to which ill-posed sampling problems such as those discussed above can be made well-posed through the inclusion of strong signal models. These techniques have already been successfully applied to SAR image reconstruction for target detection and super resolution. The broad aim of this proposal is to explore the application of compressed sensing reconstruction techniques to SAR image formation when spatial and/or frequency notches are introduced into the transmitted/received signals. Ultimately we hope to gain a general understanding of the limits to which the SAR data acquisition system can be so modified without incurring serious performance degradation."
	},
	{
		"grant":154,
		"ID": "EP/H012516/1",
		"Title": "Real Time Model Adaptation for Non-Stationary Systems",
		"PIID": "-81370",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2012",
		"Value": "105294",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Systems Engineering",
		"OrgID": "113",
		"Investigators":[
		{"ID": "-81370", "Role": "Principal Investigator"},
		{"ID": "98457", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposed research describes a new approach of modelling for the non-stationary data sets, which are commonly generated in many systems including Radar, Sonar, communications, instrumentation, seismic exploration, speech processing and recognition, etc. This proposal falls into the general area of  non-stationary processing  and particularly addresses the challenges that signals are to be extracted from non-stationary, spatially and temporally correlated noise. It also relates to the issue of  low size, weight and power . Signal processing functions usually perform based on a pre-set model, or the system structure is fixed. Although this provides a simple solution, it is highly inefficient especially for non-stationary systems that are common in practice. This makes it highly desirable to adapt the model so that it can capture the true underlying dynamics and predicts accurately the output for unseen data. This proposed research would particularly focus on on-line real time model adaptation approaches which are an important class of model construction algorithms that deal with model structure and/or parameter updating on the arrival of new data. Alternatively, flexible design with model adaptation for non-stationary systems is potentially possible in practical realisations due to recent technological advances where a large portion of the hardware processing is implemented in software. Therefore, when the maximum potential capability of a system is higher than the requirement, spare resource can be saved for other system functions. This is particularly useful for many modern systems in which limited computing resource is usually shared by multiple functions, which would eventually enable the use hardware of reduced size, weight and power (SWAP).Against this background, however, there is a lack of generic tools/methodologies to deal with the problem as on how to perform the model structural changes as demanded by the processes. Some nonlinear model structure identification algorithms are too slow for real-time applications. Most current real-time algorithms, on the other hand, are ad hoc rather than principled approaches. Hence the resultant model quality is not optimal from a statistical point of view.In this programme, we propose to develop a hybrid, flexible yet principled approach for optimum on-line adaptive modelling by means of minimal model structure determination and simultaneous parameter estimation. The aim of the proposal is to introduce a new technique for the adaptive modelling of complex nonlinear dynamical systems in real time and noisy environments. The proposed algorithms would be validated based on mathematical derivations, proofs and extensive simulations in comparison with competitive algorithms using numerical examples on simulated and realistic benchmark data sets."
	},
	{
		"grant":155,
		"ID": "EP/H012842/1",
		"Title": "Multi-Modal Blind Source Separation for Robot Audition",
		"PIID": "-190482",
		"Scheme": "Standard Research",
		"StartDate": "08/10/2009",
		"EndDate": "07/10/2012",
		"Value": "115288",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51",
		"Investigators":[
		{"ID": "-190482", "Role": "Principal Investigator"},
		{"ID": "8936", "Role": "Co Investigator"},
		{"ID": "114631", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal draws on expertise in blind source separation and multimodal (audio-visual) speech processing within the Centre for Vision Speech and Signal Processing at University of Surrey. The objective is to perform source separation of the target speech in the presence of multiple competing sound sources in room environments and thereby ultimately provide progress towards automatic machine perception of auditory scenes within an un-controlled natural environment. The fundamental novelty in this work is to exploit visual cues for enhancing the operation of frequency domain blind source separation algorithms. Exploitation of such audio-visual processing is targeted at mitigating the permutation problem, the underdetermined problem (i.e. when the number of sources is greater than the number of microphones), and the reverberation problem, which currently limits the practical applicability of blind source separation algorithms. The focus of the work is therefore on the signal processing algorithms and software tools that can be used to perform automatic separation of sound signals, e.g., for a robot. The body of work in this proposal is underpinned by the substantial experience of the investigators, two from the areas of blind source separation and digital speech processing, and one from the area of computer vision and pattern recognition. The outcomes of the proposed research will be of considerable value to the UK defence industry working especially in the areas of target separation, detection and multi-path mitigation (or dereverberation), with applications in, for example, human-robot interaction, security surveillance and human-computer interaction."
	},
	{
		"grant":156,
		"ID": "EP/H012877/1",
		"Title": "Advanced High Resolution Methods for Radar Imaging and Micro-Doppler Signature Extraction",
		"PIID": "14826",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2009",
		"EndDate": "31/03/2013",
		"Value": "85698",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "48",
		"Investigators":[
		{"ID": "14826", "Role": "Principal Investigator"},
		{"ID": "74261", "Role": "Co Investigator"},
		{"ID": "10528", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Imaging radars are airborne or spaceborne radars which generate a reflectivity map of an illuminated area through transmission and reception of electromagnetic energy. Among many types of microwave sensors, special attention has been paid in the past to Synthetic Aperture Radar (SAR) because of its high spatial resolution, day or night all weather operational capabilities. With its fine two-dimensional resolution capability SAR has evolved to satisfy a variety of applications for both civilian and military users. These applications centre on target imaging and terrain mapping. A target is a specific object of interest that the radar illuminates. The typical target is man-made and consists of multiple scattering centres. An imaging radar system must distinguish between single and multiple scatters located in close proximity. Resolution is, nominally, the minimum distance needed between adjacent scatters to separate them in the image. Fine resolution provides the capability to image a complex object or scene as a number of separate scattering centres. This type of image provides detailed information to detect, characterize, and identify specific objects of interest. Because of the importance of object identification in military applications, much development effort has been directed at improving radar resolution. Military SAR applications include intelligence gathering, battlefield reconnaissance, and weapons guidance. Civilian applications include topographic mapping, oil spill monitoring, sea ice characterization and tracking, agricultural classification and assessment, lands use monitoring, and planetary or celestial investigations. Normally imaging radars  provide a two-dimensional representation of a scatterer in the illuminated volume with no resolution or positioning of scatterer in the third dimension. Generally, we speak of monstatic (the transmitter and receiver are co-located) radar resolution in the range and cross-range or azimuth directions.    Bistatic radars, where the transmitter and receiver are positioned in different physical positions have several operational advantages. In particular such bistatic systems help to increased receiver survivability while minimising receiver cost. Furthermore when one or both of the platforms are manoeuvring in an non linear planar path allows the resolution to be computed in the 3rd dimension thus facilitating the acquisition of target height information as well range and cross range resolution.When a radar interrogates a moving target it is traditional to exploit the target's Doppler for identification and characterisation. If the target possesses additional rotational, vibration or other internal motions then these induce additional spectral components separate from the main Doppler. These are termed microdoppler components and reside as additional sidebands around the main Doppler. A human walking or running will exhibit microdopplers due to swinging arms and leg movements. A military tank will exhibit microdopplers due to the wheel tracks while a helicopter and engine target will exhibit key microdoppler components. The use of time frequency signal representation such as the short time Fourier transform  and wavelet analysis has been used to examine these microdopplers in the past. Good quality microdoppler signatures are important in new automatic target identification and recognition systems. Quality is directly related to the extracted  microdoppler resolution extracted.The aim of this work is to explore new signal processing techniques which can be used to improve the resolution of the imaging radars algorithms and microdoppler signature extraction.  The work will derive new mathematical relationships for bistatic spotlight SAR image formation and microdoppler signature extraction  based on the Fractional Fourier transform and empirical mode decomposition.  An FrFT compute engine will be realised and the algorithms will be  tested on simulated and real data."
	},
	{
		"grant":157,
		"ID": "EP/H012907/1",
		"Title": "UNI-TRAVELLING CARRIER PHOTOMIXERS FOR THZ DETECTION",
		"PIID": "119760",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2012",
		"Value": "101229",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "119760", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Although terahertz (THz) (a thousand billion oscillations in a second) show growing potential for applications such as imaging, sensing and security, the number of technological solutions to measure such signals is limited. Further, detectors such as Golay cells and photo-acoustic cells cannot detect fast modulation of the THz power. This programme proposes to investigate the possibility of detecting electromagnetic waves that oscillate in the THz range  using a semiconductor device that is normally used to detect light. This type of device is of interest for THz detection for three main reasons; first, as a semiconductor device it is compact and relatively cost effective, second, it operates at room temperature while most other THz detection technologies must be operated at very low temperatures and third, it has already proven to be efficient as aTHz emitter. The work will concentrate on investigating the fact that in the semiconductor device of interest the velocity of the carriers that are the result of light detection is strongly dependent on the electric field that is applied to the device. Such a difference implies that if the device receives an electric field generated by an antenna (the THz signal to be detected) it will affect the way it detect fast changes in an incoming light signal. Such an effect can result in the creation of an intermediate frequency output at a much lower frequency, that can be analysed using standard electronic measurement equipment. At the end of the programme it is expected that a fabricated device will be able to be used in a THz scanning system operating at room temperature."
	},
	{
		"grant":158,
		"ID": "EP/H01294X/2",
		"Title": "Information and neural dynamics in the perception of musical structure",
		"PIID": "43134",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2011",
		"EndDate": "30/04/2013",
		"Value": "344472",
		"ResearchArea": "Human Communication in ICT",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "43134", "Role": "Principal Investigator"},
		{"ID": "-128228", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Music is one of the things that makes us human. No known human society exists without music; and no other species seems to exhibit musical behaviour, in the same sense as humans. It is an open question where music came from (in terms of evolution), but it is self-evident that it arises from the human brain:  for there to be music, a brain was involved somewhere, even if only in listening.  What is not evident at all is how brains (or the minds to which they give rise) make, or even perceive, music.  This project aims to understand how human musical behaviour can be modelled using computers, by building programs which embody theories of how the musical mind works, and then comparing them with humans engaged in musical activity and also by comparing their predictions with those of an expert music analyst.  This means that the project will contribute to various areas of study:  computer music, statistical methods for cognitive modelling (and therefore to cognitive linguistics, because the same kinds of models can be used there), musicology, and neuroscience (both in a better understanding of brain function and with new methods for neural signal analysis).  Long term outcomes are likely to be computer systems that help music education, that play music musically, and that interact with human musicians musically; understanding that helps musicians do what they do more effectively; and understanding that helps brain scientists and psychologists understand more about how the brain and the mind work. Above all, since musicality is so fundamental to humanity, the project aims to help understand some of what it means to be human."
	},
	{
		"grant":159,
		"ID": "EP/H014381/1",
		"Title": "Learning and computation in disordered networks of memristors: theory and experiments",
		"PIID": "84339",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2010",
		"EndDate": "30/06/2013",
		"Value": "643827",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Environment and Technology",
		"OrgID": "21",
		"Investigators":[
		{"ID": "84339", "Role": "Principal Investigator"},
		{"ID": "103942", "Role": "Co Investigator"},
		{"ID": "50432", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Memristor (memory resistor) is a device whose resistance changes depending on the polarity and magnitude of a voltage applied to the device's terminals and the duration of this voltage's application. The memristor is a non-volatile memory because the specific resistance is retained until the application of another voltage.  A memristor implements a material version of Boolean logic and thus any logical circuit can be constructed from memristors. We propose to fabricate in laboratory experiments an adaptive, self-organized disordered network of memristors. This practical fabrication will be backed up by rigorous computer simulation experiments. The memristor network is comprised of a conglomerate of conductive polymer fibres interspersed with particles of solid electrolyte. The conglomerate is placed on a matrix of micro-electrodes capable of recording voltage and generating current sources and sinks. Machine learning techniques will be applied in order to design logical schemes and basic arithmetical circuits."
	},
	{
		"grant":160,
		"ID": "EP/H015795/1",
		"Title": "Compact diode-laser-pumped THz source based on a novel photomixer device.",
		"PIID": "77900",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2010",
		"EndDate": "31/01/2014",
		"Value": "525059",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic Engineering and Physics",
		"OrgID": "37",
		"Investigators":[
		{"ID": "77900", "Role": "Principal Investigator"},
		{"ID": "13195", "Role": "Co Investigator"},
		{"ID": "32116", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Compugraphics International Ltd"},
		{"Number": "1", "Name": "Innolume"},
		{"Number": "2", "Name": "M Squared Lasers Ltd"}
		],
		"Summary": "Within this project we will carry out theoretical and experimental studies of novel types of compact THz sources where semiconductor laser diodes will be combined with a novel dipole/spiral and waveguide photoconductor devices.  Specifically, we propose to design, fabricate and assess the performance of the material systems of amorphous/nanocrystalline silicon and quantum-dot semiconductors as materials having short electron-hole photocarrier lifetimes and high photosensitivity at 780-1300nm.  In addition to fully characterising and optimising the dual- and multi-longitudinal modes diode lasers we will evaluate the performance of these lasers in the optical pumping of the dipole/spiral and waveguided antennae for obtaining THz power levels in the >100 microwatts range with optimised beam characteristics. The overall objective of this work is to produce a diode-laser-driven, ultra-compact THz source having some potential for additional functionality through its frequency tunability."
	},
	{
		"grant":161,
		"ID": "EP/H016015/1",
		"Title": "Information flows and Information bottlenecks in Network Coding",
		"PIID": "94763",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "30/09/2012",
		"Value": "343705",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "94763", "Role": "Principal Investigator"},
		{"ID": "2493", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Transport of information is fundamentally different from transport of traditional commodities, since information can be copied and transformed during transmission while ordinary commodities cannot. The main challenge is to develop results and techniques for handling digital information. The main technical challenge and motivation is to use the insights and results in Network Coding to attack the matrix transposition problem as well as Valiant's shift problem which are both long standing open questions in circuit complexity theory.The purpose of the proposal is to investigate how information can most efficiently be transmitted through various types of communication networks.  The project will use recent computer generated results in Information theory to identify and reason about information bottlenecks. The main motivation for the work is to understand information flows and information bottlenecks in a context  that is relevant to long standing open questions in (Circuit) Complexity Theory. The project also aims at bridging the gab between these theoretical questions and potential applications of Network Coding."
	},
	{
		"grant":162,
		"ID": "EP/H016317/1",
		"Title": "Cool: Coalgebras, Ontologies and Logic",
		"PIID": "14151",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "31/07/2013",
		"Value": "393699",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "14151", "Role": "Principal Investigator"},
		{"ID": "-22759", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Microsoft Research"}
		],
		"Summary": "The main theme that underlies this research project is automatedreasoning, an applied  sub-discipline of mathematical logic.  Logichas found applications in many areas of computer sciencesuch as the verification of digital circuits, reasoning aboutprograms and knowledge representation.  One of the most fundamentalaspects in this context is to automatically decide whether aparticular formula is a logical consequence of a given set ofassumptions.  The set of assumptions may describe complex relationsbetween diseases and their symptoms, and one possible reasoning taskwould be to confirm or reject a diagnosis based on observed symptomsand medical history.In this research project, we investigate applications ofmathematical logic in knowledge representation.  One of the primechallenges in this area is to design logical formalisms that strikea balance between the two conflicting goals of expressiveness (theability to formally represent the application domain) andcomputational tractability. The family of modal logics, conceived ina broad way, combines both aspects and serves as the mathematicalfoundation of a large number of knowledge representation formalisms.The core ingredient of modal logic is the possibility to qualifylogical assertions to hold in a certain way.  Depending on thecontext, we may for instance stipulate that assertion holds `alwaysin the future', `with a likelihood of at least 50%' or `normally'.Together with names for individual entities, this allows us toformulate assertions like `the likelihood of congestion on Queen'sRoad  is greater than 30%', and complex knowledge bases arise bycombining different logical primitives.  Automated reasoning thenallows us to mechanically verify e.g. the consistency of scientifichypotheses against an existing knowledge base.  Our goal is to builda modular and practical knowledge representation system  that allowsto represent and reason about knowledge represented in this way,based on a large and diverse class of logical primitives, includinge.g. the coalitional behaviour of agents, quantitative uncertainty,counterfactual reasoning and default assumptions.  This goes waybeyond the current state of the art, where only logical primitiveswith a relational interpretation are supported by automated tools.Recent research has shown these new logical features can beaccounted for in a uniform way by passing to a more generalmathematical model, known as `coalgebraic semantics'. This richerframework does not only provide a uniform umbrella for a largenumber of reasoning principles, but also supports a richmathematical theory that has by now matured to the extent which putsthe development of automated tools within reach. The researchchallenge that this proposal addresses is the further development of thesetheoretical results as to bring them to bear on practical applications.As a concrete case study, we will use the Cool system to formalisequantitative models in Systems Biology."
	},
	{
		"grant":163,
		"ID": "EP/H016368/1",
		"Title": "Terahertz Spectroscopy of Semiconductor Nanowires",
		"PIID": "76609",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2009",
		"EndDate": "30/11/2013",
		"Value": "755087",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Oxford Physics",
		"OrgID": "106",
		"Investigators":[
		{"ID": "76609", "Role": "Principal Investigator"},
		{"ID": "98559", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Australian National University"}
		],
		"Summary": "Nanostructures such as carbon nanotubes and ZnO nano-particles are already being used in commercialproducts such as tyres and sunscreens. However, despite progress in understanding the mechanical andoptical properties of nano-materials we are still at the dawn of the fields of nano-optoelectronics andnano-photonics. Advances in understanding the fundamental materials science of these nano-materials todaywill therefore have a major impact on a wide range of commercial products over the next 30 years. One of thedifficulties with developing nano-optoelectronic components is the complexity of measuring their electricalproperties. Traditionally, new materials and devices have been tested via electrical transport measurements.Unfortunately, it is extremely difficult to make electrical contacts on a 30nm diameter nano-wire or anano-particle. Indeed even if the contacts are made it is then difficult to separate the properties of thenano-material from those of the contact. Additionally, such measurements are plagued by reproducibilityproblems. Thus there is a pressing need for techniques that can quickly and reliably extract the electricalproperties of nano-structured materials. The availability of such techniques would greatly accelerate thedevelopment of new materials and allow devices based on these materials to be brought to the market sooner.We propose to solve these problems by applying the technique of optical pump terahertz probe spectroscopy(OPTPS) to semiconductor nano-wires, and by developing refined models to extract the most importantdevice-specific electrical properties from the measured data. The knowledge we gain will help us develop newoptoelectronic devices based on semiconductor nano-wires."
	},
	{
		"grant":164,
		"ID": "EP/H016872/1",
		"Title": "Silicon-based Integrated Single-Spin Quantum Information Technology",
		"PIID": "-179793",
		"Scheme": "Standard Research",
		"StartDate": "04/05/2010",
		"EndDate": "30/11/2013",
		"Value": "1009703",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-179793", "Role": "Principal Investigator"},
		{"ID": "-234738", "Role": "Co Investigator"},
		{"ID": "-111019", "Role": "Co Investigator"},
		{"ID": "-130158", "Role": "Co Investigator"},
		{"ID": "497", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hitachi Cambridge Laboratory"},
		{"Number": "1", "Name": "NTT Basic Research Laboratories"}
		],
		"Summary": "The aim of this project is to realize a world-first Si-based integrated single-spin quantum bit (qubit) system on ultrathin silicon-on-insulator (SOI). We develop a precisely-controlled single-electron transfer technique to initialize truly single-electron spin (single-spin) states, micro electron spin resonance (micro-ESR) for single-spin manipulation, and a 'spin-to-charge' conversion technique for readout. These challenging technical requirements will be met by synergistically combining the expertise of the University of Southampton on cutting-edge silicon-based nanofabrication and single-electron devices, the University of Cambridge and the Hitachi Cambridge Laboratory on solid-state qubits and the associated low-temperature & RF measurements, and the NTT Basic Research Laboratories on single-electron / spin control technology.The first Si-based qubit was proposed by Kane using nuclear spins of phosphorous donor atoms in Si (Si:P qubits). This proposal attracted much interest due to the very long decoherence time of nuclear spins in Si. However, challenging bottom-up nanotechnologies, e.g. STM lithography, are required to control the number and position of P atoms embedded in silicon relative to surface control gates. Rather than using donors, which are atomic-like species, it is also possible to confine electrons in nano-fabricated structures known as quantum dots (QDs). An exquisite degree of control over single-electron spins (single-spins) has been demonstrated in QDs made from gallium arsenide. Unfortunately gallium arsenide is a nuclear spin rich environment leading to a rapid loss of coherence from electron spins. Recently, QDs capable of confining few electrons have also become feasible in silicon based materials, which have a low nuclear spin density, therefore providing a motivation for this research proposal. The recent appearance of isotopically pure Si materials (28Si 99.9%) also works in favour of Si-based systems by further increasing spin decoherence time. In order to develop the Si-based integrated single-spin qubit system, which has never been achieved, we fully exploit the unique set of state-of-the-art nanotechnologies brought together in our project team. Firstly, single-electron turnstile technology is adopted in order to prepare the well-defined initial single-spin states. Secondly, a high-speed charge detection technique is introduced using the radio-frequency single-electron-transistor (RF-SET). Thirdly, the detection of a single-spin state is realized based on the spin-to-charge conversion method. We propose a revolutionary SOI-based technology platform for integrated single-spin qubits, which features double single-spin turnstile devices (SSTDs) built as two parallel SOI-nanowires (SOINWs) with their edges interconnected by another short SOINW. The SSTDs are co-integrated with three other key components: (1) an in-plane single-electron electrometer formed adjacent to the edge of one of the SSTDs, (2) a micro-ESR device formed by using a metallic waveguide and placed near the SOINW interconnect, and (3) a nanomagnet which generates a magnetic field gradient across the single-spin qubits. By integrating all the building-blocks in a nanoscale footprint, we fully investigate initialization, selective manipulation and readout of the single-spin qubits for the first time on Si."
	},
	{
		"grant":165,
		"ID": "EP/H016945/1",
		"Title": "Low complexity delay-tolerant space-time block coding",
		"PIID": "97324",
		"Scheme": "Standard Research",
		"StartDate": "17/05/2010",
		"EndDate": "03/10/2013",
		"Value": "359114",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Systems Engineering",
		"OrgID": "113",
		"Investigators":[
		{"ID": "97324", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BT Group"}
		],
		"Summary": "Future wireless systems are expected to involve a plethora of small, low-cost communication nodes which will be widely distributed in the infrastructure of cities to provide truly pervasive and seamless communication and other services such as sensor networks. Perhaps the most fundamental challenge in these systems as in all wireless communications is channel fading. In these pervasive wireless systems, however, the individual nodes may be equipped with only a single antenna due to cost and size constraints. One powerful strategy to combat channel fading in the above systems is to apply space-time block coding (STBC) in a distributed fashion: creating and harnessing space diversity by enabling a cluster of wireless nodes to relay signals for each other and effectively create a distributed (or virtual) antenna array - with each relay node serving as one antenna element in the STBC array. A major challenge to distributed STBC is that the system is fundamentally asynchronous: signals from the relay nodes tend to arrive at the destination node at different times. Most existing works so far have focused on recerver based schemes. To tackle the above challenge more effectively, this proposal will employ a more fundamental and flexible approach: developing coding and modulation structures which are inherently delay-tolerant (coherent or non-coherent). In this way we move at least part of the problem from the receiver to the transmitter. Considering the fact that most wireless nodes are powered by batteries, equally important is to ensure low complexity both at the relays and at the receiver. The results of this project will enable the so far largely theoretical benefits of cooperative diversity to be realised in practical wireless networks."
	},
	{
		"grant":166,
		"ID": "EP/H017402/1",
		"Title": "CARDyAL: Cooperative Aerodynamics and Radio-based DYnamic Animal Localisation",
		"PIID": "32038",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2014",
		"Value": "422852",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "32038", "Role": "Principal Investigator"},
		{"ID": "-2886", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The primary scientific objectives of the CARDyAL project lie (i) in the challenges inherent in creating very lightweight sensing devices capable of accurate localisation; (ii) in the use of those devices in measuring the dynamically changing relative location of a cooperative group of animals in a range of different contexts for which GPS-based localisation techniques are inappropriate; (iii) relating that dynamic behaviour to scientific questions of cooperative fluid dynamics, energetics, and social biology using appropriate physiological data; and (iv) using the measured information about team organisation to inform a particle swarm optimisation model that will be used as a potential way of reducing energy consumption within and outside this application domain. The ability to make such measurements, and thus to undertake science in these areas, is currently very limited and the problem of achieving this is both challenging in research and engineering terms, and likely to be dependent on the environment for which the particular solutions are created. However, the applications for such technologies are numerous, varied and of significant scientific and strategic importance to the biological sciences community and to the public at large.The technology developed in this project is intended to be inherently transferrable: the hardware and software base for tags capable of relative localisation (a) indoors (b) in long-term deployments (c) in high dynamic environments has applications both to other animal models - understanding the dynamics of groups for purposes of conservation, ecology, welfare or epidemiology - and to the monitoring of humans for better facilities management, workplace design, emergency service and military use, amongst other things. The data reduction and analysis methodologies are applicable to a range of situations in which dynamic and social structure is important, and the relationship between true biological data and a derived PSO model is of substantial scientific interest, and of potential applicability to optimisation problems of many sorts, e.g. energy minimisation, in the computer graphics industry in terms of more accurately representing swarm movements, and in swarm robotics of various types, in path planning and control. In terms of the biology, the availability of this technology in a usable form factor allows research to be performed in a way that is not currently possible, and, consequently, allows the potential to ask scientific questions that are not currently capable of being answered.Thus CARDyAL involves research and engineering in the field of lightweight wireless sensing and wireless localisation that would lack specificity without the constraints and demands of a real application domain, and it involves research in the field of animal sciences that it would not be possible to conduct without the sensing devices and their associated algorithms. There are advances to be made in each of the fields that could not be made without the active engagement of the other and this co-dependency is innately both translational (high risk) and high reward since it is founded on a truly symbiotic programme of research in which neither of the constituent research fields dominates."
	},
	{
		"grant":167,
		"ID": "EP/H017461/1",
		"Title": "High-integrity Java Applications using Circus",
		"PIID": "103370",
		"Scheme": "Standard Research",
		"StartDate": "22/06/2010",
		"EndDate": "30/09/2015",
		"Value": "1029474",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "103370", "Role": "Principal Investigator"},
		{"ID": "16787", "Role": "Co Investigator"},
		{"ID": "36990", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "AWE Plc"},
		{"Number": "1", "Name": "IBM Canada Ltd"},
		{"Number": "2", "Name": "Praxis Systems Ltd"},
		{"Number": "3", "Name": "Sun Microsystems Ltd"}
		],
		"Summary": "The use of computers and computer programs is pervasive nowadays, but every computer user knows that programs go wrong.  While it is just annoying when our favourite text editor loses a bit of our work, the consequences are potentially much more serious when a computer program that, for instance, controls parts of an airplane goes wrong.  Software validation and verification are central to the development of this sort of application. In fact, the software industry in general spends a very large amount of money in these activities. One of the measures taken to promote correctness of programs is the use of a restricted set of features available in programming languages.  This usually means that most of  the more recent advances in software engineering are left out.  In this project, we propose to provide development, validation, and verification facilities that allow object-orientation and a modern real-time computational model to be used for the programming of safety-critical systems.  In particular, we will work with one of the most popular programming languages: Java, or more specifically, its profiles for high-integrity engineering proposed by the Open Group.  As our main case study, we will verify parts of the controller of the first Java Powered Industrial Robot, developed by Sun. One of our collaborators, a senior engineer in Sun tells in an interview that  Distributed Real-Time Systems are really hard to build and the engineering community doesn't really know how to build them in a coherent repeatable way. (java.dzone.com/articles) Real-Time Java is entering the industrial automation and automotive markets. Lawyers did not allow the Java Robot to get anywhere near a human, even in a JavaOne conference demo. To proceed in that kind market, better support is needed.Programming is just one aspect of the development of a modern system; typically, a large number of extra artefacts are produced to guide and justify its design.  Just like several models of a large building are produced before bricks and mortar are put together, several specification and design models of a program are developed and used before programs are written.  These models assist in the validation and verification of the program. To take our civil engineering metaphor one step further, we observe that, just like there can be various models of a building that reflect several points of view, like electricity cabling, plumbing, and floor plans, for example, we also have several models of a system.  Different modelling and design notations concentrate on different aspects of the program: data models,  concurrent  and reactive behaviour, timing, and so on.  No single notation or technique covers all the aspects of the problem, and a combination of them needs to be employed in the development of large complex systems. In this project, we propose to investigate a novel integrated approach to validation and verification. Our aim is to provide a sound and practical technique that covers data modelling, concurrency, distribution, and timing.  For that, we plan to investigate the extension and combined use of  validation and  verification techniques that have been successfully applied in industry.  We do not seek an ad hoc combination of notations and tools, but a justified approach that provides a reliable foundation for the use of practical techniques.  We will have succeeded if we verify a substantial part of the robot controller: using a model written in our notation, we will apply our techniques to verify parts of the existing implementation,  execute it using our verified implementation of Safety-critical Java. Measure of success will be provided by our industrial partners and the influence of our results in their practice or business plans."
	},
	{
		"grant":168,
		"ID": "EP/H017585/1",
		"Title": "Verification of Shared-Memory Concurrent Software",
		"PIID": "-202368",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "28/02/2014",
		"Value": "428481",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-202368", "Role": "Principal Investigator"},
		{"ID": "-114981", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Software products are becoming increasingly complex. One of the chiefreasons for this is the demand for concurrent software thatefficiently exploits multiple execution cores. Such systems, such asIntel's Core Duo, have become ubiquitous over the last two or threeyears. Unfortunately, developing reliable concurrent programs is adifficult and specialised task, requiring highly skilled engineers,most of whose efforts are spent on the testing and validationphases. As a result, there is a strong economic andstrategic incentive for software houses to automate parts of theverification process.Random simulation and testing, while automated, has severelimitations, particularly in the case of concurrent software, in whichthe plethora of possible thread interleavings often conspires toconceal design flaws. Formal verification, on the other hand, can alsobe automated, and tools that implement it check a concurrent programfor all its possible behaviours.Numerous tools to hunt down functional flaws in hardware designs have beenavailable commercially for a number of years. The use of such tools iswidespread, and there is a broad range of vendors. In contrast, the marketfor formal tools that address the need for quality software---and even moreso for concurrent software---is still in its infancy.The proposed research project focuses on shared-variableconcurrency, i.e., eliminating programming errors related tomulti-threaded programs in which the threads communicate via a sharedportion of the memory. This programming paradigm is frequently used,and is the predominant form of concurrency on commodity computingsystems. Furthermore, errors relating to concurrency often depend onthe process schedule, which is difficult to control. As a consequence,such errors are difficult to test for and to reproduce, yet can havewide-ranging and potentially devastating consequences.We propose to investigate (i) verification by means of automatedsummarisation of threads, (ii) identification of transactions,enabling partial-order reductions, and (iii) Craiginterpolation to derive thread invariants. Our primary target arelow-level applications written in C/C++, and we will supportboth the POSIX thread API and the WIN32 thread API to maximizethe applicability of our research. We will evaluate the benefit of ourmethods and tools in collaboration with industrial users."
	},
	{
		"grant":169,
		"ID": "EP/H017690/1",
		"Title": "Query-driven Data Acquisition from Web-based Data Sources",
		"PIID": "-185626",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2014",
		"Value": "499027",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-185626", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The functioning of entities as diverse as enterprises and government agencies depends onobtaining high-quality data.Increasingly these entities depend on external sourcesfor their operational data: critical datais obtained dynamically via web services, is extracted from web pages,or is purchased from third parties.  These sources can differ radicallyin their completeness,  accuracy, and availability. It is not possible for applications to indexand explore data from each source in advance of querying:there are too many sources, they are too costly to access, and the data in themmay be refreshed constantly. How should data acquisition proceed in such situations?In this project we will develop algorithms for answering queries in the presence of large numbers ofweb-based data sources, sources that may overlap substantially in their datasetsbut have different access restrictions and costs. Our approach will make use of schema information about thedata an application is querying:  data format, integrity constraints, and any prior knowledge of costs that maybe available. The core of the project will be algorithms for answering a query by interactively exploring the sources,dynamically pruning out irrelevant or exhausted sources in the process."
	},
	{
		"grant":170,
		"ID": "EP/H018190/1",
		"Title": "FTIR USING ASYNCHRONOUS FEMTOSECOND OPOS: A NEW PARADIGM FOR HIGH-RESOLUTION FREE-SPACE MID-INFRARED SPECTROSCOPY",
		"PIID": "46128",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2010",
		"EndDate": "31/01/2013",
		"Value": "347735",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "46128", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal for an EPSRC-NPL Postdoctoral Research Partnership addresses the call theme identified as,  Photonic Technologies for Optical Remote Sensing in Carbon Capture and Storage and Other Climate Change Applications,  and will be conducted in partnership with Dr Tom Gardiner, head of NPL's Environmental Measurement Group.Using their unique infrared differential absorption LIDAR (DIAL) system, NPL's Environmental Measurement Group carries out remote monitoring for the qualification of airborne emissions from industrial installations such as land-fill repositories, petrochemical stacks, cement factories etc.  Such quantitative monitoring is an essential procedure in the government's compliance - via  carbon taxes  imposed on polluters - with the reductions in CO2 and CH4 emissions mandated by the Kyoto Protocol.While being a mature technique, infrared DIAL has reached the limits of its detector technology, and moreover is seriously constrained by its inability to simultaneously measure different gases, a direct consequence of the use of two narrow-linewidth laser wavelengths in the technique.  DIAL lacks the ability to obtain true spectroscopic information in a single measurement, and therefore cannot benefit from the sophisticated analysis tools that exist in FTIR to extract the concentrations of multiple species from an absorption spectrum.  A system combining the bandwidth and spectral resolution of FTIR, with the remote-sensing facility of DIAL would enhance and extend NPL's technical capability to monitor the emissions of greenhouse gases implicated in climate change.We propose to address this requirement by combining...(a) our world-leading work in femtosecond (fs) FTIR spectroscopy (we were the first group to demonstrate the technique)(b) our world-leading expertise in mid-infrared OPO frequency combs (again, we were the first group to demonstrate a fs OPO frequency comb)Normally FTIR spectroscopy is carried out with thermal sources (globars etc) that produce poorly collimated IR beams that are incompatible with free-space propagation over extended distances.  By contrast, the output from a fs OPO has a broad IR bandwidth, approaching that of a thermal emitter, but - critically - has the spatial coherence of a laser, permitting free-space propagation over long distances. A fs OPO therefore provides a route to implementing free-space FTIR across several hundred cm-1 in a single measurement - sufficient to probe the characteristic absorption lines of many chemical species in a single measurement.Conscious of the practical challenges of implementing an interferometric technique over long distances, we propose a novel technical approach, to implement a robust, no-moving-parts FTIR system, which we will progress, in stages, from a bench-top demonstrator to a free-space field-trial at NPL's Teddington site.The concept is based around replacing a mechanically-scanned optical delay line with two asynchronous, phase-coherent mid-IR pulse sequences derived from identical OPOs. Uniquely, this approach is robust, and has the potential to acquire a high (< 0.01 cm-1) resolution spectrum in only a few milliseconds. It offers precisely range-gated detection over propagation distance of potentially 100's of metres, and perhaps further using infrared time-correlated single-photon-counting detection (a technique which the project will have access to at Heriot-Watt).In comparison with DIAL, the technique offers greater spectral discrimination and the ability to simultaneously probe for multiple gases. Furthermore, it is all-solid-state, providing a potentially more compact and efficient solution than DIAL, and avoids the use of dye laser technology, eliminating the associated carccarcinogen hazard. It will extend sensing to new chemical species where suitable DIAL lines are unavailable, and is compatible with heterodyne / RF lock-in detection techniques for improved signal:noise performance."
	},
	{
		"grant":171,
		"ID": "EP/H018557/1",
		"Title": "EPSRC Network on Vision and Language (V&L Net)",
		"PIID": "103570",
		"Scheme": "Network",
		"StartDate": "01/03/2010",
		"EndDate": "31/10/2013",
		"Value": "104261",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing, Engineering & Maths",
		"OrgID": "19",
		"Investigators":[
		{"ID": "103570", "Role": "Principal Investigator"},
		{"ID": "-114384", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The amount of digital information accessible on the web and, more generally, in data repositories of various sorts isgrowing at an ever faster pace. Increasingly, digital information means visual content (image and video), and thisdevelopment has resulted in a situation where computational solutions are lagging behind a diverse range ofcurrent image/video search, processing and management needs. There is a big, and as yet unbridged, semanticgap between visual content and language. Finding solutions for image/video retrieval, automatic image/videoannotation and similar challenges will require this gap to be bridged, and this in turn will require expertise from boththe computer vision (CV) and natural language processing (NLP) fields. Yet, while language and vision are the twoprimary modalities for human perception and computer-mediated communication, the two corresponding computingscience disciplines hardly talk to each other, and this is part of the reason why the language-vision gap is still sowide: NLP research is perhaps not aware enough of the range of possible applications involving visual content andtheir specific language processing requirements; CV can tend to underestimate the complexity of thelanguage processing problem, and currently uses mostly basic language processing technology, whereassophisticated, high-performance tools exist.We propose an EPSRC Network on Vision and Language, V&L Net, to create a forum for researchers from CVand NLP to meet and exchange ideas, expertise and technology. The UK has some of the world's leadingresearchers in NLP and CV. V&L Net aims to tap this body of expertise to create new strategic partnerships aimed atnarrowing the language-vision gap by developing the theory required for solutions to the difficult challenges posedby our increasingly multi-modal world. A successful network will place the UK at the forefront of developing solutionsat the language-vision intersection which have clear commercial potential.Our overarching goal in V&L Net is the creation of a new interdisciplinary research community working towardscomputational solutions for challenges that involve both language and vision. By (i) bringing researchers from thetwo currently separate disciplines of computer vision and language processing together, (ii) facilitating access torelevant information, expertise, and resources, and (iii) stimulating research and pump-priming individual researchprojects, we aim to engender a substantial increase in interdisciplinary research activity. Through this increase inwork bringing to bear expertise from both computer vision and language processing, we expect to see a stepchange in progress towards solutions for a range of real-world challenges as well as theoretical questions. Whilethe latter will tend to have a more long-term impact (laying the groundwork for future breakthroughs), the formerhave substantial potential to result in ground-breaking new products and services that will improve people's qualityof life in diverse ways even in the short to medium term. People with impairments in sight, hearing and cognitive ability will benefit from assistive technology that will help them access multiple modalities. Improvements in image search and retrieval will enhance online search experience, as well as help institutions such as hospitals and police forces to cope with the massive amounts of images and videos they deal with daily."
	},
	{
		"grant":172,
		"ID": "EP/H019162/1",
		"Title": "Sandpit:  The Programmable Rhizosphere",
		"PIID": "-2799",
		"Scheme": "Standard Research",
		"StartDate": "09/12/2009",
		"EndDate": "08/06/2013",
		"Value": "972909",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Plant Sciences",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-2799", "Role": "Principal Investigator"},
		{"ID": "98121", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Humans have striven for centuries to control and exploit living organisms for their own purposes. Agricultural practices have been developed to maximise the yield of plants and animals. More recently, microbial systems have been manipulated to increase their utility in the food, biotech and brewing industries. Many of these changes have been achieved through breeding and chance selection for improved agronomic characters. Recent developments in genetic engineering have allowed scientists to apply  precise perturbations that lead to beneficial changes in an organism. However, the complexity of biological systems makes it difficult to manually design and implement large changes that predictably produce an intended phenotype using conventional genetic engineering techniques. Our ability to synthesise DNA far outstrips our ability to design new genetic systems. Synthetic Biology holds the promise of rational design and reproducible fabrication of biological circuits that can be used to introduce a desired function in an organism. One of the main premises of this approach is that engineering principles should be applied to the design of modular circuits from well-characterized parts and components, using defined composition rules. A framework that enables this approach to the engineering of biology has, to date, been lacking. In this project, we propose to develop such a framework, and a unique library of new DNA parts. Specifically, we propose to tackle the problem of how cellular circuits in organisms (such as microbes and plants) can be designed in to self-organise and interact with other organisms in a predictable and robust fashion. To this end we will develop novel mathematical and computational approaches that automatically transform a quantitative description of a desired function into a circuit design that implements this function in bacteria. In addition we will generate a collection of DNA parts that will allow the construction of new channels of communication between different cell populations or organisms, and the pathways for symbiotic exchange of nutrients. There are many situations where improvements in the ability to regulate cells, and to form stable new ecologies, would be of benefit to humans. These range from applications in tissue engineering through to bioremediation, biotechnology and bioenergy. In this project we have chosen to focus on the relationship between plants and soil bacteria that normally live alongside the root system. We wish to engineer communication between a model bacterium and model plant, to allow negotiation and establishment of a new symbiotic relationship. The system would have many applications for improvements in sustainable agriculture, bioproduction and food security, such as improvements in soil use, pest resistance, weed suppression and creation of new crop plants capable of nitrogen fixation."
	},
	{
		"grant":173,
		"ID": "EP/H020055/1",
		"Title": "Delta-doped diamond structures for high performance electronic devices",
		"PIID": "8103",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "30/09/2013",
		"Value": "552730",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "London Centre for Nanotechnology",
		"OrgID": "81",
		"Investigators":[
		{"ID": "8103", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Diamond Microwave Devices Ltd"}
		],
		"Summary": "The combination of extreme electronic and thermal properties found in synthetic diamond produced by chemical vapor deposition (CVD) is raising considerable excitement over its potential use as a semiconductor material.  Experimental studies have demonstrated charge-carrier mobilities of >3000cm2V-1s-1 and thermal conductivities >2000 Wm-1K-1. The material has been predicted to have a breakdown field strength in excess of 10 MVcm-1. These figures suggest that, providing a range of technical challenges can be overcome, diamond would be particularly well suited to operation as a semiconductor material wherever high frequencies, high powers, high temperatures or high voltages are required.  This proposal addresses the novel use of 'delta-doping' to realise such devices.In conventional device technology a major limitation to the magnitude of mobility values within a given semiconductor is the presence of ionised impurities which cause carrier scattering.  However, it is these ionised impurities that are the origin of the free carriers within n- or p-doped material.  It is the physical separation of the impurities from the free carriers, such that less scattering occurs and mobility values increase, that lies at the heart of recent improvements in high frequency device performance using III-V semiconductor technology.  One approach to achieve this the formation of very thin, highly doped regions within a homostructure.  Provided the doped, or d, layer is only a few atom layers thick, carriers will move in a region close to, but outside, this layer.  The resultant separation between carriers and the donor/acceptor atoms that created them leads to enhanced mobility.  The advantages offered by d doping in other systems will be valid for diamond, with the additional feature that the problem with the large activation energy of boron can be overcome, as very high concentrations are desirable in the d-layer.  However, the molecular beam epitaxy (MBE) techniques that can be used for III-V semiconductor growth cannot be used with diamond; the need to use plasma-enhanced CVD processes significantly complicates the approach needed to realise atomic-scale modulation-doped diamond structures.While Si and GaAs devices dominate the solid-state microwave device market, they cannot match the power performance of the vacuum tube. One driver for diamond as a semiconductor stems from an interest in replacing vacuum tubes in niche applications. The development of a solid-state alternative would have many benefits including small size, low weight, low operational voltage (compared with vacuum tube devices), and greater robustness.  Current vacuum tube designs, such as magnetrons, klystrons, and traveling-wave tubes (TWT) are usually bulky, often fragile, and expensive (with the exception of magnetrons for microwave ovens, which are manufactured in huge volumes and cost only $10-20/kW). If the intrinsic properties of diamond could be fully exploited through novel delta-doped device design and fabrication, it could compete not only with existing wide-bandgap devices (based on SiC and GaN) but also with TWTs in the entire radio frequency (RF) generation market up to 100 GHz.  The control of power at high voltages is another potential use of the diamond devices that may arise from the proposed programme of study. Theoretically, a single diamond switch could be used to switch power at voltages approaching 50 kV. This is not currently achievable with any other electronic material."
	},
	{
		"grant":174,
		"ID": "EP/H020217/1",
		"Title": "Refactoring and Neutrality in Genetic Programming",
		"PIID": "85889",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2010",
		"EndDate": "31/01/2013",
		"Value": "305240",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "27",
		"Investigators":[
		{"ID": "85889", "Role": "Principal Investigator"},
		{"ID": "15747", "Role": "Co Investigator"},
		{"ID": "103407", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computer programming is difficult. Despite many years of experience, certain problems are very difficult for programmers to solve. To address this, researchers have developed methods where computers automatically create program code from a description of the problem to be solved. One of the most popular forms of automated program creation is called Genetic Programming (GP). In GP, a population of potential solutions is created, tested on the problem, and a new population created by processes inspired by biological evolution, such as genetic mutation and the crossover of genes between individuals. This process is repeated until an effective program is found.GP has proven effective in a number of areas: GP systems have created programs that are as good or better than those created by human programmers in a number of areas, such as robot control, bio-medical data analysis, and the design of electronic circuits. It has been applied in real-world technologies such as the design of antennas for satellites, the analysis of currency markets, improving the design of chemical engineering systems, and the detection of unexploded devices such as landmines. The aim of this project is to make GP more effective by including in the evolutionary process a technique that has risen to prominence in human-based software engineering, known as refactoring. Refactoring means changing the structure of computer programs without changing what they actually do. This is important in the development of software because it means that programs can be simplified and their structure made clearer before programmers work on changing the behaviour. By separating out these two aspects, the programming process is made clearer. These techniques have not been systematically applied to GP in the past.An important reason why this idea is likely to succeed is because it has already been shown that something called neutrality is important for evolution. Neutrality means that there are many more possible genes than there are proteins that can be created by those genes, and therefore one protein can be encoded for by many genes. During evolutionary history, many of the changes that occur are these neutral changes-changes to the genetic encoding, rather than the behaviour that results from that encoding. As part of the project we want to understand how this idea of neutrality can be used to understand the development of these computer programs during their evolution.Overall, we want to rigorously test whether this enhancement - refactoring - is capable of improving the efficiency and effectiveness of this increasingly important technology for the automated creation of computer programs."
	},
	{
		"grant":175,
		"ID": "EP/H020780/1",
		"Title": "Automated Reasoning with Very Large Theories",
		"PIID": "27394",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2009",
		"EndDate": "28/02/2013",
		"Value": "315403",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "27394", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our proposal focuses on first-order reasoning with Very Large Theories (VLTs).A VLT is a collection of formalised knowledge expressed in a logical language.For example, such a VLT can be extracted by programs from largecollections of documents in some domain, such as biology, or from a largecollection of Web sites.Reasoning with such theories means answering queries based on thelogical semantics of the knowledge as opposed to the keyword search.If the project turns out to be successful, it may give rise to new waysof Web search where search for user's queries will be based onsemantics and reasoning."
	},
	{
		"grant":176,
		"ID": "EP/H021426/1",
		"Title": "Combinatorial Optimization Algorithms for Hereditary Graph Classes",
		"PIID": "85034",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2010",
		"EndDate": "31/12/2012",
		"Value": "96293",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "85034", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Developing efficient algorithms for solving combinatorial problems has been of great importance to the modern technological society. The applications include diverse areas such as transportation, telecommunication, molecular biology, industrial engineering, etc. Problems such as assigning frequencies to mobile telephones, can be modeled using graphs, and then the problem is reduced to coloring the vertices of the graph so that no two adjacent vertices receive the same color (of course the interest is in the minimum number of colors, and this is called the chromatic number of a graph). Many other applications in the real world reduce to the same coloring problem on graphs. Unfortunately, finding the chromatic number of a graph and some other optimization problems such as finding the size of a largest clique in a graph, are NP-complete in general. This means that it is highly unlikely that these problems can be solved efficiently on a computer (i.e. it is unlikely that polynomial time algorithms for these problems exist). One of the ways to deal with this situation is to find classes of graphs for which these problems can be solved in polynomial time.This project will focus on developing techniques for obtaining combinatorial optimization algorithms by expoliting structural analysis of hereditary graph classes. Many important graph classes are hereditary (i.e. closed under taking induced subgraphs), such as perfect graphs. For a difficult optimization problem, such as finding the chromatic number of a graph or the size of its largest clique, to be solvable in polynomial time for a given class, it means that this class must have some strong structure. The proposed research is about trying to understand what is this strong structure that will allow polynomial time combinatorial optimization algorithms, and developing techniques for obtaining the desired structure theorems and using them in algorithms."
	},
	{
		"grant":177,
		"ID": "EP/H02171X/1",
		"Title": "The Social Complexity of Immigration and Diversity",
		"PIID": "-8384",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2015",
		"Value": "2710226",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Social Sciences",
		"OrgID": "93",
		"Investigators":[
		{"ID": "-8384", "Role": "Principal Investigator"},
		{"ID": "-8030", "Role": "Co Investigator"},
		{"ID": "43558", "Role": "Co Investigator"},
		{"ID": "-169397", "Role": "Co Investigator"},
		{"ID": "10573", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Ours has been dubbed the 'age of migration'. Immigration is a major political issue, with increasing media coverage, rising anti-immigration sentiment and the rise of anti-immigration political parties. The issue of migration sits centrally within the wider debate about ethnic and religious diversity and its effects on social cohesion. We are still, though, a long way from understanding these issues and their potential consequences. They seem to rest on beliefs about national identity and ethnicity, but cannot be divorced from the effects of social class, education, economic competition and inequality, as well as the influences of geographical and social segregation, social structures and institutions.This project will integrate the two very different disciplines, social science and complexity science, in order to gain new understanding of these complex, social issues. It will do this by building a series of computer simulation models of these social processes. One could think these as  Serious Sims  programmes that track the social interactions between many individuals. Such simulations allow 'what if' experiments to be performed so that a deeper understanding of the possible outcomes for the society as a whole can be established based on the interactions of many individuals. A difficulty with the computer simulation of complex systems is that if they are made realistic (in the sense of how people actually behave) it becomes very complex, which makes the simulation hard to understand, whilst if they are made simple enough to understand they can be too abstract to mean anything useful in terms of real people. This project aims to get around this by making  chains  of related models, starting with a complex, 'descriptive' model and then simplifying in stages, so that each simulation is a model of the one  below  it. The simpler models help us understand what is going on in the more complex ones. The more complex models reveal in what ways the simpler ones are accurate as well as the ways they over-simplify. In this way this project will combine the relevance of social science with the rigour of the  hard  sciences, but at the cost of having to build, check and maintain whole chains of models.Building on an established collaboration between social and complexity scientists in Manchester, this project will integrate the two disciplines to produce new insights, techniques and approaches for policy makers and their advisors. However this will require both the complexity and social scientists to develop new techniques. The complexity scientists will develop new families of computer models that capture several aspects of society in one simulation, including: how the membership of different groups, origins, classes, etc. are signalled by people (e.g. the way they dress, or their attitudes); the advantages and disadvantages of belonging to several different social groups at the same time; how different but parallel social networks might relate to each other; and how the views of people on specific issues might change in response to their friends, wider group and even politicians. The social scientists will develop ways of relating these kinds of models to the rich sources of social data that are available, and will collect additional social data where these sources prove inadequate. They will also ensure that the modelling results are interpreted meaningfully and usefully, in particular in ensuring that they are not over-interpreted. By bringing together the social science evidence, the layers of simulation models and the combined expertise of the researchers this project aims to make real progress in understanding the complex, important yet sensitive issues surrounding the processes that underlie the effects of immigration and diversity on social cohesion and integration. From the beginning it will involve policy experts and decision makers to help guide the project and ensure its relevance."
	},
	{
		"grant":178,
		"ID": "EP/H021779/1",
		"Title": "Evolution and Resilience of Industrial Ecosystems (ERIE)",
		"PIID": "51931",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2010",
		"EndDate": "31/05/2016",
		"Value": "3344524",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Sociology",
		"OrgID": "51",
		"Investigators":[
		{"ID": "51931", "Role": "Principal Investigator"},
		{"ID": "-109887", "Role": "Co Investigator"},
		{"ID": "43761", "Role": "Co Investigator"},
		{"ID": "89326", "Role": "Co Investigator"},
		{"ID": "-167925", "Role": "Co Investigator"},
		{"ID": "-99485", "Role": "Co Investigator"},
		{"ID": "47898", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Evolution and Resilience of Industrial Ecosystems programme (ERIE) will address a series of fundamental questions relating to the application of complexity science to social and economic systems. Our programme of research aims to embed cutting-edge complexity science methods and techniques within prototype computational tools that will provide policymakers with realistic and reliable platforms for strategy-testing in real-world socio-economic systems. The programme includes the gathering of data from case studies, the development and application of appropriate theoretical and computational techniques, simulation using agent-based models and the incorporation of all these elements into 'serious games' for use by policymakers. We will study the negotiation of policy goals and options, explore the role of models in policymaking and involve policymakers in the design and testing of our strategy tools.The programme will focus on a crucial aspect of the UK economy: the way in which firms are interdependent on each other, with the interrelationships being multi-level and multi-valued. Within an industrial 'ecosystem', there can be relationships of supply and demand; the transfer of knowledge; competition for labour; the transfer of materials down supply chains; negotiation over standards; collaboration in trade associations and unions; and innovation, product differentiation and branding. We will use mathematical and computational approaches to model these layered, nested, multiscale systems, where the links between actors are dynamic and the exchanges between them are unpredictable, fluctuating and perhaps sporadic. Within this context we will examine concepts and measures of resilience (the ability to recover from external shocks), emergence (the ways in which social institutions arise from individual activities) and immergence (the ways in which individuals react to institutional constraints). This leads us to some of the most intriguing open questions of complexity science. We will seek answers inspired by the real-world industrial ecosystems as captured in our case studies. Our vision is to provide models of multi-level socio-economic systems that are useful for decision-makers aiming to 'steer' towards policy-relevant goals. It is not our intention to provide 'the' policy solution to policy problems (specifically, it is not our intention just to show how particular ecosystems may be made more resilient or more sustainable), but rather to provide a suite of tools which will allow decision makers and their representatives to investigate alternative scenarios given a set of assumptions and initial conditions.We will apply the methods of data assimilation, largely developed in the context of weather forecasting, to incorporate the inevitably incomplete data from case studies into agent-based models, on an ongoing basis, with the aim of providing 'predictive' tools that are continually updated with real-world data. By 'prediction' here we mean the identification of alternative scenarios along with estimates of the probability that each will be realised over given time frames, and estimates of the sensitivity of these to uncertainties in the data and underlying model. It is an integral part of ERIE to study - and involve - those involved in the case study sites. One research stream is concerned with studying those with a stake in the system, as controllers, decision makers, customers, workers, etc., their goals, policy options and their links with the industrial ecosystems that they are interacting with.The research programme is divided into four streams, each consisting of a number of cross-disciplinary projects.  Four post-doctoral researchers and a project officer will work on the programme, with seven Investigators from the disciplines of mathematics, computing science, environmental science and sociology, and 9 PhD research students, the latter funded from internal University of Surrey resources."
	},
	{
		"grant":179,
		"ID": "EP/H022031/1",
		"Title": "Participation in healthcare environment engineering",
		"PIID": "-169518",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2010",
		"EndDate": "31/08/2015",
		"Value": "1122267",
		"ResearchArea": "Built Environment",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-169518", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Coventry and Warwickshire NHS PT"},
		{"Number": "1", "Name": "Edward Cullinan Architects"},
		{"Number": "2", "Name": "Enprecis"},
		{"Number": "3", "Name": "NHS West Midlands"},
		{"Number": "4", "Name": "Ove Arup & Partners Ltd"},
		{"Number": "5", "Name": "Plymouth Hospital NHS Trust"}
		],
		"Summary": "This research programme will positively affect health and well-being and improve healthcare effectiveness to benefit the UK population.  The aim is to create a team to deliver high impact research to improve the user experience of healthcare environments, through user participation in design, engineering and decision making.  This research will produce (1) better healthcare environment designs; and (2) new methods for end user participation in engineering.Engineering produces things (environments, products, processes) to improve our quality of life, yet the people who will ultimately use these things are often not involved in their design (or if they are, this often amounts to  tokenistic consultation , rather than embedded best practice).  Decision making needs to directly involve the people who use these things, to capture their subjective opinions, ideas, language, feelings, and needs and translate these into a format meaningful for engineers.  Involving people in engineering can have a transformative effect on new products and environments, but since this is not traditionally part of formal engineering training, the benefits of participation still have huge, untapped potential. Furthermore, the notion that engineering can be enhanced through working with other disciplines is only just beginning to have an impact in engineering practice. A radical step change is needed now, to equip our next generations of young engineers with the know-how to think in new creative waysParticipation is most powerful when it contributes to improving quality of life, and healthcare is the most timely and relevant application of this. The UK has been left with a legacy of aged hospital buildings that are unsuitable for the needs of today's increasing and ageing population.  The design of healthcare environments can be linked to health outcomes so it is increasingly important to optimise the design and user experience of new build and redeveloped healthcare projects.  The challenges faced by healthcare environment design are complex.  Infection control, safety, security and environmental issues all impose constraints, and now the advent of patient choice means that the whole hospital environment must effectively  sell  the hospital as a carefully packaged experience. Improving healthcare design through participation requires a highly inter-disciplinary approach.  This research programme draws together engineering with design, architecture, psychology, science, ICT and healthcare.   Hospitals and industry will provide real life users and opportunities for piloting novel participatory design approaches (for example, in creating a better experience for patients in the Emergency Department).    Government involvement will help to drive forward policy change, and crucially, end users (patients, staff, decision makers) are involved throughout. This programme of research is executed through 4 core research themes: (1) methods of participation, including exploiting developments in Information and Communication technology (ICT) as an enabler to participation; (2) best-use of representations of future healthcare environments for co-designing with, and presenting concepts to stakeholders; (3) data capture from these representations, and the best use, re-use and presentation of data to decision-makers; and (4) production of an evidence-base for this research by measuring the effects of engineering and design interventions on health and healthcare effectiveness.  The ultimate vision is that this work will launch a step-change in engineering research, which will impact upon practice and education.  This programme will set a precedent for user involvement in engineering, demonstrating how highly inter-disciplinary research teams can inject creativity and humanity into the creation of environments, products and services in new ways - which will lead to true innovation in design and engineering in the 21st Century."
	},
	{
		"grant":180,
		"ID": "EP/H022236/1",
		"Title": "Illuminating Colour Constancy: from Physics to Photography",
		"PIID": "49980",
		"Scheme": "Standard Research",
		"StartDate": "15/05/2010",
		"EndDate": "14/05/2014",
		"Value": "643759",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "102",
		"Investigators":[
		{"ID": "49980", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Imsense Ltd"},
		{"Number": "1", "Name": "International Commission on Illumination"}
		],
		"Summary": "In daily life, we depend on colour images which represent the real world, from photographs of key personal events to pictures of possible purchases. In general, these are poor approximations of the real thing. Our aim is to understand better how we perceive colours in the real world, and how to recreate that perception with images. Central to these aims is colour constancy,  a fundamental phenomenon which keeps object colours stable even under large changes in the colour of the illumination - we see an apple as red whether it is under bluish daylight or yellowish tungsten light. Camera sensors, which faithfully record the changing light signals, do not naturally possess colour constancy. But digital cameras are often equipped with special colour balancing modules to cope with changes in lighting, and the photographs they produce may be further processed to remove colour casts. In computer vision, such 'color correction' algorithms are necessary to enable machines to use colour as a reliable cue - for example, in automated grading of manufactured goods such as tiles. Human vision and computer vision are typically studied in isolation from each other: the first aims to understand why colours appear as they do to humans, and the other to make them as useful as possible to machines, regardless of how they appear. These two goals are generally not identical, because neither human nor computer colour constancy is perfect.To bridge colour constancy from humans to machines we will perform an innovative set of experiments. First, we will systematically study illuminant metamerism. Metamerism is what makes all image reproduction work: two stimuli with vastly different colour spectra can induce the same colour percept. The light invoking a white percept on a TV has a highly spiky spectrum compared to the flat spectral reflectance of a piece of white paper in daylight. Yet, illuminants which look the same when shining on white paper can sometimes make other surfaces change appearance. We experience this phenomenon when we buy clothes which look good under the artificial shop lights but less satisfactory when we take them outdoors. We will quantify this effect for real scenes under real lights using a new 'tuneable' spectral illuminator with which we can generate any light spectrum. Our second innovation is to make use of newly available High-Dynamic-Range (HDR) displays. In contradistinction to the real world where the brightest point in the scene may be a 100000 times as bright as the darkest point, most displays struggle to produce a dynamic range of even 1000:1 and printed photographs are at most 100:1. Yet we know that colour perception depends on the overall dynamic range of the scene. The new HDR displays can output contrast ratios of 100000:1 and we will use them to measure constancy in lab conditions but with real world brightnesses. A third challenge that we face in making colour photographs match our perception of the real world is the inaccuracy of colour memory. Typically, when we view a photograph, we do not have the real thing to compare it with, but must recall the original scene from memory. The imperfections of our memory then may taint our judgment. It is well known that our memory colours for familiar objects such as sky, grass, and skin tend to be 'over-saturated' -- grass may be remembered as greener and the sky as bluer than they actually are. Thus, when we test colour correction algorithms by asking people which image they prefer, we might find that they do not prefer the one that most accurately reproduces the original scene, but instead matches their imperfect memory. We will quantify these effects of memory and preference. Finally, our research will, at all stages, consider how measured percepts of colour might be predicted by mathematical models. Ultimately, we will design algorithms to automatically see colours as we do, making for better photographs and more useful vision machines."
	},
	{
		"grant":181,
		"ID": "EP/H022384/1",
		"Title": "Complex Photonic Systems II (COPOS II)",
		"PIID": "16932",
		"Scheme": "Platform Grants",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2015",
		"Value": "1203033",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "16932", "Role": "Principal Investigator"},
		{"ID": "12308", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Ericsson UK"}
		],
		"Summary": "This proposal seeks the renewal of the COPOS Platform grant at the Centre for Photonic Systems at Cambridge University. The rationale for the work is the premise that the field of photonics must progress from focussing on single function systems (for example optical fibre links), where advances in performance have been primarily due to developing bespoke high speed components, to one where much more complex systems can be constructed using greater levels of integration. Such advances should benefit from the expanding range of optical materials available, especially flexible materials. It is increasingly accepted that should such goals be achieved, then photonics can become a technology of ubiquitous application in the 21st Century, as electronics became in the 20th Century. For it to be fully adopted however, not only should the integration technology deliver the performance and functionality required, but it should use readily available processes. In short the technology should become commoditised and available to a much wider range of manufacturers.As a result, COPOS II will seek to build on past research at Cambridge to develop integration fabrication techniques which can be used by non-photonic companies, in particular those involved in printed circuit board manufacture. The work will develop exemplar sub-systems which will not only allow greater functionality in performance, but can be made at low cost (even for low volumes), whilst addressing growing issues such as energy consumption. There will be two main integration technologies at the heart of the project: (i) the use of siloxanes formed directly on large area printed circuit boards so that multilayer electronic and photonic circuits can be formed using low cost processing, with the optical and electronic components populated using pick-and-place techniques, and (ii) the use of quantum dot III-V material systems for forming integrated circuit functions (such as large port count active routers) which cannot be realised using the siloxane technology, but which can be readily integrated with it. In the case of (ii), it is intended later in the project to print active components (for example organic LEDs and detectors) directly onto the board and to introduce capacitive coupling to electronic components for the lowest possible power consumption. As a result, by developing these two integration approaches, we believe that we can meet the great majority of future integration requirements for photonic systems.It should be noted that a series of specific application goals including interconnect, Ethernet, healthcare and imaging systems, have been set to encourage the research to advance in as adventurous a manner as possible. In introducing a more challenge- based approach to our research, we are keen to extend the level of electronic and photonic integration, such as: (i) much greater use of complementary electronic signal processing. (ii) introduction of printing techniques both for electronic and optical integration. (iii) the introduction of much greater levels of (non-wavelength) parallelism in optical circuits.In addition to enabling us to develop our research, the platform grant will also allow us to innovate in our research practice and hence deliver additional benefits. For example, the grant would enable us to develop new techniques to: (i) manage our research and develop it strategically while flexibly engaging in new concepts. (ii) retain the wide range of skills which are so important in this type of activity whilst empowering key members of our group to build up their own careers by broadening their expertise. (iii) grow our outreach activities. (iv) engage in new industrial and international academic collaborations, whilst developing our existing ones."
	},
	{
		"grant":182,
		"ID": "EP/H023631/1",
		"Title": "Biologically inspired transportation: a distributed intelligent conveyor",
		"PIID": "84339",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2011",
		"EndDate": "31/12/2013",
		"Value": "360895",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Environment and Technology",
		"OrgID": "21",
		"Investigators":[
		{"ID": "84339", "Role": "Principal Investigator"},
		{"ID": "71420", "Role": "Co Investigator"},
		{"ID": "50432", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A parallel manipulator is a massive array of simple individual actuators with a small power density that collectively transport and position objects with masses considerably higher than the force generated by a single actuator alone. This design is inspired by the biological phenomena of cilia, small hair-like structures on the surface of cells which can either sense local properties such as in the rod photoreceptors for vision or in olfactory neurons for smell, or can move in coordinated wave action to move liquid over their surface, as in the trachea and kidneys. Employing these capabilities in an analogous array of micro-actuators will produce a conveyor of parallel intelligent manipulation able to sense object properties, move them in different directions and effectively sort objects according to their properties. Crucially, the actuator array will be capable of communicating local information about objects to other parts of the array to enable coordinated action.Parallel intelligent manipulation plays an increasingly important role in intelligent robotics, computer science and intelligent manufacturing systems. Significant advantages of the distributed manipulating system are task flexibility (it can be dynamically reprogrammed to implement another task); massive-parallelism (it can process several different objects simultaneously, and different parts of the manipulator can perform separate tasks concurrently); fault tolerance (faults in single actuators do not restrict performance of the system as a whole, so operation of the system can be maintained); autonomy; and the ability to process objects simultaneously and independently.The overarching aim of the project is to build an intelligent autonomous massively parallel manipulator for distributed sensing, recognition, analysis, sorting, transportation and manipulation of light-weight objects. A paradigm of reaction-diffusion computing, i.e. information processing and computation by spreading wave-patterns in non-linear media, will be employed in the control system of the manipulator.Using evolutionary computation and machine learning, we will develop new principles and implementations for non-linear medium based control, and introduce a range of algorithms for distributed sensing (of object properties such as shape), filtration (sorting different objects according to common characteristics), orienting (ensure objects are facing and moving in the correct direction), positioning (moving objects into the correct path of travel on a different part of the manipulator) and shape-determined transportation of the objects.This manipulator system not only has the potential to impact upon the academic community in terms of the advancement of evolutionary algorithms, reaction-diffusion computing, and intelligent robotic systems, but also has ready application domains in industry such as high-tech manufacturing, enabling an advanced network of sensors to control dynamics of mechanical components, automation of assembly of nano-devices, and medical applications such as prostheses and computer controlled implants."
	},
	{
		"grant":183,
		"ID": "EP/H02364X/1",
		"Title": "Room Temperature Terahertz Quantum Cascade Lasers on Silicon Substrates",
		"PIID": "45941",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2010",
		"EndDate": "31/12/2013",
		"Value": "643144",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Electrical Engineering",
		"OrgID": "49",
		"Investigators":[
		{"ID": "45941", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Kelvin Nanotechnology"},
		{"Number": "1", "Name": "Teraview Ltd"}
		],
		"Summary": "The THz part of the electromagnetic spectrum has a number of potential applications which include oncology (skin cancer imaging), security imaging, THz bandwidth photonics, production monitoring and astronomy. The U.K. has been one of the pioneering countries in THz research but also in the exploitation of the technology with a  number of companies including TeraView, QMC Instruments and Thruvision. At present most commercial imaging and spectroscopy systems use expensive femtosecond lasers with photoconductive antenna which fundamentally limits the power output to the microWatt level. Virtually all the applications referenced above require room temperature sources with over 10 mW of output power if parallel, fast, high performance imaging and/or spectroscopy systems are to be developed.While interband recombination of electrons and holes in Si and Ge are inefficient due to the indirect bandgap of the semiconductors, intersubband transitions provide an alternative path to a laser for low energy radiation such as THz frequencies. Intersubband unipolar lasers in the form of quantum cascade lasers have been demonstrated using III-V materials. Powers up to 248 mW at 10 K have been demonstrated at THz frequencies but due to polar optical phonon scattering and the associated reduction in intersubband lifetimes as the temperature is increased, such devices only operate at cryogenic temperatures. Previous work has been undertaken on p-type Si/SiGe quantum cascade lasers but due to large non-parabolicity and large effective mass (0.3 to 0.4 m_0) in the valence band, significant gain above 10 cm^-1 is difficult to engineer.In this proposal, we propose to use pure Ge quantum well designs and L-valley electrons for the first experimental demonstration of a n-type Si-based quantum cascade laser grown on top of a Si substrate. We demonstrate that the low effective of 0.118 m_0 and long non-polar lifetimes in the Ge/SiGe system potentially provide gain close to values demonstrated in GaAs THz quantum cascade lasers at 4 K and also potentially allow 300 K operation. Further the cheap and mature available Si process technology will allow at least a x100 reduction in the cost of THz quantum cascade lasers compared to GaAs devices. Such devices could be further developed into vertical cavity emitters (i.e. VCSELs) for parallel imaging applications or integrated with Si photonics to allow THz bandwidth telecoms. Finally we propose optically pumped structures which have the potential for broadband tunability, higher output powers and higher operating temperatures than THz quantum cascade lasers.This programme has brought together the modelling and design toolsets at Leeds University with the CVD growth expertise at Warwick University combined with the fabrication and measurement expertise of SiGe devices at Glasgow University to deliver internationally leading research. We have a number of industrial partners (AdvanceSis, Kelvin Nanotechnology and TeraView) who provide direct exploitation paths for the research. Successful room temperature quantum  cascade lasers are an enabling technology for many new markets for THz applications including oncology (skin cancer imaging), security imaging, production monitoring, proteomics, drug discovery and astronomy."
	},
	{
		"grant":184,
		"ID": "EP/H023666/1",
		"Title": "Ferroelectrics for Nanoelectronics (FERN)",
		"PIID": "11775",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2010",
		"EndDate": "30/11/2013",
		"Value": "528499",
		"ResearchArea": "Functional Ceramics and Inorganics",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "11775", "Role": "Principal Investigator"},
		{"ID": "21304", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "CPI Ltd"},
		{"Number": "1", "Name": "Intel Ireland Ltd"},
		{"Number": "2", "Name": "National Microelectronics Institute"},
		{"Number": "3", "Name": "National Semiconductor U K Ltd"},
		{"Number": "4", "Name": "Neocera Inc"}
		],
		"Summary": "The evolution of silicon technology since the 1960's has focussed on doubling performance and functionality every 18-24 months through miniaturization.  Critical dimensions measured in tens of nanometres are now common place and billions of components connected by miles of wiring can be packed onto a wafer no larger than a thumb nail. Today the focus is shifting away from more scaling (called more Moore after the founder of Intel, Gordon Moore) towards increasing functionality through the introduction of mixed technologies on silicon (called more than Moore).  This project investigates the incorporation of ultra thin ferroelectric materials into silicon nanoelectronics and two of its many applications.Capacitance is the rate of change of charge with voltage.  It is the defining property of capacitors which are necessary in many electronic systems but are relatively large.  Ferroelectrics can shrink capacitors by three orders of magnitude, because their electric permittivity is so high.  More than that, their capacitance can be made to vary depending on the applied voltage so very small and tunable capacitors can be made, which can find applications in hand held electronics products in order to reduce power consumption.  If they could be integrated onto a silicon microchip there would be further space savings.   Thin layers are expected to produce even higher capacitance.  However there is evidence that capacitance starts to reduce below 50 nm as  dead layers  are said to form near the interface with electrodes, but this may be an interface effect which can be lessened through engineering. Recently there has been experimental evidence that effective negative capacitance can be seen in ultra-thin ferroelectric films. If such material can be incorporated into a transistor then it would be able to reduce the voltage needed to switch a transistor between its on and off states (the sub-threshold slope).  This would transform silicon technology, allowing a new generation of more powerful single core processors.  Modern computers have dual or multi-core processors.  A single core processor would generate too much heat but is still desirable for many applications.  Capacitance places a lower limit on the sub-threshold slope.  The consequence is that transistors need a larger applied voltage to be on and/or will leak current and so can never be fully switch off.  This leads to increased power loss and heating as more transistors are crammed onto the same area of silicon, which limits component density.  Integrating a ferroelectric film with negative capacitance into the gate of a transistor would reduce the overall capacitance and thus the sub-threshold swing.   The need to understand and produce high quality ferroelectric ultra-thin films is imperative for each of these applications.   Atomic Layer Deposition (ALD) at Newcastle and Pulsed Laser Deposition (PLD) at Imperial College will be used to deposit thin films of the ferroelectric materials barium titanate (BTO) and barium strontium titanate (BST).  Both allow deposition thicknesses with atomic level precision.  Extensive characterisation is needed to assess quality of these ferroelectric films.  First principles computer simulation will be used to gain a better understanding of the films and to direct experiments. The deposition and thermal parameter space will be mapped to identify best ferroelectric properties for given constraints laid down by the silicon fabrication.  Transistors will be made incorporating the best ferroelectric films to confirm the reduction in sub-threshold slope.  Ferroelectric capacitors integrated onto silicon will be demonstrated, quantifying the capacitance increase per unit area and examining the fabrication constraints needed to maintain high transistor performance.  This will also help identify integration issues, which also include equipment contamination and the development of ferroelectric etches."
	},
	{
		"grant":185,
		"ID": "EP/H024050/1",
		"Title": "AI4FM: using AI to aid automation of proof search in Formal Methods",
		"PIID": "46590",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "30/09/2014",
		"Value": "467296",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "46590", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Formal Methods bring to software engineering the level of scientific underpinning familiar in other engineering disciplines. Such methods use specification languages with precise meaning and thus open the possibility of proving that a design (ultimately the implementation) satisfies the specification.Such formal methods have come a long way since their inception and they are now used in applications far more common than the safety-critical systems where they were first deployed. Significant awareness of their potential has come from the use of push-button, post facto, methods that derive from ideas of  model checking . The family of methods that can be thought of as  top-down  have more potential pay-off but are also more challenging for users. Any post-facto method has to face the prospect of incorrect programs - extracting their specifications can be unedifying. Furthermore, an enormous amount of the waste in software development derives from scrap and rework when design errors are discovered after their insertion (but possibly before there is even code to execute). Both post-facto and top-down approaches are important: we choose to address the latter and tackle a key cost in their deployment.In justifying a top-down step of design, the user has to discharge so-called  proof obligations . These are small proofs that can often be discharged by an automatic theorem prover. But where they are not discharged automatically, an engineer is faced with the unfamiliar task of constructing a formal proof. Improvements in so-called heuristics can help increase the power of theorem provers. This project aims to use  learning  techniques from artificial intelligence to record and abstract how experts do proofs in order to increase the proportion of cases where proofs are constructed without (or with minimal) human intervention."
	},
	{
		"grant":186,
		"ID": "EP/H024107/1",
		"Title": "Molecular-Metal-Oxide-nanoelectronicS (M-MOS): Achieving the Molecular Limit",
		"PIID": "73981",
		"Scheme": "Programme Grants",
		"StartDate": "01/02/2010",
		"EndDate": "31/07/2014",
		"Value": "3567075",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Chemistry",
		"OrgID": "49",
		"Investigators":[
		{"ID": "73981", "Role": "Principal Investigator"},
		{"ID": "107959", "Role": "Co Investigator"},
		{"ID": "491", "Role": "Co Investigator"},
		{"ID": "57784", "Role": "Co Investigator"},
		{"ID": "71158", "Role": "Co Investigator"},
		{"ID": "-133775", "Role": "Co Investigator"},
		{"ID": "45941", "Role": "Co Investigator"},
		{"ID": "58309", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "National Semiconductor U K Ltd"}
		],
		"Summary": "Our vision is to demonstrate functional circuits using molecular metal-oxides (MMOs), connecting self-assembled MMOs into top-down, lithographically defined CMOS architectures with the ultimate aim of achieving the molecular limit in data storage and processing: i.e. realising inorganic, single molecule transistors. Our proposal is unique because: (i) it identifies a new class of inherently CMOS-compatible and functional molecules that have not previously been considered (or even patented) for 'beyond-Moore' applications; (ii) it aims to address key practicalities of scalability, interfacing, stability and reproducibility that are often omitted from schemes aiming simply to construct a single demonstrator device; and (iii) it is underpinned by a strongly-collaborative team with complementary expertise in molecular synthesis, modelling and device fabrication. This project is highly creative and adventurous, proposing that inorganic molecules could be reliably used in the fabrication of nano-electronic devices that take advantage of the intrinsic electronic properties of molecules as switchable molecular semiconductors (EPSRC success feature 1). It supports talent at all levels - from senior professors to early career researchers - in a highly supportive and collaborative context (EPSRC success feature 2). Initially, we propose to design hybrid devices combing CMOS embedded with bistable MMOs and to examine the interplay between 'bulk' and nano-molecular semiconducting units. Our approach is both innovative and practical because it embeds molecular electronics within the current the state-of-the-art, allowing us to address practical issues and develop know-how in this new field, before down-scaling to 'beyond-Moore' dimensions down to the molecular limit with collaborations that achieve a two-way flow of knowledge between the research base and industry (Building collaborations that achieve a two-way flow of knowledge between the research base and industry (EPSRC success feature 3) and at the same time this proposal encourages and supports research that crosses the borders between disciplines (EPSRC success feature 4). Theoretical studies of both single clusters and arrays will allow us to predict their behaviour and design new architectures; surface studies and device measurements will enable us to assess the electronic characteristics of devices and drive us towards viable nanoelectronics that can be mass-produced therby developing a shared vision of tomorrow's major challenges and opportunities with stakeholders: society, industry, universities and other partners (EPSRC succes feature 5). We aim to show that MMO-CMOS (herein called M-MOS) can function with 'embedded' molecular units and we plan towards the single molecule limit. This potential will be assessed and exploited within the Glasgow Nano EPSRC KTA (EP/H500138/1) allowing 'real-time' technology transfer allowing us to immediately seize any commercial development opportunities thereby building a better understanding of where we should focus our effort to benefit both UK society and the UK economy and increase its global competitiveness (ESPRC success feature 6).Finally this programme will directly train 7 PDRAs and 4 PhDs and indirectly train 8 further PhDs and 24 undergraduate / erasmus students thereby creating and sustaining research scientists and engineers in the UK so that they are recognised worldwide as leaders in their field (EPSRC success feature 7)."
	},
	{
		"grant":187,
		"ID": "EP/H026266/1",
		"Title": "Novel Adaptive Filtering Techniques for Multidimensional Signals",
		"PIID": "87220",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2010",
		"EndDate": "31/01/2013",
		"Value": "329847",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "87220", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "University of Tokyo"}
		],
		"Summary": "This proposal seeks to develop a rigorous theoretical and computational framework for statistical signal processing of  three- and four-dimensional real world signals. This will be achieved in the  quaternion domain, benefiting from its division algebra, and thus promising a quantum improvement in the modelling of such signals.  Particular emphasis will be on solutions for adaptive signal processing problems, whose accuracy will be enhanced through the use of quaternion statistics and  the associated special forms of correlation-  and eigen-structures. Current algorithms are less than adequate for the very large class of processes with noncircular (rotation dependent) probability distributions, and for signals whose components exhibit coupling and large unbalanced dynamics; these are common in array signal processing, wind modelling, motion tracking, and chaos engineering.The proposed research will enable unified modelling of three- and four-dimensional signals, together with better understanding of the associated nonlinear dynamics and geometry of learning, and will also serve as a framework for simultaneous modelling of  heterogeneous data sources. The fundamental novelty of this work is our recently proposed quaternion least mean square (QLMS)  algorithm, which makes full use of quaternion algebra, and thus allows for additional degrees of freedom and enhanced accuracy in the modelling of real world phenomena. This will also serve as a framework to design a suite of novel adaptive filtering and tracking algorithms, based on both standard and  widely linear  models, which will be suitable to deal with the generality of quaternion valued signals. Comprehensive theoretical evaluation and practical testing will be performed in order to prove the worthwhileness of the proposed approach. Practical applications considered will be short term wind forecasting in renewable energy and trajectory tracking from motion sensors in smart environments; particular gains are expected when dealing with large and intermittent dynamics at multiple scales (turbulence, gusts, multiple coupled rotation trajectories).This research proposal, based at Imperial College and in collaboration with an internationally leading research group from University of Tokyo Japan, will find solutions to these problems and will also open new possibilities for advances in a number of emerging areas dealing with uncertainty, complexity and multidimensional data natures."
	},
	{
		"grant":188,
		"ID": "EP/H026835/1",
		"Title": "Descriptive Complexity with Algebraic Operators",
		"PIID": "50283",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2010",
		"EndDate": "31/03/2014",
		"Value": "424833",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "50283", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The study of computational complexity is concerned with understanding what makes certain computational tasks inherently difficult to solve.  In this context, problems that can be solved by means of algorithms that take a number of steps bounded by a polynomial are considered to be feasible, or efficiently solvable.  A long standing research concern in the field of descriptive complexity has been to give a complete classification of those problems that are feasible.  Such a complete classification would take the form of a formal language (or a logic) in which one could define all the feasible problems, but no others.  It has been known for some time that languages based on induction and counting are not sufficient for this purpose and it has recently been discovered that adding certain operators based on linear algebra can extend the power of such languages.  Our research aims at building on this breakthrough by understanding the power of these extended languages.  To do this we will develop new methods to analyse the expressive power and use this to determine whether or not the newly defined languages do capture the power of feasible computation.We will also investigate whether these languages capture feasible computation in interesting special cases."
	},
	{
		"grant":189,
		"ID": "EP/H026878/1",
		"Title": "A generic transducer-based approach to modelling and verifying infinite-state systems: techniques, applications, and tools",
		"PIID": "-247515",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2013",
		"Value": "250894",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-247515", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computers have become so complex today that the likelihood of subtle errors is greater than ever before. Moreover, since computerised systems are ubiquitous (e.g. planes, railways, and nuclear power plants), the impact of such errors will certainly be far-reaching. In the past such errors have resulted in a loss of time, money, and even human lives. The development of (fully-automatic) model checking technologies --- pioneered by the recent ACM Turing Award winners Clarke, Emerson, and Sifakis --- has been so influential in minimising the likelihood of subtle errors that model checking has been adopted by major companies including IBM, Intel, Motorola, and NASA. Despite its success, model checking suffers from the inherent state-explosion problem, which remains difficult today even though a substantial progress has been made in the past two decades. The past fifteen years have seen an increasing level of awareness amongst researchers that modelling computerised systems as infinite-state systems is not only more suitable, but might also help circumvent the notorious state-explosion problem. Such a modelling approach views the parameters that cause the state-explosion problem as potentially unbounded or infinite.  These include the sizes of arrays, stacks, queues, integer or real valued variables, discrete-time or real-time clocks, and the number of processes in distributed protocols. Instead of the state-explosion problem, such abstractions as infinite-state systems yield undecidability in general. The field of infinite-state model checking aims to develop tools and techniques to deal with this problem. Broadly speaking, approaches to infinite-state model checking can be classified as follows:1. Restrictions to decidable formalisms.2. General (undecidable) formalisms with the aid of decidable semantic restrictions or semi-algorithmsMost research in infinite-state model checking thus far adopts only one of these approaches without seriously considering the other. Moreover, little has been done to see the connections between these two approaches. This is unfortunate since both approaches have their own disadvantages that can be considerably minimised only by considering both approaches in parallel. The project proposes a particular hybrid approach taking into account the two aforementioned approaches simultaneously. The goal is to systematically develop generic tools and techniques for infinite-state model checking aiming for both sound theoretical foundations and practical applicability. This project focuses on general formalisms that are inspired by various notions of finite-state transducers, as they are known to be clean, expressive, and most amenable to theoretical analysis."
	},
	{
		"grant":190,
		"ID": "EP/H026975/1",
		"Title": "Garbage Collection for Multicore Platforms",
		"PIID": "27964",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "28/02/2014",
		"Value": "383969",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "27",
		"Investigators":[
		{"ID": "27964", "Role": "Principal Investigator"},
		{"ID": "-10302", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Developers are increasingly turning to languages like Java and C# for their ease of development, deployment and maintenance.  Most applications for the foreseeable future will be written in languages supported by managed runtimes, running on multicore hardware.  Particular benefits of managed runtimes include support for automatic dynamic memory management, or 'garbage collection' (GC) and threads. GC allows programs to recycle unused memory automatically, without error-prone programmer intervention. Threading allows a program to run different sequences of instructions in parallel; for instance, a web server might employ a separate thread for each incoming request from internet browsers.One of the most significant recent developments for language implementers is the development of multicore processors, with  the number of cores deployed in commodity platforms expected to increase significantly over the next 5 years.  The complexity of the processor's access to memory has also increased, in terms of levels of memory hierarchy and in the technology interconnecting processors.  However, modern runtime technology has not evolved as fast as hardware technology, and how to fully exploit hardware parallelism remains an open question.This research asks, how can we exploit hardware parallelism, in particular by running multiple user program ('mutator') and GC threads? How can we avoid paying penalties for non-local memory access, but still benefit from multiple paths to memory? How can we take proactive advantage of locality properties? How can we minimise synchronisation between mutator and GC threads?Today's concurrent GC techniques avoid relocating live objects, so as to minimise the need for this synchronisation, but this leads to poor use of memory with many small holes but nowhere to accommodate larger objects ('fragmentation'). The standard fragmentation solution - periodic compaction phases - has high overheads, and often lacks portability or leads to throughput slumps before mutator threads can operate at full speed again. Memory management will be a bottleneck for the next generation of increasingly thread parallel software unless the problem of high performance GC for multicore can be solved. This proposal aims to address this key problem, reconciling compaction with concurrency and performance.We believe that the key to exploiting modern multicore architectures is a judicious division of effort between mutator and GC threads, in order not simply to avoid paying the price of accessing non-local memory, but proactively to process data while it is in the cache.  It is almost always worth paying the cost of executing a few more instructions in order to avoid accessing non-local data.  Thus, if a mutator thread is about to access data (and hence it is or soon will be in the cache), it should perform some GC work on that data immediately. Other data should be left to be handled by separate GC threads. At no time should all mutator threads be halted waiting for the collector. Our aim is therefore to provide high throughput and very low pauses, by utilising parallel copying collector threads, running concurrently and carefully coupled with mutators in order to leverage locality.This research will benefit GC researchers by broadening the design space and devising and evaluating new concurrent GC techniques, and developers in the broad community through enabling them to tune their applications and GCs to modern architectures. A high-performance GC tuned to modern multicore hardware will also lower the barrier to deployment of future software applications that expect to exploit multicore hardware fully. We will make all code developed freely available under an Open Source license. As well as disseminating our results through journals and conferences, we shall organise two workshops in order to build UK research strength in this field."
	},
	{
		"grant":191,
		"ID": "EP/H027203/1",
		"Title": "Structured Sparsity Methods in Machine Learning an Convex Optimisation",
		"PIID": "120114",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2014",
		"Value": "220703",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "120114", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Over the past ten years theoretical developments in machine learning (ML) have had a significant impact in statistics, applied mathematics and other fields of scientific research. In particular, fruitful interactions between ML and numerical optimisation have emerged that are expected to lead to theoretical and algorithmic breakthroughs with the potential to render ML methodologies significantly more applicable to many problems of practical importance. The proposed project aims to make significant UK contributions at a crucial juncture in this emerging interdisciplinary field that has so far been dominated by the US and France. Many ML techniques can be cast as problems of minimising an objective function over a large set of parameters. Examples include support vector machines as well as more recent techniques for semi-supervised learning and multi-task learning. Often the objective function is convex. Consequently, ideas from convex optimisation are becoming increasingly important in the design, implementation and analysis of learning algorithms. Up to now, however, ML has almost exclusively resorted to  off the shelf  methods for convex optimisation, without substantially exploiting the rich theory which lies behind this field.  A thesis of this proposal is that there is a need for a deeper interplay between ML and numerical optimisation. Ultimately, bridging the two communities will facilitate communication and the power of core optimisation will be more easily brought to bear in ML and lead to new frontiers in optimisation. An area in which the interplay between ML and optimisation has a particularly important role to play is in the use of sparsity inducing optimisation problems. A rationale that drives the use of sparsity-inducing models is the observation that when the number of model parameters is much larger than the number of observations, a sparse choice of parameters is strongly desirable for fast and accurate learning. Building on this success, we believe that the time is now right for the development of a new line of algorithms for matrix learning problems under structured sparsity constraints. This means that many of the components of the parameter matrix or a decomposition thereof are zero in locations that are related via some rule (e.g the matrix may be constrained to have many zero rows, many zero eigenvalues, to have sparse eigenvectors, etc.).Perhaps the most well-know examples in which structured sparsity has proven beneficial are in collaborative filtering, where the objective function is chosen to favour low rank matrices, and in multi-task learning where the objective function is chosen to favour few common relevant variables across different regression equations. These types of optimisation problems have only recently started to be addressed in ML and optimisation, and several fundamental problems remain open, most importantly the study of efficient algorithms which exploit the underlying sparsity assumptions and a statistical learning analysis of the methods.Our proposal is multidisciplinary and involves substantial exchange of ideas between Computer Science (Machine Learning) and Mathematics (Numerical Optimisation), with three main goals. Firstly, we aim to develop novel and efficient algorithms for learning large structured matrices; fast convergence of the algorithms should be guaranteed when applied to problem data that have a sparse solution. Secondly, in the cases where the assumed sparsity structure leads to NP-hard problems and the first goal is unachievable (this is often the case under low-rank assumptions), we aim to identify tractable convex relaxations and understand their impact on sparsity. Thirdly, we aim for models and algorithms that have a more natural interpretation than generic solvers (e.g., a minimax statistical justification), which should make it more likely that practitioners will embrace the new methodology."
	},
	{
		"grant":192,
		"ID": "EP/H027351/1",
		"Title": "Multiprocessors: From Microarchitecture to Semantic Theory",
		"PIID": "-249355",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "10/06/2010",
		"EndDate": "15/07/2013",
		"Value": "269820",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-249355", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "ARM Ltd"},
		{"Number": "1", "Name": "IBM"},
		{"Number": "2", "Name": "IBM Austin"},
		{"Number": "3", "Name": "University of Texas at Austin"}
		],
		"Summary": "Modern computers have become predominantly multicore, providing programmers with many simultaneously-executing threads of execution sharing the system memory. Concurrent programming is challenging, and this trend in hardware should have been an ideal application target for previous academic work on software verification. Concurrent programming has been extensively studied, with the development of areas such as model checking, process calculi, separation logic, and rely-guarantee reasoning. Unfortunately, realistic architectures violate a key assumption of most such work, that all threads have a consistent view of memory, an assumption often referred to as Sequential Consistency. Instead, programmers are actually faced with a  Relaxed Memory Model , with only approximate consistency, making programming and verification significantly more challenging.Relaxed memory models arise, ultimately, from hardware (and compiler) implementation features designed for performance. Unfortunately, there is no clear understanding of how the microarchitectural choices made in hardware implementations lead to programmer-observable consequences. This is an instance of a more general problem, that modern hardware is too complex to understand without formal proofs, as witnessed by a succession of errata in deployed processors. Hardware verification is itself a field that has received a great deal of study. However, the field has still not reached the point of verifying global semantic properties, such as relaxed memory models, that emerge from the interaction of multiple disparate subsystems, each of which suffers from imprecise specifications.I propose to create a general theory of the programmer-observable behaviour which emerges from the microarchitectural choices and optimizations made in multiprocessor implementations. I will develop abstract mathematical models of the implementations, and explain with proof the observable consequences of those implementations. With specifications derived from that theory, I will go on to study the hardware verification problem, establishing techniques and methods for verifying global programmer-visible properties. Throughout, I will address not just safety properties, but also progress and liveness properties that sophisticated programmers rely on.Such a general theory is critical for a better understanding of multiprocessor semantics, by both programmers and hardware implementers, and for enabling these different communities to communicate their knowledge to each other.  It is also an essential prerequisite for sound theories of concurrent programming. An improved understanding and communication of these issues is imperative for reliable multiprocessor usage."
	},
	{
		"grant":193,
		"ID": "EP/H027408/2",
		"Title": "Text Entry by Inference: Eye Typing, Stenography, and Understanding Context of Use",
		"PIID": "-249907",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "28/03/2011",
		"EndDate": "27/05/2013",
		"Value": "183157",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "119",
		"Investigators":[
		{"ID": "-249907", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "My research is based on the observation that our daily interaction with computers is highly redundant. Some of these redundancies can be modelled and exploited by intelligent user interfaces. Intelligent text entry methods use AI techniques such as machine learning to exploit redundancies in our languages. They enable users to write quickly and accurately, without the need for a key press for every single intended letter.In this programme I propose to develop two new intelligent text entry methods. The first is a system that enables disabled users to communicate efficiently using an eye-tracker. The second system is a novel intelligent text entry method that is inspired by stenography.In addition, I propose to explore text entry methods' broader context. The research literature has concentrated on inventing text entry methods that promise high entry rates and low error rates. Now that we have text entry methods that have reasonably high entry rates it is time to complement this objective function by discovering other aspects of text entry. I propose to use social-science techniques, such as diary and field-studies, to understand how users would prefer to use text entry methods  in the wild.  System 1: Eye-typing by inferenceThis is a system that will potentially increase the entry rate in eye-typing systems. Current eye-typing systems are inherently slow (due to the dwell timeouts), and users perceive them as frustrating. I propose to build a system that enables users to eye-type without the need for a dwell timeout at all. Potentially, my method will be faster than any other eye-tracker based method in the world.With my proposed system users write words by directing their gaze at the intended letter keys, in sequence. Users' intended words are transcribed when they look at a  result area  positioned above the keyboard. Users can write more than one word. They can also write sequences or words, or even stop short within a word. They may go to the spacebar key between words but this is not strictly necessary for the system to be able to correctly infer users' intended words.System 2: Stenography by inferenceThis system will be a stenography system for pen or single-finger input. The primary application is mobile text entry. However, I strive to create a system that to some extent can replace the desktop keyboard, should users so desire. Potentially it will be faster than any other pen-based text entry method.The idea behind this method is to enable users to write words quickly by gesturing patterns they have previously learned. Such open-loop recall from muscle-memory is much faster than the closed-loop visually-guided motions users are required to perform when they tap on, for example, an on-screen keyboard. My proposed system will enable users to quickly and accurately articulate gestures for individual words. These gestures will be fixed for a particular word. That is, each word is associated with a single (prototypical) unique gestural pattern. A user's input gesture is recognised by a pattern recognizer. The word whose closest pattern best match the user's input gesture will be outputted by the system as the user's intended word.Understanding the broader context of text entryThe last component of my proposed programme serves to contribute new perspectives to the text entry research field. As previously discussed, context of use is largely unexplored in text entry. I intend to explore this topic using a range of qualitative methods. I intend to perform interviews, conduct field studies (e.g. studying participants trying a prototype mobile speech recognizer at a caf), and diary-studies. The latter will be conducted with a system that provides users of a choice of a few text entry methods that I hypothesize will be useful for different situations. I also intend to read literature on design and architecture to further my understanding of the complete design space of text entry."
	},
	{
		"grant":194,
		"ID": "EP/H027955/1",
		"Title": "Towards Industrial Applications of Modular Languages for Biology",
		"PIID": "-249414",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2013",
		"Value": "250340",
		"ResearchArea": "Synthetic Biology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Biological Sciences",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-249414", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Systems biology is a relatively new field which seeks a systems level understanding of organisms: rather than studying individual entities such as proteins and genes in isolation, we ask how these interact in order to form the complex emerging behaviour that living systems exhibit. Such an understanding is for example important for the development of new drugs and to predict how these impact on an organism. Synthetic biology is a related and emerging field which seeks to engineer new organisms for practical purposes. Examples include bacteria that produce energy from sunlight; cells which organise to form e.g. artificial human tissue; and crops which are resistant to bugs. Both systems and synthetic biology rely on mathematical models. In systems biology, models are used to formalise our knowledge about an existing biological system and to allow computer simulations which predict its behaviour. In synthetic biology a model similarly allows us to test the behaviour of a new system before it is implemented in the lab. As we gain more biological knowledge and models become larger, more structured modelling methods are also needed. Computer science has a rich history in inventing and applying formal languages which support modelling in engineering, and over the last decade, substantial research efforts have gone into creating such formal languages for biology. However, these languages have so far been applied mainly to small-scale, proof-of-concept examples that yield little or no biological insight. In particular, formal languages have not yet gained traction in industry. The vision of the proposed project is to change this situation. I propose to do so by taking two existing languages that have been the subject of my PhD, one for systems biology and one for synthetic biology, and targeting the development of these languages towards industrial applications. The languages are good starting points for this purpose because they have already been designed with three key features in mind: 1) they are modular, so large systems can be described systematically in terms of their components; 2) they are intuitive to use and can be understood by non-specialists; and 3) they are defined mathematically and can hence be read by computers. The synthetic biology language furthermore allows the translation of a model to DNA sequences that can be put to work in living cells.To achieve the aim of industrial applicability in systems biology, I propose to work closely with industry. In particular, I will work with Novo Nordisk/HRI on the development of a model of insulin signalling used in their diabetes research, and through this I will obtain valuable insight into the requirements for a modelling language. I will also work and exchange ideas with Plectix BioSystems, which is developing another formal language that complements the languages in this proposal. For synthetic biology, an attempt to bring a dedicated formal language to industry would be premature given that the field is currently in its infancy. Instead, my aim is here to develop a language which can be of practical use in the annual  international Genetically Engineered Machines  (iGEM) competition projects. The projects are carried out by undergraduate teams from over 20 different countries and lie at the cutting edge of synthetic biology. This proposed work will involve the design of a new database of genetic parts that is necessary for the translation to DNA, and on the biological side this database will be developed at Cambridge University. The work will also focus on efficient methods of translation to DNA since the existing methods do not scale to real applications; and finally the work will study new ways of describing cell-cell communication and the impact of a synthetic circuit on host cells. This will be done in collaboration with Microsoft Research."
	},
	{
		"grant":195,
		"ID": "EP/H028056/1",
		"Title": "Pattern matching algorithms for streaming data",
		"PIID": "-249634",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/03/2011",
		"EndDate": "28/02/2014",
		"Value": "222288",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-249634", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Imagine that I give you the following task: read the Complete Works of Shakespeare and write down all occurrences of the phrase  my good lord . The task is known in Computer Science as exact pattern matching; here the phrase  my good lord  is the pattern. If I asked you to find all phrases similar to the phrase  my good lord , you may decide write down the phrases  my noble lord ,  my gracious lord  and simply  my lord . This is approximate pattern matching, a problem whose complexity is, of course, dependent on how we define the word similar. The definition considered depends on the application and much of the breadth and depth of the field arises from this.Now imagine that I am going to read the Complete Works of Shakespeare to you and expect you to write down similar phrases as you hear them. This is online approximate pattern matching and is the focus of this proposal. The proposal is applicable to Internet related applications where a vast quantity of data passes though a computer constantly - a field known as data streaming. Here the data is far too large to be stored and results must be computed on the fly as the data arrives. In the reading analogy, if you mishear a paragraph, I'm not going to reread it to you.The aim of this proposal is to bring these fields together to search for patterns quickly in streaming data. Continuing the analogy, we will be considering finding patterns in a number of circumstances:1. As before I am going to read you a book but this time much faster. I know that you can't write down all the occurrences fast enough but I want you to guarantee you will catch most of them.2. Many people will read books out loud to you at the same time. Any time any of them say the pattern you are looking, for you have to write it down.3. I am going to read you a book but I make no promise to read the words in order:   page 6 line 3 word 6 is good ,   page 39 line 1 word 2 is happy ,  page 6 line 3 word 5 is my ...Of course, these problems sound strange and counter-intuitive phrased in plain English, but the underlying Computer Science problems are highly significant for many emerging applications such as traffic shaping, firewalls, Internet monitoring and malicious content detection."
	},
	{
		"grant":196,
		"ID": "EP/H028900/1",
		"Title": "Rigorous Runtime Analysis of Nature Inspired Meta-heuristics",
		"PIID": "-214321",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/08/2010",
		"EndDate": "30/09/2013",
		"Value": "264223",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-214321", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BYG.DTU"},
		{"Number": "1", "Name": "Max Planck"}
		],
		"Summary": "A rigorous runtime analysis of different nature inspired meta-heuristics will be analysed in this projectin order to gain a deeper understanding of when and why a given meta-heuristic  is expected to perform well or poorly. Various nature inspired meta-heuristics have been applied successfully to combinatorial optimisation in many scientific fields.However, their computational complexity is far from being understood in depth. It is still unclear how powerfulthey are for solving combinatorial optimisation problems, and where their real power is in comparison with the more traditional deterministic algorithms.Evolutionary Algorithms (EAs), Ant Colony Optimisation (ACO) and Artificial Immune System (AIS) algorithms will be studied in this project.Since the knowledge level of their computational complexity is at very different stages, two different types of results will be produced.One is the computational complexity results of realistic EAs, not (1+1)-EAs, on selected well-known combinatorial optimisation problems. A setup of complexity classes will be built revealing what classes of problems are hard (or easy) for which kind of EAs.The other is a setup of the first basis for a systematic computational complexity analysis of ACO and AIS other popular nature inspired meta-heuristics for which very few runtime results are available.The expected outcomes of this project will not only provide a solid foundation, but also insights and guidance in understandingwhich meta-heuristic should be preferred  for a given problem and in the design of more efficient variants."
	},
	{
		"grant":197,
		"ID": "EP/H029001/2",
		"Title": "Maximising Efficiency of Resource Usage Under Uncertainty in AI Planning",
		"PIID": "-202908",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "01/11/2011",
		"EndDate": "30/04/2013",
		"Value": "103506",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Informatics",
		"OrgID": "78",
		"Investigators":[
		{"ID": "-202908", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In recent years planning technology has a enjoyed significant increase in real-world application, with industry and general science research benefiting from the great depth theoretical work done in this area over many decades.   The core problem of deciding which activities to carry out and when occurs in a vast range of domains; the research area of planning is concerned with developing generic problem solving technology that automates the task of performing this core reasoning.  Planning technology has been employed in a wide range of domains including controlling printing presses, power management for the UK National, train scheduling on the Hungarian Railway Network, scheduling aircraft landing in airports, and autonomous robotic control, both in space and in the oceans.  Experience in these areas has given rise to two key observations.  First, the existing theoretical work done in AI Planning has been extremely valuable, allowing planning technology to begin to solve real world problems. Planning is a fundamental component of intelligent autonomous behaviour and as such planning technology  has real potential for application in many different areas, both now and in the future.  The second is that whilst one can observe that planners can now begin to be applied to these problems, there is still a great need for improvement of the underlying technology, in terms of expressivity and performance, in order to be able to create greater autonomy by allowing reasoning about an uncertain world.At the heart of this lies deeply theoretical computer science research: a planner is a generic problem solving system, consisting of search algorithms and heuristics.  Of particular interest is reasoning about time and resources, something key to many areas of computer science, from compilers and programming languages to web services and optimisation.  In order to tackle application problems well, reasoning effectively about these is essential.  Of specific interest here is uncertainty in time taken and resources consumed.  This occurs in many application domains, and in each of these a similar approach is taken: conservatism about time and resource availability in order to guarantee success.  This, however, comes at a cost.  By way of example, when planning for autonomous Martian exploration, the models used by both the ESA and NASA are pessimistic, underestimating the amount of power the rover will receive from the sun, and overestimating the amount of energy and time each activity will take.  The result is that the equipment is highly under-utilised, with fewer science targets being achieved than could have been with better on-board reasoning.  Given the expense of placing rovers on Mars and the limited equipment lifespan, this is a great cost to mankind's exploration of space.  A similar problem occurs when deploying renewable energy generation: wind farms are assumed to provide 10% of their maximum output, even thought the reality is almost always greater than this.  This conservative assumption, there to ensure power is always provided, causes great environmental and economic cost, as extra production capacity must be available through other sources regardless of whether it is required.The major benefit of planning is in generating a generic problem solving  technology.  Developing several bespoke solvers would take many years, and incur great financial cost.  By developing efficient planning systems, a single domain-independent problem-solving core is built, capable of solving many problems without the cost of developing a bespoke solver for each.  The core of this research is addressing challenges in solving the general planning problem that will allow future application of planning, extending the range of problems to which this generic technology can be applied."
	},
	{
		"grant":198,
		"ID": "EP/H031464/1",
		"Title": "Ultra high detectivity single carrier multiplication InAs avalanche photodiodes for IR optical detection",
		"PIID": "120483",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2010",
		"EndDate": "30/04/2014",
		"Value": "381589",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "120483", "Role": "Principal Investigator"},
		{"ID": "3826", "Role": "Co Investigator"},
		{"ID": "-19880", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "SELEX Galileo"},
		{"Number": "1", "Name": "University of Virginia"}
		],
		"Summary": "The ability to detect very low light level in the infrared (IR) wavelengths, down to a single photon has numerous applications ranging from enabling highly secured communication that relies on detection of a single photon, measurement of very weak fluorescence in biomolecule identification to high resolution 3 dimensional imaging based on laser ranging. Conventional semiconductor photodiodes do not have the sensitivity required for these photon-starved applications. Therefore it is necessary to use photodiodes designed with internal amplification or gain, called avalanche photodiodes (APDs), to convert the signal from a few photons to a large current that can be detected by an external electronics. In most semiconductors this amplification process also introduces excess noise. However Silicon APDs were able to produce high gain with low excess noise and therefore have been used in many applications to provide detection down to a single photon in the visible wavelengths. This is because, in Silicon the gain is provided predominantly by the electron multiplication process which reduces the excess noise. Unfortunately no commercial IR APD with performance similar to, or better than, Silicon is available despite various proposals to achieve Silicon-like APDs over the last 20 years. This exciting proposal will address this void by developing a new class of APDs based on InAs, a semiconductor with unique band structure features, to achieve high gain with negligible excess noise that is lower than that of Silicon.  This proposal aims to provide IR APDs with extremely high performance, capable of detecting a single photon in the wavelength range of 1100 nm to 3000 nm. For instance they can provide low cost high performance large format imaging arrays for IR applications such as LIDAR, a technique that can provide excellent images and range measurements, non-invasive blood glucose sensing, atmospheric CO2 concentration monitoring as well as eye-safe free space optical communication. We therefore expect our APDs to generate new applications and provide highly competitive IR APDs.  Based on the understanding of the InAs bandstructure, our APDs will be designed such that only electron will undergo impact ionisation to produce high avalanche gain with negligible excess noise. In addition to excellent gain, our devices can be operated at low voltage, making them compatible with off-the-shelf readout circuits. This could pave the way to a highly sensitive and affordable IR camera. To enhance the exploitation and the gain characteristics we will grow a novel InAsSb APDs on GaAs substrate which is significantly larger and cheaper than InAs substrate. This, if successful, will enable integration with commercial GaAs electronics. To propel our InAs APDs towards exploitation in the applications mentioned above we will;I) Optimise the crystal growth method to achieve high quality InAs materials with low level of impurities.II) Develop fabrication and surface passivation techniques to yield devices with low leakage current, leading to higher sensitivity.III) Pioneer techniques to implant ion species and to perform dopant diffusion to control the electric field in the InAs devices leading to high reliability.IV) Control growth conditions such as temperature and atomic pressure to achieve low crystal defect formation during the growth of InAsSb APDs on GaAs.This exciting project will be carried out by a highly skilled research team, comprising UK universities (Sheffield, Heriot-Watt and Surrey), American university (Virginia) and UK companies (Selex-Galileo and Thales Optronics) with years of experience in research and development of sensing applications. Thus, one of the outputs of the project is to provide a leading IR sensor technology to the research communities to facilitate new research and to the industry to maintain a lead in the IR sensor market."
	},
	{
		"grant":199,
		"ID": "EP/H032436/1",
		"Title": "SANDPIT: Multi-scale dynamics and gene communities",
		"PIID": "10573",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "31/08/2013",
		"Value": "609731",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "93",
		"Investigators":[
		{"ID": "10573", "Role": "Principal Investigator"},
		{"ID": "128104", "Role": "Co Investigator"},
		{"ID": "-168471", "Role": "Co Investigator"},
		{"ID": "-187845", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Evolution has mostly been studied in the context of the higher organisms, where the transfer of genetic material is almost always from parents to offspring. However some years ago it was discovered that bacteria, by contrast, are able to transfer genetic material directly between individuals. The first type of transfer is called vertical and the second type horizontal. So when we talk about bacteria evolving, we are referring to the genes which they contain, and horizontal transfer is a way of changing this content. From the point of view of the genes, the bacteria are habitats where the genes can reside, just like an ecosystem such as a forest or lake is a habitat where plants and animals can reside. So while the survival and reproduction of genes and the bacteria they live in are related, the fact that genes are not bound to a specific organism means that their interests are not identical. So genetic material transferred between bacteria can evolve strategies that can either help or hurt the bacteria. These are referred to as mutualism and parasitism respectively. This means that the mechanism of natural selection may take place at two levels: at the genetic level and at the level of the organism, and that these may be in conflict with each other. This conflict is played out in the ways that the genes interact with each other, and how these interactions determine the properties of the bacterium in which they are found. Evolution is a process which is random, and there is evidence that randomness is important in the horizontal transfer of genes. The analysis of systems made up of a large number of interacting objects, which have a degree of randomness, is an important area of applied mathematics. So far, however, progress in analysing these systems mathematically has lagged behind their study using computer simulations. We propose to develop an appropriate mathematical analysis of horizontal gene transfer in bacteria. We will be guided by computer simulations and also by databases of the distribution of genes in a wide range of different organisms. The final theory, although developed for a specific biological situation, can provide a model for mathematical treatments of other interacting random systems, which are found widely in physical, biological, and social contexts. It will also address an important theoretical problem in biology. We have already referred to the fact that selection can happen at the lower (genetic) level and the higher (organism) level. There could in principle be more than two levels, leading to what is called multi-level selection. In this case entities (genes, bacteria,...) at each level can reproduce, mutate and compete with each other. We will also gain greater understanding of the way that genes in bacteria move around, and so gain some insight into the current diversity of bacteria which is observed. This should allow for the monitoring and managing of new infections and antibiotic resistance."
	},
	{
		"grant":200,
		"ID": "EP/H035745/1",
		"Title": "Ultrafast Multi-GHz Waveguide Lasers",
		"PIID": "59852",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "28/02/2014",
		"Value": "449071",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "59852", "Role": "Principal Investigator"},
		{"ID": "4633", "Role": "Co Investigator"},
		{"ID": "110324", "Role": "Co Investigator"},
		{"ID": "110087", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Elforlight Ltd"}
		],
		"Summary": "In this work we will develop very compact and efficient laser sources with a footprint of just a few square centimetres that are capable of producing more than a billion pulses of light per second, each having a duration of less than one tenth of one trillionth of a second.  Such sources have a wide range of applications, including microscopy of biological cells, precise measurement of optical frequencies, spectroscopy and telecommunications, which can all take advantage of both the very short duration of the pulse and its high repetition rate.  Many of these applications currently rely on large and relatively inefficient lasers which necessarily limits application design.  The miniaturised sources that we propose are based on a waveguide geometry that confines the laser light to very small dimensions, in a similar fashion to that used in glass optical fibres.  However, these waveguides are based in crystals such as titanium-doped sapphire and ytterbium-doped tungstate, which have proven themselves capable of providing the very short pulses of light that we are interested in.  The waveguide geometry is also capable of supporting components that are integrated into one monolithic device that can both act as the laser gain material necessary to generate the light and provide the ultrafast switching that is required to give short pulses.  The waveguides will be fabricated by growing thin layers of doped crystal on undoped substrates, with the dopant providing both the laser gain and the refractive index increase necessary to confine the light to the thin layer.  Advanced waveguide structures, based on etching of these layers and re-growth, will be fabricated to give optimum laser performance and allow pumping by high-power diode lasers.  The integrated switching components will be based on saturable absorbers that give low loss for high-intensity short optical pulses and high loss for low-intensity continuous wave light.  Optimisation of the switching properties of these absorbers and their integration with the waveguide laser will form a major part of this work.  We will also investigate the use of the Kerr effect in simple thin-film waveguides to achieve short optical pulse production by using laser resonator designs that take advantage of the fact that the high intensity of the short optical pulses will modify the refractive index such that a focussing effect is achieved.  Finally, having developed a number of devices, we will be in a good position to apply them to nonlinear microscopy of biological cells and demonstrate that the high repetition rate of the pulses provides advantages in terms of producing high optical signals without causing damage to the specimens under study."
	},
	{
		"grant":201,
		"ID": "EP/H037853/1",
		"Title": "Development of an integrated optical E-Probe for GaN power transistor reliability analysis",
		"PIID": "56547",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "31/03/2014",
		"Value": "305012",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "56547", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Fraunhofer Institute IAF"},
		{"Number": "1", "Name": "QinetiQ"},
		{"Number": "2", "Name": "SELEX Sensors and Airborne Systems Ltd"},
		{"Number": "3", "Name": "TriQuint Semiconductor"},
		{"Number": "4", "Name": "University of California Santa Barbara"}
		],
		"Summary": "AlGaN/GaN high electron mobility transistors (HEMT) are a key technology currently envisioned for future radar, satellite and communication applications, ranging from civilian to military use. Although the performance of AlGaN/GaN HEMTs presently reaches power levels up to 40W/mm at frequencies as high as 2-10 GHz, i.e., a spectacular performance enabling disruptive changes for many system applications, long-term reliability of AlGaN/GaN HEMTs is still a serious issue, not only in the UK and Europe, but also in the USA and Japan. There are several key factors affecting AlGaN/GaN HEMT reliability resulting in a variety of different failure mechanisms, including trap generation, metal migration and others. These are accelerated by: (i) device temperature, (ii) local stresses / strains (converse piezo-electric and thermal), (iii) high electric fields. Knowledge of these parameters is essential for reliability testing, in particular, for accelerated lifetime testing to predict mean time to failure (MTTF). The CDTR in Bristol developed and pioneered Raman thermography, to probe temperature and stress/strain with sub-micron spatial and nano-second time resolution in the only a few micron size active device area of AlGaN/GaN HEMTs, but there is presently no non-invasive probe available for experimentally quantifying electric field strength and its lateral distribution in particular when operating devices at high voltages. Therefore presently only simulation can be used to estimate electric field strength. The key aim of this research project is to develop, test and employ a non-invasive novel optical probe (E-probe) to quantify electric field strength and its lateral distribution in the device channel of AlGaN/GaN HEMTs, and to integrate it into Raman thermography, to enable simultaneous electric field, temperature and stress analysis of AlGaN/GaN HEMTs, to develop a unique and highly beneficial analysis technique for AlGaN/GaN HEMT reliability research. Experiments on degrading / stressing of devices to probe the resulting changes in the electric field strength and its distribution will be performed for state-of-the-art reliability research. Charge carrier traps generated during stressing change electric field strength which we expect to be able to probe directly here for the first time. Carrier trapping times range from milliseconds to seconds. We aim, a higher-risk component of this project, to also developing the ability to probing time-dependent changes in the electric field with carrier trapping / detrapping in the devices."
	},
	{
		"grant":202,
		"ID": "EP/H040536/1",
		"Title": "INTelligent Energy awaRe NETworks (INTERNET)",
		"PIID": "37134",
		"Scheme": "Programme Grants",
		"StartDate": "01/06/2010",
		"EndDate": "31/12/2015",
		"Value": "5997917",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "37134", "Role": "Principal Investigator"},
		{"ID": "-250444", "Role": "Co Investigator"},
		{"ID": "7631", "Role": "Co Investigator"},
		{"ID": "16932", "Role": "Co Investigator"},
		{"ID": "106606", "Role": "Co Investigator"},
		{"ID": "3618", "Role": "Co Investigator"},
		{"ID": "12308", "Role": "Co Investigator"},
		{"ID": "78454", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Avago Technologies"},
		{"Number": "1", "Name": "BBC Research and Development"},
		{"Number": "2", "Name": "Broadcom Corporation"},
		{"Number": "3", "Name": "BT plc"},
		{"Number": "4", "Name": "Cisco Systems Inc"},
		{"Number": "5", "Name": "Ericsson Ltd"},
		{"Number": "6", "Name": "Oclaro Technology UK"},
		{"Number": "7", "Name": "Solarflare Communications"},
		{"Number": "8", "Name": "Telecom New Zealand Limited"}
		],
		"Summary": "Energy efficient processes are increasingly key priorities for ICT companies with attention being paid to both ecological and economic drivers. Although in some cases the use of ICT can be beneficial to the environment (for example by reducing journeys and introducing more efficient business processes), countries are becoming increasingly aware of the very large growth in energy consumption of telecommunications companies. For instance in 2007 BT consumed 0.7% of the UK's total electricity usage.   In particular, the predicted future growth in the number of connected devices, and the internet bandwidth of an order of magnitude or two is not practical if it leads to a corresponding growth in energy consumption. Regulations may therefore come soon, particularly if Governments mandate moves towards carbon neutrality. Therefore the applicants believe that this proposal is of great importance in seeking to establish the current limits on ICT performance due to known environmental concerns and then develop new ICT techniques to provide enhanced performance. In particular they believe that substantial advances can be achieved through the innovative use of renewable sources and the development of new architectures, protocols, and algorithms operating on hardware which will itself allows significant reductions in energy consumption. This will represent a significant departure from accepted practices where ICT services are provided to meet the growing demand, without any regard for the energy consequences of relative location of supply and demand. In this project therefore, we propose innovatively to consider optimised dynamic placement of ICT services, taking account of varying energy costs at producer and consumer. Energy consumption in networks today is typically highly confined in switching and routing centres. Therefore in the project we will consider block transmission of data between centres chosen for optimum renewable energy supply as power transmission losses will often make the shipping of power to cities (data centres/switching nodes in cities) unattractive. Variable renewable sources such as solar and wind pose fresh challenges in ICT installations and network design, and hence this project will also look at innovative methods of flexible power consumption of block data routers to address this effect.  We tackle the challenge along three axes: (i) We seek to design a new generation of ICT infrastructure architectures by addressing the optimisation problem of placing compute and communication resources between the producer and consumer, with the (time-varying) constraint of minimising energy costs. Here the architectures will leverage the new hardware becoming available to allow low energy operation. (ii) We seek to design new protocols and algorithms to enable communications systems to adapt their speed and power consumption according to both the user demand and energy availability. (iii) We build on recent advances in hardware which allow the block routing of data at greatly reduced energy levels over electronic techniques and determine hardware configurations (using on chip monitoring for the first time)  to support these dynamic energy and communications needs. Here new network components will be developed, leveraging for example recent significant advances made on developing lower power routing hardware with routing power levels of approximately 1 mW/Gb/s for ns block switching times. In order to ensure success, different companies will engage their expertise: BT, Ericsson, Telecom New Zealand, Cisco and BBC will play a key role in supporting the development of the network architectures, provide experimental support and traffic traces, and aid standards development. Solarflare, Broadcom, Cisco and the BBC will support our protocol and intelligent traffic solutions. Avago, Broadcom and Oclaro will play a key role in the hardware development."
	},
	{
		"grant":203,
		"ID": "EP/H042512/1",
		"Title": "Elastic Sensor Networks: Towards Attention-Based Information Management in Large-Scale Sensor Networks",
		"PIID": "55263",
		"Scheme": "Standard Research",
		"StartDate": "14/06/2010",
		"EndDate": "13/12/2013",
		"Value": "471777",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "55263", "Role": "Principal Investigator"},
		{"ID": "52025", "Role": "Co Investigator"},
		{"ID": "-253112", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project, which will be co-funded by the Institute of Security Science and Technology at Imperial College London, aims to develop a novel theoretical framework and associated computational model for information management in Large-scale Sensor Networks (LSSN). Applications of such networks are drawing wide attention from both academia and industry ranging from home monitoring to industry sensing, including environment and habitat monitoring, security, traffic control and health care. A key challenge in managing such networks is that of avoiding information overload as the amount of information monitored increases. A second key challenge is how to effectively maximize the value of collected information under resource and real-time constraints. Addressing both challenges requires developing effective and efficient methods for organizing information collection and information processing that focus on analyzing only information relevant to the user needs.Our key hypothesis in this proposal is that an analogy between information processing by humans, in particular their well-evolved human attention mechanism, and information processing in sensor networks would lead to the development of novel and highly effective information management strategies for LSSNs. This analogy would enable us to exploit effectively the relationship between local and global information, avoid information overload in the application and also minimize unnecessary resource consumption (processing and communication) in the network. Our interest in developing and using an attention-like mechanism in sensor networks is driven by the fact that it could be mapped easily to a concise and robust Bayesian formulation. Such a formulation would enable us and other researchers to reason about the correctness of the approach and also to reason about its extensions and potential improvements beyond this project.Our work in this project thus focuses on addressing a number of key challenges both at the theoretical and practical levels, including the extension and application a standard Bayesian probabilistic framework to the LSSN setting, developing   the foundations for an elastic resource allocation model for such networks and supporting a decentralized approach for our implementation that scales to large scale networks implementations.In addition to developing the theoretical foundations, our work will also include developing functional prototypes of a distributed LSSN information management system using both simulations and real sensor hardware. The evaluation of our methods will proceed using case studies from two application areas: multi-modality security monitoring and urban pollution monitoring. The evaluation will be conducted in close collaboration with end users in the Institute of Security Science and Technology (ISST) and the Cenre of Transportation Studies (CTS)  at Imperial College London as well as with collaborators in three international institutions (Rutgers University, Harvard University and Monash University). The evaluation will be based on real and simulated data sets to compare the efficacy and efficiency of the proposed approach against traditional and competing methods."
	},
	{
		"grant":204,
		"ID": "EP/H043594/1",
		"Title": "Logical Difference for Ontology Versioning",
		"PIID": "-10366",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2010",
		"EndDate": "15/01/2014",
		"Value": "326691",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-10366", "Role": "Principal Investigator"},
		{"ID": "109145", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Ontologies are used to provide a common vocabulary for a domain of interest together with descriptions of the meaning of terms built from the vocabulary and relationships between them. A wide range of information technologies in areas such as medical informatics, bio-informatics, and the semantic web and grid are now dependent on ontologies to capture domain semantics and promote interoperability. A major example is the NHS Connecting for Health Programme with the strategic aim of adopting the healthcare ontology SNOMED CT across all NHS clinical systems.Modern applications often require large and complex ontologies (sometimes, containing more than 400,000 different term definitions), and, in practice, ontologies constantly evolve. They are regularly being extended, updated, corrected, and refined. Engineering, maintaining, and using such ontologies is a highly complex and laborious task, which is practically infeasible without appropriate tool support. Among the tools required, automated support for ontology versioning is one of the most critical and challenging.The aim of this research proposal is to develop a novel approach to ontology versioning. Most current ontology editors and ontology management systems, such as Protege, SWOOP, OBO-Edit, and OntoView, support ontology versioning either natively or through plugins. Though helpful, it is generally agreed that current support is unsatisfactory: it lacks unambiguous semantic foundation, it is syntax dependent, and it does not capture the non-local implications of differences between ontology versions. Moreover, it cannot take into account the fact that often one and the same ontology is used in different applications, which may require different comparison techniques. This research proposal aims at developing a logic-based approach to representing the difference between ontologies as a basis for ontology versioning. Under this view, ontologies provide answers to queries about some vocabulary of interest with the help of reasoning services.  If two versions of an ontology give the same answers to a class of queries relevant to an application domain, they may be deemed to have no difference regardless of their syntactic or structural form; and queries producing different answers from the versions may be considered as the characterisation of the difference itself. The main advantage of the logical diff over known differencing techniques is that it is determined by the logical semantics of the ontology and query language and does not depend on their syntactic form. Moreover, this approach is of greater flexibility.  By choosing an appropriate query language and vocabulary, one can detect exactly the differences visible when querying instance data, exactly the differences expressed by subsumptions between concepts, or even exactly the differences expressed in very expressive languages such as first-order logic.To achieve these aims, a number of challenging theoretical and practical problems need to be solved. In particular:- Novel algorithmic approaches are required to detect whether two versions of a logical theory are logically different.- Novel techniques to succinctly characterise the logic-based difference between ontology versions are required.- A mechanism tracing the differences in logical consequences back to axioms of the ontologies is required.- Logical meta-properties of distinct notions of logical difference have to be understood.Solutions to these research challenges are of great interest not only for ontology versioning but also for other areas of logic,knowledge representation, and automated reasoning such as ontology debugging, theory update, and theory modularisation."
	},
	{
		"grant":205,
		"ID": "EP/H043748/1",
		"Title": "Automated Prover Generation",
		"PIID": "58111",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2010",
		"EndDate": "31/03/2014",
		"Value": "410798",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "58111", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Reasoning is needed in many areas of computing from verification of programs, ontology reasoning for the semantic web, multi-agent systems to artificial intelligence and text mining and also other fields including mathematics, computational linguistics and philosophy. Successful automated theorem provers can ensure a much higher level of trust in complex mathematical proofs, they can enable us to automatically eliminate large classes of errors from software systems, and communication protocols, and they can be used to support the creation of and reasoning in large knowledge bases and web-based repositories.Implementing a fully-automated reasoning tool is a time-consuming and difficult undertaking. Currently, researchers requiring an automated reasoning tool for an application can develop a prover from scratch; they can extend and adapt an existing prover; they can use an existing prover for a wider logic into which their logic can be embedded, e.g., a prover for first-order logic or higher-order logic; or they can use logical frameworks or prover development platforms. While these approaches have led to impressive successes, all these approaches are difficult to adopt for users without significant expertise in logic and automated reasoning.As an alternative the aim of this project is to automatically generate implemented provers. The idea is that users will be able to define a logical formalisation of their application. This is automatically transformed first into a deduction calculus and then into an implemented prover. The way this is envisaged to work is similar to a compiler or interpreter, which automatically transforms a program written in a high-level programming language into machine-code or byte-code. The difference is that the input to the prover generator is not a set of instructions, but a high-level specification of a logic or a theory. This project will focus on the automated generation of tableau provers. The foundation for tableau prover generation will be a tableau calculus synthesis framework that we developed in recent work and have already applied to a number of logics. Based on this framework the software to be developed will be able to automatically generate a sound and complete tableau calculus for the given logic or theory.  We believe that it is now possible to go further and develop software that will automatically generate an implemented automated theorem prover for this calculus. This will give non-expert users and developers the ability to obtain implemented provers with relative ease in a supported way. For the user crucially this removes the burden of being concerned with proving soundness, completeness and decidability results, and implementing or adapting a prover.We will extend the theoretical foundation of the framework in various ways in order to make it more comprehensive and extend the scope of tableau methods as decision procedures. A series of tools will be developed: a tableau prover generator program and auxiliary software to ensure reliability, correctness and efficiency of the generator and the generated provers. To allow for the rapid employment of the technology and receive feedback from users a first prototype will be completed and released within one year and two tutorials will be organised. Several case studies will be conducted with the prototype to identify important issues that may need to be changed and improved in the prototype. Areas where it is crucial to find optimisations are rule refinement, prover efficiency, and generalisation of the framework. To quantify the success of the project, the utility of the developed tools and the generated provers, the acceptance by users and the practical benefit of generated provers an evaluation will be conducted.Overall, automated prover generation is a new and difficult research problem, but we believe that it is now a real possibility that we intend to realise in this project."
	},
	{
		"grant":206,
		"ID": "EP/H045368/1",
		"Title": "Making Light Deliver: translation of methods of photoporation",
		"PIID": "45650",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2014",
		"Value": "964449",
		"ResearchArea": "Clinical Technologies (excluding imaging)",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "45650", "Role": "Principal Investigator"},
		{"ID": "116335", "Role": "Co Investigator"},
		{"ID": "84429", "Role": "Co Investigator"},
		{"ID": "-44498", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The cell membrane represents the outer extremity of all cells. In mammals, this is a thin bi-layer film of lipids, embedded with various protein molecules at interspersed locations. The membrane encloses the cell, defines its boundaries and maintains the essential physico-chemical differences between the cytoplasm and the extracellular environment. Under normal circumstances, the lipid nature of the cell membrane acts as an impermeable barrier to the passage of most water-soluble molecules. Thus, the selective introduction of therapeutic agents to the inside of dysfunctional or diseased cells remains a key challenge. The St Andrews-Dundee team's original Basic Technology grant was aimed at developing two inter-related strands of research that would enable targeted drug delivery to cells and tissue at will. The studies have resulted in excellent research and papers and the methods of using tightly focused laser beams to create minute transient pores that permit therapeutic agents to be introduced has been the most promising. This technique, termed photoporation,  offers a powerful method for single cell targeted transfection in a sterile fashion.  In this next proposal we aim to translate our results and the developed technology into devices that will be common place in both universities and industry. This will be because these optical devices will be able to deliver biologically important material, ranging from genes through to prototype drugs, into a host of different biological systems such as its involvement in stem cell research, plant cells, agriculture through to in vivo mammalian tissue."
	},
	{
		"grant":207,
		"ID": "EP/H046623/1",
		"Title": "Synthesis and Verification in Markov Game Structures",
		"PIID": "-238808",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "30/11/2013",
		"Value": "335487",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-238808", "Role": "Principal Investigator"},
		{"ID": "-214796", "Role": "Co Investigator"},
		{"ID": "49970", "Role": "Co Investigator"},
		{"ID": "25618", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "To meet the objectives of our project, we will divide our research into five work packages.The first work package is devoted to representing the control problems that we want to approach and to identify benchmarks and case studies as guidelines for relevant demands and measures of success for the applied aspects of our project. The starting point for our models will be- a generalisation of interactive Markov chains to 2.5 player games, a model in which the decisions of the different players are physically separated by assigning them to different states, and- a generalisation of Markov decision processes to Markov games, a model in which the decisions of both (or, more generally, of all) players are entangled and represented in the same node.We will extend these models by representations of the observational and computational power of the controllers under consideration, and formalisations of the--simple--objectives we want to meet.Additionally, we will develop benchmarks and case studies to guide the applied aspects of our project, and to root it in different communities--in particular in engineering and IT--by reflecting their respective demands.Work packages two, three, and four from the theoretic core of our work. Our second work package will address the simple question of constructing controllers with complete information, while a third work package will address the generalisation of these techniques to controllers with incomplete information but, for distributed controllers, equivalent information about the system state.Different to discrete systems, the abstraction (or restricted observability) of time plays a paramount role when considering incomplete information of these systems. This particular type of abstraction has proven to often simplify the construction of optimal strategies: The construction of optimal time-abstract strategies (and the proof of their existence) is much simpler than the construction of time dependent ones.The fourth work package refers to the extension of these results to distributed schedulers with different observational power.For work packages two, three, and four, we will study the decidability of quantitative and qualitative safety and reachability properties.In a fifth work package we will focus on algorithmic aspects like the development and selection of appropriate data structures of the model checking and optimisation problems, and develop prototype implementations that solve as a proof-of-concept for their applicability for a selection of the developed approaches. These proof-of-concept implementations will also play an important role in determining the applicability and potential of the techniques developed in the project on the target implementations defined in the first work package, and as means to communicate our results for dissemination and exploitation purposes."
	},
	{
		"grant":208,
		"ID": "EP/H046658/1",
		"Title": "COSMOS Technology Translation Proposal",
		"PIID": "38924",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2010",
		"EndDate": "31/05/2013",
		"Value": "864033",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "38924", "Role": "Principal Investigator"},
		{"ID": "16932", "Role": "Co Investigator"},
		{"ID": "5587", "Role": "Co Investigator"},
		{"ID": "45311", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The primary aim of the original COSMOS Basic Technology Grant was to develop a new generation of compact laser light sources that were either continuously tunable or consisted of multiple wavelengths, and were based on soft materials such a polymers and liquid crystals. The emergence of such a technology will provide a light source that combines key features that do not exist in any present-day laser device namely, compact size, high quality output and a wide range of colour tunability. These lasers consist of two essential features: a light harvester that can be either excited optically or electrically and acts as a gain medium, and a colour selective structure (such as a photonic bandgap) that provides optical feedback. The COSMOS project involved the chemistry, physics and engineering of both the materials and the devices used as these laser sources.  Over its duration the project synthesised new materials for light generation as well as characterised different devices for both laser and light emitting diode (LED) structures in the context of applications such as flat panel and projection displays, telecommunications and biomedical systems.This translational grant is designed to expand some of the key results from COSMOS and develop them further both in terms of their chemical components, physical structure and their overall implementation into different applications.  One such thread is the further development of multi wavelength sources for holographic projection.  The use of holograms to generate 2D and 3D images is a recently developed disruptive technology, with many advantages but with a major drawback of having to use 3 laser light sources to provide the 3 colour components in the image.  The use of lasers sources improves the colour quality immensely but current semiconductor lasers are limited, expensive and difficult to mass produce.  The lasers developed on COSMOS are ideal for this application as they can produce 3 simultaneous laser colours from the same single device cheaply, efficiently and also in a way that could well provide a solution to a further problem with these projectors; the image degradation due to laser induced speckle.The progress made by COSMOS into the materials suitable for infra-red (IR) lasing devices is also another area being expanded upon by this proposal.  The ability to produce cheap tuneable IR sources is of great interest to many different biomedical applications as both human blood and tissue have unique signatory responses to different colours of light in the IR spectrum.  One such biomedical system is optical coherence tomography (OCT) which allows a tissue sample to be imaged not only in cross section but also in depth (ie through the different layers of the sample).  This is a very powerful tool in many medical areas such as retinal imaging where OCT can be used to diagnose pathologies such as diabetes and glaucoma. In the case of OCT, not only is the tuneable colour in the IR an important feature but also the coherence of the IR laser source is a vital parameter in the performance of the system.  A key advantage of the soft materials used in the COSMOS laser sources is that the coherence properties can be optimised through both the choice of chemical components as well as the overall device structure.  This adds another level of versatility into the already powerful diagnostics capabilities of an OCT system.  Finally, the soft nature of these IR lasers also makes them an ideal candidate for embedding into polymer waveguide based systems such as those being developed for telecommunication network components.  The ability to integrate the laser into the waveguides is an exciting new development which will be further explored by the translational grant proposed."
	},
	{
		"grant":209,
		"ID": "EP/H047816/1",
		"Title": "Materials World Network to Optimize the Growth of InGaN Quantum Dots within High Quality Optical Micro-Cavities",
		"PIID": "132139",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2011",
		"EndDate": "31/12/2013",
		"Value": "587425",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials Science & Metallurgy",
		"OrgID": "24",
		"Investigators":[
		{"ID": "132139", "Role": "Principal Investigator"},
		{"ID": "15550", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Harvard University"}
		],
		"Summary": "Materials scientists have been studying crystals - large and small - for many years. However, very tiny crystals - crystals only a few atoms across - exhibit some really surprising properties, which we are only just starting to understand. In terms of their optical properties, these very small crystals, which we call quantum dots, exhibit behaviour more similar to that of an individual atom, than that of a large crystal. This surprising observation - which is a consequence of the confinement (or trapping) of charge carriers within a very small region - is more than just a weird academic curiosity. Scientists hope to exploit quantum dots to allow improved performance in light sources such as laser diodes, and to develop completely new light sources which might be used in novel computers or in secure communication. For light sources emitting in the red or infra-red, researchers are already starting to realise some of these goals using a material called indium gallium arsenide. However, for light emission in the blue - which is particularly relevant to applications such as high density data storage and satellite-based communications networks - quantum dots made from different materials are required. For light emission in the blue spectral region, quantum dots made from indium gallium nitride (or InGaN) could be used. Quite apart from their convenient wavelength of emission, InGaN quantum dots might be rather flexible, since their emission can be adjusted by applying an external electric field. Also, by surrounding the InGaN quantum dots with an optimal matrix material, it may be possible to force them to exhibit their peculiar properties at room temperature, whereas quantum dots emitting in the red usually have to be cooled down to temperatures more than 200 degrees below freezing before they work properly. Unfortunately, InGaN quantum dots also have disadvantages. They are usually formed on top of layers of another semiconductor - gallium nitride. Gallium nitride is quite difficult to make, and contains many mistakes, or defects, in the crystal. The defects may become electrically charged, and the presence of this charge alters the properties of the quantum dot. Since the electrical charge on the defect varies with time, so does the behaviour of the quantum dot - leading to problems with the operation of a quantum dot device. In order to try to understand the properties of the InGaN quantum dots more thoroughly, and to improve the properties of quantum dot devices, we have decided to incorporate the quantum dots into optical cavities. An optical cavity is a structure within which light may be confined. By trapping the light emitted by the quantum dot within a small volume, we can force the quantum dot and the light to interact strongly, and this can lead to more efficient emission from the quantum dot. By understanding the interactions between the light and the quantum dot, we can also use the cavity as a tool to probe the details of the quantum dot's behaviour and its interactions with any defects in its immediate surroundings. We hope to use the cavities to tailor the quantum dots' properties so that they are easier to exploit in future applications. However, making the cavities is very challenging, particularly since we have to find routes to do this which do not damage the quantum dot. Since this is a very complex problem, we have set up an international collaboration in order to attack it more effectively. Two British research groups with expertise in InGaN quantum dots will collaborate with an American research group which has world-leading capability in cavity fabrication. Together, we hope to be able to develop quantum dot - cavity systems which allow very strong interactions between the quantum dot and the cavity. In the future such systems will be used not only as a probe to study the quantum dot properties but as a major building block of novel light sources."
	},
	{
		"grant":210,
		"ID": "EP/H047964/1",
		"Title": "Millimeter-Waves: The Vision for the Future - From Electrons to Volcanoes",
		"PIID": "53237",
		"Scheme": "PPE",
		"StartDate": "01/10/2010",
		"EndDate": "31/03/2013",
		"Value": "182527",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "53237", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "How can the same sort of light be used to interact with objects as different in size and scale as an electronand a volcano? What can you tell by looking at electrons anyway? How can nuclear fusion solve the 21stcentury energy crisis? How do state of the art security scanners work and what can they see? How do youobserve how a volcano is changing underneath cloud or smoke? What technology is used to build car radarfor cruise control or collision avoidance? How can we see how stars form in the outer reaches of the galaxyor understand what the Universe looked like after the Big Bang?Millimetre wave technology, a significant but little known area of Physics research, lies at the heart of ourability to do all of this and more. This project aims to convey the incredible range of imaging and sensingprovided by mm-wave technology to as wide a cross section of the public as possible, targeting audiencesthat would not normally be associated with science outreach. Our proposal seeks to deliver a high impactmajor public education program on technologies and applications associated with these very high frequencymicrowaves - in the mm-wave and sub-mm-wave regime. This area of the electromagnetic spectrum hasseen substantial funding from the research councils and is central to many topical areas of science andsociety. The program will build on a previous highly rated PPE project where a substantial amount oflecture material, demonstration equipment, video footage, teaching material, interactive exhibits and schoolworkshops were developed and enhanced over a 2 year period.There are three principal aims to this project: (1) a fresh remit to reach non 'self-selecting' audiences usingstate of the art research imagery and live demonstrations of mm-wave radar; (2) introduce the majorscience research topic and industrial application of magnetic resonance; (3) deliver a stand-alone interactiveexhibition of all this material to achieve long term sustainability in our outreach mission."
	},
	{
		"grant":211,
		"ID": "EP/H049371/1",
		"Title": "High Power Fiber Based Mid-IR Supercontinuum Sources",
		"PIID": "132142",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2011",
		"EndDate": "30/09/2012",
		"Value": "100626",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "132142", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Lasers have the unique capability to deliver highly collimated light across a wide range of wavelengths (colours).  Laser imaging does not currently extend to the mid-infra red region, which although beyond the detection range of the eye can be useful in a similar way that night-vision is of practical assistance today. Furthermore, when the light from an object is analysed, the specific  colours  enable identification of different molecules by their absorption  signatures . Conventional glass fibers are opaque at these wavelengths, but new glasses and fiber designs have recently become available with improved infra-red transmission properties. The proposed research will use this technology to create wavelength tunable laser sources. As the complexity and cost of current lasers severely restricts the number of users the proposed research will use glass fibers to develop more practical laser sources for the future. Glass is a good laser material since it is an inexpensive solid, and by using a fiber  light-pipe  geometry to maintain alignment complex mirror arrangements can be avoided.  To create such sources research is needed in a study of the characteristics of both the lasers and the fiber geometries to obtain an optimum combination. Laser development requires new materials, theoretical modelling, and experimental work.  The novel materials currently emerging from the host University's laboratories would be used to develop fiber laser sources that may become the preferred choice for future scientific and industrial laser users."
	},
	{
		"grant":212,
		"ID": "EP/H049665/1",
		"Title": "Audio and Video Based Speech Separation for Multiple Moving Sources Within a Room Environment",
		"PIID": "36938",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "31/12/2013",
		"Value": "300747",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90",
		"Investigators":[
		{"ID": "36938", "Role": "Principal Investigator"},
		{"ID": "69275", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Human beings have developed a unique ability to communicate within a noisy environment, such as at a cocktail party.  This skill is dependent upon the use of both the aural and visual senses together with sophisticated processing within the brain.  To mimic this ability within a machine is very challenging, particularly if the humans are moving, such as in a teleconferencing context, when human speakers are walking around a room.  In the field of signal processing researchers have developed techniques to separate one speech signal from a mixture of such signals, as would be measured by a number of microphones, on the basis of only audio information with the assumption that the humans are static and typically no more than two humans are within the room.  Such approaches have generally been found to fail, however, when the human speakers are moving and when there are more than two in number.  Fundamentally new approaches are therefore necessary to advance the state-of-the-art in the field.  Professor Chambers and his team at Loughborough University were the first in the UK to propose a new approach on the basis of combined audio and video processing to solve the source separation problem, but their preliminary approach identified major challenges in audio-visual speaker localization, tracking and separation which must be solved to provide a practical solution for speech separation for multiple moving sources within a room environment.  These findings motivate this new project in which world-leading teams at the University of Surrey, led by Professor Kittler, and at the GIPSA Lab, Grenoble, France, headed by Professor Jutten, are ready to work with Professor Chambers and his team at Loughborough University to advance the state-of-the-art in the field.In this new project, two postdoctoral researchers will be employed, one at Loughborough and another at Surrey.  The first will focus on the development of fundamentally new speech source separation algorithms for moving speakers by using geometrical room acoustic (for example location and number of sources, descriptions of their movement) information provided by the second researcher.  The research team at Grenoble will provide technical guidance on the basis of their considerable experience in source separation throughout the project and will work on providing an acoustic noise model for the room environment which will also aid the speech separation process.  To achieve these tasks,  frequency domain based beamforming algorithms will be developed which exploit microphone arrays having more microphones than speakers so that new data independent superdirective robust beamformer design methods can be exploited using mathematical convex optimization.  Additionally, further geometic information will be exploited to introduce robustness to errors in the localization information describing the desired source and the interference.  To improve the localization information an array of collaborative cameras will be used and both audio and visual information will be used.  Advanced methods from particle filtering and probabilistic data association will be exploited for improving the tracking performance.   Finally, visual voice activity detection will be used to determine the active sources within the beamforming operations.  We emphasize that this work is not implementation-driven, so computational complexity for real-time realization will not be a focus; this would be the subject of a future project.All of the new algorithms will be evaluated both in terms of objective and subjective performance measures on labelled audio and visual datasets acquired at Loughbourgh and Surrey, and from the CHIL seminar room at the Karlsruhe University (UKA), Germany.  To ensure this pioneering work has maximum impact on the UK and international academic and research communities all the algorithms and datasets will be made available through the project website."
	},
	{
		"grant":213,
		"ID": "EP/H050442/1",
		"Title": "Word segmentation from noisy data with minimal supervision",
		"PIID": "-203779",
		"Scheme": "Standard Research",
		"StartDate": "24/01/2011",
		"EndDate": "23/04/2014",
		"Value": "281783",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-203779", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In recent years, the field of natural language processing (NLP) has made great advances in a wide range of areas, such as machine translation, document summarization, and topic identification.  However, much of this success is due to systems that are built using large quantities of human-annotated data in a supervised machine learning approach.  This means that languages with fewer annotated resources (low-density languages) are left without much useful language technology.  An important direction in NLP research is therefore to improve our ability to develop successful systems using as little annotated data as possible.  Research on completely unsupervised systems is particularly interesting not only for its potential to broaden the reach of NLP technology, but also because it may shed light on the ways in which human infants manage to learn language with little or no explicit instruction.We propose to focus on the particular problem of word segmentation, and to develop a new type of probabilistic model, the infinite noisy channel model, for solving this problem in settings where little or no annotated data is available.  Word segmentation refers to the problem of identifying word boundaries in either text or speech.  It arises in NLP systems for many Asian languages, where words are not separated by whitespace, and also for infants learning language, because most spoken words are not separated by pauses.  Previous work on unsupervised word segmentation has assumed that every time a particular word occurs, it is realized in exactly the same way.  However, this is not the case for infants learning language (since words are subject to phonetic variability and noise in pronunciation), nor is it always true in NLP (if the input text contains errors, such as those produced by an optical character recognition system).  Our new model will address this shortcoming by simultaneously performing word segmentation and correction of noise and variability, to recover a sequence of de-noised words from the unsegmented noisy input.  We plan to develop two different versions of our model.  One of these will be designed to correct for phonetic variability, and will be evaluated as a cognitive model of human language acquisition.  With this model, we hope to gain insight into the computational mechanisms that allow infants to successfully extract words from noisy input, and in particular to show that the Bayesian inference techniques used in our model are a plausible explanation of infants' learning behavior.  The second version of our model will be designed to correct for errors resulting from optical character recognition, and will be evaluated as a word segmentation and error-correcting NLP application in several different languages.  We hope to show that the model reduces the number of character errors in the document while also producing successful segmentations.  We expect these improvements to be particularly pronounced in low-density language situations."
	},
	{
		"grant":214,
		"ID": "EP/H050655/1",
		"Title": "Exploring Short Wavelength Limits for High Performance Quantum Cascade Lasers",
		"PIID": "27910",
		"Scheme": "Standard Research",
		"StartDate": "11/10/2010",
		"EndDate": "10/04/2014",
		"Value": "500498",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "116",
		"Investigators":[
		{"ID": "27910", "Role": "Principal Investigator"},
		{"ID": "-2868", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cascade Technologies Ltd"}
		],
		"Summary": "The realisation of high performance quantum cascade laser (QCL) sources at the short wavelength end of the 3-5 micron atmospheric transmission window is of major interest for a wide range of technological applications. Many of these are potentially of great significance for healthcare, security and the environment. However, conventional QCL materials systems such as InGaAs-AlInAs are fundamentally unsuitable for such short wavelength devices, as they do not have sufficiently deep quantum wells to support the high energy intersubband transitions required. Consequently, in recent years, attention has turned to alternative QCL materials systems based on III-V antimonides. At Sheffield we have established considerable expertise in the InGaAs-AlAsSb materials system.  In addition to very deep quantum wells (~1.6 eV), this system provides lattice matched compatibility with InP-based waveguide and device fabrication technology. In this project we will develop short wavelength InGaAs-AlAsSb QCLs that will redefine the state of the art for semiconductor lasers in the 3-4 micron region, and provide unprecedented levels of performance and functionality for trace gas sensing and countermeasures applications. We will also exploit the potential of such deep QW devices for new developments in intersubband non-linear optics, in particular the demonstration of QCL operation at telecommunications wavelengths via intracavity second harmonic generation."
	},
	{
		"grant":215,
		"ID": "EP/H050795/1",
		"Title": "Long-term, High Order Visual Mapping",
		"PIID": "11374",
		"Scheme": "Standard Research",
		"StartDate": "10/06/2010",
		"EndDate": "09/06/2014",
		"Value": "767540",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "11374", "Role": "Principal Investigator"},
		{"ID": "46023", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "By observing a static scene with a moving video camera, it is possibleto estimate the trajectory of the camera through the environment,simultaneously build a  map  of the environment simply by repeatedobservation of the motion of visual  features  in the videoimages. The robotics community generally refers to this problem asvision-based Simultaneous Localisation and Mapping (SLAM) and thecomputer vision community calls it Structure From Motion (SFM).It is currently possible perform real-time camera localisation andmapping (i.e. visual SLAM) in a small environment on any modernlaptop, suitable for applications such as augmented reality orproviding visual odometry to a service robot. However, currently themaps that are built to enable localisation are usually short-lived,difficult to interpret by a human, and are rarely re-used by devicesother than the ones used to build the map. These maps are also easilycorrupted by moving objects or other dynamics such as cyclic changes.A significant challenge now faced is to build maps that can adequatelyrepresent a non-static environment by taking account of movingobjects, evolve over long periods of time, and provide more usefulrepresentations of the world with high level information. Applicationswhich would benefit from the longer lasting, semantically meaningfulmaps include: personal robotics and assistive devices, beneficial inan aging society to confer greater independence to the infirm; changedetection for military and civilian surveillance such as in IED(imporvised explosive device) detection; driver assistance tools, forautomatic reasoning for vehicles in complex and clutteredenvironments.  Our research therefore aims to address fundamentalissues in visual mapping in order to provide sound underpinnings forthe above commercial applications.  More specifically, the focus ofthe current proposal is:(i) to build representations that can be updated to take into accountchanges in appearance and structure and be used by different visualsensors;(ii) to investigate representations and algorithms for extraction ofhigh-level semantic information to improve the mapping process,develop scene understanding, to provide a more natural interface tocognitive processing.The use of a mobile observer taking continuous measurements of theenvironment provides opportunities for learning and leveraging contextin novel ways.  We aim to achieve our goals through combined use ofgeometric data (map), photometric data (the image stream) andhigh-level contextual information (in which observations areunderstood in terms of their relationship to the  bigger picture ).In doing so we expect to build an environment model not as a simpleunstructured point cloud, nor as a  flat  collection of visual featuredescriptors, but as a semantically meaningful hierarchy.  Such arepresentation would clearly be of great benefit for high-levelreasoning and human interaction. Moreover we argue that it is onlythrough such higher-level representations that the robustness requiredfor truly long-term mapping will emerge.We aim to support these theoretical and practical advances inalgorithms and representations for visual mapping, with real, robustimplementations, which run in real-time."
	},
	{
		"grant":216,
		"ID": "EP/H051155/1",
		"Title": "Softcore Streaming Processors for FPGA",
		"PIID": "-119314",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "28/02/2014",
		"Value": "450323",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10",
		"Investigators":[
		{"ID": "-119314", "Role": "Principal Investigator"},
		{"ID": "17488", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern FPGA represent the pinnacle of the embedded device market for computationally intensive applications; providing an unprecedented 1.2TMACs of fixed-point on-chip performance, and 28Tbits/s memory bandwidth, they offer massive resources with which to build embedded realisations of applications such as Digital Signal Processing (DSP), and the ability to optimise the cost of the embedded architecture in an application specific manner.They are, however, critically hamstrung by programming problems. In stark contrast to the software-based programming style of the majority of alternative devices, FPGA normally require the use of hardware design languages and techniques to derive the best implementation.This project proposes a new approach to solve this problem for FPGA DSP. Having shown that simple programmable processors can achieve real-time performance and cost similar to dedicated hardware on FPGA, we propose a project which uses these processors to build FPGA DSP architectures. We propose to develop an ultra-efficient processor architecture and a design methodology to translate a dataflow graph DSP application model into a heterogeneous network of the simple processors, tailored to the application requirements. Further, we will automate this process and apply to real world application demonstrators from our industrial partners Xilinx Inc., the FPGA device market leader and National Instruments, a premier global supplied of FPGA embedded products."
	},
	{
		"grant":217,
		"ID": "EP/H051511/1",
		"Title": "ExODA: Integrating Description Logics and Database Technologies for Expressive Ontology-Based Data Access",
		"PIID": "71179",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2011",
		"EndDate": "31/03/2014",
		"Value": "704521",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "71179", "Role": "Principal Investigator"},
		{"ID": "-185626", "Role": "Co Investigator"},
		{"ID": "-192991", "Role": "Co Investigator"},
		{"ID": "-106108", "Role": "Co Investigator"},
		{"ID": "-266029", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Sources of semi-structured, overlapping, and semantically-related data on the Web are currently proliferating at a phenomenal rate, which has created a demand for more powerful and flexible information systems (ISs). This new generation of ISs will need to integrate incomplete and semi-structured information from heterogeneous sources, employ rich and flexible schemas, and answer queries by taking into account both knowledge and data.Ontology-based data access has recently been proposed as an architectural principle for such systems. The main idea is to develop a unified view of the data by describing the relevant domain in an ontology, which then provides the vocabulary used to ask queries. The IS can use ontological statements, such as the concept hierarchy, to derive new facts and thus enrich query answers with implicit knowledge. This idea has been incorporated into systems such as QuOnto, Owlgres, ROWLKit, and REQUIEM, and ontology reasoners such as RACER, FaCT++, Pellet, and HermiT.Such systems suffer from two main problems. First, the modelling capabilities of ontology languages are often insufficient for practical use cases. In order to achieve favourable computational properties, ontology languages are usually capable of describing only  tree-shaped  relationships; furthermore, (with some notable exceptions) they usually support only unary and binary predicates. Finally, ontology languages typically employ the open world assumption; however, when answering queries over large amounts of data, the closed world assumption (CWA) is often more appropriate.Second, query answering facilities in existing ontology-based ISs typically do not scale to data sets commonly encountered in practice. Up to now, approaches to addressing this problem have focused on reducing the expressivity of the ontology language even further in order to obtain formal tractability guarantees.  This obviously exacerbates the first problem (restricted modelling capabilities), while not necessarily delivering robust scalability in practice.Database theory and practice can provide partial solutions to these problems.  In databases, complex domains can be described using dependencies.  Dependencies are used in a number of different ways: they are often used as integrity constraints--checks that verify whether a database instance includes all data specified in the domain description; however, dependencies can also be used similarly to ontologies to derive implicit knowledge. Treating dependencies as integrity constraints and answering queries under CWA has allowed practical relational database management systems (RDBMSs) to scale to very large data sets.Database techniques alone do not, however, satisfy all the requirements for an ontology-based IS.  In particular, dependencies often cannot model arbitrarily large structures and thus do not cover all practical modelling use cases.  Furthermore, generalising the query answering techniques used in practical RDBMSs to the case where information deriving dependencies must be taken into account is still an open problem.We therefore believe that the next generation of ontology-based ISs should be based on a synthesis and an extension of ontology and database systems and techniques, providing data handling capabilities similar to current RDBMSs, but with schemas that are rich, flexible, and tightly integrated with the data. In order to achieve this ambitions goal, however, a number of challenging fundamental problems must be solved. First, ontology and dependency languages need to be unified in a coherent theoretical framework.  Second, it will be necessary to identify fragments of the framework that are likely to exhibit robust scalability but can still support realistic use cases. Third, it will be necessary to devise effective algorithmic techniques that can form the basis of practical ISs."
	},
	{
		"grant":218,
		"ID": "EP/H051678/1",
		"Title": "Programmable Transponder for Future Optical Networks (PATRON)",
		"PIID": "-11022",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2011",
		"EndDate": "28/02/2013",
		"Value": "101183",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30",
		"Investigators":[
		{"ID": "-11022", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Today's telecom operators are facing an increasing need to provide users with dynamic and high-capacity network connectivity services. This is mainly driven by deployment of the FTTx technologies and the emergence of ever more bandwidth demanding, dynamic and heterogeneous e-services such as 3D games, virtual worlds, and network based high quality media applications such as HD and UHD IPTV.As 100 Gbps Ethernet is becoming commercially available, researchers are focusing on 1Tbps transport by investigation solutions for achieving bit rates at 100Gbps and beyond per wavelength channels which can be deployed in a dynamic network environment. A promising solution for transmission of high bit rate at 100Gbps and beyond per channel is optical orthogonal frequency division multiplexing (O-OFDM). It can take advantage of multi-level modulation schemes; enable the efficient compensation of transmission impairments and also adaptation of data rate and transmission format of the transmission channel. This features make the O-OFDM technology an attractive solution for future optical Internet where dynamic bandwidth allocation is requiredThe PATRON proposal combines advantage of multi-level modulations, Optical OFDM multiple Access (O-OFDMA) concept and network elements programmability and proposes an advanced programmable optical transmitter and receiver capable of transmitting different user data with different bit rates and different modulation format in a dynamic O-OFDMA core network. The research novelties of this project are:1- Design and implement a programmable O-OFDMA transmitter and receiver that can be attached to a commercial electronic edge router or optical cross connect and constitute edge or core router of an O-OFDMA network.2-Deployment and adaptation of O-OFDMA technology in core optical networks by proposing a novel transmitter/receiver capable of mapping real time data from metro/access network into high speed subcarriers at different wavelength channels in a core O-OFDMA network3- Core network programmability through on-demand and programmable network partitioning and bandwidth allocation with fine granularity at wavelength and subcarrier level enabled by the proposed transmitter/receiverThe major outcome of this project will be an implementation of a programmable O-OFDMA transmitter and receiver supporting the above functionalities, which will be demonstrated and validated in a lab trial.The outcome of PATRON project can potentially enable network network operators to partition their network to several segments, each with different bandwidth and QoS capability. The partitioning criteria such as bandwidth and QoS can be defined based on user or application requirementsThis proposal aims to open a new field of research for the Principle Investigator (PI) inline with the current activities within High Performance Networking Group (HPNG) in University of Essex. . The outcomes of PATRON will enable the PI to advance the HPNG rich portfolio in two dimensions: 1- high-speed optical transport for future core optical network based on optical OFDM technology and 2- programmable and application-ware optical networking for future Internet based on optical OFDMA technology. It will enable the PI to build his own independent research field by further developing the novel concept of optical core network infrastructure partitioning using O-OFDMA networking technology."
	},
	{
		"grant":219,
		"ID": "EP/H051988/1",
		"Title": "A Predictive Modelling based Approach to Portable Parallel Compilation for Heterogeneous Multi-cores",
		"PIID": "51232",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2010",
		"EndDate": "31/05/2014",
		"Value": "494120",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "51232", "Role": "Principal Investigator"},
		{"ID": "-22950", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern computers at their heart consist of multiple processingelements. These multi-core processors have the capability ofdelivering high performance with reduced energy consumption, but arehighly challenging to program. As the number of cores is relativelysmall in number at present, operating systems can make good use ofthem. In the near future, however, the number of cores will rise andvary in type and capability.  Currently, this means that programmerswill soon have to think in parallel and work out how to partitiondifferent parts of their programs to run on different types ofcores. Each time this program is run on a new platfrom or the currentone is upgraded, this task will have to be repeated. Due to the sheercomplexity of this process, as hardware increases in size andcomplexity, software will not be able to utilise its potential, leadingto  software stagnation.This project aims to prevent this software stagnation by investigatingnew techniques to automatically learn how to utilise new multi-coreplatforms.  Using ideas and techniques first developed in artificialintelligence, we will develop a system that automatically learns howto adapt software to work on new platforms. It uses statisticalmachine learning to determine what type of cores to use to give thebest performance and also predicts when software is out-of date.If successful it will be of significant benefit to academics workingin the area, UK industry and in the long term applicationsprogrammers."
	},
	{
		"grant":220,
		"ID": "EP/H052089/1",
		"Title": "Near infrared single photon detection using Ge-on-Si heterostructures",
		"PIID": "2220",
		"Scheme": "Standard Research",
		"StartDate": "10/01/2011",
		"EndDate": "08/01/2015",
		"Value": "367877",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "2220", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Semiconductor-based photon-counting detectors have risen to prominence in the last decade as new application areas have emerged, such as quantum information processing, and in particular quantum cryptography. These photon-counting detectors - mainly fabricated from silicon - have also taken over from photomultipliers in a number of laboratory applications where their room temperature operation, fast timing, small footprint and low power consumption have proved advantageous in a host of applications, for example fluorescence lifetime imaging. New photon-counting applications areas in ground-based, airborne and even satellite-borne laser-induced reflection techniques have been developed in recent years (eg for detection of trace gas concentrations), as well as significant developments in low-power optical imaging and high-resolution depth imaging.  In the near-infrared spectral region - where silicon-based detectors are highly inefficient - there remain substantial issues with available single-photon avalanche diode (SPAD) detectors.  Their performance deteriorates due to the high noise levels associated with thermal excitation of carriers across the relatively narrow bandgaps, as well as the effects of mid-gap trapping centres causing the deleterious effects of afterpulsing, further contributing to detector noise levels. This project aims to establish a new class of germanium/silicon SPADs that will operate efficiently in the near-infrared, particularly at the strategically important telecommunications wavebands, and combine the advantages of low-noise Si single-photon avalanche multiplication with the infra-red sensing capability of Ge. This new class of detectors will take advantage of recent advances in epitaxial Ge/Si growth and be fabricated in conjunction with the recently-created UK Silicon Photonics consortium (UKSP), which offers world-class device growth and fabrication facilities. The detectors will be validated on existing state-of-the-art testbeds for quantum key distribution and time-of-flight ranging/depth imaging.  The project leverages the combined expertise and facilities of existing UK Silicon Photonics consortium to do additional and new work, thus adding value to that consortium."
	},
	{
		"grant":221,
		"ID": "EP/I001107/1",
		"Title": "Scene Understanding using New Global Energy Models",
		"PIID": "72946",
		"Scheme": "Standard Research",
		"StartDate": "07/09/2011",
		"EndDate": "06/09/2015",
		"Value": "439228",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Tech, Design and Environment",
		"OrgID": "54",
		"Investigators":[
		{"ID": "72946", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "OMG plc"}
		],
		"Summary": "This proposal concerns scene understanding from video. Computer vision algorithms for individual tasks such as objectrecognition, detection and segmentation has now reached some level of maturity. The next challenge is to integrate all thesealgorithms and address the problem of scene understanding. The problem of scene understanding involves explaining thewhole image by recognizing all the objects of interest within an image and their spatial extent or shape in 3D.The application to drive the research will be the problem of automated understanding of cities from video usingcomputer vision, inspired by the availability of massive new data sets such as that of Google's Street Viewhttp://maps.google.com/help/maps/streetview/, Yotta http://www.yotta.tv/index.php (who have agreed to supply OxfordBrookes with data) and Microsoft's Photosynth http://labs.live.com/photosynth/. The scenario is as follows: a van drivesaround the roads of the UK, in the van are GPS equipment and multiple calibrated cameras, synchronized to capture andstore an image every two metres; giving a massive data set. The task is to recognize objects of interest in the video, fromroad signs and other street furniture, to particular buildings, to allow them to be located exactly on maps of the environment.A second scenario would be to perform scene understanding for indoor scenes such as home or office, with video taken froma normal camera and Z-cam."
	},
	{
		"grant":222,
		"ID": "EP/I00193X/1",
		"Title": "ULTRA-SENSITIVE GRAPHENE NANO-BIOSENSORS",
		"PIID": "126042",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2012",
		"Value": "101163",
		"ResearchArea": "Graphene and Carbon Nanotechnology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "125",
		"Investigators":[
		{"ID": "126042", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Illnesses like cancer, cardiovascular diseases, Alzheimer's, Parkinson's, and diabetes have negative effects on patients, society and insurance systems. The ever-ageing population has a critical demand for more efficient, affordable healthcare.Nanotechnology is building nanometre sized devices for electronics and medical applications, raising expectations for better diagnostics and 'smart' treatments. Nanosensors could identify disease biomarkers at the earliest possible stage, potentially intercepting more serious illness. Available therapeutic options informed by early detection are greatly enhanced e.g. prostate cancer, the 2nd largest killer of male cancer patients, better than 70% chance of successful treatment if detected early. Biomarkers are often present at very low concentrations. Monitoring of biomarkers like prostate-specific-antigen, glucose or other blood-based markers at clinical concentrations - possible using nano-sensor arrays - could provide genuine benefits to patients. Non-invasive smart-testing using urine or breath samples would have a hugely beneficial impact on chronic patients and for monitoring biomarker levels after treatment. Current diagnosis of disease biomarkers is based on detection of fluorescently labeled probe molecules which interact with specific substrate-bound receptors. This is expensive and time-consuming, with limited sensitivity. Nanoscale electrochemical biosensors can achieve much higher sensitivities, using label-free biomarkers. We will exploit our recent advances in graphene growth and semiconductor surface functionalisation, to develop wafer-based, graphene/Silicon Carbide (SiC) technology for novel biosensor applications. Graphene, a single atomic layer of graphite, has some exceptional electronic properties such as its zero bandgap and very high carrier mobilities at room temperature - similar to carbon nanotubes. Graphene can be grown on SiC substrates using silicon sublimation. The principal advantage of graphene/SiC over conventional graphene - from exfoliation of graphene flakes from graphite substrates - is the capacity for graphene growth on flat, large-area wafers (up to 100mm in diameter), on which devices can subsequently be fabricated using standard semiconductor processing. Nano-channel graphene electronic devices will be fabricated on SiC substrates. We have shown that graphene can be bio-functionalized and will use surface functionalisation chemistry to attach  bio-receptors  to nano-channel graphene devices. Because of their high surface/volume ratio and quantum confinement properties, the electrical properties of graphene nano-channels have increased signal/noise ratios and are strongly influenced by minor perturbations, enabling biomarker detection at ultra-sensitive (fM) analyte levels. Current transport through the nanosensor should show extreme sensitivity to its local environment.A proof-of-concept nano-channel bio-sensor will be developed using antibody-functionalized graphene, capable of specific and selective interaction with the target prostate cancer biomarker, 8-hydroxydeoxyguanosine (8-OHdG). When the bio-molecule binds to the functionalised surface, it produces an electrical signal, which can be detected. The key to an effective biosensor is that only specific bio-molecules will bind to the receptor and produce a response. Selectivity can be controlled by choosing the correct bio-receptor.The functionalisation chemistry proposed allows for attachment of a variety of antibody, enzyme, or aminoacid bio-receptors. Our graphene/SiC biochip could eventually provide an ultra-sensitive, fast-diagnosis, cost-effective test for numerous disease biomarkers - potentially revolutionizing healthcare by bringing diagnosis and monitoring to the point-of-care. Once trialed, mobile monitoring systems, transmitting a signal to a hand held readout display, could allow translation of healthcare from hospitals to patients in their own homes."
	},
	{
		"grant":223,
		"ID": "EP/I001964/1",
		"Title": "UCT for Games and Beyond",
		"PIID": "98328",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "28/02/2014",
		"Value": "466368",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "98328", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Artificial Intelligence (AI) research and the development of the multi-billion dollar video games industry have gone hand in hand for many years. Video games are by far the most prevalent way that the public encounter AI techniques on a day to day basis, and the desire for better video games has driven AI research in areas such as move/path planning, decision making, non-player character (NPC) behaviour and the automated generation of game content. A recent development of Monte Carlo methods called the Upper Confidence Bounds for Trees (UCT) method promises to have a profound impact on AI for games. Applications of UCT are not limited to games and have potential benefits for almost any domain where simulation and statistical modelling can be used to forecast outcomes, such as planning, decision support, economic modelling, behavioural analysis, and so on.Since it appeared in 2006/7, UCT has revolutionised the demanding problem of move planning for computer Go to produce artificial players able to beat professional players for the first time this year, a feat previously thought infeasible. UCT has also been successfully applied to the less specialised domain of General Game Playing (GGP) to produce the 2008 and 2009 world champion GGP programs. This success in Go, where substantial problem-specific knowledge is used, and in GGP, where it is impossible to use problem-specific knowledge, points to the tantalising possibility of the broad use of UCT between these two extremes. Game AI researchers are now starting to take such a great interest in UCT that we are seeing the birth of a new research field of Monte Carlo Tree Search (MCTS). However, there has been to date no unified effort to fully understand and exploit the UCT algorithm and related MCTS methods, a state of affairs that we plan to redress.The proposed research will develop and evaluate novel extensions of the UCT method to increase its applicability to a broad range of game-related domains including: its use for move planning and decision making in infinite, continuous real-time environments; its application to situations involving uncertainty and incomplete information; and its application to multi-objective and ensemble planning approaches. We will also investigate its use for more general game-related problems including the detection and optimisation or correction of suboptimal game designs and game content, and the automated generation of new high quality games and game content. Further, we will demonstrate how the techniques we develop can be applied to broader non-game domains by demonstrating their application to robotic control and automated music generation, in particular the creatively challenging task of jazz improvisation.The potential impact of UCT and MCTS cannot be overstated. Landmark events that have driven AI research include the introduction of tree search methods which have been the backstay of AI decision making since the inception of this field in the 1950s, and the formalisation of Monte Carlo methods in the 1970s for simulation-based decision making in a broader range of more general and less well-defined problems. UCT/MCTS promises to be the next major breakthrough in AI methods that combines the power of tree search with the generality of simulation-based search."
	},
	{
		"grant":224,
		"ID": "EP/I003614/1",
		"Title": "Liquid-Crystal-Based Beam Steerable Planar Antennas for 60 GHz Wireless Networks",
		"PIID": "10962",
		"Scheme": "Standard Research",
		"StartDate": "02/01/2011",
		"EndDate": "01/07/2014",
		"Value": "453065",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30",
		"Investigators":[
		{"ID": "10962", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The huge bandwidth available in the unlicensed 57-66 GHz spectrum can support multi-gigabit transmission.  Depending on the operating environment and application, 60 GHz wireless networks with ranges between 10 m to 100 m are attractive and currently are under research and development.  They can have immense applications in consumer and professional electronic markets.  Not only do wireless networks eliminate cabling between communicating terminals, but they permit high definition (HD) uncompressed video streaming over air, can support wireless HD video interactive games, allow secure multi-gigabit data link between close city offices, and can provide safe and fast data, audio and video wireless distribution in planes, trains, cars, offices, hospitals and many other places of interests.  Restricted to these applications alone, the sale of 60 GHz chipsets is estimated to run into billion of units.  In the 60 GHz band the external interference is extremely low as the absorption of electromagnetic signals by oxygen and obstacles are extremely high.  Unfortunately, the high attenuation together with the low transmitter power adversely affects the transmission quality.  As flagged by the industry, this problem can be overcome by the implementation of directional antennas in 60 GHz transceivers, capable of steering their beams automatically for maximum signal reception.  The aim of this work is to develop beam steerable planar antenna arrays for the 60 GHz band using nematic liquid crystals as a means for controlling the phase distribution over the antenna elements and hence, to provide prototype antennas which are compact, low-power and low-cost for multi-gigabit 60 GHz wireless networks.  For this purpose, we will investigate, develop and compare two types of phase shifters based on liquid crystals, and design and build beam steering antenna arrays integrating the best alternative. Both the optimisation of the phase shifter performance and the development of the antenna arrays require detailed liquid crystal and electromagnetic modelling for which there is no commercial software available.  In this work, our own advanced liquid crystal and electromagnetic modelling methods are extended and software tools capable of dealing with fully inhomogeneous and anisotropic dielectrics (on a point by point basis) will be developed in order to analyse and design accurately and reliably the 2D and 3D liquid-crystal-based devices including phase shifters and antennas."
	},
	{
		"grant":225,
		"ID": "EP/I004084/2",
		"Title": "Quantum Simulations of Future Solid State Transistors",
		"PIID": "-188808",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "18/07/2011",
		"EndDate": "17/03/2016",
		"Value": "640570",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "125",
		"Investigators":[
		{"ID": "-188808", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Gold Standard Simulations"},
		{"Number": "1", "Name": "Institute of Material Sciences Barcelona"},
		{"Number": "2", "Name": "Swiss Federal Institute of Technology"},
		{"Number": "3", "Name": "Universites d'Aix-Marseille Paul Cezanne"},
		{"Number": "4", "Name": "University College London"},
		{"Number": "5", "Name": "University of Southampton"}
		],
		"Summary": "Computers and electronic gadgets, such as the iphone, have transformed modern life. The silicon transistor is at the core of this revolution, having been continuously made faster and smaller over the last forty years. In a chip, millions of them are squeezed into an area the size of a pinhead, switching a billion times in one second. Transistor size has now reached nanometre dimensions; one nanometre is only ten time larger than an atom. Moore's law, which dictates that transistor size halves every two years and is the driving force behind the success of the electronics industry, has come to a halt. The happy and easy days of transistor scaling are now gone. Quantum mechanical laws conspire against transistor function making it leak when switched off and generating poor electrical control. Also, our inability to control the precise atomic structure of interfaces and chemical composition during fabrication makes transistors less predictable. Hence semiconductor companies are searching for alternative, non-planar (multigate) transistor architectures and novel devices such as nanowires, nanotubes, graphene and molecular transistors, which will ultimately break through the nano-size barrier resulting in a completely new era of miniaturization. There is a significant gap between our ability to fabricate transistors and to predict their behaviour.The simulation and prediction of the silicon transistor has become an vital mission. Current planar transistor architecture presents serious problems in scalability regarding leakage and controllability. Transistors of nanometre dimensions are more vulnerable to the atomic nature of matter than their previous cousins of micrometre dimensions. Furthermore, at nanoscales heat transfer is a source of  heat death  for novel transistor applications due to the decrease of thermal conductivity. Within this context I propose to develop a Quantum Device simulator, with atomic resolution that will enable the accurate prediction of present and future transistor performance. The simulator will deploy a quantum wave description of electron propagation, treating the interaction of electrons with crystal lattice vibrations (heat) at a fully quantum mechanical level. It will have the capability of describing the electron interactions with the roughness of the semiconductor/dielectric interface and with each other under the effect of a high electric field. Devices will be properly tested and optimised regarding materials, chemical composition and geometry without the high costs implicit in fabrication. A wide range of transistors will be explored from planar, non-planar and novel. This is timely as existing computer design tools lack predictive capabilities at the nanoscale and the industrial build-and-test approach has become prohibitively costly. Efficient quantum-models/algorithms/methodologies and tools will be developed.These are dynamic times as device dimensions move closer to the realm of atoms, which are inherently uncontrollable. In this regime two streams collide: the classical and quantum worlds making the need for new regularities and patterns vital as we strive to conquer nature at this scale. This offers exiting opportunities to merge an engineering top-to-bottom approach with a physics bottom-up approach. As 21st century environmental concerns rise, the need for greener technology is increasing. My proposal addresses the lowering of power consumption, raw material reductions delivering more functionality and the provision of a cheaper way to assess new design technologies. Collectively, these will help companies to provide a greener alternative to consumers."
	},
	{
		"grant":226,
		"ID": "EP/I004157/2",
		"Title": "Breaking the Copper Bottleneck: Computer Architecture and Power Implications of Photonic Interconnect",
		"PIID": "-97308",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/06/2011",
		"EndDate": "30/09/2015",
		"Value": "588413",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-97308", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Xilinx Corp"}
		],
		"Summary": "The provision of future services in the digital economy is reliant on achieving more power efficient computers.  Recent bandwidth improvements in electronic interconnects have only been achieved at the expense of dramatic increases in latency and power consumption [19].  Photonic technologies appear essential to make chip-to-chip communication sustainable for ever-higher data rates due to inherently lower power operation.  Recent advances in silicon photonics, photonic printed circuit boards (PCB) and 3D integration technologies indicate great promise for short distance photonics.  However, given the radical changes in computer design brought about by chip multiprocessors (CMP)  and the fundamental differences between electronic and photonic communications, the design implications for complete computer systems are not clear.  The proposed research will study the implications of upcoming photonic technologies on the power consumption and architecture of large computer systems such as high performance computers and data centres.  The uniqueness of the proposal is its method and the holistic results that it should produce.  I will start with a firm scientific foundation based on characterisation of emerging photonic devices leading to models of existing and predicted future components.  FPGA-based emulation will enable investigation of complete multi-chip, multi-core computer systems and interconnect running at around 1/100th of real time.  The outcome will be the knowledge to build large computer systems optimised for minimum power consumption.  This multidisciplinary research therefore underpins several EPSRC themes: digital economy, next-generation healthcare and energy efficiency as well as responding to the EPSRC signposted Moore-for-Less microelectronic grand challenge."
	},
	{
		"grant":227,
		"ID": "EP/I004246/1",
		"Title": "Foundations of Secure Web Programming",
		"PIID": "-108456",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/08/2010",
		"EndDate": "31/07/2015",
		"Value": "591978",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-108456", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many important activities in our lives involve the web. We socialize on Facebook, have fun on YouTube, bank online, store our work in the cloud, find a job on LinkedIn and some of us even get married on Second Life. What makes web technology so exciting is that people and companies keep finding new and creative ways of using it for applications not foreseen by its designers: for example, using the web to make phone calls and mobile phones to browse the web.Unfortunately, for this very reason, the software and protocols on which web applications are based are not designed with the appropriate level of security in mind. Some of the information we share with web applications is very valuable, and should be protected carefully. News stories often remind us how cyber-crime negatively affects our finances, privacy and well-being.Web companies are strongly innovation-driven and focus on delivering new applications and features as quickly as possible, selecting which ones to maintain based on popularity or profitability. While the importance of security is acknowledged, the most common approach is to enforce security by monitoring the system and intervening when a security violation is detected. As this industry matures, there is a raising awareness that security must to be built into the languages and tools used to program web applications, and there is a growing need to gain some level of confidence that an application is effectively secure.In my career so far, I have studied in depth the foundations and principles for understanding computer programs and making sure that they work correctly without security breaches. Over the next few years, I will face the challenge of applying these principles to lay web programming on a sound formal ground. I want to understand deeply the current and emerging technologies that are used on the web, find ways to make them more secure, and contribute to the design of future web technologies and tools. This process will involve lots of creative thinking, and lead to innovative scientific results, because a secure web application must combine securely non-trivial components such as databases, internet protocols, scripting languages and web browsers.Here is an example of a first step in the direction of my proposal. Facebook users write Facebook applications in JavaScript (the language that sits inside web pages and makes them interactive) and share them with other users. This raises the problem of restricting such JavaScript, written by a potentially malicious user, to make sure that it is safe for all the other Facebook users. With colleagues in Stanford, I modelled JavaScript as a set of simple mathematical formulas with a very precise meaning, and once I understood the language and its security properties (by proving several mathematical results), I studied the way Facebook restricts JavaScript and found several flaws. A malicious user could have written  bad  Facebook applications, able to steal information and damage the profile or the web browser of other users. I contacted the Facebook team and discussed possible solutions, and they modified their restriction mechanism accordingly.This is just an example of how the work I am proposing consists in original foundational research that also has direct impact on the life of millions of people. Following a similar approach I will also model the languages that are used to program web servers, such as PHP, and the browser with its DOM libraries, and study their security properties. I will participate in the definition of standards related to web security, and influence the design of several major web applications such as the future versions of the iGoogle portal, Yahoo!'s advertising platform and the Microsoft Web Sandbox framework for mashups. I have already met researchers from these companies, all interested in receiving input from this line of research."
	},
	{
		"grant":228,
		"ID": "EP/I004343/1",
		"Title": "Light unlimted - active and passive exploitation of light at the nanometre scale",
		"PIID": "126765",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2015",
		"Value": "1077391",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "126765", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Friedrich-Schiller University of Jena"},
		{"Number": "1", "Name": "Regents of the Uni California Berkeley"}
		],
		"Summary": "Light and the various ways it interacts with matter is our primary means of sensing the world around us. It is therefore no surprise that many technologies are based on light; for example submarine optical fibres make up the backbone of the Internet and display technology delivers affordable and compact crystal clear televisions. However, light itself has a limitation that we are still trying to overcome: light cannot be imaged or focused below half its wavelength, known as the  diffraction limit . To see smaller objects we must use shorter wavelengths. e.g. Blue-ray, uses blue lasers (405 nm) to store more information than DVDs, which use longer wavelength red lasers (650 nm). Today, we are learning to overcome this limit by incorporating metals in optical devices. The proposed research investigates the use of metals to shatter the diffraction limit for creating new technological products, expand the capabilities of computers and the internet and deliver new sensor technologies for healthcare, defense and security.We often take for granted just how strongly light can interact with metals. Electricity, oscillating at 50 Hz (essentially very low frequency light), has a wavelength of thousands of kilometers, yet a wall-plug is no larger than a couple of inches; well below the diffraction limit! The relatively new capability to structure metal surfaces on the nanoscale now allows us to use this same phenomenon to beat the diffraction limit in the visible spectrum. Metals do this by storing energy on the electrons that collectively move in unison with light, called surface plasmons. This approach has recently re-invigorated the study of optics at the nano-scale, feeding the trend to smaller and more compact technologies.So what sets nano-optics aside from low frequency electricity if they share the same physics? I believe the paradigm of nano-optics is the capability to reduce the size of visible and infrared light so that it can occupy the same nano-scale volume as molecular, solid state and atomic electronic states for the first time. Under natural conditions the mismatch makes light-matter interactions inherently weak and slow. With nano-optics, interactions not only become stronger and faster but weak effects once difficult to detect are dramatically enhanced. This goal of this proposal is to strengthen such weak effects and utilize them to realize new capabilities in optics.With any new type of control come caveats. Firstly, it is difficult to focus light from its normal size beyond the diffraction limit. Secondly, having overcome the first challenge, light on metal surfaces is short lived due to a metal's resistance. My research plan is geared to directly address these challenges. The first thrust develops a concept that I recently proposed to mitigate the problem of energy loss to the point where surface plasmons become useful. Building on Silicon Photonics, a well-established commercial optical communications architecture, I can use established techniques to seamlessly transfer light between the realms of conventional and nano-optics with the potential for short term impact on photonics technology. The second thrust exploits my recent breakthrough on surface plasmon lasers, which can generate light directly on the nano-scale and sustain it indefinitely by laser action. This overcomes both challenges in nano-optics simultaneously. While conventional lasers transmit light over large distances, it is the light inside surface plasmon lasers that is unique. I want to use this light for spectroscopy at single molecule sensitivities. Just as ultra-fast lasers, serving as scientists' camera flash, have given us snap shots of Nature's fleeting processes, so surface plasmon lasers will allow us to probe Nature with unprecedented resolution and control at the scale of individual molecules. Exploring optics at untouched length scales is an exciting opportunity giving us the potential to make fundamentally new discoveries."
	},
	{
		"grant":229,
		"ID": "EP/I005021/1",
		"Title": "Life-Long Infrastructure Free Robot Navigation",
		"PIID": "109735",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2015",
		"Value": "1655490",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "109735", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BAE Systems"},
		{"Number": "1", "Name": "Department for Transport"},
		{"Number": "2", "Name": "Guidance Navigation Ltd."},
		{"Number": "3", "Name": "Massachusetts Institute of Technology"},
		{"Number": "4", "Name": "Navtech Radar Limited"},
		{"Number": "5", "Name": "Nissan Motor Company"}
		],
		"Summary": "In the future, autonomous vehicles will play an important part in our lives. They will come in a variety of shapes and sizes and undertake a diverse set of tasks on our behalf. We want smart vehicles to carry, transport, labour for and defend us. We want them to be flexible, reliable and safe. Already robots carry goods around factories and manage our ports, but these are constrained, controlled and highly managed workspaces. Here the navigation task is made simple by installing reflective beacons or guide wires. This project is about extending the reach of robot navigation to truly vast scales without the need for such expensive, awkward and inconvenient modification of the environment. It is about enabling machines to operate for, with and beside us in the multitude of spaces we inhabit, live and work. Even when GPS is available, it does not offer the accuracy required for robots to make decisions about how and when to move safely. Even if it did, it would say nothing about what is around the robot and that has a massive impact on autonomous decision-making.Perhaps the ultimate application is civilian transport systems. We are not condemned to a future of congestion and accidents. We will eventually have cars that can drive themselves, interacting safely with other road users and using roads efficiently, thus freeing up our precious time. But to do this the machines need life-long infrastructure-free navigation, and that is the focus of this work.We will use the mathematics of probability and estimation to allow computers in robots to interpret data from sensors like cameras, radars and lasers, aerial photos and on-the-fly internet queries. We will use machine learning techniques to build and calibrate mathematical models which can explain the robot's view of the world in terms of prior experience (training), prior knowledge (aerial images, road plans and semantics) and automatically generated Web queries. The goal is to produce technology which allows robots always to know precisely where they are and what is around them. Robots have a big role to play in our future economy, but underpinning this role will be life-long infrastructure-free navigation."
	},
	{
		"grant":230,
		"ID": "EP/I005102/1",
		"Title": "The Neural Marketplace",
		"PIID": "-253807",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2015",
		"Value": "1134774",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Bioengineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-253807", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern computers are more powerful than many ever dared expect. So it is remarkable how much today's computers still can't do. Strangely, some of the hardest tasks for computers are effortless to humans. Problems like vision, natural language comprehension, and walking control will undoubtedly require massive computing power. But the real difficulty is our inability to write down sets of rules that a computer can follow to perform these tasks. The only solution may be to develop computer systems that, like us, learn by example and by trial and error, without needing explicit instructions. The brain contains roughly as many neurons as there are transistors in a modern supercomputer. These cells are computationally more sophisticated than was once believed. But what is most amazing is their ability to organize into large, functionally coherent networks, that constantly learn and adapt to an animal's changing circumstances. This happens with no central point of control, suggesting that something about neurons causes them to automatically assemble into information-processing systems. This fellowship proposal is based on a new hypothesis, derived from neurobiological research, for how this self-organization occurs through competitive processes analogous to those of a market economy. A typical neuron in the cerebral cortex receives about 10,000 inputs, which it integrates to produce a single output, broadcast in turn to about 10,000 targets. Our new hypothesis is for a mechanism by which a neuron receives feedback from its targets, signalling how useful the information it carries is to the rest of the network. Several lines of evidence suggest that in the brain, molecules called neurotrophins can act as carriers of this feedback signal. According to the hypothesis, neurons throughout the brain constantly experiment with new information processing strategies. In most cases, the new information will not be required by the neuron's targets, no feedback will be received, and the neuron will return to its prior state. A few neurons, however, will happen upon information that is useful to the larger network, and will receive feedback causing the recent changes to be retained. In a market economy, interactions like this allow autonomous agents (people and firms) to organize into networks. A firm that makes cars buys parts from suppliers, who buy components from their own suppliers, and so on. At each stage of the supply chain, multiple firms compete to produce the best products, experimenting with new designs that, if successful, will increase market share. The decisions required to build a good car are thus distributed over a large number of agents. No one person has to understand every part of the manufacturing process; instead, decisions made by multiple individual agents cause the system to organize itself. Improvements and adaptations occur by experiments with new approaches at all levels. Scale this picture up, and you have a global economy encompassing billions of individuals. Could similar interactions organize the billions of cells in the brain into a single coherent system? And could they allow us to build scalable learning machines to solve currently intractable problems in computing?The current proposal will answer these questions by constructing a series of increasingly large  market-based  neural network systems, to solve a series of increasingly challenging tasks from speech recognition and robot control. This research will have impact far beyond these domains, informing the construction of learning systems for applications as diverse as vision and medical diagnosis, as well as to domains such as internet routing that require scalable self-organization of multiple computing devices. Confirming the computational validation of the hypothesis would also provide a step-change in our understanding of how the brain processes information, potentially yielding new approaches to disorders of brain organization."
	},
	{
		"grant":231,
		"ID": "EP/I005102/2",
		"Title": "The Neural Marketplace",
		"PIID": "-253807",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2015",
		"Value": "706947",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Neurology",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-253807", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern computers are more powerful than many ever dared expect. So it is remarkable how much today's computers still can't do. Strangely, some of the hardest tasks for computers are effortless to humans. Problems like vision, natural language comprehension, and walking control will undoubtedly require massive computing power. But the real difficulty is our inability to write down sets of rules that a computer can follow to perform these tasks. The only solution may be to develop computer systems that, like us, learn by example and by trial and error, without needing explicit instructions. The brain contains roughly as many neurons as there are transistors in a modern supercomputer. These cells are computationally more sophisticated than was once believed. But what is most amazing is their ability to organize into large, functionally coherent networks, that constantly learn and adapt to an animal's changing circumstances. This happens with no central point of control, suggesting that something about neurons causes them to automatically assemble into information-processing systems. This fellowship proposal is based on a new hypothesis, derived from neurobiological research, for how this self-organization occurs through competitive processes analogous to those of a market economy. A typical neuron in the cerebral cortex receives about 10,000 inputs, which it integrates to produce a single output, broadcast in turn to about 10,000 targets. Our new hypothesis is for a mechanism by which a neuron receives feedback from its targets, signalling how useful the information it carries is to the rest of the network. Several lines of evidence suggest that in the brain, molecules called neurotrophins can act as carriers of this feedback signal. According to the hypothesis, neurons throughout the brain constantly experiment with new information processing strategies. In most cases, the new information will not be required by the neuron's targets, no feedback will be received, and the neuron will return to its prior state. A few neurons, however, will happen upon information that is useful to the larger network, and will receive feedback causing the recent changes to be retained. In a market economy, interactions like this allow autonomous agents (people and firms) to organize into networks. A firm that makes cars buys parts from suppliers, who buy components from their own suppliers, and so on. At each stage of the supply chain, multiple firms compete to produce the best products, experimenting with new designs that, if successful, will increase market share. The decisions required to build a good car are thus distributed over a large number of agents. No one person has to understand every part of the manufacturing process; instead, decisions made by multiple individual agents cause the system to organize itself. Improvements and adaptations occur by experiments with new approaches at all levels. Scale this picture up, and you have a global economy encompassing billions of individuals. Could similar interactions organize the billions of cells in the brain into a single coherent system? And could they allow us to build scalable learning machines to solve currently intractable problems in computing?The current proposal will answer these questions by constructing a series of increasingly large  market-based  neural network systems, to solve a series of increasingly challenging tasks from speech recognition and robot control. This research will have impact far beyond these domains, informing the construction of learning systems for applications as diverse as vision and medical diagnosis, as well as to domains such as internet routing that require scalable self-organization of multiple computing devices. Confirming the computational validation of the hypothesis would also provide a step-change in our understanding of how the brain processes information, potentially yielding new approaches to disorders of brain organization."
	},
	{
		"grant":232,
		"ID": "EP/I00520X/1",
		"Title": "Trusted Autonomous Systems",
		"PIID": "-7328",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2015",
		"Value": "1066423",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-7328", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "IBM Watson Research Centre"},
		{"Number": "1", "Name": "Polish Academy of Sciences"},
		{"Number": "2", "Name": "Sysbrain"},
		{"Number": "3", "Name": "University College London"},
		{"Number": "4", "Name": "University of Rome La Sapienza"},
		{"Number": "5", "Name": "University of Southampton"}
		],
		"Summary": "Fully autonomous systems are here. In the past 50 years we have quickly moved from controlled systems, where the operator has full control on the actions of the system (e.g., a digger), to supervised systems that follow human instructions (e.g., automated sewing machines), to automatic systems performing a series of sophisticated operations without human control (e.g., today's robotic car assembly lines), to autonomous systems. Autonomous systems (AS) are highly adaptive systems that sense the environment and learn to make decisions on their actions displaying a high degree of pro-activeness in reaching a certain objective. They are autonomous in the sense that they do not need the presence of a human to operate, although they may communicate, cooperate, and negotiate with them or fellow autonomous systems to reach their goals.The objective of this fellowship is to develop the scientific and engineering underpinnings for autonomous systems to become part of our every-day's life. There is a clear benefit for society if repetitive or dangerous tasks are performed by machines. Yet, there is a perceived resistance in the media and the public at large to increasingly sophisticated technology assisting key aspects of our lives. These concerns are justified. Most people have first hand experience of software and automatic devices not performing as they should; why should they be willing to delegate to them crucial aspects of their needs?In a recent influential report published by the Royal Academy of Engineering in August 2009 and widely discussed in the media it is argued that there is a real danger that these technologies will not be put into use unless urgent questions about the legal, ethical, social, and regulatory implications are addressed. For instance, the report highlights the issue of liability in case of collisions between autonomous driverless cars. Who should be held responsible? The passenger?  The software?  The owner?  The maker of the vehicle? Quite clearly society and the government need to engage in a maturedebate on several of these issues. However, the report identifies an even more fundamental point:``Who will be responsible for certification of autonomous systems? [Royal Society, Autonomous Systems, August 2009, page 4]While there are complex regulatory aspects to this question, its underlying scientific implication is that we, as computer scientists and engineers, urgently need to offer society techniques to be able to verify and certify that autonomous systems behave as they are intended to. To achieve this, four research objectives are identified:1) The formulation of logic-based  languages for the principled specification of AS, including key properties such as fault-tolerance, diagnosability, resilience, etc.2) The development of efficient model checking techniques, including AS-based abstraction, parametric and parallel model checking, for the verification of AS. 3) The construction and open source release of a state-of-the-art model checker for autonomous systems to be used for use-cases certifications.4) The validation of these techniques  in three key areas of immediate and mid-term societal importance: autonomous vehicles, services, and e-health.This fellowship intends to pursue novel techniques in computational logic to answer the technical challenges above. A success in these areas will open the way for the verification of AS, thereby opening the way to their certification for mainstream use in society."
	},
	{
		"grant":233,
		"ID": "EP/I005706/1",
		"Title": "LogMap: Logic-based Methods for Ontology Mapping",
		"PIID": "-186548",
		"Scheme": "First Grant Scheme",
		"StartDate": "10/01/2011",
		"EndDate": "09/11/2012",
		"Value": "101657",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-186548", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "EMBL - European Bioinformatics Institute"}
		],
		"Summary": "In computer science, an ontology is a formal description of some aspect ofthe world in a format that a computer can process. For example, a bio-medical ontologymay contain information such as   polyarticular arthritis is a kind of arthritis that affects at least five joints'', juvenile arthritis is a kind of arthritis that affects children up to the age of 13'', and polyarticular juvenile arthritis is the kind of arthritis that is both polyarticular and juvenile''.Ontologies are extensively used in biology and medicine.  Aprominent example of a bio-medical ontology is SNOMED CT, which is a core component of the NHS patient recordservice. Other examples include the Foundational Model of Anatomy(FMA) and the National Cancer Institute Thesaurus (NCI).Ontologies such as SNOMED CT, FMA, and NCI are gradually superseding the existing medical classificationsand are becoming core platforms for accessing, gathering, and sharing medical knowledge and data.For example, ontologies can be used to process data (e.g., electronic patient records in the case of a medical application) in a more intelligent way:  if JohnSmith's medical record states that he is a 10 years old patient suffering from arthritis, and who has damage in hisknee, ankle, wrist, elbow, and hip joints, then an ontology can be used to conclude that hesuffers from a kind of polyarticular juvenile arthritis.To exchange or migrate data between ontology-based applications,it is crucial to establish correspondences (or mappings) between their ontologies.For example, a mapping between NCI and FMA should establish that the FMA term Cardiac Muscle Tissue'' and the NCI term  Myocardium'' are synonyms. Usingthis mapping, a computer program would then be able, for example, to migrate the datastatement  Paul Williams has suffered from an infarction affecting the Myocardium'' from an NCI-based application to an FMA-based application.Creating such mappings manually is often unfeasible due to the size and complexity of modern ontologies.Therefore, the problem of automatically generating mappings between ontologies (often referred to as the ontology matching, ontology alignment, or ontology mappingproblem) has been investigated extensively in recent years.Despite the already mature state of the art,  bio-medical ontologies still poseserious challenges to existing techniques.Our ultimate goal in this project is to meet these challenges and lay thefoundations for the development of new generation bio-medical informationsystems.Our main research hypothesis is based on the observation that existing techniques for ontology mapping oftendisregard the logic-based semantics of the input ontologies. As a result, they fail to take advantage ofthe available semantics, and of the highly effective reasoning services for modernontology languages. We are proposing to rethink the foundations underlying the current state-of-the art in the field by incorporating logical reasoning in each of the steps of the ontology mapping process. We also intend to go even further and make our techniquespractical and ready to be used in applications.The research is based on our preliminary empirical evidence which suggests the potential benefitsof logic-based reasoning when analysing existing mappings between real-world ontologies.We expect that our results will be directly relevant to the users of ontology-based systems inthe bio-medical domain, where knowledge and data integration is a matter of major concern."
	},
	{
		"grant":234,
		"ID": "EP/I005838/1",
		"Title": "PAnDA: Programmable Analogue and Digital Array",
		"PIID": "16159",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2014",
		"Value": "1220676",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics",
		"OrgID": "55",
		"Investigators":[
		{"ID": "16159", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Gold Standard Simulations Ltd"}
		],
		"Summary": "Moore's law states that, since their invention in 1947, every two years the number of transistors on an integrated circuit doubles. This is due to the shrinking of devices through advances in technology. However, as these devices are approaching the atomistic level, intrinsic variations are becoming more abundant, leading to a lower production yield and higher failure rates. In order to accommodate the increased variability of individual device characteristics there is a need for novel device architectures and circuit design methodologies. For example, Intel were forced to make the biggest change in transistor technology since the 1960s in order to reach the 45nm CMOS technology node. These predictions and issues were originally focussed on large-scale integration, mainly connected with microprocessor design. However, in the last 10 years the rise of Field Programmable devices (e.g. Field Programmable Gate Arrays - FPGA) both in terms of technology advances and application domains has meant that these issues are now relevant to these devices as well. Hence, the proposal focuses upon one of the current greatest challenges in electronic design: taking physical effects of intrinsic variability into account when the shrinking of device sizes approaches atomistic levels, in order to achieve functional circuit designs. Both process and substrate variations impose major challenges on the reliable fabrication of such small devices. These variations fall into two categories; deterministic variability, which can be accurately modelled and accounted for using specific design techniques, and stochastic variability, which can only be modelled statistically and is harder to overcome. The proposal will develop a reconfigurable design platform that can be manipulated at the device and digital abstraction levels in order to further understand and tackle the effects of stochastic variability in hardware upon next generation designs.The research proposal comprises four threads that build upon each other:- Design of a simulation model for a variability tolerant architecture, - Hardware realisation of this model,- Development of a comprehensive software framework, which will be able to interface the simulation model as well as the chip,- Development of bio-inspired approaches to tackle variability tolerant design. At its conclusion the project will have developed an understanding of how stochastic variability will affect circuit design in the future and will propose novel design methodologies to overcome stochastic variability. A novel, variability tolerant architecture will have been developed and realised as a simulation model and as a prototype in hardware. Both are vital steps towards next generation FPGA architectures."
	},
	{
		"grant":235,
		"ID": "EP/I006583/1",
		"Title": "Generative Kernels and Score Spaces for Classification of Speech",
		"PIID": "71480",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2011",
		"EndDate": "31/07/2014",
		"Value": "392127",
		"ResearchArea": "Speech Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "71480", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of this project is to significantly improve the performance of automatic speech recognition systems across a wide-range of environments, speakers and speaking styles. The performance of state-of-the-art speech recognition systems is often acceptable under fairly controlled conditions and where the levels of background noise are low. However for many realistic situations there can be high levels of background noise, for example in-car navigation, or widely ranging channel conditions and speaking styles, such as observed on YouTube-style data. This fragility of speech recognition systems is one of the primary reasons that speech recognition systems are not more widely deployed and used. It limits the possible domains in which speech can be reliably used, and increases the cost of developing applications as systems must be tuned to limit the impact of this fragility. This includes collecting domain specific data and significant tuning of the application itself.The vast majority of research for speech recognition has concentrated on improving the performance of hidden Markov model (HMM) based systems. HMMs are an example of a generative model and are currently used in state-of-the-art speech recognition systems. A wide number of approaches have been developed to improve the performance of these systems under speaker and noise changes. Despite these approaches, systems are not sufficiently robust to allow speech recognition systems to achieve the level of impact that the naturalness of the interface should allow. This project will combine the current generative models developed in the speech community with discriminative classifiers used in both the speech and machine learning communities. An important, novel, aspect of the proposed approach is that the generative models are used to define a score-space that can be used as features by the discriminative classifiers. This approach has a number of advantages. It is possible to use current state-of-the-art adaptation and robustness approaches to compensate the acoustic models for particular speakers and noise conditions. As well as enabling any advances in these approaches to be incorporated into the scheme, it is not necessary to develop approaches that adapt the discriminative classifiers to speakers, style and noise. One of the major problems with speech recognition is that variable length data sequences must be classified. Using generative models also allows the dynamic aspects of speech data to be handled without having to alter the discriminative classifier. The final advantage is the nature of the score-space obtained from the generative model. Generative models such as HMMs have underlying conditional independence assumptions that, whilst enabling them to efficiently represent data sequences, do not accurately represent the dependencies in data sequences such as speech. The score-space associated with a generative model does not have the same conditional independence assumptions as the original generative model. This allows more accurate modelling of the dependencies in the speech data.The combination of generative and discriminative classifiers will be investigated on two very difficult forms of data that current systems perform badly on. The first task is adverse environment recognition of speech. In these situations there are very high levels of background noise which causes severe degradation in system performance.  Data of interest for this task will be specified in collaboration with Toshiba Research Europe Ltd. The second task of interest is large vocabulary speech recognition of data from a wide-range of speaking styles and conditions. Google has supplied transcribed data from YouTube to allow evaluation of systems on highly diverse data. The project will yield significant performance gains over current state-of-the-art approaches for both tasks."
	},
	{
		"grant":236,
		"ID": "EP/I006761/1",
		"Title": "Sustainable domain-specific software generation tools for extremely parallel particle-based simulations",
		"PIID": "8710",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2010",
		"EndDate": "30/06/2014",
		"Value": "439206",
		"ResearchArea": "Computational & Theoretical Chemistry",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "8710", "Role": "Principal Investigator"},
		{"ID": "-23955", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This research project emerged from the EPSRC Sandpit on Extreme Computing that took place in Jan 2010.  The sandpit process facilitated discussion between a group of chemists and civil engineers who all use particle-based numerical simulation approaches.  The applications motivating these scientists' simulations vary greatly (from quantum mechanics to dam engineering), very different scales are simulated (atoms in some cases, mm sized sand particles in others) and the interactions between the particles are modelled in significantly different ways.  However all of the methods have a common basis (they simulate particles and their interactions) and they all require implementation in a high performance computing (HPC) environment.  In fact the similarities in the simulation approach mean that there are key common challenges to achieving effective HPC implementations. Recent and likely future developments in HPC are making massively parallel computations viable; consequently there is a real possibility of achieving a step change in the level of complexity and realism in these numerical models with huge impact on industry and society.  However, at the same time, the increased complexity of HPC hardware poses a barrier to being able to fully harness this resource.  Working with the chemists and engineers, two computer scientists developed a strategy to address these challenges.  The proposed approach is to develop a new Particle Science Language (PSL) to act as a bridge between the scientists who are developing particle based models and the computer science required to implement them in a rapidly evolving HPC environment.   The scientists and engineers will implement their physical models in this relatively simple PSL and an automatic code generation system then automatically generate optimized code to effectively run the simulations on high performance systems.  The research strategy is to rapidly develop the first implementation of the PSL and prototype it on micron to millimetre scale Discrete Element Modelling (DEM) dynamics code. From this early foundation, we will develop the PSL, with the key objectives of demonstrating generality in the breadth of applications within the particulate dynamics domain and portability to multiple HPC architectures.  A range of particle based methods (PBM) are currently used to simulate materials in chemistry, engineering, physics and biophysics. The 4 types of PBM considered directly in the proposed are molecular dynamics (MD), the ONETEP quantum mechanics-based program, discrete element modelling (DEM), and smoothed particle hydrodynamics (SPH). The overall research objective is to develop a sustainable tool that will deliver, in the future, cutting edge research applicable to applications ranging from dam engineering to atomistic drug design. Measureable achievements will also be made during the current project.  In fact, we anticipate at least 3 physical science  firsts  in the project lifetime:(1) The first protein drug simulations that describe the entire protein-drug complex and solvent from first principle quantum mechanics.(2) The first rationalization, at an atomic level, of polymer solubility in CO2, with chemical accuracy. (3) Simulation of the vibrational spectrum of a carbon nanotube from first principles quantum mechanics.(4) Simulation of triaxial compression tests with 1,000,000 particles From a Computer Science perspective the project will lead to a novel high-level compiler optimisation framework that captures dependence, data distribution and loop nest optimisation in a context of highly-irregular data structures beyond the scope of classical approaches such as the polyhedral model, or even new results in shape analysis and ownership-type abstractions."
	},
	{
		"grant":237,
		"ID": "EP/I00680X/1",
		"Title": "Music-Games: Supporting New Opportunities for Music Education",
		"PIID": "-162851",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/05/2011",
		"EndDate": "30/04/2013",
		"Value": "97675",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering & Built Environment",
		"OrgID": "50",
		"Investigators":[
		{"ID": "-162851", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The digital revolution is creating new ways to engage with music, transforming music-making opportunities in the classroom and wider musical environment. The challenge for music educators is to capitalise on the evident motivation for informal music-making with digital technology, as a tool to create new opportunities to inspire and engage learners with music in educational contexts. This project will investigate the potential of music-games to inspire and engage learners with music, developing theoretical and practical knowledge to support and enrich authentic and inclusive classroom participation in line with defined curriculum goals. Music-games present a highly pervasive new digital platform to create, perform, appreciate and transmit music through peer and online communities, and one of the biggest selling video game genres. Previous research highlights the power of music participation to enrich cognitive, social and emotional wellbeing, while a growing body of work highlights the educational potential of digital games to scaffold and enrich personalised learning across the curriculum. However the music-game synergy has been neglected by research, presenting a platform to investigate the processes, experiences, educational opportunities and potential outcomes of both music participation and digital game participation in one context. The aims of the project will be achieved by completing the following main objectives:1. Develop recommendations and materials to guide educators on effective and innovative employment of music-games in the classroom, and guide industry on enhanced design of educationally appropriate future music-games.2. Develop and investigate  a co-created scenario of music-making with music-games in the classroom, identifying processes of participation and music-game features which support engaging, authentic and inclusive experience.3. Develop two theoretically informed models: a model of Music-Game Flow, and a model of the potential educational opportunities and outcomes with music-games, by integrating the collective data from the contextual enquiry and the empirical investigation with theoretical insight.The project objectives will be achieved through four stages of work. Stage 1 involves a comprehensive stakeholder enquiry (Educator, Learner and Games Industry) to establish attitudes, uses and requirements with music-games, and generate user-driven scenarios of use for music-games as a tool to support music-education. Stage 2 involves the comprehensive generation and empirical investigation of a music-making scenario with Guitar Hero 5 in situ of the classroom as an authentic and inclusive learning context. Stage 3 involves evaluation of the collective results to develop and refine recommendations for educators and industry on effective and innovation employment and design of music-games. Further, Stage 3 involves the development of two theoretically informed models: a model of Music-Game Flow, and a model of the potential educational opportunities and outcomes with music-games. This will be achieved by integrating the collective data from Stage 1's stakeholder enquiry and Stage 2's empirical investigation, with theoretical insight. Stage 4 involves the dissemination of the findings through cross-disciplinary national and international peer reviewed publication, conference presentation, organised research colloquiums, proposed edited text, and public awareness events.In conclusion, this project aims to develop greater theoretical and practical understanding of the potential of music-games to engage learners with music, identifying processes and factors that shape and constrain authentic and inclusive music-making in line with curriculum goals."
	},
	{
		"grant":238,
		"ID": "EP/I009310/1",
		"Title": "Dual Process Control Models in the Brain and Machines with Application to Autonomous Vehicle Control",
		"PIID": "79853",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2011",
		"EndDate": "30/04/2014",
		"Value": "352599",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Science and Mathematics",
		"OrgID": "121",
		"Investigators":[
		{"ID": "79853", "Role": "Principal Investigator"},
		{"ID": "21243", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Industrial Systems and Control Ltd"},
		{"Number": "1", "Name": "SciSys Ltd"}
		],
		"Summary": "The field of autonomous vehicle control (AVC) is a rapidly growing one which promises improved performance, fuel economy, emission levels, comfort and safety.Application of conventional control methods can generate adequate results under restricted circumstances,but have high design and computational costs and are fragile under real environmental changes (winds, proximity of other vehicles etc).There is therefore a pressing need for alternative approaches to AVC.One particularly promising alternative is to break the task into a set of sub-tasks,each valid over a restricted range of conditions, and to switch between them when required.Dr Hussain's group in Stirling has been developing a novel framework for such'modular learning controllers'over the last few years.The problem of selecting from amongst a set of actions or behaviours is also a central problem for animals.There is growing evidence that a set of central brain nuclei -the basal ganglia- are used by all vertebrates to help solve this problem.Research in Prof Gurney's lab has,over the last decade,been developing computational models of how the basal ganglia support behavioural selection.Thus,we believe that the basal ganglia act as a central 'selector' or 'switch' in all vertebrate brains,in that they examine requests for behaviour and allow the most urgent or salient requests to be selected for behavioural expression Given the similarity between the two problems' domains of AVC and action selection in animals, this project aims to leverage new results from psychology and neurobiology (discovered in Prof Gurney's lab) and apply them to the AVC controllers under development in Dr Hussain's group.One aspect of action selection which appears particularly promising in this respect has to do with there being two general modes of behavioural selection.To see this,consider the following scenarios.First,imagine making tea soon after getting out of bed in the morning in your own kitchen.You probably know exactly what to do without having to consciously be aware of it--the location of the tea,milk,sugar,kettle and water-tap are all well learned, as is the motor actions required to interact with the objects in these locations.Introspection after the event leads us to use terms such as;`I did it in my sleep' or `I was on auto-pilot'.Now consider doing the same task if you are staying at a friend's house for the first time.A completely different strategy appears to be used.Thus,we have to be alert, explore, and use high level cognitive knowledge that we hope generalises well (for example,we hope the tea will be in a cupboard near the sink, not in the living room)These two modes of control are well recognised in the psychological literature as automatic and controlled or executive processing respectively.There is also growing neurobiological evidence for the existence of different control regimes, supported by different brain systems.In addition, the new AVC systems developed at Stirling have two major components:a high level 'supervisory' controller and a set of basic (but adaptable) controllers that direct the actual vehicle behaviour.We believe the similarities with the biological notions of executive and automatic control are highly indicative of a mutually fruitful interaction between neuroscientific and control theoretic domains in this regard.Thus, while our general aim is to exploit a range of similarities between systems in control engineering and the animal brain, we will focus specifically on the concepts of automatised and controlled (or executive) processing and how they might map onto modular AVC solutions of the kind described above.The outcome should be a new generation real-time AVC controller, more directly inspired by the biological ideas. We will work with our industrial partners (Industrial Systems Control and SciSys) to evaluate the benefits of these novel controllers within the context of regular road driving and planetary rover vehicles."
	},
	{
		"grant":239,
		"ID": "EP/I010084/1",
		"Title": "Multiscale Modelling of Metal-Semiconductor Contacts for the Next Generation of Nanoscale Transistors",
		"PIID": "96850",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2011",
		"EndDate": "31/10/2014",
		"Value": "289985",
		"ResearchArea": "CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "125",
		"Investigators":[
		{"ID": "96850", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "IBM"},
		{"Number": "1", "Name": "IMEC - REALITY"},
		{"Number": "2", "Name": "TSMC Ltd"},
		{"Number": "3", "Name": "University of Glasgow"}
		],
		"Summary": "Contacts, made up of metal-semiconductor interfaces, are integral parts any semiconductor device. Compatibility of the metal and semiconductor components, homogeneity of structural and electrical characteristics of their interfaces, and robustness and durability of the contacts are crucial for the device proper functionality.Optimal operation of the contacts is a key to realisation of novel devices and development of new device concepts, including high mobility semiconductors based CMOS, tunnelling and spin-based transistors, tunnelling diodes, gas and infrared carbon-nanotube detectors, etc. Two major current trends in the semiconductor industry - miniaturisation of the devices and shift to new materials - pose the challenges for the contact technology: (i) robustness and stability of operation in ever smaller devices and (ii) compatibility of metal and semiconductor components. For example, the resistance of present day contacts is strongly affected by fluctuations in the currently being developed sub-22 nm technology. This problem is getting worse for smaller devices. On the other hand, introduction of new materials for high-mobility channels, e.g., Ge and III-Vs, necessitates the search for compatible metals and brings new challenges related to the contact fabrication. Therefore, understanding the dependence of the nanoscale metal-semiconductor interface properties on the atomic structure of this interface, chemical composition disorder, and defects is a key to formulating and exploiting new device concepts. In particular, this understanding is imperative for the developing of optimal contact fabrication procedures for nano-scale semiconductor devices.Primary aims of the proposed research are i) enabling and carrying out multiscale modelling of the optimal chemical compositions and structures of metal-semiconductor interfaces such that the Schottky barrier is minimal;ii) analysis of the role of interface defects, strain, and disorder on the carrier transport in CMOS devices.We will first develop a methodology which bridges ab initio simulations of atomic-scale structures and electronic properties of interfaces at 1-3 nm scale and simulation of device current-voltage characteristics at the scale of 5-50 nm. The results of the ab initio calculations will be transferred into 3D Monte Carlo (MC) transport simulations, which will allow us to make a realistic representation of the metal-semiconductor interface and develop a physical model of source/drain contacts. This model, in turn, will be incorporated into a 2D MC device simulator to predict the device performance and thus allow one for the straightforward comparison with experimental data obtained directly from the operating devices. Such methodology will allow us:  i) to consider explicitly effects of point defects (<0.5 nm scale), composition disorder (~1 nm scale), and metal granularity (~1-2 nm scale) on the electronic properties of selected metal-semiconductor interfaces, ii) to incorporate these effects into 3D MC transport simulations through the metal-semiconductor interfaces,iii) to develop realistic models for source/drain contacts, carry out 2D MC device simulations, and to optimise device performance with respect to the properties of the contacts.The methodology will be first tested on the case of Ti metal contact with an archetypal III-V semiconductor GaAs and the results will be validated using experimental data provided by our project partners. Then other systems of increasing complexity will be investigated: interfaces of Ti metal with unary Si and Ge, doped GaAs, and ternary InGaAs semiconductors and, finally, interfaces of TiN metal alloy with InGaAs. Our theoretical predictions will be validated by and compared to experimental results at each scale: Transmission Electron Microscopy (TEM) data for the interface structures, resistance measurements for the transport through the interface, I-V characteristics for the device simulations."
	},
	{
		"grant":240,
		"ID": "EP/I010165/1",
		"Title": "RE-COST: REducing the Cost of Oracles for Software Testing",
		"PIID": "47895",
		"Scheme": "Standard Research",
		"StartDate": "03/01/2011",
		"EndDate": "02/01/2014",
		"Value": "353915",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "47895", "Role": "Principal Investigator"},
		{"ID": "116951", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Berner and Mattner"},
		{"Number": "1", "Name": "Motorola"},
		{"Number": "2", "Name": "Sogeti UK Limited"}
		],
		"Summary": "Testing involves examining the behaviour of a system in order to discover potential faults. The problem of determining the desired correct behaviour for a given input is called the Oracle Problem. Since manual testing is expensive and time consuming there has been a great deal of work on automation and part automation of Software Testing. Unfortunately, it is often impossible to fully automate the process of determining whether the system behaves correctly. This must be performed by a human, and the cost of the effort expended is referred to as the Human Oracle Cost.RE-COST will develop Search-Based Optimisation techniques to attack the Human Oracle Cost problem quantitatively and qualitatively. The quantitative approach will develop methods and algorithms to both reduce the number of test cases and the evaluation effort per test case. The qualitative approach will develop methods and algorithms that will reduce test case cognition time.The RE-COST project seeks to transform the way that researchers and practitioners think about the problem of Software Test Data Generation. This has the potential to provide a breakthrough in Software Testing, dramatically increasing real world industrial uptake of automated techniques for Software Test Data Generation."
	},
	{
		"grant":241,
		"ID": "EP/I010297/1",
		"Title": "Evolutionary Approximation Algorithms for Optimisation: Algorithm Design and Complexity Analysis",
		"PIID": "76174",
		"Scheme": "Standard Research",
		"StartDate": "29/04/2011",
		"EndDate": "28/04/2015",
		"Value": "469838",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "76174", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In the last two decades, many evolutionary algorithms (EAs), including ant colony optimization, particle swarm optimization and artificial immune systems, have been proposed to tackle NP-hard combinatorial optimization problems. Many papers have been published. Some commercial successes of applying these algorithms in the real world have also been reported. However, the vast majority of such studies rely on computational experiments. Current  theoretical studies of EAs are mainly restricted to genetic algorithm and evolutionary strategy, especially for non-population based EAs. Rigorous results about the computational complexity for other types of EAs, e.g. ant colony optimization, artificial immune systems and estimation of distributionalgorithms, have been few. The limited theoretical analysis in recent years has primarily concentrated on the runtime analysis of EAs in finding the exact optimal solution to an optimization problem. Since EAs are not expected to find exact optimal solution to all instances of any NP-hard problem efficiently, the fundamental research challenge here is to study what kind of approximation solutions EAs can find to NP-hard optimization problems, which is the topic of this proposal. Our focus will be on analyzing theoretically what types of problems can be solved approximately and efficiently using what kind of EAs, and why. We are particularly interested in the relationship between problem characteristics and algorithmic features (such as selection, mutation and crossover). As Papadimitriou and Steiglitz pointed out in their 1998 book on Combinatorial Optimization: Algorithms and Complexity:  Developing the mathematical methodology for explaining and predicting the performance of these heuristics is one of the most important challenges facing the fields of optimisation and algorithms today.  Few theoretical studies exist in anaylsing EAs as approximation algorithms. This project is highly adventurous in trying to tackle the theoretical issue by bringing traditional theoretical computer science and evolutionary computation together. It will study four types of population-based EAs, including genetic algorithms, artificial immune algorithms, ant colony optimization and estimation of distribution algorithms. They are chosen in the proposal because they are all used with success in the real world and because of the need to understand what makes them successful on some problems but not on others and whether they are really different theoretically (or what the fundamental differences are among these algorithms, if any). Two important optimization problems, i.e., scheduling and routing, will be used as case studies in the proposal. These two problems are different but strongly related. Scheduling was the first problem studied for approximation algorithms in 1966 and has wide applications in the real world. Routing is another hard problem with numerous applications in transportation, utility and communication networks, where we have some research experiences. The expected outcomes of the proposed research will deepen our understanding of why, how and when an evolutionary approximation algorithm works significantly."
	},
	{
		"grant":242,
		"ID": "EP/I010491/1",
		"Title": "PATRICIAN: New Paradigms for Body Centric Wireless Communications at MM Wavelengths",
		"PIID": "3257",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2010",
		"EndDate": "13/07/2014",
		"Value": "620844",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical and Computer Eng",
		"OrgID": "14",
		"Investigators":[
		{"ID": "3257", "Role": "Principal Investigator"},
		{"ID": "34539", "Role": "Co Investigator"},
		{"ID": "9133", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The future communications world has two scales. One is global, in which the internet gives universal interconnectivity, and access for all to vast amounts of information. The other is personal, where the user can be supported by the global, in a wide range of activities and situations. To some extent this is already with us and is reflected in terminology such a metropolitan, local, personal and body area networks, (MAN, LAN, PAN, BAN). Whilst such classifications clarify the communications from large to small and vice versa, there is another challenge, that of interfacing the user needs to the wider network, in terms of personalisation of the communications and shaping the global support to the human level. These will also be cognitive, in that the personal system will have high levels of awareness of the user state and will control the connections and data flow to match the personal needs. User awareness will depend on the application of the personal system, such as health, business, entertainment, and special occupations including defence and emergency services. Personal awareness could involve body mounted sensors, wirelessly connected through a BAN and connected to the external network through a PAN. We refer to these two domains as body centric communications.In its widest implementation there will be large numbers of body centric systems, and as is normal in human activity, these will at times congregate in the same place, which will also have many other wireless communications equipment. Hospitals, public transport, sports and entertainment events and shopping malls will have high densities of personal systems, working in an electromagnetically cluttered environment. The personal system must be immune to interference and not interfere with other users or important local wireless systems. In the hospital environment these problems may be life threatening. In defence applications, at which this proposal is especially directed, there is a need for the soldier wearing the sensor network to be invisible in a wireless sense. If electromagnetic energy from his systems is picked up by enemy observers, he can be located and attacked.Body area networks at low microwave frequencies have, by the rules of electromagnetics, large antennas that cannot control the spread of energy well, and will not be able to meet the requirements for many close proximity BANs. Communication chip sets are becoming available at 60 GHz that have small antennas, with narrow beams that will be used, for example, to distribute HD TV around the home. We propose, in this project, to investigate the use of 60 GHz and above for body area networks. The challenges are daunting. At these frequencies, diffraction around the body is weak and so shadowing by the body can prevent communication. The solution is to use reconfigurable antennas. For example, for communications between network nodes on the front and back of the body, a path bouncing off the floor might be chosen over one that attempts to propagate around the body surface. As the body moves, this choice may need to be changed as the systems seeks to switch from shadowed paths to successful ones. Similar propagation path searching can be used for communications from the body to local base stations. To realise this sort of agile networking requires very good knowledge of the way the energy propagates around the body and the surroundings, and the design of switchable antennas with narrow beams. Computer based design of future systems requires digital models of both the energy propagation and the moving body, and again the high frequencies throw up difficulties that make this beyond current computational capabilities. The research teams of the Universities of Birmingham, Durham and Queen Mary University of London are well placed to undertake the study having experience in radiowave propagation measurements, antenna design and numerical computation."
	},
	{
		"grant":243,
		"ID": "EP/I010920/1",
		"Title": "High performance X-ray detectors with sub-100eV energy resolution",
		"PIID": "-19880",
		"Scheme": "First Grant Scheme",
		"StartDate": "07/03/2011",
		"EndDate": "06/03/2013",
		"Value": "80181",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-19880", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Soft X-ray detection is used in a wide range of applications ranging from material research, medical imaging, to industrial manufacturing inspection. This proposal aims to develop a new class of detectors for soft X-ray (<20keV) with potential to achieve better energy resolution and sensitivity than existing cooled Ge and Si detectors. These new X-ray detectors wil be made using InAs, a III-V semiconductor material which has a rare combination of properties for X-ray detection superior to Si, including high X-ray absorption coefficients, high number of carriers generated per absorbed photon, and ability to produce high gain without introducing excess noise (recently discovered at Sheffield).These InAs X-ray avalanche photodiodes (APDs) will be cooled to reduce unwanted leakage current. Our InAs APDs have the potential to achieve the fundamental Fano factor limited energy resolution rather than avalanche gain limited, thus improving on the best data from Si X-ray APDs. This could be the enabling technology to harvest the benefit of high internal gain without incurring significant energy resolution degradation, which has long been the main drawback of X-ray APDs. Its attenuation length, the distance by which the 63% of the X-ray is absorbed, is more than 6 times shorter than that of Si at 5.9keV photon energy, with increasing advantage at higher energies. This enables much shallower InAs pixel to be fabricated and could lead to a new generation of large format arrays that are compatible with digital X-ray imaging. Thus the ultimate aim of this proposal is to demonstrate shallow (<30micron) single-pixel InAs APDs with high gain and excellent energy resolution for soft X-ray detection. This new capability could underpin the next generation X-ray detection for applications such as non-destructive imaging (failure analysis and inspection in material research, electronic component/circuit board inspections and mechanical parts/assemblies), medical imaging (large area imaging of patients, angiography to selectively show blood vessels in the body), real-time imaging (to assist surgeries), and security screening of concealed objects through imaging.Developing InAs X-ray APDs is now finally feasible, owing to exciting developments in wafer growth and device fabrication at Sheffield in the recent years for InAs infrared APDs. Although there are significant different challenges in the development of InAs X-ray APDs, progress in both projects will be accelerated by the synergy. The investigator, who has years of experience with APDs and is currently developing AlGaAs X-ray detectors for room temperature operation, is well-placed to carry out the work. In addition to demonstration of high performance InAs X-ray APDs, this project aims to develop a device simulator of X-ray APDs which will be available to other X-ray detector researchers. This simulator has the unique ability to predict energy resolution limit, taking into account the statistical contribution of the avalanche gain process. In addition to the X-ray APD simulator, the proposed work will establish wafer growth conditions and device fabrication procedures appropriate for InAs X-ray APDs and eventually demonstrate devices with high gain and excellent energy resolution."
	},
	{
		"grant":244,
		"ID": "EP/I011005/1",
		"Title": "Automatic Proof Procedures for Polynomials and Special Functions",
		"PIID": "12193",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2010",
		"EndDate": "31/10/2014",
		"Value": "533183",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "12193", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "An engineering design is expected to satisfy safety constraints, many of which can be expressed as mathematical and logical formulas. Computer software exists that can check some such formulas automatically, but although they can have a large and complicated logical structure, the mathematical component currently has to be linear: in other words, involving nothing more complicated than addition. Real-world engineering problems involve sophisticated mathematical concepts, such as polynomials and transcendental functions.The investigators have developed software (called MetiTarski and RAHD) that can solve such problems in many cases. The current project will extend the scope of this software, increasing its power and targeting it at specific real-world application areas. One such application is analogue circuitry, which is widespread in consumer electronics. The project will investigate many other potential applications In engineering and the mathematical sciences."
	},
	{
		"grant":245,
		"ID": "EP/I011528/1",
		"Title": "Computational Counting",
		"PIID": "49970",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2011",
		"EndDate": "28/02/2014",
		"Value": "327205",
		"ResearchArea": "Logic and Combinatorics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "49970", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computational complexity is typically concerned with decision problems, but this is a historical accident, arising from the origins of theoretical computer science within logic. Computing applications, on the other hand, typically involve the calculation of numerical quantities. These applications broadly fall into two types: optimisation problems, in which the goal is to maximise or (minimise) the value of a function, and counting problems, broadly interpreted, by which we mean computing sums, weighted sums, and integrals including, for example, the expectation of a random variable or the probability of an event.This project will consider all aspects of computational counting, with a particular focus on foundational questions in computational complexity (characterising the inherent difficulty of problems) and the development of algorithmic techniques. The goal of the project is to provide a better understanding of the inherent computational difficulty of counting (including approximate counting) and to provide efficient algorithms where they exist."
	},
	{
		"grant":246,
		"ID": "EP/I011587/1",
		"Title": "HUMAN-AGENT COLLECTIVES: FROM FOUNDATIONS TO APPLICATIONS [ORCHID]",
		"PIID": "29356",
		"Scheme": "Programme Grants",
		"StartDate": "01/01/2011",
		"EndDate": "31/12/2015",
		"Value": "5537003",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "29356", "Role": "Principal Investigator"},
		{"ID": "37987", "Role": "Co Investigator"},
		{"ID": "132855", "Role": "Co Investigator"},
		{"ID": "13520", "Role": "Co Investigator"},
		{"ID": "51356", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Austrailian Centre for Field Robotics"},
		{"Number": "1", "Name": "BAE Systems"},
		{"Number": "2", "Name": "PRI Ltd"}
		],
		"Summary": "With a reported 5 billion mobile subscriptions worldwide, access to communication technologies has reached unprecedented levels and has fundamentally altered the ways in which we experience computational systems. Once delivered through a desktop machine to an office worker, computing has become an interwoven feature of everyday life across the globe in a way that profoundly affects us all. We are now interconnected using mobile devices; we routinely invoke remote services through a global  cloud  infrastructure and increasingly rely on computational devices in our everyday life. Computational devices monitor our health, entertain us, guide us and keep us safe and secure. However, this explosive growth in these devices and on-line services is only a precursor to an  era of ubiquity,  where each of us will routinely rely upon a plethora of smart and proactive computers that we carry with us, access at home and at work, and that are embedded into the world around us. As computation increasingly pervades the world around us, it will profoundly change the ways in which we work with computers. Rather than issuing instructions to passive machines, we will increasingly work in partnership with highly inter-connected computational components (aka agents) that are able to act autonomously and intelligently. Specifically, humans and software agents will continually and flexibly establish a range of collaborative relationships with one another, forming human-agent collectives (HACs) to meet their individual and collective goals. This vision of people and computational agents operating at a global scale offers tremendous potential and, if realised correctly, will help us meet the key societal challenges of sustainability, inclusion, and safety that are core to our future. However, these benefits are mirrored by the potential of equally concerning pitfalls as we shift to becoming increasingly dependent on systems that interweave human and computational endeavour.As systems based on human-agent collectives grow in scale, complexity and temporal extent, we will increasingly require a principled science that allows us to reason about the computational and human aspects of these systems if we are to avoid developments that are unsafe, unreliable and lack the appropriate safeguards to ensure societal acceptance.Delivering this science is the core research objective of this Programme. In more detail, it seeks to establish the new science that is needed to understand, build and apply HACs that symbiotically interleave human and computer systems to an unprecedented degree. To this end, it brings together three world-leading academic groups from the Universities of Southampton, Oxford and Nottingham (with multi-disciplinary expertise in the areas of artificial intelligence, agent-based computing, machine learning, decentralised information systems, participatory systems, and ubiquitous computing) with industrial collaborators (initially BAE Systems, PRI Ltd and the Australian Centre for Field Robotics) to collectively establish the foundational scientific underpinnings of these systems and drive these understandings to real-world applications in the critical domains of future energy networks, and disaster response."
	},
	{
		"grant":247,
		"ID": "EP/I011811/1",
		"Title": "Learning to Recognise Dynamic Visual Content from Broadcast Footage",
		"PIID": "94934",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2011",
		"EndDate": "31/08/2015",
		"Value": "489782",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51",
		"Investigators":[
		{"ID": "94934", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This research is in the area of computer vision - making computers which can understand what is happening in photographs and video. As humans we are fascinated by other humans, and capture endless images of their activities, for example home movies of our family on holiday, video of sports events or CCTV footage of people in a town center. A computer capable of understanding what people are doing in such images would be able to do many jobs for us, for example finding clips of our children waving, fast forwarding to a goal in a football game, or spotting when someone starts a fight in the street. For Deaf people, who use a language combining hand gestures with facial expression and body language, a computer which could visually understand their actions would allow them to communicate in their native language. While humans are very good at understanding what people are doing (and can learn to understand special actions such as sign language), this has proved extremely challenging for computers.Much work has tried to solve this problem, and works well in particular settings for example the computer can tell if a person is walking so long as they do it clearly and face to the side, or can understand a few sign language gestures as long as the signer cooperates and signs slowly. We will investigate better models for recognising activities by  teaching  the computer by showing it many example videos. To make sure our method works well for all kinds of setting we will use real world video from movies and TV. For each video we have to tell the computer what it represents, for example  throwing a ball  or  a man hugging a woman . It would be expensive to collect and label lots of videos in this way, so instead we will extract approximate labels automatically from subtitle text and scripts which are available for TV. Our new methods will combine learning from lots of approximately labelled video (cheap because we get the labels automatically), use of  contextual  information such as which actions people do at the same time, or how one action leads to another ( he hits the man, who falls to the floor ), and computer vision methods for understanding the pose of a person (how they are standing), how they are moving, and the objects which they are using.By having lots of video to learn from, and methods for making use of approximate labels, we will be able to make stronger and more flexible models of human activities. This will lead to recognition methods which work better in the real world and contribute to applications such as interpreting sign language and automatically tagging video with its content."
	},
	{
		"grant":248,
		"ID": "EP/I01196X/1",
		"Title": "Transforming the Internet Infrastructure: The Photonic HyperHighway",
		"PIID": "12208",
		"Scheme": "Programme Grants",
		"StartDate": "01/11/2010",
		"EndDate": "31/10/2016",
		"Value": "7288218",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "12208", "Role": "Principal Investigator"},
		{"ID": "28779", "Role": "Co Investigator"},
		{"ID": "41202", "Role": "Co Investigator"},
		{"ID": "-10060", "Role": "Co Investigator"},
		{"ID": "30827", "Role": "Co Investigator"},
		{"ID": "45632", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BBC Research and Development"},
		{"Number": "1", "Name": "Fianium Ltd"},
		{"Number": "2", "Name": "Oclaro Technology UK"}
		],
		"Summary": "Our vision is to develop the disruptive component technologies and network concepts that will  enhance our communications infrastructure 1000-fold to meet our 20-year needs, avert network grid-lock and reduce energy consumption.With continued steep growth in transmitted data volumes on all media, there is a widely-recognized and urgent need for more sophisticated photonics technologies in both the core and access networks to forestall a 'capacity crunch' in the medium term.  Our Programme involves two world-class groups ideally positioned to satisfy this need and reinforce the traditional leadership of the UK in this area.  All-optical technologies can also save considerably on the rapidly-rising energy consumption of communications systems (several % of global energy consumption, similar to air transport!), as well as substituting for travel, (e.g. Cisco's ultrawideband telepresence system has halved their large worldwide travel budget). This proposal is therefore focused on one of the most important challenges facing our modern society - an energy-efficient, ultra-high capacity ICT infrastructure able to connect people and businesses seamlessly everywhere. Traffic on the global communications infrastructure continues to increase 80% year-on-year, driven by rapidly expanding and increasingly-demanding applications: YouTube, MMS, iPlayer, new concepts such as cloud computing, tele-surgery, the introduction of the iPhone alone proved a severe drain on the capacity of major carriers. Bandwidth growth in the access network is starting to overwhelm the available capacity in the core. In the last 10 years, the number of broadband subscribers worldwide has grown 100-fold. We are now rapidly approaching the fundamental data carrying capacity of current optical technology; moreover, the energy required to support today's growing, power-hungry, ICT infrastructure is looking worryingly unsustainable. It is time to ask hard questions about some long-held assumptions.We propose a radical transformation of the physical infrastructure underpinning today's networks by developing devices capable of 1000-fold improvements in performance, starting with a critical re-examination of some of the most basic transmission building blocks - the optical fibres, amplification and regeneration, and nonlinear switching and distribution. Since the inception of optical telecommunications 40 years ago, the silica fibre has been its work-horse. However, as it nears its capacity limits, a radical rethink can reap dividends in non-linear threshold, transmission window breadth and loss through new materials and designs, leading to 1000-fold improvements. In addition, current power-hungry electronic switches are bottlenecks that photonics can alleviate.  Although immensely challenging, the new technologies that we propose have the potential to lead to major advances and benefits in many other important areas - including security, the environment, manufacturing and healthcare.  If we are successful in achieving our objectives, the Programme will surely establish the UK firmly as the world leader in optical communications and networking technologies for decades to come."
	},
	{
		"grant":249,
		"ID": "EP/I012036/1",
		"Title": "Custom Computing for Advanced Digital Systems",
		"PIID": "40151",
		"Scheme": "Platform Grants",
		"StartDate": "01/10/2010",
		"EndDate": "30/09/2015",
		"Value": "1267382",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "40151", "Role": "Principal Investigator"},
		{"ID": "2867", "Role": "Co Investigator"},
		{"ID": "91284", "Role": "Co Investigator"},
		{"ID": "-264123", "Role": "Co Investigator"},
		{"ID": "-128335", "Role": "Co Investigator"},
		{"ID": "8710", "Role": "Co Investigator"},
		{"ID": "100961", "Role": "Co Investigator"},
		{"ID": "-182481", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Altera Europe"},
		{"Number": "1", "Name": "ARM Ltd"},
		{"Number": "2", "Name": "J.P. Morgan"},
		{"Number": "3", "Name": "Maxeler Technologies"},
		{"Number": "4", "Name": "Nokia Devices R&D"},
		{"Number": "5", "Name": "Xilinx Corp"}
		],
		"Summary": "Advanced digital systems provide many exciting opportunities for UK economic growth. Our current Platform Grant has enabled us to implement a strategy of developing novel custom computing solutions, which involve customising the latest hardware and software elements, to meet demanding requirements from many applications. These include embedded systems applications such as software-defined radio and patient monitoring, as well as high-performance computing applications such as financial modelling and medical imaging. Continued Platform Grant funding will allow us to build on our success, to support strategic development of our team, and to extend our lead in custom computing technology to cover a wide variety of advanced digital systems for healthcare, environment, and security applications.There are three new strategic directions on which we are uniquely capable of making major impacts. We plan to conduct exploratory research to identify promising projects for responsive-mode funding for the following:1. customisable heterogeneous architectures, including design space exploration of devices and systems, relevant development methods and tools, and prototyping platforms and design portability enhancement;2. self-adapting design, including architecture innovations, adaptation policies and optimisation strategies, and design and verification flow;3. security-aware systems, including architecture enhancements, compilation and test generation environments, and experimental facilities and demonstration flow.The  added value  aspects for this Platform Grant proposal include: (a) providing continuity of support, (b) exploring significant strategic directions, (c) contributing to research infra-structure, (d) attracting fresh talents, (e) pioneering and strengthening international collaborations, and (f) accelerating technology transfer."
	},
	{
		"grant":250,
		"ID": "EP/I01246X/1",
		"Title": "New fibres for new lasers - photonic crystal fibre optics for the delivery of high-power light",
		"PIID": "50579",
		"Scheme": "Standard Research",
		"StartDate": "03/10/2011",
		"EndDate": "02/04/2015",
		"Value": "321075",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "50579", "Role": "Principal Investigator"},
		{"ID": "-21466", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Fianium Ltd"},
		{"Number": "1", "Name": "PowerPhotonic Ltd"},
		{"Number": "2", "Name": "Renishaw Plc"}
		],
		"Summary": "Lasers are rapidly becoming more useful - they are widely available at shorter wavelengths, emitting shorter pulses and at higher energies and powers than ever before. These characteristics make them especially useful for several industrial applications such as velocimetry, micro-machining and welding, where the beam characteristics delivered to the workpiece are critical in determining the success and efficiency of the process. Unfortunately, the very characteristics that make these laser pulses so useful - their short pulse lengths, low wavelengths and higher energy and power - make them absolutely impossible to deliver using conventional fibre optics. This means that those wishing to exploit the new laser systems would currently have to do so using bulk optics - typically, several mirrors mounted on articulated arms to deliver the pulses to the workpiece.We propose to use an alternative optical fibre technology to solve this problem. Hollow-core fibres which guide light using a photonic bandgap cladding have roughly 1000 times less nonlinear response than conventional fibres, and have far higher damage thresholds as well. In previous work, we concentrated on longer nanosecond pulsed lasers, and demonstrated that we could use these fibres to deliver light capable of machining metals. However, it is with the picoscond and sub-picosecond pulse laser systems now becoming more widespread that the hollow-core fibres really come into their own. For these shorter pulses, transmission through conventional fibres is limited not only by damage, but first by pulse dispersion and optical nonlinear response. These problems can only be surmounted using hollow-core fibre - no competing technology has come even close.Our work programme has several strands, with the common objective being to devise systems capable of delivering picosecond-scale pulses through lengths of a few metres of fibre, at useful energies and powers. To do this, we need to be able to efficiently couple light into the fibres and transmit them, single-mode, over a few metres of fibre with low attenuation. We plan to focus our attention on doing this in the wavelength bands around 1060nm and 530mn, and to investigate the possibility of extending the work to shorter wavelengths. We will work closely with several collaborators from the industrial/commercial sector, ranging from a UK-based supplier of relevant laser systems through to a company developing machining systems and indiustries which actually use such systems. In this way, we plan to provide UK-based industry with a competitive edge on teh global stage, by providing them with access to an academic area where the UK is an acknowledged world leader."
	},
	{
		"grant":251,
		"ID": "EP/I012591/1",
		"Title": "Lighting the Future",
		"PIID": "7875",
		"Scheme": "Programme Grants",
		"StartDate": "01/12/2010",
		"EndDate": "30/11/2015",
		"Value": "6361650",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials Science & Metallurgy",
		"OrgID": "24",
		"Investigators":[
		{"ID": "7875", "Role": "Principal Investigator"},
		{"ID": "132139", "Role": "Co Investigator"},
		{"ID": "-201321", "Role": "Co Investigator"},
		{"ID": "21646", "Role": "Co Investigator"},
		{"ID": "48498", "Role": "Co Investigator"},
		{"ID": "273", "Role": "Co Investigator"},
		{"ID": "32110", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Next time you change a tungsten filament light-bulb, give a brief thought to the amount of energy that bulb will have used up over its lifetime: more than 200 million Joules (about the same amount of energy as is contained in 3 tonnes of coal)! If you consider how many light bulbs must be in use in the world at this moment, it becomes clear that a huge amount of energy is spent keeping our homes and offices lit. In fact, about a fifth of electricity usage in the UK is for lighting. Hence, improvements in something as simple as light bulb efficiency could have an enormous impact on the UK's greenhouse gas emissions and use of fossil fuels.A new semiconductor material - gallium nitride (GaN) - provides a potential solution to the lighting problem. GaN is used to make white light-emitting diodes (LEDs). These solid state-light sources are already much more efficient than conventional tungsten filament light bulbs, and could potentially yield efficiency improvements of more than ten times (and be three times more efficient than compact fluorescent lamps). To achieve these vast improvements in efficiency, we need to thoroughly understand the material from which we make the LEDs, and how its structure, composition and properties influence LED performance. We also need to design devices which make the best possible use of everything we learn about the material. A large number of different factors influence the efficiency of LEDs, and in this programme, scientists from Cambridge, Manchester, Bath and Strathclyde are pooling their expertise to understand what limits the efficiency and find solutions which will benefit all of us, by providing sensibly-priced, highly-efficient lighting units which will be long-lasting and provide attractive high-quality light in homes and offices.To do this, we are breaking down the question of LED efficiency into a number of inter-linked scientific projects. GaN LEDs are based on thin layers of material grown on other materials such as silicon or sapphire. Electric current is passed into the active region of the LED, from which the light is emitted. The active region consists of very thin alternating layers of GaN and another semiconductor - indium gallium nitride (InGaN). The InGaN layers are only ten atomic layers thick and are called quantum wells. In the InGaN layers, positive and negative charge carriers become trapped and hence combine with one another giving out light. The GaN and InGaN crystals are not perfect, however, and defects in their structure can disrupt the light emission process, resulting in the production of heat rather than light and a reduction in LED efficiency. In the early years of our programme, we plan to tackle fundamental questions relating to light emission efficiency, by pursuing projects concentrating on the defects in the material, the detailed small-scale structure of the InGaN quantum wells and the electric fields which arise in those quantum wells. The effect of all of these factors will be dependent on the amount of electricity injected into the LED, and a major problem is that LED efficiency drops at high injection currents. Since high brightness LEDs for lighting require high electric currents, we will also need to understand this question before solid state lighting can reach its full potential.The new materials and structures we develop will have to be integrated into working LED devices, and the architecture of those devices is also a key factor affecting efficiency. Hence, another of our research projects will address device design. By bringing the new ideas we develop about materials and devices together we aim to produce LEDs that are highly efficient and thus beneficial to our environment, cheap to buy and, unlike compact fluorescent lamps, produce an attractive colour of light to make homes and offices pleasant and healthy places to be. In the end, we hope the products of our research really will be lighting your future!"
	},
	{
		"grant":252,
		"ID": "EP/I012702/1",
		"Title": "Coherent Optical SIgnals for extremely high-capacity NEtworks (COSINE)",
		"PIID": "14115",
		"Scheme": "Standard Research",
		"StartDate": "01/08/2011",
		"EndDate": "31/07/2014",
		"Value": "484760",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "14115", "Role": "Principal Investigator"},
		{"ID": "119760", "Role": "Co Investigator"},
		{"ID": "126618", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Oclaro Technology UK"},
		{"Number": "1", "Name": "u2t Photonics UK Ltd."}
		],
		"Summary": "High-speed fibre-optic cables link cities, countries and continents across the globe, underpinning the Internet and the fixed and mobile phone networks that enable and enrich our lives today.  Historically, increasing the overall data rate transmitted on a single optical fibre has dramatically reduced the cost of data transmission, and this is one factor that has enabled high data rate connections to be available at reasonable cost to end users.  As services like social networking, music downloads, and video-on-demand capture the public's attention, they in turn create demand for greatly increased capacity on these networks.  We can expect this cycle to continue as the installed fibre capacity is pushed to its limit.To achieve high transmission capacity on a single fibre, data is transmitted on several wavelength channels (wavelength division multiplexing, WDM).  For compatibility with the components used in existing systems, and to avoid having to manage a huge number of wavelengths, it is preferable to increase the amount of data transmitted by increasing the data rate per channel, rather than by packing the wavelength channels closer and closer together.  WDM networks with 40 Gb/s line rate are now being deployed, and there is currently considerable activity directed towards research and standardisation of 100 Gb/s Ethernet (100 GE) on a single wavelength.By extending the approach proposed for 100 GE by using advanced modulation schemes like those used in wireless communications, it may be possible to squeeze data transmission rates of several hundred Gb/s onto each wavelength, but the technological challenges posed will be significant.   To move beyond this - towards 1 Tb/s (1,000 Gb/s) per wavelength - will require new techniques.In this work, we will investigate one approach to achieving this, which also eases some of the stringent demands on the optical transmitters and receivers imposed by current methods.  Each wavelength channel will be divided into a number of sub-channels, and advanced modulation formats used to transmit data at a high rate in the narrow spectral band of each sub-channel.  It will be necessary to generate the optical signals that define the sub-channels at the transmitter efficiently and cost-effectively, and to produce synchronised optical signals at the receiver to recover the data.  To do so, we will generate all the sub-channels at the transmitter from a single laser that defines the frequency of the overall channel, and we will use one sub-channel to transmit information to allow an identical set of optical signals for channel demodulation to be created at the receiver.  In this way, the sub-channels are synchronised (phase locked) to each other within the overall channel, as are the transmitter and receiver.  This means that the sub-channels can be packed as closely together as possible and behave as a single unit, while recovering the data at the receiver is simplified.By this means we expect to increase the overall fibre transmission capacity to 135 Tb/s, more than an order of magnitude greater than the current state of the art for commercial long haul transmission systems. The work will mainly be carried out experimentally, investigating the key technical elements of the proposal in stages before combining them to show that the full scheme could deliver the anticipated increase in transmission capacity if fully implemented.  Areas that will be examined include new ways of generating phase-locked sub-channels at the transmitter; methods for generating and synchronising the corresponding optical signals at the receiver; and modulation and de-modulation techniques giving high data rate transmission in a narrow spectral band.  The experimental demonstration will be supported by computer simulations of the system, which will also allow new applications enabled by the approach - too advanced to be demonstrated experimentally at this stage - to be investigated."
	},
	{
		"grant":253,
		"ID": "EP/I012923/1",
		"Title": "An Intelligent Integrated Navigation and Autopilot System for Uninhabited Surface Vehicles",
		"PIID": "21886",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2011",
		"EndDate": "30/09/2014",
		"Value": "354231",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Marine Science & Engineering",
		"OrgID": "109",
		"Investigators":[
		{"ID": "21886", "Role": "Principal Investigator"},
		{"ID": "3661", "Role": "Co Investigator"},
		{"ID": "-79164", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Atlantic Inertial Systems Ltd"}
		],
		"Summary": "Global positioning systems (GPSs) are very useful navigational aids for both civilian and military vehicular systems. However, owing to the weakness of the radio signals from the satellites they are susceptible to signal loss when operations are being conducted in confined areas such as rivers, mountains and canyons, and are extremely exposed to being deliberately jammed by terrorists or aggressors during times of extreme tension or conflict. Thus given the vulnerability of GPSs to signal loss, it is prudent not to be totally reliant upon them in the design of navigation subsystems for autonomous vehicles (AVs). One solution to this problem is to enhance the subsystem with an algorithm based on simultaneous localization and mapping (SLAM) techniques. SLAM being the process of simultaneously building a feature based map of the operating environment and utilizing it to estimate the location of an AV. To further improve the capability and performance envelope of an AV it is appropriate to combine a SLAM reinforced navigation subsystem with an adaptive control subsystem thereby transforming them into one fully integrated system.This research proposal aims to design and build a new advanced intelligent integrated navigation and autopilot (IINA) system with adaptive capabilities for uninhabited surface vehicles (USVs). The existing intelligent navigation (IN) subsystem will be complemented with a SLAM algorithm which will be newly designed and also capable of interfacing with other types of more traditional navigation system. The new improved IN subsystem will be benchmarked against the existing navigation subsystem and a navigation system enhanced with an inertial measurement unit supplied by Atlantic Inertial Systems (AIS) who are the industrial collaborator for this project. Whilst the intelligent integrated system will be designed and developed for a marine application, the technology evolved will be able to be transferred and used in other types of AV.A common feature with many SLAM algorithms is the reliance upon extended Kalman filters to act as the information data collection mechanism. In this research proposal an interval Kalman filter (IKF) which uses interval calculus in its design will be employed instead and enhanced using artificial intelligence techniques to construct a fuzzy IKF (FIKF). Scene information extraction for SLAM can be from visual or non-visual sources. Visual SLAM has the benefit of selecting and using robust features gained from video imagery of the local scene. A novel aspect of this work will be the design of a feature-matching algorithm (FMA) that will be capable of operating in night-time conditions. Thus, an information data collection mechanism based on a FIKF will be integrated with the FMA to form the overall SLAM enhancement algorithm.Whilst there are a number of methods for introducing adaptability into an autopilot design, in this research project the approach to be taken will be based on a combination of on-line closed loop identification and model predictive control. Thus the methodology described herein represents a new conceptual framework for the design of marine autopilots. It should also be noted that, to date, all uninhabited marine vehicle system identification trials have been performed in the open loop. Upon the successful completion of the design of the adaptive autopilot, it will be merged with the SLAM enhanced IN subsystem to form the IINA system."
	},
	{
		"grant":254,
		"ID": "EP/I013067/1",
		"Title": "Linear Algebra and Optimization: Structure, Sparsity, Algorithms and Software",
		"PIID": "54563",
		"Scheme": "Standard Research",
		"StartDate": "03/10/2011",
		"EndDate": "02/10/2015",
		"Value": "1489217",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computational Science & Engineering",
		"OrgID": "36",
		"Investigators":[
		{"ID": "54563", "Role": "Principal Investigator"},
		{"ID": "23213", "Role": "Co Investigator"},
		{"ID": "22892", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed program of work is to develop algorithms, supporting theory and software for solving large-scale problems as may occur in science, engineering, planning and economics. Real-life applications that can benefit from our work abound. Engineers aim to build bridges that are as light as safely possible. Manufacturers seek maximum efficiency in the design of their production processes. Investors aim at creating portofolios that avoid high risk while yielding a good return.  Experimentalists are interested in how proteins hold, and in detecting hidden structure in vast data sets. Finding the 'best' solution commonly involves constructing a mathematical model to describe the problem. These models are usually complicated and often large scale, depending on alarge number of parameters. Models with millions and billions of variables and restrictions are not uncommon, but neither are relatively small but fiendishly difficult ones. It is therefore imperative to implement the model on a computer and to use computer algorithms for solving it. The latter task is at the core of the proposed activities.Nearly all such large-scale problems exhibit an underlying mathematical structure or sparsity. That is to say, the interactions between the parameters of a large system are often localized and seldom involve any direct interaction between all the components. For example, an electrical network can be represented by a graph where nodes are equivalent to branches in the network and components are on the edges. This graph will be sparse in as much as most nodes are only connected to very few other nodes. Engineering structures, and many other problems, can be represented by a similar graph. To efficiently solve the systems and models represented in this way involves developing algorithms that are able to exploit these underlying 'simpler' structures, which often reduces the scale of the problems, and thus speeds up their solution. This enterprise commonly leads not only to new software that implements existing methods, but to the creation of new theoretical and practical algorithms. At the other extreme, some problems involve interaction between all components, and while the underlying structure is less transparent, it is nonetheless present. For example, atomistic models may have to account for interactions between each atom, however small. In these cases, the computational burden may be very high and such problems may generally only be solved by sophisticated use of massively parallel computers.The methods we will develop will aim to solve the given problem efficiently and robustly.  Since computers cannot solve most mathematical problems exactly, only approximately, a priority will be to ensure the solution obtained by applying our algorithms is highly accurate, that is, close to the 'true' solution of the problem. But it is also vital  that we solve problems fast without sacrificing accuracy; this is particularly true if a simulation requires us to investigate a large number of different scenarios, or if the problem we seek to solve is simply a component in an overall vastly-more-complicated computation. Developing algorithms that are both fast and accurate on multicore machines presents a key challenge.The software that will be produced under this grant will be included in the internationally renowned mathematical software libraries HSL and GALAHAD, which are freely available to academics for research and teaching. These libraries are extensively used by the scientific and engineering research community in the UK and abroad, as well as by some commercial companies (including Aspentech, Wolfram Research, Ziena Optimization, Altair Engineering, and IBM). In the UK, in the last four years, more than 80 university departments have used HSL for teaching or research. The areas in which it has been employed include computational chemistry, engineering design, fluid dynamics, portfolio optimization, circuit theory."
	},
	{
		"grant":255,
		"ID": "EP/I013504/2",
		"Title": "Community Organisation of Events in Communications, Mobile Computing and Networking with EPSRC ICT Portfolio",
		"PIID": "10612",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "31/03/2014",
		"Value": "179592",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "10612", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "4-6 monthly co-ordinating brainstorming community meetings for the academics in  Communications, Mobile Computing and Networking  which also encompass linking into RF and Microwave Technologies, Photonics and Electronics. These aim to build, with active EPSRC staff participation, future  vision  and  roadmap  documents for our overall research community to complement the existing EPSRC grand challenge documents in microelectronics and silicon technology. We envisage holding 3 of these meetings in year 1 followed thereafter by 6 monthly meetings with each meeting attended by 25-30 academic and industrial participants. Six 1 or 1.5 day specialist research intensive briefings to address future burning topics. These would be theme based with a morning of  presentations  from recognized experts in the chosen theme subject area followed by a highly structured follow-on discussion to identify new research themes which should be actively discussed for forwarding up to the above vision meetings and then on to EPSRC SAT and consideration for inclusion in future planning. For year 1 we suggest these meetings could individually address topics such as:  Spectrum ,  Integration of RF and Photonics  and  Software in Systems . These workshops would be co-ordinated with EPSRC staff to ensure that we included any workshop topics they wished to initiate. The participants in the two sets of above meetings will include early career researchers and EPSRC/RAEng research fellows to ensure that we engage with the full community and brief future stars on the planning to deliver an effective research programme of international standing. Arrange an education day for  communications  researchers to make the community more aware of the difficulties in successfully funding proposals which receive somewhat mixed peer reviews. Organise an annual  Communications and Networking  5 day Summer School for (new) PhD students in each of the 3 years. This would be organized and coordinated by Prof Barham Honorary at Lancaster and Dr Harald Haas at Edinburgh. We would be aiming here give the participants a wide range appreciation of related topics by including here at least three half day tutorials on topics such as  Photonics ,  RF and microwave  and  Networks . We would also like to indicate to PhD students how research is organized and delivered, probably with an after dinner speaker, to give them appreciation of wider organizational issues, over and above their own in-depth PhD studies. These more specialist tutorials and presentations would be delivered by appropriate academic or industrial colleagues. These Summer Schools are aimed at first year PhD students to give them access to presentations by key UK academics and industrialists as appropriate and thus provide a much improved grounding in the overall background to the research techniques that they will be developing further in their personal PhD programmes."
	},
	{
		"grant":256,
		"ID": "EP/I013512/1",
		"Title": "The Creative Speech Technology Network",
		"PIID": "4697",
		"Scheme": "Network",
		"StartDate": "01/03/2011",
		"EndDate": "28/02/2013",
		"Value": "52518",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "4697", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "People are increasingly expected in every-day like to interact with synthetic voices. These encounters are often not very satisfactory. The suggestion is that a major reason for this is that these voices do not sound 'right'. Synthetic voices have improved greatly in recent years in that they sound more human-like. Yet there remain two (conflicting) problems: On the one hand they are still not entirely human-like; most people spot the fact that they are listening to synthesizer within a few words.At the same time, even if it were possible to re-create a perfectly human-sounding voice, it would convey the wrong associations for what are computer-generated utterances. The computer does not have all of the other attributes of a person, such as intelligence, a sense of humour, empathy etc. Put another way, a computer voice should sound like a computer voice. The question is: how should that sound?The objective of this network is to bring together people who know about (human) voices and how to use and manipulate them - from an artistic background - with technicians who know how to generate synthetic voices. By establishing and developing a language through which these disparate people can communicate, it will be possible to create new voices for specific purposes. The focus of the work will be on creating artistically inspired artefacts which will challenge the potentials of synthetic voice production. Those artefacts will also be used to disseminate the results of the work - to expose it to members of the public. This will share the results of the research as well as allowing the final evaluation of the voices to be given to the people who matter - those who have to put up with this kind of interaction on a daily basis."
	},
	{
		"grant":257,
		"ID": "EP/I013601/1",
		"Title": "Analytic Descriptions of the Ionospheric Impact on Space-Based Synthetic Aperture Radar",
		"PIID": "73003",
		"Scheme": "Standard Research",
		"StartDate": "08/03/2011",
		"EndDate": "07/03/2014",
		"Value": "434992",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical and Computer Eng",
		"OrgID": "14",
		"Investigators":[
		{"ID": "73003", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "FOI - Swedish Defence Research Agency"},
		{"Number": "1", "Name": "QinetiQ Ltd"}
		],
		"Summary": "Current space based observations of the Earth, whilst providing consistent and global coverage of land use, do not accurately measure forest biomass. This is because high frequency electromagnetic radiation (including X-band radar, optical and infra red sensors) measure scattering from surface features, such as leaves, and do not measure the biomass contained beneath. Although the situation is better with lower frequency C-band radar (such as Envisat) and even lower frequency L-band radar (such as PALSAR), the scattering that they measure still saturates at low levels of biomass. To overcome this limitation, longer wavelength (~1m, P-band) signals, which penetrate deeper into the forest, are needed. The backscatter from such radars saturate at higher levels of biomass, thus enabling accurate measurement. Low frequency SAR also has potential military applications, most notably as a counter to camouflage, concealment under foliage, and deception, and in planetary exploration missions.The overriding disadvantage of using long wavelengths, apart from antenna design issues due to their proportionally larger size, is the degrading impact of the ionosphere. The ionosphere is a highly variable and turbulent medium which at these frequencies primarily affects the phase of a radar signal with amplitude affects due to diffraction. The degrading effects are most prevalent at high and equatorial latitudes and in the evening sector. Judicious choice of the orbit may mitigate the ionospheric impact but this is not always possible for operational reasons, including the requirements of other payloads. PALSAR is an example of a satellite in an orbit which is affected by ionospheric turbulence as well as gradients. In this project, the generic problems identified above will be addressed by a three pronged attack. (a) The development of novel analytical expressions of the effect of the ionosphere on SAR imaging. (b) Comparison and verification of the analytic expressions through numerical simulation (facilitated through a full-diffraction parabolic-method). (c) Comparison and verification of the analytical expressions through comparison with experimental SAR images of known calibrated targets which have been imaged through the turbulent equatorial ionosphere.The analytic theory will be verified by a full simulation of the ionosphere that includes diffraction effects, ideally required for P-band frequencies and below. The experimental validation of our model will use L-band PALSAR imagery (made available by ESA), since there is no P-band SAR in orbit; for this, calibrated corner reflector targets will be used. To link analytic, numerical and experimental data we will need a measure of the TEC and ionospheric strength of turbulence. This will be provided through Global Positioning Satellite (GPS) measurements of signal phase. Further, we will utilize satellite beacon measurements of scintillation at 150 MHz and 400 MHz to link measurements at L-band (SAR and GPS) to frequencies most pertinent to a low frequency SAR. The measurements will be made in the equatorial region where the effects are largest.Once the analytic theory has been developed and verified, it will be applied to biomass measurement accuracy estimates and more generally to the design of SAR systems optimized to mitigate the ionosphere. Algorithmic developments and improvements will follow."
	},
	{
		"grant":258,
		"ID": "EP/I014268/1",
		"Title": "Quantum Multiplexer",
		"PIID": "27169",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2011",
		"EndDate": "30/06/2014",
		"Value": "520899",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "27169", "Role": "Principal Investigator"},
		{"ID": "28187", "Role": "Co Investigator"},
		{"ID": "14612", "Role": "Co Investigator"},
		{"ID": "8454", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "National Physical Laboratory NPL"}
		],
		"Summary": "The split gate transistor was invented 22 years ago and has ben in wide-spread use ever since in the investigations of the fundamental physics of electron transport in low-dimensional structures.  At most a few such transistors have ben used in very small circuits.  We have devised a way of making over 1000 split gate devices on the same chip and the way to bias each gate, while needing only 40 contacts to read out the transport properties of the entire array.  This advance will greatly accelerate both the physics investigations and the exploration of new device concepts, taking fully into account a statistical analysis of the results as a function of the intrinsic fluctuations from gate-to-gate."
	},
	{
		"grant":259,
		"ID": "EP/I01490X/1",
		"Title": "Synthetic materials using metallic and non-metallic nanoparticles at microwave frequencies",
		"PIID": "16234",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2011",
		"EndDate": "31/05/2015",
		"Value": "495773",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90",
		"Investigators":[
		{"ID": "16234", "Role": "Principal Investigator"},
		{"ID": "-176916", "Role": "Co Investigator"},
		{"ID": "44767", "Role": "Co Investigator"},
		{"ID": "-85963", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "European Space Agency"},
		{"Number": "1", "Name": "IMST GmbH"}
		],
		"Summary": "The aim of this research is to fabricate microwave radiating antennas and substrates using nanomaterials.  These novel dielectric substrates will facilitate electromagnetic advantages.Antennas are becoming increasingly prevalent in our modern, wireless and digital society; they are crucial for voice and data communication, GPS information and the provision of wireless communication between components of larger integrated systems. Antennas are subject to constant market forces which demand that products and their antennas become cheaper and smaller with improved functionality. With multiple antennas with multiband and MIMO capabilities whilst in very close proximity, for example on a mobile phone, the isolation between the different antennas also requires technological advances for improvement. The establishment of a novel technique to create antennas with improved radiation efficiency would reduce energy consumption.Nanoparticles are typically smaller than one millionth of a metre in at least one dimension and can be combined to form nanomaterials. Yet because the size of nanoparticles is so small and their resultant surface area-to-volume ratio so extremely large, nanomaterials possess a range of very useful and exciting properties. These include proportionately increased electrical conductivity, strength, heat and scratch resistance. Note, we will not be using nano-powders so the health risks will be minimal - and we will take all necessary steps to further minimise them.The use of nanomaterials will fundamentally allow increased versatility and improve functionality by design innovations. This area of research is highly novel as the use of nanomaterials as proposed here has not previously been reported at the application-rich microwave frequencies (wavelength ~ 30cm >> 1 micron). Using such nanomaterials for microwave antennas would allow manufacturing benefits as the antenna, the substrate and RF circuitry can be constructed together and integrated into one process. Currently, antennas designs are limited to certain specific fixed substrate properties. By constructing the substrate from non-metallic nanomaterials, advantageous, novel and heterogeneous substrates, with low losses and desirable electric and magnetic properties, can be produced, which can then be tailored for specific applications. Creating antennas from nanomaterials enables highly conductive and thinner than conventional layers.Intensive simulations using high performance computers will enhance Loughborough University's (LU) recent pilot study of how these novel antennas can behave. When these preparatory stages have been completed, prototype samples and antennas will be fabricated. Initially, geometrically simple antenna designs such as dipoles and patches will be used, enabling extrapolation to more complex antenna geometries later in the project. Once these are created their characteristics will be measured using LU's anechoic chamber, and compared with the simulation results.LU is ideally placed to research this exciting new area. The Communications Group has extensive expertise of simulating, design and measuring antennas and metamaterials. We have assembled an extremely strong multi-disciplinary team which has over 700 journal publications and more than 100 patents and book chapters. The Centre for Renewable Energy Systems Technology (CREST) has the capabilities to produce and characterise our specially made nanostructures. We also have close contacts with Patras University in Greece, which can fabricate nanostructures by an alternative (but viable) method using polymers."
	},
	{
		"grant":260,
		"ID": "EP/I017461/1",
		"Title": "Monolithic Resonant TeraHertz Detectors",
		"PIID": "71158",
		"Scheme": "Standard Research",
		"StartDate": "29/09/2011",
		"EndDate": "28/03/2015",
		"Value": "589217",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Electrical Engineering",
		"OrgID": "49",
		"Investigators":[
		{"ID": "71158", "Role": "Principal Investigator"},
		{"ID": "110614", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Selex Sensors and Aiborne Systems Ltd"},
		{"Number": "1", "Name": "TeraView Limited"}
		],
		"Summary": "Visible light is only a very small part of the whole electromagnetic spectrum.  The radio spectrum is also very familiar to most people, but less well known is the range of wavelengths in between.  In this project we are particularly interested in a part of the spectrum that has come to be known as the terahertz band, so called because the frequency is around 1 THz.  Light in the terahertz band can pass through materials that are opaque to visible light, but yet, the wavelength is still small enough to resolve features smaller than 1 mm. Because of this terahertz has attracted a great deal of interest for applications where we need to see through materials, but also take good sharp pictures.  Applications include medical and security imaging, particularly because terahertz is non-ionising so can be safely used with humans.Unfortunately terahertz technology suffers from some significant difficulties that requires research to overcome.  Bright terahertz sources are difficult to make, so considerable effort is needed to improve what we have at the moment.  Terahertz is energetically similar to ambient radiated heat, so sensors have to be both sensitive and highly descriminating.  In a complete terhertz imaging system all aspects of the technology and its components are important in determining the overall performance.  This project is therefore dedicated to improving sensor performance.There are a number of attributes that we would like for a good sensor.  It should be small, consume little power, be very sensitive, and ideally, if it it to be used in an camera, fast enough to allow video rate imaging.  We propose to use the optical properties of semiconducting materials and carefully designed metallic structures to capture terahertz radiation.  We will demonstrate that these structures can be used to make an array of sensors, just as you would find in a normal camera, and that the sensors are sensitive and selective to terahertz.  In the same way that mainstream photography has benefited from microelectronics to make digital cameras possible, we will also be able to make use of integrated circuit technology so that many sensors can be cheaply and efficiently put on to a single chip.Our project has attracted support from leading UK companies including Teraview and Selex-Galileo that have immediate routes to market for successful technology.  Our aim is to complete the research that will demonstrate new technologies to the point where further investment will enable the creation of new products that can be used by scientists, clinicians and the security services in the not to distant future."
	},
	{
		"grant":261,
		"ID": "EP/I017860/1",
		"Title": "Silicon Valleytronics",
		"PIID": "-250817",
		"Scheme": "First Grant Scheme",
		"StartDate": "05/05/2011",
		"EndDate": "04/08/2013",
		"Value": "100023",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "7",
		"Investigators":[
		{"ID": "-250817", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "NTT Basic Research Laboratories"}
		],
		"Summary": "Harnessing the interface between silicon and its thermal oxide has had, and continues to have, enormous impact on humankind through the crucial role it plays in metal-oxide-semiconductor transistors. These transistors can be found in anything containing integrated circuitry like computers. Due to the importance of this interface, vast efforts were made to understand the underlying physics around three decades ago and as such, it is often assumed that most of the pertinent physics is well understood.However, along with extreme miniaturisation and the developing interest in quantum information processing, we are now entering into an exciting new era where our understanding of silicon is tested in ways only dreamt about thirty years ago. Miniaturisation has come so far, that cross-sectional micrographs of cutting-edge transistors can now show individual atomic structures on the same picture encompassing an entire device. In such small devices, and indeed in emergent devices aimed at manipulating quantum information, details such as atomic layer fluctuations of an interface and quantum mechanical effects come to the fore. These have enormous effects on device properties, rendering further progress critically dependent on our ability to understand and control them. This is leading to a world-wide revival of interest in the basic physics of silicon. As an unexpected and surprising result of this endeavour, recent experiments have revealed, that when the interface is prepared in a particular manner, the band-structure of silicon is profoundly altered in a completely new way which we call  giant valley-splitting . The band-structure of a material lies at the very heart of the physics of any crystalline solid - it dictates all properties involving electrons such as how suitable the material is for use in a transistor, and what colour of light the material absorbs and emits.Despite silicon-silicon dioxide being one of the most important interfaces in the infrastructure of modern society, at present, we know very little about this effect. We do not have a quantitative theory to explain it; we do not know what microscopic structural parameters determine it; we do not know what exact preparation parameters determine it, and we do not know how it affects other physical properties except limited aspects of electrical conductivity. In this respect, this silicon-silicon dioxide interface is a new material with yet-unknown properties.The aim of this project is to understand the origin and consequences of this  new  interface so that we can harness it as a new ingredient for physics of low dimensional systems and technology of semiconductor devices. Since the material is made from silicon and silicon dioxide, it is automatically compatible with the vast arsenal of cutting-edge silicon technology. New properties and resulting functionalities can be embedded into existing silicon based systems at the deepest level of integration which is impossible with any other material."
	},
	{
		"grant":262,
		"ID": "EP/I017909/1",
		"Title": "A new approach to Science at the Life Sciences Interface",
		"PIID": "53062",
		"Scheme": "Standard Research",
		"StartDate": "05/05/2011",
		"EndDate": "04/05/2016",
		"Value": "3989307",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "53062", "Role": "Principal Investigator"},
		{"ID": "8392", "Role": "Co Investigator"},
		{"ID": "30452", "Role": "Co Investigator"},
		{"ID": "110695", "Role": "Co Investigator"},
		{"ID": "21769", "Role": "Co Investigator"},
		{"ID": "35009", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The key scientific challenges of the next century require that we fundamentally advance current understanding of complex natural systems - ranging from how cells work and why/how they go wrong, to how the interaction of climate and ecosystems regulates the planet's life support system. We wish to understand how these systems behave at the functional level, and how this behaviour arises as a result of highly dynamic, strongly non-linear, tightly coupled interactions between component processes occurring across multiple spatial and temporal scales. Entirely new kinds of exploratory and predictive models and research strategies are needed to address these challenges. A new generation of theoretical methods and tools are needed to enable the recognition and exploitation of synergies and similarities that allow the translation of solutions between different scientific problem domains. Biological theories are evaluated against often imperfect data sets. There is a need for judicious selection and validation of the test bed against which any model is evaluated and the development of novel technologies for gathering new data identified as critical to complex system behaviour. The investigation of systems-level behaviour requires the identification of biological hypotheses that could not have been expressed by looking at individual phenomena alone. The unprecedented degree of complexity will mean that these quantitative models will be analytically intractable, and exploring their behaviour will be possible only through a computational approach. In short, a novel computational approach and environment is needed for doing this kind of science - and this does not exist in academia today. Progress requires both a cultural and technological change in the way in which mathematical and computational models, tools and software are developed, and a concomitant change in the way in which groups of scientists are trained to develop and use these approaches. This work can only be done in the context of real biological problems. This proposal brings together a consortium of partners from academia and industry, each of whom has begun to focus on differing but complementary aspects of these problems. These centres are the ideal community to attempt such a cultural shift since they are already dedicated to training the next generation of scientists who will pioneer this kind of science."
	},
	{
		"grant":263,
		"ID": "EP/I018417/1",
		"Title": "AMORPHOUS CHALCOGENIDE-BASED OPTOELECTRONIC PLATFORM FOR NEXT-GENERATION OPTOELECTRONIC TECHNOLOGIES",
		"PIID": "92211",
		"Scheme": "Standard Research",
		"StartDate": "24/11/2011",
		"EndDate": "30/04/2014",
		"Value": "414776",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "ATI Electronics",
		"OrgID": "51",
		"Investigators":[
		{"ID": "92211", "Role": "Principal Investigator"},
		{"ID": "6534", "Role": "Co Investigator"},
		{"ID": "7590", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Varian Semiconductor Equipmnt Associates"}
		],
		"Summary": "Materials discovery, development and modification has been a key factor in developing the world we live in. The study of materials which exhibit electrical or optical properties has played a major role in enabling all of modern technology and in particular electronics, computing and communications. As these technologies have been developed existing materials have also been modified and pushed close to their limits of what is technical feasible. An example of this is the advances made in silicon (Si) based microelectronics which has led to speed, which relates to power of processing, becoming critical, with a reduction in the size of the microelectronics used to achieve this. This approach is ultimately limited as sizes reduce; thus alternative methods must be sought. Optical communication and data transfer is widely known as being much quicker as information can be moved at the speed of light. However, whenever it interacts with electronics, such as when broadband optical fibre is connected to a computer the data transfer and processing must slow down to the speed of the microelectronic processors. There is a strong desire and compelling argument therefore to develop an 'optoelectronic' technology which is a hybrid of the optical and electronic systems but without the current limitations imposed by the two current technologies working independently.  This proposal will seek to apply one of the most developed materials modification tools that is fundamental to modern microelectronics, ion-implantation, to a class of materials that show unique potential for enabling future optoelectronic technologies. These materials, known as chalcogenides, are already widely used in applications such as photovoltaics (solar cells), memory (e.g. DVDs), and advanced optical devices (e.g. lasers). Currently however they are used solely as either electronic materials or optical materials, with different types of chalcogenides used for each. Their properties that allow use in these separate application types gives them the potential to be developed so that the excellent optical properties of one material can be combined with the excellent electronic properties of another and vice versa. One of the reasons that this has yet to be done is that it has proved to be extremely difficult to modify their electronic properties during the material preparation which typically involves melting at high temperatures. Anything that is added to the materials, referred to as doping, is ineffective under these conditions due to the ability of the material to reorder itself whilst melted to cancel out the desired effect. In this programme of work, we will modify the properties by introducing dopants into the chalcogenide materials below their melt temperature, thus not allowing the material to reorder. This will be undertaken using ion-implantation which allows precise control of the type of impurity introduced. As a result of this work, we will develop for the first time an understanding of how these unique materials can be modified in a controlled way. We will then use this to develop better models of the origin of the materials' electronic and optical properties which will allow us to develop optimised materials. We will also develop prototype devices that will lead the way to the development of a truly optoelectronic technology.  This programme will establish the UK as leaders in this field and therefore directly contribute to the continuing growth of the knowledge economy. We will train the next generation of scientists and engineers in state-of-the-art techniques to ensure that the UK maintains the expertise base required for this, aim to ensure the impact of this work is maximised and accelerated where possible, and communicate the results widely including to all stakeholders in this research."
	},
	{
		"grant":264,
		"ID": "EP/I020357/1",
		"Title": "Reliable Numerical Computation with Parallel Unreliable Technologies",
		"PIID": "100961",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2011",
		"EndDate": "30/06/2016",
		"Value": "1002462",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "100961", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Imagine a world where mathematical dynamical models of processes in medicine, transport, finance and energy, are routinely constructed and refined, on the fly, by personalised hardware devices. With sufficient computational power, such devices could make intelligent decisions, for example utilising advanced control techniques for real-time intervention in diabetic patients and for pacemakers or defibrilators, or to automatically re-route personalised transport options based on up-to-the-minute information. The vision is a powerful one, but highlights many limitations of today's digital technologies, limitations that will not be overcome with simple scaling of technology, but which need a fundamental rethink of the way in which massively parallel computation can be performed. The focus of this proposal is therefore not on any one of the above grand challenges, but rather on the fundamental scientific problems enabling this revolution. The proposed research could enable computation at orders of magnitude better energy/performance ratio than would be possible by extending today's techniques to tomorrow's technologies, opening up a step change in current practices at both the low power end of computation (sophisticated personal devices) and the high power end (advancements in computational physics, chemistry, biology and finance).Over the next decade, there will be a number of radical shifts to the way in which computation is performed. In particular, rather than computing with a single computational core equipped with a numerical computational unit, or with a traditional inter-processor network of such cores, two differences are becoming increasingly apparent. Firstly, if we allow process technology to scale at the rate it could, then each computational device will become increasingly unreliable. Secondly, under the same assumption, this lack of reliability will be compensated by massive parallelism. Independently,  each of these issues requires considerable research effort to overcome the lack of reliability and the current inability to make efficient use of massive parallelism in a portable manner, and there is much ongoing international research in these distinct fast-moving areas.These seemingly distinct challenges, however, have an untapped common research core. Massive parallelism in numerical computation mandates a way to effectively deal with numerical imprecision in computation, even in fully reliable circuitry. One must have in mind a specification of tolerable numerical accuracy. Once such a specification exists, and can be formalised, it opens up the potential to use this specification as input to a powerful specialised optimising compiler, capable of optimising numerical hardware and software in order to achieve the specification with maximum performance within a given power envelope. Lack of reliability in future devices will propagate to numerical computation as noticeable numerical errors in the hardware. One way of overcoming these errors is to utilise parallelism in some kind of redundant fashion. A radical alternative is to `hide' the numerical errors caused by unreliable components within the tolerance specification already required. For a fixed silicon area, the emerging multi-core revolution in computational hardware brings to the fore a tension between numerical precision and computational performance; one can no longer afford to pay the price in performance for over-designed hardware.At the core of this research will therefore be the development of mathematical techniques to reason about the compilation of numerical software into parallel hardware, broadening applicability to general classes of nonlinear and control-intensive algorithms, requiring the application of nonlinear systems theory to reason about the convergence of classes of algorithm under numerical perturbation, and mechanising algebraic approaches to reasoning about accuracy in numerical algorithms."
	},
	{
		"grant":265,
		"ID": "EP/I022562/1",
		"Title": "Phase modulation technology for X-ray imaging",
		"PIID": "-87745",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2011",
		"EndDate": "31/08/2016",
		"Value": "1436518",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "London Centre for Nanotechnology",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-87745", "Role": "Principal Investigator"},
		{"ID": "13522", "Role": "Co Investigator"},
		{"ID": "-151895", "Role": "Co Investigator"},
		{"ID": "-304931", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We plan to develop coherence-based X-ray imaging methods, which have great potential for solving a wide range of physical problems and are expected to see wide usership, especially at Diamond's I13 beamline.  Coherence is a relatively new addition to the capabilities of synchrotron facilities and not all of its applications have been explored.  There is much to learn by transferring technology from existing visible-light capabilities, as we plan.  Gratings for X-rays, which need high aspect ratio and small (50-nanometer) feature sizes, can be manufactured on site at Harwell.  Grating-based measurement techniques are in the early stages of development and will be developed under this proposal for the benefit of the materials and biological imaging communities.Microfabrication has always played an important role in modern x-ray imaging.  From the 1952 proposal by Albert Baez to use Fresnel zone plates to build an x-ray microscope, x-ray imagers have profited from the rapid advances in the microfabrication industry.  X-ray zone plates are now the key technology for x-ray microscopes.  The modern practice of making them by electron-beam writing was pioneered by the MIT and IBM groups in the early eighties and can now deliver soft X-ray zone plates with an outer zone width (which roughly equals the resolution) of 10 to 15 nm.  Other types of microfabricated objects, such as resolution test patterns, Zernike phase rings, uniformly redundant arrays, and the list continues to grow.  Diamond's I-13 beamline is a world-class facility which can provide the highly coherent beams needed to advance the technology.  Its two branches correspond to the two basic modes of imaging, in real and reciprocal space, which will provide the baseline capabilities:i) X-ray computed tomography (CT) for 3D volume imaging with amplitude or phase contrast.  A rotational series of projection views is assembled into a 3D image using computation.  ii) Coherent Diffraction Imaging (CDI) by phasing and subsequent inversion of diffraction patterns, either in the forward direction or around Bragg peaks from a crystalline sample.  CDI, however, does require a computational phasing step, for which good algorithms are available now and better ones are under active development.On a longer time scale, the proposed methods development will interface with Diamond beamline B-24, for cryogenic Transmission X-ray Microscopy (TXM).  This project needs the highest possible quality Fresnel Zone Plates operating in the  water window , which could be achieved with a well-planned upgrade pathway for the Harwell microfabrication facility.  Since x-ray imaging programs are totally dependent on the fabricators of these items, it is vital that routine access to the fabrication facilities should be available to UK researchers, preferably within the UK, and preferably within STFC.  Having a dedicated full-time person in place working on X-ray optics within this project will be a first step in this direction.The research enabled by the new methods we will develop will see wide application in visualising the details of processes involved in biological, medical and materials science.  The flexibility of the methods we will develop will also enable undertaking dynamical studies of similar processes, as well as the imaging of samples requiring sophisticated manipulation, housing and/or extreme pressure/temperature conditions.  Our project has very broad scope, ranging from nanoscale structures accessible only though diffraction (CDI) to macroscale whole animal studies, necessarily requiring a large field of view.  All these X-ray imaging modalities are seeing rapid growth at the present time.  This project focuses on enhancements achievable by phase modulation, either by improving image contrast or by broadening the range of accessible samples by allowing them to imaged (phased) in the first place."
	},
	{
		"grant":266,
		"ID": "EP/I022791/1",
		"Title": "Ultra-precision optical engineering with short-wavelength semiconductor disk laser technology",
		"PIID": "115843",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2011",
		"EndDate": "30/04/2016",
		"Value": "996024",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Photonics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "115843", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "It has been 50 years since the first operation of the laser, yet there are still many new applications being made possible by continued innovation in laser technology. A range of exciting optical engineering techniques are currently being developed by scientists and engineers to achieve ever greater precision in sensing, manufacturing, and measurement: from the fabrication of nanometre-scale crystal structures created by laser light patterns to the probing of atomic energy levels to define the time and frequency standards used for communications and navigation. Such visible- and ultraviolet-based (short wavelength) research is very active; however, investigators are currently making do and having to become rather adept at converting current lasers with complex systems for beam shaping, amplification and frequency conversion which generally fall short of the desired wavelength, power and finesse, and confine this technology to the lab. This programme will develop a new class of simplified and tailored short wavelength laser systems in collaboration with these scientists and engineers in order to address a gap in the laser toolbox, dramatically improve capability, and bring these currently specialist techniques out of the lab to the level of widely deployed technology.The core laser technology for the optical engineering systems targeted will be semiconductor disk lasers (SDLs). SDLs are distinct from conventional high performance lasers in that the gain material is engineered on the nanometre scale. Rather than a laser crystal (millimetres long), a flow of dye, or a pressurised tube of gas, light amplification is provided by several quantum wells (QWs): ultra-thin (few nanometres thick) layers of semiconductor, positioned with nanometre-scale accuracy with respect to the light field in the laser. Aside from commercial advantages in terms of compactness, cost and wavelength flexibility, this set-up is fundamentally suited to the very high coherence, low noise laser performance required for ultra-precision optical engineering.Nearly all SDLs operate in the near- or mid-infrared regions of the spectrum; however, many more applications will open up if their full potential for visible and ultraviolet operation is realised. The unique capability in short wavelength SDLs that Dr. Hastie's team has developed over the past 5 years means that she is now in a position to push the technology to target genuine applications for wider benefit. She has identified UK and international research partners for the realisation of high finesse semiconductor laser systems in the visible and UV, together with end users at research institutions in the UK. The Challenging Engineering award will provide the platform necessary to lead this research network and address the identified challenges.Three different optical engineering systems will be targeted initially:* interference lithography - an effective, low-cost method of fabricating nanostructures over a large area and widely deployed in the fabrication of circuits in the semiconductor industry* ultraviolet spectroscopy - for measuring the concentrations of important atmospheric trace gases* optical clocks - for the improvement in time and frequency standards used for communications, satellite navigation and testing of fundamental physics.These areas are complementary in terms of the required laser engineering and performance, will achieve a step-change in capability through the application of short wavelength SDLs, and are sufficiently diverse to provide scope to actively pursue multiple promising research directions and applications, many not yet predicted."
	},
	{
		"grant":267,
		"ID": "EP/I023186/1",
		"Title": "Extreme light-matter interaction in the solid-state for quantum technologies",
		"PIID": "-189537",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2011",
		"EndDate": "30/04/2016",
		"Value": "1057014",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "-189537", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hitachi Cambridge Laboratory"},
		{"Number": "1", "Name": "National Physical Laboratory NPL"},
		{"Number": "2", "Name": "Renishaw Plc"},
		{"Number": "3", "Name": "University College London"}
		],
		"Summary": "Optoelectronic devices are vital in our modern society for processing and transmitting information. Electronic signals are manipulated at GHz frequencies with semiconductor devices and signals are transmitted over large distances via optical fibres using semiconductor lasers and detectors. However, new approaches to process and transmit information are required to keep pace with the daunting increase in the volume of information and the continued miniaturization of devices. To address this, radical ideas which exploit coherent quantum states are being developed for a diverse range of applications including communication, information processing, and metrology. As with the existing digital economy, semiconductor heterostructures will be central to future commercialization of quantum technologies. The most feasible approach to implement quantum technologies is to interface flying bits of quantum information, photons, with the semiconductor quantum states. Hence, quantum photonic devices with extremely efficient light-matter interaction (at the single photon level) are paramount for the future digital economy. This Challenging Engineering programme aims to engineer ideal quantum photonic devices and exploit them for an array of quantum technologies. Success will be a major boost to UK competitiveness in a key frontier research area and strategies are in place to impact multiple academic disciplines, UK industry, and multiple segments of the general public."
	},
	{
		"grant":268,
		"ID": "EP/I026959/1",
		"Title": "UnderTracker: Underground Animal Tracking and Mapping in 3D",
		"PIID": "-283783",
		"Scheme": "Postdoc Research Fellowship",
		"StartDate": "29/03/2012",
		"EndDate": "28/03/2015",
		"Value": "263646",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-283783", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Austriamicrosystems"},
		{"Number": "1", "Name": "University of Cambridge"}
		],
		"Summary": "Wildlife tracking using wireless sensor networks has garnered a great deal of attention and research, since the seminal ZebraNet project monitored zebras with mobile nodes in 2002. However, research to date has concentrated on monitoring animals when they are above ground. It is currently impossible to automatically monitor animals whilst they are underground. The main reason for this is that radio waves are severely attenuated by layers of soil, to the point of being unusable. There is a strong need for a system that can localize burrowing animals when they are within their dens or tunnels, in order to better understand their behaviour and habits. A prime example of this is the European badger - badgers are a protected species in the UK, yet are subject to widespread culling due to their possible link to bovine TB. By monitoring internal sett conditions and animal interactions underground, a better understanding of infection could potentially be obtained. To tackle these issues, I propose the use of low frequency magnetic fields (i.e. the principle of magneto-induction, MI), which are able to penetrate soil without attenuation, to provide ultra-low power three dimensional localization of wild animals within their burrows. Data from tracking collars will be forwarded by conventional high frequency radio links when the animal is above ground, meaning that the animal does not need to be recaptured to obtain the stored information. By mapping animal movements over time, the subterranean tunnel architecture itself will be determined, something which can currently only be obtained, destructively, through excavation. Sensors within the tunnel will monitor gas concentrations and temperature gradients, which will help to explain how animals achieve suitable ventilation underground and maintain body temperature. To investigate animal behaviour, tracking collars will be equipped with miniature sensors, such as accelerometers and magnetometers, which will record motion and energetics. To reduce data volumes, tracking collars will automatically characterize animal behaviour primitives, such as walking or sleeping. To further increase the rate of  learning  this information, tracking collars will share motion features, forming a distributed knowledge base. Thus, this research proposes a broad animal monitoring and tracking system, which will reveal a complete picture of animal life underground, for the first time.To achieve the goals of the research a close collaboration with the Wildlife Conservation Research Unit at the University of Oxford will be formed. They will guide the design of the tracking collars and attach them to suitable badgers during regular research undertaken in Wytham Woods, Oxfordshire. Their expertise is also vital in framing the research to address biologically relevant questions. Data from this system will also be used by researchers in the University of Cambridge Computing Laboratory, to investigate social contact networks. Ultimately this insight into the detail of badgers' lives will help to unravel the true extent with which they interact with each other, and shed light not just on the behavioural-ecology of this species, but investigate their social systems and address important questions concerning the transmission of disease."
	},
	{
		"grant":269,
		"ID": "EP/I02798X/1",
		"Title": "Table-Top Lasers for Resonant Infrared Deposition of Polymer Films",
		"PIID": "59852",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2012",
		"EndDate": "31/12/2014",
		"Value": "511746",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "59852", "Role": "Principal Investigator"},
		{"ID": "4633", "Role": "Co Investigator"},
		{"ID": "-240453", "Role": "Co Investigator"},
		{"ID": "41202", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Covesion Ltd"}
		],
		"Summary": "Thin films of organic molecules and polymers play a critical role in a huge number of electronic, photonic, mechanical, and medical technologies that are crucial to modern life.  Significant examples include antistiction coatings for micro-mechanical systems such as computer magnetic disk drives, multi-layer films for light-emitting devices and flexible displays, thin film transistors for computer and TV displays, and biodegradable coatings for time-release drug delivery.  Current industrial technologies for deposition of such films have some significant drawbacks.  For example, thin films of conjugated polymers, used in organic light-emitting diodes and photovoltaic solar cells, have been fabricated by a variety of methods based on solution casting processes, but this leads to solvent induced conformational defects that adversely influence the optoelectronic behaviour. As a solution-free alternative, thermal evaporation is viable for short chain oligomers and small organic molecules, but is very challenging for long-chain conjugated polymers.  As another example, thin films of PTFE are desirable for a large number of applications due to its biocompatibility, low frictional resistance, chemical inertness, and low dielectric constant, and again many techniques have been developed for its deposition but each has certain drawbacks. For example, spin coating is problematic due to a lack of suitable solvents and a need for post-annealing that can be undesirable for microelectronic structures, while plasma polymerisation of fluorocarbon monomers and sputtering techniques produce fluorine deficient PTFE films.  It is for these reasons that laser-based deposition of polymer films has become important.  UV lasers may be used for pulsed laser deposition of polymers but it is difficult to grow a film with the same chemical structure as the starting material.  Typically, the polymer is converted to monomers and small oligomeric fragments in the plume and repolymerisation occurs upon deposition. However, if repolymerisation is incomplete or if there are missing groups due to direct scission photoreactions then the film will be chemically modified.  Matrix-assisted pulsed-laser evaporation aims to resolve this issue by dissolving the polymer in a volatile solvent, which is then frozen to create a solid target.  Ideally the polymer would be transparent to the incident light and the host highly absorbent, thereby limiting direct interaction between the laser and the polymer. However, this ideal situation is not easily accomplished using UV lasers.These problems have led to the development of resonant infrared pulsed laser deposition (RIR-PLD) where excitation of vibrational resonances can lead to the breaking of relatively weak intermolecular bonds and deposition of polymer films with unmodified chemical structure.  This technique can be applied directly to the polymer or in a matrix-assisted format. However, the relevant vibrational modes lie within the molecular fingerprint region of the IR spectrum (2-10um) where there is an unfortunate dearth of appropriate laser sources.  Consequently, the vast majority of RIR-PLD experiments to date have been performed using a free-electron laser. While this source is ideal for demonstration purposes it is certainly not suitable for a commercial processing facility.We therefore propose to build a novel, compact and efficient source of high-energy picosecond pulses with broad tunability in the mid-IR.  The source is based on an synchronously pumped optical parametric oscillator with a fibre feedback arm to conveniently allow long cavity lengths, relatively low repetition rates, and hence high pulse energies.  It will be pumped by a simple gain switched diode laser, scaled to high average powers by an Yb-doped fibre amplifier.  This table-top replacement for the FEL will revolutionise thin-film polymer deposition and consequently impact strongly upon a number of important applications."
	},
	{
		"grant":270,
		"ID": "EP/I028757/1",
		"Title": "REINS",
		"PIID": "117106",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2011",
		"EndDate": "01/03/2015",
		"Value": "455967",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Arts Computing Eng and Sci",
		"OrgID": "115",
		"Investigators":[
		{"ID": "117106", "Role": "Principal Investigator"},
		{"ID": "-408294", "Role": "Co Investigator"},
		{"ID": "-169936", "Role": "Co Investigator"},
		{"ID": "-268406", "Role": "Co Investigator"},
		{"ID": "44243", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Guide Dogs"},
		{"Number": "1", "Name": "South Yorkshire Fire and Rescue"},
		{"Number": "2", "Name": "Thales Netherland"}
		],
		"Summary": "The REINS project is to design and investigate haptic communicational interfaces (reins) between a human agent and a mobile robot guide. The reins will facilitate joint navigation and inspection of a space under conditions of low visibility (occurring frequently in fire fighting). The focus is on haptic and tactile human robot cooperation as it has been found that a limited visual field and obscured cameras adds to the distress of humans working under pressure. Humans naturally interact with animals using tactile feedback in scenarios such as working with guide dogs and horse riding; the REINS project aims to extend this practice to human robot interaction. Expertise from a number of different disciplines - design, engineering, robotics, and communication - will be brought to bear on the problem of designing a communicational interface which will be both sufficiently robust for the relevant physical environment and sufficiently flexible to allow for the on-the-spot exercise of human judgement and creativity.Inspired by the use of a harness for a guide dog and also the rein to ride or drive a horse, the REINS project will investigate and experiment with haptic interfaces for human-robot cooperation. The low/no visibility constraint ensures the focus is on the tactile and haptic aspects only. Currently, robots do not sufficiently enhance human confidence. In human-robot cooperation, the human (by nature) will try to 'read' the situation, and anticipate the movements of the robot companion. The robot is provided with an impedance filter and the rein enables the human to feel the robot's movements and behaviour. Experiences with remotely controlling a robot which is not directly visible show that 'operators spent significantly more time gathering information about the state of the robot and the state of the environment than they did navigating the robot'.The REINS project aims to map the communicational landscape in which humans (fire fighters, but also the visually impaired) might be working with robots, with the emphasis on tactile and haptic interaction. We adapt a semi-autonomous mobile robot for navigation in front of a human. The robot provides rich sensory data and is enabled to try the mechanical impedances of the objects it encounters. We also design and implement a soft rein (rope), a wireless rein and a stiff rein (inspired by the lead for guide dog) enabling the human to use the robot to actively probe objects. The project thus creates the means to explore the haptic Human-Robot Interaction landscape. We will work from an integrationist perspective in which the communicator is not a mere user of pre-existing signs but a sign-maker; the signs emerge in the ongoing coordination and integration of activities adapted to the particular circumstances. We review the communicational landscape occurring within a team of (human) fire fighters and in addition review literature on working guide dogs and horse riding. A research question is whether the information should be explicitly encoded as messages or can remain implicit.In the initial phase of the project the robot is adapted and the first prototypes of the reins are implemented; the emphasis in this phase is on providing rich data to the human. The second phase is dedicated to surveying the communicational landscape. The human-robot team will navigate a known environment with low visibility where unknown obstacles may occur.  At least two different types of reins are applied: one requires that messages are explicitly coded, while the other propagates the information implicitly. Based on experiences in the first trials the reins might be adapted to improve their usability. Professional fire fighters will be the first group of subjects to try the reins, later on also volunteers experienced with guide dogs may join the experimentations."
	},
	{
		"grant":271,
		"ID": "EP/I029141/1",
		"Title": "Gallium nitride enabled hybrid and flexible photonics",
		"PIID": "45306",
		"Scheme": "Platform Grants",
		"StartDate": "01/04/2011",
		"EndDate": "31/03/2015",
		"Value": "1048345",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Photonics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "45306", "Role": "Principal Investigator"},
		{"ID": "32110", "Role": "Co Investigator"},
		{"ID": "61164", "Role": "Co Investigator"},
		{"ID": "-24412", "Role": "Co Investigator"},
		{"ID": "48498", "Role": "Co Investigator"},
		{"ID": "109313", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Compound semiconductors lie at the heart of modern-day information and communications technologies, and of these none is currently more important than gallium nitride and its associated family of alloys. This material system allows the production of sophisticated optical devices (lasers, light-emitting diodes, photodiodes) covering the ultraviolet and visible spectrum for displays, optical data storage and photovoltaics; it enables the development of advanced microwave electronic devices (transistors) for high temperature, high power and high frequency operation. Most of the work currently undertaken with gallium nitride focuses on the basic material itself and the devices that can be made directly from it. Here, in a visionary programme interfacing to a wide range of other materials and disciplines, we seek to explore the unique potential of gallium nitride for 'hybrid and flexible photonics'. These two interrelated themes involve the integration of nitride semiconductor micro/nanostructures and devices with compatible hard and soft materials, which we take to include single crystal diamond, nanocomposites, polymer overlayers and substrates, printable electronics, organic resists, biopolymers, and metal/plasmonic structures. Imagine, for example, hybrid waveguide devices made from gallium nitride and diamond. These could generate and manipulate single photons of light, towards computation and communications systems exploiting the full potential of quantum mechanics, or could enable lasers to be made from diamond via the so-called stimulated Raman process. Imagine, furthermore, the transfer of gallium nitride devices onto flexible substrates and their control via printable electronics. This could facilitate large area micro-displays, and a wide range of instrumentation and communications systems. Imagine the wavelength conversion of gallium nitride emission via nanocomposites and metal-based plasmonic effects, as the basis of multi-gigahertz visible light communications systems. Imagine a range of nanophotonic sources capable of stuying fundamental energy transfer processes on a nanoscale and of performing ultra-high resolution photolithography and direct write patterning. All of these capabilities and more can be forseen by the development of hybrid technologies based on gallium nitride. They present tremendous opportunities for UK leadership in fields of science and technology as diverse as nanoscience, lasers and nonlinear optics, quantum information, bioscience and visible light communications."
	},
	{
		"grant":272,
		"ID": "EP/I029613/1",
		"Title": "Novel Multi-target Distance Metrology for Multi-probe Application using Chirped Laser",
		"PIID": "-171807",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2011",
		"EndDate": "30/11/2014",
		"Value": "425642",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Advanced Technology",
		"OrgID": "127",
		"Investigators":[
		{"ID": "-171807", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This 3 year project intends to develop a new high repetition mode hop free chirped laser that allows major enhancements in an combined multilateration (CM) instrument.  This will enable additional distance measurements to be carried out at an accuracy of 0.1mm over a distance of 10m.  This project will require an experienced RA to develop the laser and detection system so that a state of the art instrument can be realised. The RA will be expected to work closely with NPL and be based at their facility in the third year. The research will produce an instrument that greatly simplifies manufacturing measurements without compromising accuracy. Commercially, this instrument will have a market potential of 10m/year and will provide enormous added value to the UK's manufacturing potential."
	},
	{
		"grant":273,
		"ID": "EP/I030328/1",
		"Title": "Persistent topological structures in noisy images",
		"PIID": "-149756",
		"Scheme": "First Grant Scheme",
		"StartDate": "08/08/2011",
		"EndDate": "07/08/2013",
		"Value": "99975",
		"ResearchArea": "Geometry & Topology",
		"Theme": "Information and Communication Technologies",
		"Department": "Mathematical Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-149756", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed research is on the interface between topology and algorithms.The methods are topological coming from the recently developed theory of persistent topology.The key idea of persistent topology is identifying features of geometric  objects that remain stable for a wide range of varying parameters. The research objectives are applied and involve designing novel  learning algorithms to recognise topological graphs in noisy images.A natural application is identifying a text-based advanced CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) widely used on the web to prevent automatic posting to blogs.A desired learning algorithm will be parallel like the human brain analysing local persistent properties of pixels in a noisy image.Hence it is the next step towards reverse-engineering the human brain.Other potential applications include recognising handwritten mail addresses and designing computer glasses reading signs of directions for blind people."
	},
	{
		"grant":274,
		"ID": "EP/I030921/1",
		"Title": "Approximate product-forms and reversed processes for performance analysis (APROPOS)",
		"PIID": "6874",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2012",
		"EndDate": "28/02/2015",
		"Value": "334971",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "6874", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Metron Technology Ltd"}
		],
		"Summary": "The need for models in the quantitative design of complex computer and communicaton systems is indisputable but their construction is hampered by the lack of a uniform methodology for building large models from smaller component-models - mirroring the design process of the systems themselves.  Various specific, often ad hoc, techniques have been developed over the last four decades in response to contemporary design features, beginning with queueing networks that modeled multiprogrammed mainframe systems through to network-calculus descriptions of mobile networks and stochastic models of telecommunication systems and the internet.  Unfortunately, such models are prone to exponential (or higher) growth in computational complexity, a phenomenon often referred to as  state-space explosion .The present proposal aims to provide techniques and tools for deriving efficient, mainly approximate, solutions to models of modern networks.  As in almost all analyses of such complex systems, the aim is to find separable solutions, which allow subsystems (components) to be solved separately and their solutions to then be combined in a simple way.  More specifically, we propose to develop the recent significant results obtained by the proposer and Dr Andrea Marin through a three-year project supporting Marin as the RA.  The said results have led to a number of research and tutorial papers that have been accepted and/or presented at the highest quality international venues in our research area.  It is the proposer's view that the research is at a knee in an upward curve and the opportunity to work directly with a rising-star such as Marin is one not to be missed if at all possible; in addition to the above papers and tutorials, Marin has won best paper awards at two international conferences for his work in this area in the last 12 months.  Furthermore, the presentation at Sigmetrics 2010 in New York was well received and led to several discussions with leading international researchers and plans for specific collaboration - see the Case for Support.  The theoretical research proposed will supplement existing analytical techniques, such as queueing network modeling (QNM), which are still relevant but are lacking in expressive power for modeling today's systems.  For example, Stochastic Petri Nets (SPNs) are suitable for describing virtual resource systems, as used in cloud computing for example, which is not the case for standard queueing networks.  Similarly, (non-standard) networks with batch movements are important in models of energy-efficient systems (see the Case for Support), but in general do not have separable (or other efficient) solutions; preliminary results relating to this will be presented as a poster at Performance'10 in November.  Last, optimisation is facilitated by the ability of our unique approach, using the Reversed Compound Agent Theorem (RCAT), to perturb specifications so as to create separable solutions, admitting the possibility of searching for a  best-fit  product-form solution.Based on these theoretical and practical developments, the first objective of the proposed project is to enhance the RCAT-approach to product-forms and semi-product-forms for application in models with SPN specifications and in networks with batch-movements.  Perhaps even more importantly, the probability density function of the response time of tasks in passing along a path will be investigated in a new class of networks; preliminary results have already been obtained and will also be presented in the aforementioned poster.  This work is a substantial advance on established approaches, such as Boxma and Daduna's as well as that of the proposed PI previously.  We believe that three years of uninterrupted collaboration between the proposer and Marin in the AESOP research group will attain the goals summarised above and listed under  Objectives ."
	},
	{
		"grant":275,
		"ID": "EP/I031170/1",
		"Title": "Synthesizing and Editing Photo-Realistic Visual Objects",
		"PIID": "-120390",
		"Scheme": "Standard Research",
		"StartDate": "26/09/2011",
		"EndDate": "25/03/2015",
		"Value": "536961",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-120390", "Role": "Principal Investigator"},
		{"ID": "-119581", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Adobe Systems Incorporated"},
		{"Number": "1", "Name": "Autodesk Inc"},
		{"Number": "2", "Name": "BBC Research and Development"},
		{"Number": "3", "Name": "Harvard University"},
		{"Number": "4", "Name": "University of Toronto"}
		],
		"Summary": "Current computer graphics techniques allow us to render almost any object at near photo-realistic quality. However, the standard approach necessitates that the user painstakingly specifies all aspects of the geometric and material properties of the object. This is time-consuming and needs skilled human operators. It is hard to edit the resulting models at anything other than the low level of geometry and materials at which they are specified. Moreover, we cannot edit real photographs without reverse engineering the underlying model and this is very difficult.In this proposal we investigate a radically different pipeline for computer graphics that will allow non-experts to rapidly create and edit photo-realistic two dimensional images of objects. The crux of our approach is to provide the computer with a deeper understanding of the class of objects under consideration. This knowledge (which takes the form of a statistical model) is then leveraged to help the user achieve their goals more easily. The impact of this project is potentially enormous. Such a technology could become a standard tool installed on every home and business computer. Some of the many potential applications are:- Conceptual design. Manufacturing industries often need to sketch new product ideas and refine existing designs. Our system could help a fashion designer produce and manipulate photo-realistic images of new garments.- Clipart objects. Stock images are required for on-line and real-world publishing and these are often sought via search engines (e.g. Google Images). However, the returned results are often not ideal and may be subject to copyright. Our approach will allow the user to design bespoke images to exactly their specifications.- Photo and movie editing. Digital editing of images and movies is commonplace, but requires considerable skill. Our techniques could be used to modify facial expressions in portrait photography or apply digital cosmetics in movie post-production.- Content for virtual worlds. The trend towards larger 'sandbox' environments in video games has created an explosive demand for graphical content. Our system could allow automated or semi-automated creation of photorealistic building facades for a large virtual environment."
	},
	{
		"grant":276,
		"ID": "EP/I031243/1",
		"Title": "Faster and higher quality global illumination",
		"PIID": "-124855",
		"Scheme": "Standard Research",
		"StartDate": "01/08/2011",
		"EndDate": "31/07/2014",
		"Value": "247013",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "College of Science",
		"OrgID": "125",
		"Investigators":[
		{"ID": "-124855", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Global illumination brings realistic and detailed lighting environments to Computer Graphics imagery at the expense of very high computational cost. Although most people are unaware, they experience its imagery when they have renderings of a new kitchen design, in public displays of new architecture, and in a variety of film, advertising and entertainment. The computer imagery produced by this technique can deliver the highest form of realism achievable by Computer Graphics as it models indirect light (reflected from other surfaces) and not just light direct from light sources in the scene.Such realism is now a standard requirement for the above applications and it is also gaining popularity in the virtual reality and scientific visualization communities as the advantages of increased perception and understanding are now recognised and outweigh the disadvantages of production time.Current approaches for fast global illumination place limits on the scene geometry, depth of light paths, numbers of samples, or the materials in the scene. Within these limitations it is possible to employ large caches for computation reuse (only in static scenes), or create simplified algorithms on the GPU, but always at the cost of reducing rendering accuracy.The aim of this research is to work towards real-time and accurate global illumination techniques by exploring new approaches to biased rendering methods. We will apply hierarchical techniques, clustering, ray and photon statistics and information, edge detection and combine other appropriate techniques to reduce the complexity and computation of global illumination whilst maintaining or enhancing visual accuracy.These improved timings and render quality will be demonstrated within a software package. The improved algorithms will allow the above application areas to produce higher quality images at reduced cost."
	},
	{
		"grant":277,
		"ID": "EP/I031782/1",
		"Title": "Creativity@Home: Collaborative Performance Analytics",
		"PIID": "88715",
		"Scheme": "Standard Research",
		"StartDate": "16/01/2012",
		"EndDate": "15/01/2013",
		"Value": "97343",
		"ResearchArea": "Engineering Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "88715", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal stems from discussions that took place through the creativity@home scheme piloted by the EPSRC. These discussions centered on three major societal trends that are destined to influence each other - the explosion of information, the emergence of mobile computing and the growth in social networking and open innovation. Clearly these three major trends open up new opportunities, but they also pose significant challenges. The first of which is how to make sense of the copious quantity of data available to organizations today.The world is becoming more instrumented - organizations are collecting data at ever more disaggregated levels. Services, such as credit and loyalty cards, allow banks and retailers to capture individualized data. Every mobile phone transmits location data, while every security camera records activity data. Products are increasingly instrumented. Sensors in cars and planes continuously transmit data on product performance and reliability. Every visit to a website leaves a footprint. Data is contained in every photo posted to Facebook and every tweet made to Twitter!A challenge for organizations is how to harness these data and extract insight from them. Today's context means that this challenge is complicated by three factors - (i) the simple volume of data, (ii) the fact that 80% of this data is unstructured and (iii) the fact that much of the data lies in disconnected source files - databases, excel spreadsheets and individual websites.The thesis that underlies this proposal is that parallel developments in mobile computing and social networking technologies, including open innovation, may offer a solution to the challenge of extracting insight from data. For while the simple challenge of making sense of data is not new, the novelty in this proposal lies in bringing together mobile technologies, with socially distributed ways of working, to explore the potential of what we are calling collaborative performance analytics. Collaborative performance analytics builds on the ideas of open innovation and socially distributed ways of working. It asks the question - how might organizations open up their performance data to a wider community and engage them in the analytics process. Clearly a pre-requisite would be for the wider community to have to have access to technologies that enable socially distributed working. Hence the thrust of this proposal - that we should bring together mobile technologies, with social networking applications to enable collaborative performance analytics. To explore this concept this proposal explains how we intend to create a demonstrator scenario to illustrate the potential of collaborative performance analytics."
	},
	{
		"grant":278,
		"ID": "EP/I032495/1",
		"Title": "PLanCompS: Programming Language Components and Specifications",
		"PIID": "128182",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2011",
		"EndDate": "31/10/2015",
		"Value": "695781",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "College of Science",
		"OrgID": "125",
		"Investigators":[
		{"ID": "128182", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Microsoft Research Ltd"}
		],
		"Summary": "Software comes in many different shapes and sizes: ranging from games and social networking systems through databases and office software to control system for vehicles and medical instrumentation. Regardless of its purpose, software is almost always written in high-level programming languages, which are significantly easier to use than the low-level machine languages which can be executed directly by computers.Before a program written in a high-level language can be run on a particular computer, the language needs to have been implemented on that computer. The implementation could be a compiler, which translates high-level programs to machine code; alternatively, it might directly interpret them, simulating their intended behaviour.Many hundreds of programming languages have been designed and implemented since the 1950s, and dozens are currently in widespread use. Major ones introduced since 1995 include Java, C#, Python, Ruby, OCaml, Delphi, and VBScript. Older languages evolve to incorporate new features: new versions of Fortran, Cobol, Ada, C++, Scheme and Haskell appear at intervals ranging from one to 10 years. New programming languages are continually being designed and implemented, with the aim of making it easier (or at least quicker and cheaper) for programmers to write useful software. So-called domain-specific languages (DSLs) are designed for use in a particular sector, such as banking or engineering, or particular application areas, e.g., interactive web pages; they are often obtained by extending general-purpose languages with features that correspond closely to standard concepts or notation in the targeted sector.The documentation of a language design is called a language specification. This usually consists of a succinct formal grammar, determining the syntax of the language (i.e., which sequences of characters are allowed as programs, and how they are to be grouped into meaningful phrases), together with a lengthy informal explanation of their semantics (i.e., the required behaviour when programs are run), written in a natural language such as English. Unfortunately, such explanations are inherently imprecise, open to misinterpretation, and not amenable to validation.This project will employ innovative techniques for specifying the semantics of languages completely formally. The main novelty will be the creation of a large collection of reusable components of language specifications. Each component will correspond to a fundamental programming construct, or funcon, with fixed semantics. Translation of program phrases to combinations of funcons determines the semantics of the programs, and specifying this translation is much simpler - and much less effort - than specifying their formal semantics directly. The project will test and demonstrate the advantages of this component-based approach to language specification using case studies involving specification of major programming languages (including C# and Java) and DSLs.Sophisticated tools and an integrated development environment will be designed and implemented by the project to support creation and validation of component-based language specifications. The tools will support automatic generation of correct prototype implementations directly from specifications, allowing programs to be run according to their formal semantics. This will encourage language designers to experiment with different designs before initiating a costly manual implementation of a particular design, which may lead to development of better languages.Funcon and language specifications will be stored in an open-access repository, and a digital library interface will support browsing and searching in the repository. The library will also provide access to digital copies of existing formal specifications of programming languages using previous approaches."
	},
	{
		"grant":279,
		"ID": "EP/I032606/1",
		"Title": "An information-dynamical approach to characterise and model complex systems",
		"PIID": "-260000",
		"Scheme": "Standard Research",
		"StartDate": "12/10/2012",
		"EndDate": "11/10/2015",
		"Value": "414743",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "0",
		"Investigators":[
		{"ID": "-260000", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project has as its main goal the determination of the pathways for the information flow in complex systems and complex networks. We also aim at proposing ways to determine how much information can be exchanged among the many subsystems forming those complex systems. We will then apply this knowledge to characterise, predict, and model data coming from complex systems.  These applications can be imagined as a sophisticated, simple and innovative way for conditional monitoring a complex system. By a COMPLEX NETWORK we mean networks formed by nodes whose dynamical description is known in a way that if needed we can simulate the system in the computer. Examples of complex networks that we will be considering in this project are neural networks, chaotic networks, and ecological stochastic networks.  By a COMPLEX SYSTEM we mean a system formed by a large number of subsystems (which we conveniently refer to as nodes) that interact with other subsystems in a complicated manner and is subject to the influence and interactions with its environment.  Its equations of motion, assumed to be higher dimensional, are unknown. Examples of complex systems we will be considering in this project are the weather, cardiac cells, and the human heart and brain.  The reason for studying simultaneously complex networks and complex systems is essentially because our fundamental and more mathematical oriented work in complex networks will permit us to construct and test theoretical tools to be applied in the analysis of the data coming from complex systems. Besides, the data will guide us in the right direction to derive theoretical tools aimed in solving long-lasting problems in complex systems.Measuring information transfer in complex networks and complex systems is a very difficult task.  In the real world data sets do not always contain a sufficient amount of points with a sufficient resolution to calculate accurate probability of events.  As a result, standard techniques provide often biased results with the value for the amount of information obtained depending on the resolution of the experiment and on the number of points.  For that reason, we have developed an alternative approach for measuring the amount of information exchanged between two nodes in a complex network or the amount of information shared between two data sets, without having to calculate probabilities. And this project  will test, apply and generalise this approach within the context of complex systems. For example, we will show that our approach allows one to measure information exchange not only between two nodes in a complex network but also between groups of nodes. This will allow us to measure how much information two data sets share even when the data sets have different dimensions. Another example of how the proposed approach will be helpful for treating complex systems is because it can be applied to data sets acquired with completely different sampling rates. We will separate this work into two Research projects.  Research Project 1 is devoted to study complex networks, and it will be mainly carried out by one post-doc, whereas Research Project 2 is devoted to study complex systems, and it will be mainly carried out by 2 PhD students (with assistance from the post-doc). One of the main outcomes of this project is to provide minimal models for the complex networks and the complex systems studied by creating the Network of Measured Data, which is the network that represents how nodes in the networks are functionally connected and how data in the complex systems are also functionally connected. The Network of Measured Data can be imaged as the  skeleton  of the system which will assist us in the modelling of data coming from complex systems. These models will not yield the dynamics responsible for a given data set, but will approximately reconstruct a data set based on another data set that is functionally connected to the data set to be modelled."
	},
	{
		"grant":280,
		"ID": "EP/I032916/1",
		"Title": "An integrated model of syntactic and semantic prediction in human language processing",
		"PIID": "-22951",
		"Scheme": "Standard Research",
		"StartDate": "15/09/2011",
		"EndDate": "28/02/2015",
		"Value": "329562",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-22951", "Role": "Principal Investigator"},
		{"ID": "102151", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "When humans process language, they do so incrementally: they compute the meaning of a sentence on a word-by-word basis, rather than waiting until they reach the end of the sentence. As a consequence, readers and listeners have to constantly update their interpretations as new input becomes available. Experimental evidence shows that they also make predictions about upcoming input: for example, when hearing a verbs such as  eat , the listener predicts that an object such as  soup  is likely to follow. The prediction process has two components: syntactic prediction, i.e., the structure of the upcoming input is anticipated (after  eat , an object is likely, but a subject isn't), and semantic prediction, i.e., the meaning of the upcoming input is anticipated (after  eat , a noun referring to edible things is likely, but one referring to abstract things isn't).Previous research has developed computational models of either syntactic or semantic prediction in human sentence processing. But there are currently no models that capture both processes in a single framework, despite clear experimental evidence that humans rely on both types of information when generating predictions. The aim of this project is to develop a model of human sentence processing that integrates syntactic and semantic prediction; such a model will not only make it possible to investigate an important theoretical question in psycholinguistics, but it also has important potential applications in natural language processing.Our model will bring together two key approaches in sentence processing. On the syntactic side, we will develop an incremental, probabilistic parser that generates syntactic predictions. This parser will be based on an extension of the Tree-adjoining Grammar (TAG) formalism, which in previous work has been shown to capture prediction data. The parser will be combined with a distributional model of semantics, which is the standard way of modeling word meaning in cognitive science; we will extend this model to capture sentential meaning, thus making it amenable to integration with a parser. Three distinct ways of achieving such an integration will be pursued, each corresponding to a theoretical position in psycholinguistics: the autonomous processing view, which holds that syntax and semantics operate independently, the syntax-first view, which holds that semantic processing has access to syntax, but not vice versa, and the interactive processing view, according to which the two components freely exchange information.By implementing these three approaches, and evaluating the resulting predictions against data from eye-tracking and priming experiments, we will be able to shed light on a key question in psycholinguistics, viz., how syntactic and semantic processing interact.Apart from this theoretical contribution, the project also has a practical aim: a computational model of human sentence processing can be used to determine which parts of a text are hard to understand. This information can be used to provide feedback to human writers, score essays, or correct the output of automatic language generation systems. In order to assess the potential for such applications, we will focus on one particular problem, viz., text simplification. We will develop a system that takes input text and makes it easier to read, e.g., for language-impaired readers or for language learners. Our integrated model of syntax and semantics will be used to pinpoint the difficult parts of a text, which will then be replaced by simplified passages using a technique called integer linear programming, which has previously been used successfully for text rewriting. The resulting simplified texts will be evaluated for their intelligibility in studies with human readers."
	},
	{
		"grant":281,
		"ID": "EP/I033688/1",
		"Title": "GISMO: Genetic Improvement of Software for Multiple Objectives",
		"PIID": "47895",
		"Scheme": "Standard Research",
		"StartDate": "28/10/2011",
		"EndDate": "27/10/2015",
		"Value": "502415",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "47895", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "IBM UK Labs Ltd"},
		{"Number": "1", "Name": "Motorola"},
		{"Number": "2", "Name": "nVIDIA"}
		],
		"Summary": "Humans find it hard to develop systems that balance many competing and conflicting operational objectives. Even meeting a single objective requires automated support. For example, there has been a long and rich history of research into techniques to optimise compiled code size and speed. Unfortunately, speed and size are but two of many objectives that the next generation of software systems will have to meet. Emergent computing application paradigms require systems that are not only reliable, compact and fast, but also which optimise many different competing and conflicting objectives such as response time, throughput and consumption of resources. Humans cannot be expected to optimally balance these multiple competing constraints and may miss potentially valuable solutions. Techniques are therefore required that can either automatically create code that balances many conflicting objectives or that can provide support to the human who seeks to do so.  The GISMO project seeks to do both. It will develop automated techniques to produce new versions of components of existing systems that meet newly defined objectives. After a period of running the old and new component in parallel, the programmer may decide to adopt the newly evolved component. However, the beauty of the GISMO approach is that it does not insist that the programmer must accept the evolved solution in order to be useful. The programmer can also use GISMO to explore the multi-objective candidate  solution space, gaining insight into what can be achieved by balancing several competing constraints."
	},
	{
		"grant":282,
		"ID": "EP/I033975/1",
		"Title": "Scalability and robustness in large scale networks and fundamental performance limits",
		"PIID": "-185268",
		"Scheme": "First Grant Scheme",
		"StartDate": "17/10/2011",
		"EndDate": "31/12/2013",
		"Value": "101123",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-185268", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed research will make a contribution towards the analysis and synthesis of large scale complex networks: fundamental theory will be developed and important applications will be addressed, by extending tools from control theory. Networks are present throughout the physical and biological world, but nowadays they also pervade our societies and everyday lives. Such celebrated examples include the Internet, power networks, financial markets; many other emerging applications such as platoons of vehicles, satellite formations, sensor networks; and also examples found in nature, ranging from flocking phenomena to gene regulatory networks. Major challenges that will be addressed are:1. The engineering of large scale heterogeneous networks that are guaranteed to be robust and scalable.2. The reverse engineering of biological networks.A distinctive feature of the networks we would like to engineer, which falls outside more traditional domains in systems and control, is that of scalability. Scalability here refers to the fact that network stability and robustness must be preserved as the network evolves with the addition or removal of heterogeneous agents. Imagine, for example, having to redesign congestion control algorithms each time a new computer/router enters the Internet. A main objective of the proposed research is to develop methodologies for addressing this need for scalability, i.e. be able to guarantee robust stability of the entire arbitrary network by conditions on only local interactions. Previous results in this context show that this is indeed possible by exploiting interconnection structure. Nevertheless many questions still remain unanswered. The aim is to merge less conservative linear results, with corresponding more conservative nonlinear approaches on a common solid theoretical framework. This will lead to non-conservative designs, which are thus of practical interest. These methodologies will have a significant impact on the design of Internet congestion control protocols; improved, less conservative algorithms will lead to a better utilization of the network resources. The same abstract theory can also guarantee robust stability of other networks where scalability is an issue, with the novelty lying in the heterogeneity of the participating dynamics. These include flocking phenomena, coordination of unmanned vehicle formations, distributed computations in sensor networks and other related applications such as vehicle platoons and synchronous operation in power networks.The proposed project will also make a contribution towards the reverse engineering of biological networks at the molecular level, by focusing on the analysis of intrinsic stochasticity within the cell. Life in the cell is dictated by chance; noise is ubiquitous with its sources ranging from fluctuating environments to intrinsic fluctuations due to the random births and deaths of individual molecules. The fact that a substantial part of the noise is intrinsic (and not additive) provides a major challenge in control theoretic methodologies. How can feedback be used to suppress these fluctuations, what are the associated tradeoffs and limitations, and how does nature manage to handle these so efficiently in specific mechanisms? These are questions that will be addressed with our research by developing tools for analyzing known configurations, but more importantly, by deriving fundamental limitations that hold for an arbitrary feedback policy. These hard performance bounds are a result of simple features of these processes such as the presence of delays and noisy feedback channels. Specific feedback mechanisms, such as plasmid replication control in bacteria, will be studied using this theory, thus leading to a better understanding of the underlying functionality. More broadly, feedback is present in many biological processes and understanding the underlying principles is important."
	},
	{
		"grant":283,
		"ID": "EP/I034548/1",
		"Title": "The Quest for Ultimate Electromagnetics using Spatial Transformations (QUEST)",
		"PIID": "80862",
		"Scheme": "Programme Grants",
		"StartDate": "01/07/2011",
		"EndDate": "30/06/2016",
		"Value": "4618424",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "80862", "Role": "Principal Investigator"},
		{"ID": "13851", "Role": "Co Investigator"},
		{"ID": "29233", "Role": "Co Investigator"},
		{"ID": "117015", "Role": "Co Investigator"},
		{"ID": "12031", "Role": "Co Investigator"},
		{"ID": "27648", "Role": "Co Investigator"},
		{"ID": "80129", "Role": "Co Investigator"},
		{"ID": "6475", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BAE Systems"},
		{"Number": "1", "Name": "DSTL Porton Down"},
		{"Number": "2", "Name": "Era Technology Ltd"},
		{"Number": "3", "Name": "Flann Microwave Ltd"},
		{"Number": "4", "Name": "National Physical Laboratory NPL"},
		{"Number": "5", "Name": "STFC - Laboratories"}
		],
		"Summary": "From Marconi's first transatlantic wireless transmission through Sir Henry Tizard's radar to modern cellular communications, the rapid advance of applied electromagnetics during the 20th century has changed our world. Now, in the 21st century, a new revolution in exploiting electromagnetism (EM) is emerging; one that brings together two recent developments: spatial transformations and the design and fabrication of novel electromagnetic materials. The idea of spatial transformations (ST) is to provide entirely fresh solutions to the distribution of the spatial arrangement of materials so as to enable new ways to manipulate the emission, propagation and absorption of EM radiation.  This goes far beyond what can be accomplished with traditional materials in the form of lenses and mirrors, requiring both conventional materials and also those with properties that do not exist in nature (i.e., metamaterials). ST are at the heart of exciting ideas such as invisibility cloaking and optical illusion. To make the required exotic materials in large quantities, modern fabrication techniques will be needed, including the use of nano-composites and graded-index coatings. The material palette can be further widened by the inclusion of  active metamaterials  and superconducting dielectric composites.  As an example of the type of application one may envisage, there is an increasing demand for wireless communications anywhere and at any time.  However, many environments such as offices and crowded shopping centres contain obstacles and scatterers that lead to signals being 'confused'.  Signals either reach places they ideally should not, or worse, are not accessible where they are required. Current methods try to deal with these problems by additional signal processing of the received signals, but this can only be seen as an interim fix. A more resilient solution would be to modify the local EM environment so as to ensure quality reception at any given location by, for example, making certain obstacles or scatterers 'invisible'. Materials and devices based upon the concept of STs offer the exciting prospect of warping electromagnetic space so as to overcome problems due to obstacles and scatterers. Such applications are at the heart of the QUEST project. We will build and demonstrate several devices in collaboration with defence, aerospace and communication stakeholders in the areas of healthcare, security, energy and the digital economy. QUEST solutions will place the UK in a leading position in this exciting area, pushing the conceptual boundaries whilst at the same time exploring the practical problems of design and manufacturability.The Programme Grant will bring together a new grouping of leading UK experimentalists and theorists from physics, materials science and electronic engineering to work together on the exciting opportunities and challenges emerging in the area of spatial transformations (STs) and electromagnetism (EM). The potential of the underlying ST approaches however have much wider applicability than cloaking alone, in arguably more important applications that span communications, energy transfer, sensors and security. However, theory and concepts are outstripping practical demonstration and testing, leading to a mismatch in what may be theorised and computed and what can be realised for impact in society and commerce. We contend that the timing is now ideal for UK theorists, modellers, manufacturers and engineers to work together to maintain the UK strength in this field, with a clear focus on the reduction to practice and demonstration of potentially radical new concepts and devices."
	},
	{
		"grant":284,
		"ID": "EP/I034750/1",
		"Title": "Non-Parametric Models of Phrase-based Machine Translation",
		"PIID": "-215835",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2012",
		"EndDate": "31/08/2013",
		"Value": "101251",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-215835", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Machine Translation is the automatic process of translating human language text in one language into another language using a computer. Statistical Machine Translation (SMT) is a data-driven approach to machine translation which uses machine learning techniques to learn how to translate directly from large collections of sentences and their translations. SMT has seen a surge in popularity in the last decade, and has now matured into an invaluable means for data access and communication, as evidenced by the many successful commercial SMT systems, e.g., Google Translate and Microsoft Translator. These technologies are beginning to have a substantial impact on individuals, businesses and governments by enabling communication with foreign language speakers and enabling access to the growing amounts of foreign language data. Consequently automatic translation is rapidly becoming a key technology across all levels of the community, and improvements in its quality have the potential to further increase its impact. Although current systems can produce good translations for some language pairs, such as French-English, their performance is markedly worse for many others, e.g., Chinese-English and Basque-Spanish. There are two key reasons for this: language similarity and data availability. Predominant SMT approaches do not model the structure of the language, instead assuming that translation can be performed largely at the level of single words or small groups of words. For this reason they are unable to describe large changes in word order which are commonly required for translating between dissimilar languages. For example, in Japanese sentences typically follow a subject-object-verb order, while in English sentences are subject-verb-object. In order to translate between Japanese and English the positions of the verb and object phrase must be reversed. Another reason for SMT under-performing is data availability. Current techniques require very large collections of sentences and their translations in order to learn a good translation model (needing hundreds of thousands or millions of sentence pairs), but performance suffers when considerably less data is available. For official languages of the leading countries in the West and Asia this type of data is often plentiful, however for the remaining majority of the world's languages and dialects translation data is exceedingly rare. This problem is exacerbated for languages with large vocabularies, for instance Finnish in which each word can convey a wide range of syntactic and semantic information including the gender, syntactic case, tense, number and aspect. This project will tackle both of these issues, focusing primarily on the second issue of generalising from small training sets; The novelty of our approach will help to make inroads into the first issue of dealing with word-order differences between structurally dissimilar languages. The project will develop a translation model which reframes phrase-based translation in a novel way by using much simpler translation units than phrase-based models, primarily single words and their translations, while also modelling correlations between the translations used in a sentence. This will allow the model to describe implicitly arbitrarily large translation units and thus the approach is more general that current phrase-based translation. Additionally, our approach confers a number of further benefits. Most notably, it will make better use of training data by compiling denser and more reliable statistics, and thus will generalise more accurately from small training sets. In addition our approach will support a richer set of translation fragments than current phrase-based models, including gapping phrase-pairs which can describe syntactic divergences between structurally dissimilar languages. These benefits should lead to improvements in translation accuracy across a wide range of language pairs."
	},
	{
		"grant":285,
		"ID": "EP/I035307/1",
		"Title": "NSF Materials World Network: Semiconductor photonic materials inside microstructured optical fibers",
		"PIID": "73952",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2012",
		"EndDate": "31/12/2014",
		"Value": "430917",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "73952", "Role": "Principal Investigator"},
		{"ID": "-162039", "Role": "Co Investigator"},
		{"ID": "107145", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Pennsylvania State University"}
		],
		"Summary": "The development of optical fibres led directly to the data communications revolution of the late 20th century. Today their application base has expanded and they are now impacting many other fields from remote sensing to biomedicine. This impact is growing in part because of rapid advances in active devices for which the fibre serves not merely as a passive waveguide, but as a medium to directly modulate, generate, or otherwise manipulate light. As a result of this versatility, fibres form key components of systems in almost any applications that use light.  Materials for current active fiber devices are largely limited to those that are compatible with the fiber drawing process. This multidisciplinary and collaborative project between Penn State University and the University of Southampton Optoelectronics Research Centre is focused on incorporating new materials into optical fibers to broaden the range of possible active fiber devices, focusing particularly on mid-IR applications where the fundamental rotational and vibrational structure of many organic molecules have strong, characteristic  fingerprint  absorption features. Semiconductor filled optical fibres thus have enormous potential for robust, compact, powerful and cost-effective mid-IR including recycling management of plastics and other waste reprocessing, optical gas sensors for pollution monitoring, remote sensing, industrial process control, spectroscopy, infrared countermeasures as well as medicine and health care. The broader impacts of the research include strengthening ties across disciplines and between UK and US research efforts."
	},
	{
		"grant":286,
		"ID": "EP/I035501/1",
		"Title": "Solar cells based on InGaN nanostructures",
		"PIID": "2851",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2014",
		"Value": "429592",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "2851", "Role": "Principal Investigator"},
		{"ID": "-16288", "Role": "Co Investigator"},
		{"ID": "8761", "Role": "Co Investigator"},
		{"ID": "107247", "Role": "Co Investigator"},
		{"ID": "-189092", "Role": "Co Investigator"},
		{"ID": "5494", "Role": "Co Investigator"},
		{"ID": "27409", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Arizona State University"}
		],
		"Summary": "There is a worldwide effort to increase power generation through solar cells, to meet targets in reducing greenhouse gases. One requirement is for high efficiency multijunction solar cells (MJSCs) to extract power from concentrated solar power (CSP) plants, which are expected to become central to the delivery of solar power to national and super-grid systems. At present such MJSCs must combine different materials systems, and are usually limited by the requirement to lattice-match the individual cells to avoid efficiency losses due to defects. In this proposal we aim to circumvent these problems by investigating solar cells based on InxGa1-xN, which has a direct band gap of 0.7-3.4 eV, spanning most of the visible spectrum, thus promising MJSCs from a single materials system. To avoid the problems of lattice mismatch and of material quality, which limit prototype solar cells based on InxGa1-xN epilayers to low x (x<0.3), we will grow the InxGa1-xN in nanorod form, merging the nanorods using methods we have already developed to provide a solar cell template. The team assembled, which combines complementary expertise in growth and device fabrication (U. Nottingham), structural characterization (U. Bristol), nanoscale optical and electrical characterization (Arizona State U.) and solar cell design and characterization (NREL), aims to explore the properties of InxGa1-xN single junction cells over the full composition range (0<x<1). The team will examine key fundamental properties of InxGa1-xN nanorods, using transmission and scanning electron microscopy to determine the materials requirements for growing defect-free InxGa1-xN nanorod arrays, and overcoming the problem of lattice mismatch. The work will examine the electronic properties of InxGa1-xN nanorods using novel cathodoluminescence and electron holography studies, and time-resolved photoluminescence. Single junction solar cells will be fabricated and characterized for InxGa1-xN nanorods with low and high In content, and exploratory work will be carried out into a novel two-junction nanorod cell including a tunnel junction, thus establishing the requirements for the future development of InxGa1-xN MJSC devices."
	},
	{
		"grant":287,
		"ID": "EP/I035935/1",
		"Title": "Lithium niobate integrated quantum photonics",
		"PIID": "-109524",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2016",
		"Value": "611619",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-109524", "Role": "Principal Investigator"},
		{"ID": "98012", "Role": "Co Investigator"},
		{"ID": "-95250", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "National Physical Laboratory NPL"},
		{"Number": "1", "Name": "Nokia Research Center"},
		{"Number": "2", "Name": "Quantum Technology Research Ltd"}
		],
		"Summary": "Quantum information science has the potential to revolutionise information and communications technologies (ICT) in the 21st century via secure communication, precision measurement, and ultra-powerful simulation and ultimately computation.  Photonics is destined for a central role - the photon is an ideal quantum bit, or 'qubit', for encoding, processing, and transmitting quantum information.  However, real-world applications require integrated photonic devices, incorporating photon sources, detectors and circuits. Just as the invention of the silicon integrated circuit turned the tremendous potential of the transistor into reality, this project aims to develop all necessary components to the high levels of performance and integration required to realise quantum photonic technologies. This project will be the first to simultaneously address all components and their integration simultaneously. It will thereby overcome the major challenges to realising the tremendous potential of future quantum technologies.   A key challenge in the development and application of our approach is to integrate waveguide circuits with active components: single-photon sources, phase- and amplitude-modulators and high-efficiency single-photon detectors. Our initial benchmarking and characterisation results have identified lithium niobate (LN) as the perfect material system in which to realise all of these components and thereby to create a new paradigm for integrated quantum photonics. The goals of this proposal are to fabricate all of the key devices in the LN material system and to integrate them to realise the first prototype systems.  Telecom wavelength operation will enable interfacing with existing telecom systems (existing fibre optic networks for example) and the adoption of powerful telecom technologies (modulators, wavelength division multiplexing, arrayed waveguide gratings, etc.).    The devices and systems developed in this programme will revolutionise approaches to photonic quantum technologies, paving the way to practical applications. This project brings together all of the essential expertise required to achieve these ambitious goals in world-leading groups in quantum photonic technologies and LN device fabrication (Bristol), superconducting single-photon detectors (Heriot-Watt), and superconducting thin film growth and nanofabrication (Cambridge). This proposal builds on successful work within and between these groups and has substantial support from our exisiting industrial partners (The UK National Physical Laboratory, Nokia and Quantum Technology Research Ltd.).  Over the last several years the applicants have already made great strides towards integrated quantum photonic technologies, developing waveguide-on-chip quantum photonic circuits, combined with practical superconducting single photon detectors, and non-linear photon sources.    This research proposal is extremely timely in addressing a critical bottleneck in the development of optical quantum information technologies: a single material system that can support all of the required components and their integration.  Our research programme will provide a launching pad to a new generation of compact, high performance quantum photonic devices operating at telecom wavelengths.  We adopt a highly novel and ambitious approach in migrating from silica-on-silicon waveguide circuits to LN waveguide circuits. This will enable us to integrate periodically poled lithium niobate (PPLN) photon sources, rapidly reconfigurable waveguide circuits and high performance superconducting single-photon detectors together for the first time, and to achieve high performance operation at telecom wavelengths. This approach promises a new technology platform for realising secure communication networks, precision measurement systems, simulation of important physical, chemical and biological systems, including new materials and pharmaceuticals, and ultimately ultra-powerful computers.  "
	},
	{
		"grant":288,
		"ID": "EP/I03596X/1",
		"Title": "Structures at the Interface of Physics and Computer Science",
		"PIID": "35",
		"Scheme": "Network",
		"StartDate": "01/09/2011",
		"EndDate": "31/08/2014",
		"Value": "200890",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "35", "Role": "Principal Investigator"},
		{"ID": "108991", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal aims to fund a network to support an emerging research community working at the interface of Computer Science, Physics and Mathematics. The common theme is the use of high-level mathematical structures such as Category Theory. These structures have already proved important in various specific areas within these disciplines. It has recently been becoming clear that they can be used in a unified and very powerful way, and that the same structures occur across these disciplines. This has led to a lot of exciting research, which has been bringing established researchers together, and attracting students and young researchers into the area.  The aim of the network is to foster and support these activities, and to nurture the development of an emerging research community, centred in the UK, which is at the forefront of these exciting developments.  We will do this by supporting regular meetings, providing travel money for young researchers to attend conferences and make research visits, and managing a strong web presence which can act as a hub for the community. This will help to make the UK attractive to the best researchers in the world on the topic of the network, and will enable it to take a leading role in future developments."
	},
	{
		"grant":289,
		"ID": "EP/I036052/1",
		"Title": "Multifunctional III-nitride materials",
		"PIID": "-175388",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2011",
		"EndDate": "28/02/2014",
		"Value": "79053",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-175388", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "The University of Manchester"},
		{"Number": "1", "Name": "University of Nottingham"}
		],
		"Summary": "New device materials: why?Lighting, public transport, manufacturing and personal computing - these are central to our modern lives. Unfortunately, right now, light bulbs waste 95% of the electricity we put into them, whereas the AC motors and power supplies used in transport and industry can waste up to 45% and the RAM in personal computers can waste tens of watts even when it isn't used. Given our rising demand for energy but limited fossil fuel supplies, this is a major problem! However, major energy savings can be made by improving just two basic types of electrical device; light-emitting diodes (LEDs) and transistors. In particular, we need much more efficient green LEDs (to be combined with existing red and blue LEDs to produce white light) and we need transistors that can run efficiently at very high powers and frequencies without wasting energy on standby. Such devices could also be shrunk and adapted for use in ultra-high-density computer memory. However, current materials cannot reach the performance needed for these devices, so better materials must be found. What do novel nitrides have to offer? Materials in electronic devices usually have just one main function. For example, gallium nitride works as a semiconductor in blue LEDs and high-power transistors. However, this proposal centres on creating multifunctional nitride-based materials for use in new, improved devices. Currently, some exotic materials can simultaneously act as semiconductors, ferroelectrics (i.e. they have a spontaneous, reversible electric polarization) and as magnets, but most of them are unstable, difficult to manufacture or don't work at room temperature. Instead, existing nitride semiconductors could be modified by adding metals like scandium, which generate tiny distortions in the crystal structure. These materials are particularly exciting because the distortions can produce new ferroelectric and magnetic properties which nobody thought could coexist in the nitrides. At low metal concentrations, the new materials are stable and can emit light of the right colour to replace existing, highly defective active regions in green LEDs. At higher metal concentrations, the distortions line up and the entire crystal structure changes. Such materials could then be used in transistors, where they should produce a thin switchable layer of electrons, giving a very low 'on' resistance without drawing power when 'off'. Alternatively, by detecting the presence or absence of this electron layer, we could take away the transistor 'source' and 'drain' and create dense, stackable arrays of nanometer-sized devices which could provide record-breaking data storage densities. Depending on how the materials' magnetic and electrical properties interact, multiple bits of information might even be stored simultaneously. These new materials are expected to be both robust and compatible with existing nitride processing technology, making them of great practical value. Firstly, however, their fundamental properties must be understood more fully, in order to make the most of the fascinating new possibilities they offer for the energy-efficient devices of the future. This can be done by creating and characterising the most promising materials (starting with the (Sc,In,Ga)N materials system), understanding and controlling their fundamental properties and using this knowledge to design new energy-efficient devices that best exploit these properties. Impact: Better green LEDs could help save up to 80% of the energy we use in lighting. Along with more efficient high-power transistors for industry, transport and communications, this would reduce our dependence on fossil fuels significantly. In the long term, such energy-efficient displays and power supplies could also be combined with ultra-high-density memory to give us smaller, faster, lighter computers with enormous data storage capacities and very long battery lives, benefiting almost every part of society."
	},
	{
		"grant":290,
		"ID": "EP/I037423/1",
		"Title": "A systematic study of Physical LAyer Network coding: from information-theoretic understanding to practical DSP algorithm design (P.L.A.N)",
		"PIID": "-189859",
		"Scheme": "Standard Research",
		"StartDate": "05/03/2012",
		"EndDate": "04/03/2015",
		"Value": "239625",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "-189859", "Role": "Principal Investigator"},
		{"ID": "82983", "Role": "Co Investigator"},
		{"ID": "74076", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Bell Laboratories"},
		{"Number": "1", "Name": "BP Refining Technology"},
		{"Number": "2", "Name": "Infineon Technologies France"}
		],
		"Summary": "High spectral efficiency is the holy grail of wireless networks due to the well-known scarcity of radio spectrum. While up to recently there seemed to be no way out of the apparent end of the road in spectral efficiency growth, the emerging approach of Network Coding has cast new light in the spectral efficiency prospects of wireless networks [1]. Initial results have demonstrated that the use of network coding increases the spectral efficiency up to 50% [2, 3]. Such a significant performance gain is crucial for many important bandwidth-hungry applications such as broadband cellular systems, wireless sensor networks, underwater communication scenarios, etc. Currently network coding has received a lot of attention from the wireless communication community; however, many existing works focused on the application of network coding to upper layers and the study of its impact on the physical layer (PHY) design only began recently. The aim of this proposal is to systematically study network coding at the physical layer, where we will not only characterize the fundamental limits of physical layer network coding, but also design practical digital signal processing (DSP) algorithms to realize the performance gain promised by those theoretic results. The novelty of the proposed project lies on the fact that this project will be the first UK effort to bridge information-theoretic studies and DSP algorithm design for PHY network coding. This will be done by first deriving the capacity region of network coding, which provides us the upper bound of the system performance. With such a better understanding, we will develop efficient transmission protocols and DSP algorithms to realize such optimal performance in practice. Interference alignment, a technology recently developed to cope with co-channel interference, will be applied to network coding transmissions for further performance improvement. Information-theoretic results, such as outage and symbol error probabilities, will be developed and testbed-based experimental evaluation will be carried out, so a more insightful understanding for our developed schemes can be obtained."
	},
	{
		"grant":291,
		"ID": "EP/I037512/1",
		"Title": "A Unified Model of Compositional and Distributional Semantics: Theory and Applications",
		"PIID": "-114156",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2015",
		"Value": "345414",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-114156", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Information Retrieval Facility"},
		{"Number": "1", "Name": "Metrica"}
		],
		"Summary": "The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc)  is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.  There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some 'understanding' of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.  The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the 'structural' linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of 'meaning as use'. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example, if we take a large amount of text and see which words appear close to the word 'dog', and do a similar thing for the word 'cat', we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word 'television', for example, we will find less overlap with the contexts for 'dog'. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that 'dog' and 'cat' are much closer in the space than 'dog' and 'television', indicating that 'dog' and 'cat' are closer in meaning than 'dog' and 'television'.  The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the computational study of the mind). Hence we aim to make the following fundamental contributions:  1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;  2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text. "
	},
	{
		"grant":292,
		"ID": "EP/I038357/1",
		"Title": "eFuturesXD - crossing the boundaries",
		"PIID": "11775",
		"Scheme": "Standard Research",
		"StartDate": "28/11/2011",
		"EndDate": "27/05/2015",
		"Value": "578603",
		"ResearchArea": "CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "11775", "Role": "Principal Investigator"},
		{"ID": "5628", "Role": "Co Investigator"},
		{"ID": "17488", "Role": "Co Investigator"},
		{"ID": "491", "Role": "Co Investigator"},
		{"ID": "6605", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "An EPSRC workshop (ICT research - the next decade) held in October 2010 recognised the need for ICT communities to shape future capability. In order to help achieve this goal in the electronics community over the coming years, eFuturesXD will offer awards to support UK academics in electronics as they initiate collaborative research across their discipline boundary. This tends to be research of a high risk, high return kind.  Modest awards allowing immediate progress will be an efficient way to sift numerous options and to promote the best ideas. The outcome will be a collection of short studies, which will include varying levels of promise. This will help inform the electronics community where ICT research should be heading.  By offering a response to award requests within one month of applying, this cross-disciplinary account will give the agility necessary to compete on the global stage.  Four kinds of award will be offered: travel award (< 5k); meeting award (< 10k); facility award (< 20k); and staff award (< 50k).  These awards will offer the opportunity to scope new initiatives rapidly and to assemble the best team for longer larger research programmes.  The cross -disciplinary account will be facilitated by the existing electronics community network eFutures, building on its established infrastructure and network of technical expertise.  It will also be a way of adding value to the eFutures network.  The cross-disciplinary account will be publicised through community meetings and the eFutures web pages.  A condition of obtaining an award < 15k will be to give presentations at community events.  By linking the cross-disciplinary account to eFutures it will be possible to help build the best consortia for longer larger research programmes using the network's links to the academic and commercial sectors.  The application process will involve completing an on-line form which will cover summary, objectives, academic partners, funding requested and stratelignment.  Rapid assessments will be made by two experts: one from electronics and one from the other relevant discipline.  A review panel will be held monthly when decisions will be made and then communicated to applicants.  The eFutures steering group, comprising academic, commercial and public sector representatives, will review the distribution and balance of awards across strategically important areas where the UK may take a future lead.  Success will be seen by measuring the follow-on funding by those obtaining awards.  It will also be judged by looking at the overall ICT portfolio.  Examples of success include a more coherent range of research topics within electronics and a visibly stronger engagement between electronics and the rest of ICT.  To assess the effectiveness of awards a follow-up check will be made at least 6 months after each award is complete."
	},
	{
		"grant":293,
		"ID": "EP/I038713/1",
		"Title": "Programming and Reasoning on Infinite Data Structures",
		"PIID": "-249686",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/07/2011",
		"EndDate": "31/12/2013",
		"Value": "43050",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "-249686", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The project investigates infinite objects in formal logic and computation theory. Although computer (like humans) have a limited memory and can manipulate only finite data structures, mathematics has developed theories to describe and reason about infinite abstract objects. In recent years, the computational and constructive aspects of infinity have been studied by means of new models and techniques. In functional programming languages, it is possible to represent potentially non-well-founded data by finite circular definitions that can be unfolded as needed: these are called 'lazy' data types. Programming and reasoning techniques allow users and scientists to manipulate these objects as if they were actually working with infinities, while a solid theoretical basis provides the foundation for their concrete implementation.  The research field is that of type-based proof-assistant technology. Type theory is a collection of formal systems that subsumes formal logic, constructive mathematics, and programming languages. A type theory is at the same time a logical system, a functional programming language, and an environment for the development of formalized mathematics. Types are used to represent both data structures and logical formulas, elements denote both programs and proofs: this is known as the Curry-Howard isomorphism. Modern tools based on type theory use 'coinductive' types, families, and predicates to represent potentially infinite data structures and proofs.  Their semantic justification comes from the categorical theory of final coalgebras. The formal requirements for acceptability of these constructions are growing progressively more technical and elaborate. This project (FRIS, from here on) will explore clear semantic principles that will both simplify the implementation and extend its applicability. Two lines of research will be pursued: Study generalizations of the notion of final coalgebra, for example in the line of corecursive algebras or in the context of fibration categories, to give a good mathematical understanding of infinite structures; Apply the method of reflection to coinductive objects, that is, encode inside the system a type of syntactic expressions that are interpreted as potentially infinite objects and then exploit the correspondence between formal manipulation of the codes and mathematical operations on the interpretations. The final product of FRIS will be a programming/reasoning computer system based on new mathematical principles.  Reflection is a programming and reasoning technique in formal logic: It consists in defining, inside a formal logical system, a model of the syntax of the system itself and of its rules. This allows syntactic manipulations of terms and formulas that were not possible at the base semantic level. Although the perfect correspondence between the base system and the internal representation cannot be proved formally (this fact is the kernel of Gdel's incompleteness theorem), we can still justify many useful syntactic operations by their semantic meaning. In the specific case of coinductive types, we propose the following strategy: At the foundational level, we want to characterize them semantically as actually infinite structures; at a different level, we will construct a syntactic model that delineates how these objects can be rendered and stored.  "
	},
	{
		"grant":294,
		"ID": "EP/I038780/1",
		"Title": "Dynamic Real-World Lighting for Complex Scenes with Applications to Design of the Built Environment",
		"PIID": "-110997",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2012",
		"EndDate": "31/05/2013",
		"Value": "99952",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "WMG",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-110997", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Arup Group Ltd"},
		{"Number": "1", "Name": "goHDR Ltd"}
		],
		"Summary": "Physically-based rendering is the process of creating accurate computer generated images using physically accurate materials and light sources and computing a simulation of the lighting. Physically-based rendering is needed by any discipline that requires an accurate representation of the lighting in an computer generated environment, it makes it possible to accurately predicate how a new building will affect the environment around it, it allows designers to test their products under any possible lighting environment at the click of a button, it enables archaeologists to delve into reconstructions of the past, and it allows entertainers to create realistic environments that may have never existed before.      This 12 month project will improve the realism of physically-accurate computer generated images, and the time it takes to compute them for complex lighting scenarios. The novel methods will be used for applications in the built environment through consultancy and evaluation from Arup, a global firm providing engineering design, planning and project management services in all areas of the built environment. The realism of computer generated images has been improved by a process known as image-based lighting (IBL) which relies on the capture of High Dynamic Range (HDR) images. Unlike normal images, HDR images can represent most of the real world luminance and has made it possible to capture the entire lighting at a single point in an environment.  This captured HDR image, termed an environment map is used to accurately relight a virtual environment with the captured real world luminance. This process is relatively straightforward for scenes which are directly lit by the environment map, but can be very slow to compute for scenes where the lighting needs to be computed indirectly, such as is common in applications in the built environment, for example when computing the lighting of a view from inside an office building which is entirely lit from the outside. The first goal of this project is to create a novel rendering algorithm that uses clever sampling methods which will reduce the computation of such complex scenarios to that of the straightforward case.   The capture of environment maps for IBL has, until now, due to limitations in hardware capture, been limited to static scenarios. WMG is in possession of the world's first HDR video camera which can capture up to 20 f-stops at an HD resolution of 1920x1080 pixels at 30 frames per second, making it possible to capture dynamic environment maps. goHDR a company with HDR expertise will provide their compression method that will enable the dynamic environment maps to be stored as HDR videos. When not compressed, HDR videos would consume 24MB for a single HD resolution frame. This compression will allow HDR videos to be stored with a minimal overhead over a standard video, and make processing of dynamic environment maps feasible. This will enable the second major goal of this research which is to create algorithms that can compute physically-accurate rendered animations using dynamic environment maps for dynamically change lighting for complex scenarios. "
	},
	{
		"grant":295,
		"ID": "EP/I038845/1",
		"Title": "ICT Perspectives (an ICT Next Decade proposal)",
		"PIID": "2755",
		"Scheme": "Network",
		"StartDate": "01/06/2011",
		"EndDate": "31/05/2014",
		"Value": "186602",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40",
		"Investigators":[
		{"ID": "2755", "Role": "Principal Investigator"},
		{"ID": "49056", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project will provide new ways of viewing EPSRC's ICT (Information and Communications Technology) portfolio to enable researchers, EPSRC staff and other stakeholders to better contribute to ICT strategy.  The project is divided into two main stages. The first will develop new 'Perspectives' on the current portfolio based on existing 'Grants on the Web' data, the second will use spinout workshops to kick-start the use of the Perspectives and provide feedback on best practice from cross-ICT meetings. In the first phase we will use text mining and simple visualisation techniques to provide 'at a glance' overviews of the portfolio in the form of word-clouds or similar intuitive presentations. All terminology will be derived from the community, either from the grant summaries and titles or by canvassing the community directly. The 'at a glance' views will also provide the top level of an intuitive navigation system which will allow users to rapidly drilldown to project and investigator descriptions using the natural structure inherent in 'Grants on the Web' and derived using text mining.  As well as being viewable on the web these Perspectives will also be available as a set of standalone materials and associated tools for ICT strategy and Network workshops. They will provide an accessible overview of the portfolio and its natural structure. The latter will be presented in interactive dendrogram form to allow easy partitioning for scoping more detailed workshop breakout sessions.  The second phase of the project will focus on networking. Six 'spinout' workshops will be organised by open call that will make use of Perspectives' materials and tools to facilitate brainstorming and road-mapping type activities. Meetings that bring together two or more Networks will be especially sought. "
	},
	{
		"grant":296,
		"ID": "EP/I038853/1",
		"Title": "Structured Sampling Operators for Compressive Terahertz Imaging",
		"PIID": "-354377",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/12/2011",
		"EndDate": "31/03/2013",
		"Value": "96088",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Design",
		"OrgID": "129",
		"Investigators":[
		{"ID": "-354377", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "University of Liverpool"}
		],
		"Summary": "Terahertz (THz) waves, with a frequency range of 0.1-10 THz, have huge potentials in many applications such as medical imaging, non-destructive inspection, quality control, homeland security, etc. However, most existing THz imaging cameras have high operational cost and slow scanning speed. Even with the state-of-the-art THz time-domain cameras, 6 minutes are required to acquire a 400x400 image.  Recently, single-pixel THz cameras have been developed based on compressed sensing theory with the great potential of dramatic reduction of imaging time, power consumption and implementation cost. Despite their great promises, such systems are still at the infant stage and there still exists a large gap between theory and practice. This project aims to study practically-oriented sampling operators for compressive THz cameras. We will focus on the design and development of fast computable, memory efficient, and hardware friendly sampling operators. We will also investigate their theoretical performance limits. Moreover, software package and proof-of-concept hardware implementation will be demonstrated. The results of this project may open up new opportunities for the development of fast, relatively inexpensive and even portable THz imaging cameras."
	},
	{
		"grant":297,
		"ID": "EP/J000051/1",
		"Title": "Towards Real Applications in Broadband Quantum Memories",
		"PIID": "97323",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2016",
		"Value": "886175",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Oxford Physics",
		"OrgID": "106",
		"Investigators":[
		{"ID": "97323", "Role": "Principal Investigator"},
		{"ID": "-153445", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Imagine a banknote that cannot be forged, because the serial number is scrambled every time someone tries to read it. But if you are the banker, you can read it. Sounds like Harry Potter? Imagine a computer that predicts how drugs will behave by simulating all possible chemical reactions at once! This is not an idea from Phillip Pullman's fantasy of parallel universes. Real technologies like this are just around the corner.  This is the fascinating, counter-intuitive world of quantum physics. Huge advances in communications and computing technology over the last several decades have made this the information age and changed the way people live and interact even more drastically than did the industrial revolution. These advances have piggy-backed on the development of devices such as semiconductor transistors and lasers, devices which wouldn't be possible without the weird properties of quantum physics.  But although modern computers have far-outstripped the early technology of punch cards and vacuum tube valves, at an underlying conceptual level, they still use exactly the same type of information - strings of 0s and 1s called bits. Quantum physics will allow us go far beyond this into the strange world of quantum information, where the 'quantum bits' can be both 0 and 1 simultaneously! Computers that could work with this sort of information would be exponentially faster at performing difficult simulations or cracking codes. And communicating using quantum information can be made 'eavesdropper proof' - perfectly secure.  Over the past ten years, an enormous research effort has brought these extraordinary technologies from abstract ideas to small-scale experiments. One of the most promising ways to build a quantum computer is based on single particles of light, called photons, which can be sent over long distances in optical fibres and manipulated with ordinary lenses and mirrors. But like normal computers, quantum computers need memories to be able to synchronise different parts of a computation by storing the quantum information until it is needed. So to build a photonic quantum computer, we also need to have a quantum memory that can store single photons. What makes this difficult is that these special memories need to be able to store the fragile quantum information without destroying or even 'looking' at it (measuring it).  In this project, we will develop a quantum memory for photons which can store short pulses for long times with high efficiency and very low noise. To do this, we will use a 'Raman memory', an approach pioneered in our group which uses a strong laser pulse to cause the photon to be absorbed by a sample of atoms which is normally transparent. Because the absorption is created by the strong laser (which is not absorbed), there is no noise from excited atoms, and the atoms don't need to be specially prepared by cooling them or trapping them.  The simplicity of our design will allow us to build the first practically feasible memory, which would even potentially be capable of operating in isolated, harsh environments, such as on the ocean floor. This will also allow us to perform novel photonics experiments which are too complex to operate without the memory. We will also develop a miniaturized memory that could be mass-produced and integrated with existing telecoms fibres. Such a device will do for quantum photonics what the transistor did for conventional electronics.  Quantum memories will open the way to a new era of quantum enabled devices, with super-fast computers, perfectly secure communications and ultra-precise measurements. Our research is the key to bringing these truly magical technologies to life. "
	},
	{
		"grant":298,
		"ID": "EP/J000078/1",
		"Title": "Robustly Tractable Constraint Satisfaction Problems",
		"PIID": "83531",
		"Scheme": "Standard Research",
		"StartDate": "19/03/2012",
		"EndDate": "18/03/2015",
		"Value": "78524",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "83531", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The constraint satisfaction problem, or CSP for short, provides a general framework in which it is possible to express, in a natural way, a wide variety of problems from artificial intelligence and computer science. The basic aim in a constraint satisfaction problem is to decide whether there is an assignment of values to a given set of variables, subject to constraints on the values which can be assigned simultaneously to certain specified subsets of variables (decision version, CSP), or to find an assignment satisfying a maximum number of constraints (optimisation version, Max CSP). Nowadays, the CSP is extensively used in theoretical computer science, being a mathematical object with very rich structure that provides an excellent laboratory both for classification methods and for algorithmic techniques. One particular family of CSPs that receives a great amount of attention in complexity theory are the CSPs with a fixed constraint language, i.e. with a restriction on the types of constraints.  A polynomial-time algorithm for a CSP, in general, only needs to tell satisfiable instances from unsatisfiable, i.e. it treats all unsatisfiable instances the same. When can such an algorithm be made to also identify near-misses, i.e. almost satisfiable instances  - those where a tiny fraction of constraints can be removed to make the instance satisfiable? We call this type of tractability robust. We plan to develop a new research programme investigating a notion of tractability for CSP with a fixed constraint language that combines in a natural way two very advanced (both technically and conceptually), but so far practically disjoint, directions in the theory of computation: studying classical tractability and approximability of constraint satisfaction problems via algebraic/logical and analytic methods, respectively.   "
	},
	{
		"grant":299,
		"ID": "EP/J000086/1",
		"Title": "Digital Fabrication of UHF Electromagnetic Structures",
		"PIID": "55374",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2012",
		"EndDate": "31/12/2014",
		"Value": "417757",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering & Digital Arts",
		"OrgID": "27",
		"Investigators":[
		{"ID": "55374", "Role": "Principal Investigator"},
		{"ID": "12073", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Access Wireless Ltd"},
		{"Number": "1", "Name": "Dstl"},
		{"Number": "2", "Name": "Great Ormond Street Hospital"}
		],
		"Summary": "Radio Frequency Identification (RFID) technology uses radio waves to communicate between wall-mounted or handheld reading devices and small unobtrusive labels.  RFID has greater potential than commonly used barcodes and is increasingly used to track the whereabouts of everyday items as they move through, eg, a factory or supermarket.   Obviously, if they are to compete with barcodes, they must be very cheap and printed in one way or another.  Very thin tag designs must work sufficiently on different objects.  Fabrication technologies need to be evolved to make the RFID integrated circuit connection cheaply and easily.     Although RFID has been used to illustrate the concept of printed tags, it is envisaged that skin transfer antennas could also be designed with the terminals connected by surgical plaster. Longer term possibilities involve putting high impedance surfaces directly onto the skin to manage the radio channels around the body, and even facilitate personal stealth where the visibility of a person to radar could be altered to avoid detection in a military scenario.  The technology could also be applicable for medical sensing and as a pick up for implanted devices.   The human body is an especially challenging platform for RFID owing to its high conductivity.  Human tagging, external to the body, is usually based on wrist bands or ID badges which can be removed and given to other people.  A design concept outlined in this proposal is a thin, platform insensitive UHF RFID tag that can be mounted directly onto the skin surface in the form of a transfer patch in much the same way that a temporary tattoo could be applied. This work is at an early stage and requires research into effective inkjet printing of the tag pattern onto the transfer material and reliable mounting of the tag Integrated Circuit.     The next ten years will see a rapid gain in market share of mainstream printed RFID tags. According to 'Printed and Chipless RFID, Forecasts, Technologies & Players 2011-2021' (by IdtechEX), the numbers sold globally will rise from 12 million in 2011 to 209 billion in 2021. By value, chipless versions will rise from less than $1.38 million in 2011 to $1.65 billion in 2021, about one fifth of all income from RFID tags in 2021 because most of the increase in penetration will be by price advantage'.  Frequency Selective Surfaces (FSS): The radio spectrum is a finite resource and just as is the case with oil and gas and other fuel supplies, the expanding economies are making ever increase use of it.  But whereas with oil and gas the total amount that might be available is uncertain, with new fields discovered from time to time, the extent of the radio spectrum is limited.  Almost everyone now has a mobile phone and a wireless computer, whether at home or in the office.  They all use radiowaves to communicate with each other and so all have the potential to interfere with each other.  In buildings, the walls and office furniture hinder efficient communications but at the same time are often insufficiently hindering to prevent unwanted evesdropping.  FSS are flat pieces of thin material somewhat like wall paper with printed patterns.  These patterns if drawn correctly have the potential to either enhance or seriously reduce the travel of radiowaves from place to place in the building.  In other words they influence the electromagnetic architecture of the buildings.  By careful configuration this means that more computers can be installed in a building without interacting with each other if that is required, while at the same time unwanted interference from other computers or other cell phones can be reduced improving the speed of broadband provision and avoiding lost phone calls.  Inkjet conducting ink processes will make these technologies inexpensive and widespread."
	},
	{
		"grant":300,
		"ID": "EP/J000396/1",
		"Title": "Transparent organic electronics based on graphene",
		"PIID": "29233",
		"Scheme": "Standard - NR1",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2014",
		"Value": "90544",
		"ResearchArea": "Graphene and Carbon Nanotechnology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "46",
		"Investigators":[
		{"ID": "29233", "Role": "Principal Investigator"},
		{"ID": "-361397", "Role": "Co Investigator"},
		{"ID": "-297059", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "The University of Manchester"},
		{"Number": "1", "Name": "Tohoku University (Japan)"},
		{"Number": "2", "Name": "University College London"},
		{"Number": "3", "Name": "University of Bath"},
		{"Number": "4", "Name": "University of Southampton"},
		{"Number": "5", "Name": "University of Tokyo"}
		],
		"Summary": "Transparent organic electronic and optoelectronic devices are nowadays emerging technologies for future applications, for example in smart windows and in photovoltaic cells. The attributes of organic materials include large and ultrafast nonlinear optical responses and large colour tuneability. However, the electrical conductivity of organic materials is usually poor and this limits their utility. Here we propose to pursue a new type of organic material for such applications, a material that has a high electrical conductivity and thus has the potential to revolutionise the field: the material is graphene. This is a sheet of carbon just one atom thick, with spectacular strength, flexibility, transparency, and electrical conductivity. The proposed project is directed specifically at tuning the electronic properties of graphene in order to allow the potential of this material to be exploited in transparent electronic and optoelectronic devices. The outputs of the project, the development of graphene-based transparent devices, will be fundamental to the commercial and the economic development of transparent electronics. So far, chemical functionalization of graphene with different molecular species revealed that each molecular specie can be used to accumulate electrons or holes in graphene ( that is n- or p-type doping of graphene). This suggests the possibility that different doping of adjacent graphene areas can be used to engineer electron/hole interfaces also known as p-n junctions, which are the core of large part of nowadays electronic devices. Other chemical species such as hydrogen and fluorine atoms attached to graphene can modify its band structure by opening a band gap in the otherwise zero-gap semimetallic material, providing the opportunity to use graphene as a truly organic semiconductor. The potential afforded by the chemical functionalization of graphene materials is still in its infancy, and it holds great promise for future integrated optoelectronics.  The tremendous advantages of integrating devices on the same chip in electronics naturally suggest that the same be done with electronic and optoelectronic devices. However, integration of optoelectronic devices has proven to be a difficult challenge because of inherent incompatibilities. For example, a light-emitting diode based on a p-n structure has a structure quite different from the structure of any transistor. The exploitation of graphene will allow this incompatibility to be transcended. Intelligent schemes of functionalization of graphene hold the promise to accomplish the patterning of transparent standard resistors, capacitors and transistor structures integrated with light-emitting and detecting devices which constitutes a fundamental step towards applications such as smart windows. This pioneering research is at the core of this proposal.  "
	},
	{
		"grant":301,
		"ID": "EP/J000469/1",
		"Title": "Nonvolatile atom transistors and low-power logic systems",
		"PIID": "-179793",
		"Scheme": "Standard - NR1",
		"StartDate": "20/06/2011",
		"EndDate": "19/06/2014",
		"Value": "90574",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-179793", "Role": "Principal Investigator"},
		{"ID": "-111019", "Role": "Co Investigator"},
		{"ID": "-234738", "Role": "Co Investigator"},
		{"ID": "116967", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hitachi Cambridge Laboratory"}
		],
		"Summary": "The aim of this project is to design and demonstrate novel non-volatile and extremely low-power logic systems by hybridizing the newly developed three-terminal metal oxide device, Atom Transistor and nano-electro-mechanical (NEM) systems for future beyond von Neumann computing. The hybrid systems are investigated theoretically and experimentally using world-leading nanotechnologies of the NIMS team in Japan and the Southampton team in the UK. The operation of the systems is studied both on the device and circuit levels by using a multi-scale hybrid modelling. Basic circuits such as inverters and power management systems are designed utilizing the unique characteristics of the Atom Transistors and NEM switches. The duality of volatile and non-volatile operations of the Atom Transistors enables to design a new type of non-volatile logic systems which is similar to neurons in the brain. A novel bistable sleep transistor is also designed based on the recently developed suspended-gate silicon nanodot memory (SGSNM) technology for advanced power management architectures. Prototyping the systems is carried out jointly by transferring samples as well as technologies between the two teams, and electrical testing the circuits is conducted using state-of-the-art characterization tools at Southampton Nanofabrication Centre. The first demonstration of the revolutionary non-volatile logic systems towards information technology in the next generation will provide opportunities to companies for further development and commercialisation in both Japan and the UK. This will thereafter contribute to further enhance the existing economic relations between the two countries as well as strengthening their economies."
	},
	{
		"grant":302,
		"ID": "EP/J00104X/1",
		"Title": "Fast, Locally Adaptive Inference for Machine Learning in Graphical Models",
		"PIID": "-283417",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2014",
		"Value": "93717",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-283417", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Graphical models are a powerful tool in machine learning with successful applications in diverse areas such as medical diagnosis, natural language processing, robotics, speech recognition and analysis of genetic data.  Despite this success, modern data sets place new demands on the graphical modelling framework, because the models can be enormous, but exact inference in graphical models is intractable.  Despite the extensive literature on approximate inference, there is still a huge gap between the largest data sets that we wish to analyse and the largest graphical models that we can handle.  In order to meet the challenges of these new applications, this project concerns new approximate inference algorithms for the large-scale graphical models that arise in practical applications of machine learning.  Very few existing inference algorithms can handle extremely large models with continuous variables, and important classes of inference algorithms, such as Monte Carlo techniques, have not been scaled to such models at all.  Computationally efficient  inference would significantly expand the range of applications to which the graphical modelling framework can be applied. "
	},
	{
		"grant":303,
		"ID": "EP/J001058/1",
		"Title": "The Integration and Interaction of Multiple Mathematical Reasoning Processes",
		"PIID": "39488",
		"Scheme": "Platform Grants",
		"StartDate": "01/08/2011",
		"EndDate": "31/07/2015",
		"Value": "1140286",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40",
		"Investigators":[
		{"ID": "39488", "Role": "Principal Investigator"},
		{"ID": "77443", "Role": "Co Investigator"},
		{"ID": "79430", "Role": "Co Investigator"},
		{"ID": "-223268", "Role": "Co Investigator"},
		{"ID": "2244", "Role": "Co Investigator"},
		{"ID": "14582", "Role": "Co Investigator"},
		{"ID": "77481", "Role": "Co Investigator"},
		{"ID": "98328", "Role": "Co Investigator"},
		{"ID": "-99305", "Role": "Co Investigator"},
		{"ID": "10807", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed Platform Grant renewal will be used to provide essential  infrastructure and exploratory activities that will support a portfolio of  projects that focus on the automation of mathematical reasoning processes,  including their analysis, development and interaction. We can be broadly  classified by our 'holistic' perspective of automated reasoning. Central to  this theme is the interplay between representation and reasoning. Discovering  the 'right' representation can often dramatically simplify the reasoning  required in solving a problem. Conversely, meta-level reasoning, and in  particular proof-failure analysis, can often provide guidance in evolving  the 'right' representation.   The renewal of the Platform Grant will enable us to maintain and strengthen the momentum that has been build up around this theme -- both in terms of basic research as well as applications: The former covers a spectrum of topics, including: cognitive aspects of theory formulation and reformulation;  mathematical discovery and automatic theorem generation; ontology creation, repair and evolution; proof procedures; proof planning; AI problem reformulation; quantum computation; computational creativity; visualisation of reasoning processes.  The latter covers wide ranging applications such as: software verification; formal modelling of software intensive systems; graphics design; games design;  disaster recovery planning.   Our work is a unique blend of the techniques of artificial intelligence  and theoretical computer science -- we also believe we are unique in our holistic perspective of automated reasoning and mathematical discovery."
	},
	{
		"grant":304,
		"ID": "EP/J001384/1",
		"Title": "Towards a New Generation of Matrix Learning  Methods in Machine Learning",
		"PIID": "-251053",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2012",
		"EndDate": "26/06/2013",
		"Value": "99568",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "46",
		"Investigators":[
		{"ID": "-251053", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Machine Learning (ML) has been a very active field in computer science over the past two decades.  The interplay between ML, statistics and numerical optimisation is becoming increasingly fruitful.  In particular, the understanding of optimisation within ML is a process that has just begun and has recently received most of the attention.  This project will consider challenging optimisation issues in the context of matrix learning in ML.  Most of the research in this area has been based on 'recycling' knowledge and general-purpose softwares acquired from numerical optimisation research.    In other words, their special structures related to specific ML tasks are largely ignored which hampers their applications to large-scale datasets.  At the same time, the modern technologies are creating a huge number of high-dimensional and large-scale datasets, which is particularly true in industry,  e-commerce, life science and computer vision.  We believe that it is now the time to build on the success of the existing approaches to develop a new generation of  matrix learning methods for high-dimensional and large-scale data analysis.  The main theme in this proposal is to develop a  completely new eigenvalue optimisation framework for various matrix learning problems in ML by exploring  their  special convex structures.  This new framework  will not only provide  new insights into matrix learning problems in ML but also, most importantly, the beautiful mathematics underlying eigenvalue optimisation will greatly facilitate the design of  efficient algorithms. More powerful ML  methodologies than generic SDP solvers and exiting first-order methods will arise  from the  innovative  interaction between ML and eigenvalue optimization.  Therefore, the  proposed research theme represents a significant shift in emphasis.  Specifically, this proposal aims to develop a new line of matrix learning methods in ML  by exploring their special structures in convex optimisation which includes developing new  models,  designing efficient  optimisation algorithms and rigorously establishing their convergence characteristics.  Extensive empirical studies will be carried out to illustrate the potential of the new methods developed and refine the state-of-the-art results. In particular, we will implement the algorithms by means of user-friendly software tools, and apply them on two large and challenging application problems:  face identification (e.g. Labeled Faces in the Wild dataset from Yahoo News)  and collaborative filtering (prediction of users'  preferences to products).   "
	},
	{
		"grant":305,
		"ID": "EP/J001597/1",
		"Title": "LIQUID CRYSTALLINE HYBRID DIELECTRICS FOR MONODOMAIN ORGANIC SEMICONDUCTORS",
		"PIID": "11778",
		"Scheme": "Standard Research",
		"StartDate": "19/03/2012",
		"EndDate": "18/03/2015",
		"Value": "517190",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physical Sciences",
		"OrgID": "57",
		"Investigators":[
		{"ID": "11778", "Role": "Principal Investigator"},
		{"ID": "42156", "Role": "Co Investigator"},
		{"ID": "-245138", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Flexink Ltd."},
		{"Number": "1", "Name": "Merck Speciality Chemicals Ltd"}
		],
		"Summary": "The twentieth century saw an explosion in semiconductor electronics from the first transistor, which was used in hearing aids, to the ultrafast computers of today. A similar surge is anticipated for Plastic Electronics based on a new type of semiconducting material which is soft and flexible rather than hard and brittle. Plastic Electronics is considered a disruptive technology, not displacing conventional electronics, but creating new markets because it enables the printing of electronic materials at low temperatures so that plastic, fabric, paper and other flexible materials can be used as substrates. Printing minimises the waste of materials and low cost roll-to-roll manufacturing can be used because the substrates are flexible. New applications include intelligent or interactive packaging, RFID tags, e-readers, flexible power sources and lighting panels. The organic field effect transistor (OFET) is the fundamental building block of plastic electronics and is used to amplify and switch electronic signals. The organic semiconducting channel connects the source and drain electrodes and is separated from the gate electrode by an insulating dielectric. A positive/negative gate voltage induces negative/positive charges at the insulator/semiconductor interface and so controls the conductivity of the semiconductor and consequently the current flowing between the source and drain. The future success of the industry depends on the availability of high performance solution processable materials and low voltage device operation. The semiconductors must have high electron and hole mobility (velocity/electric field) achieved by the hopping of carriers between closely spaced molecular sites. A new class of lamellar polymers, mostly developed in the UK, provides the required state-of the art performance because of their macromolecular self-organisation. However a major problem is that the materials are only well-ordered in microscopic domains; trapping in grain boundaries and poor interconnectivity between domains substantially reduce performance and reliability. The low voltage operation of OFETs requires that the gate insulators have a high dielectric constant.  We propose novel insulating dielectrics for OFETs to simultaneously align the plastic semiconductors and ensure low voltage operation. They will be solution processable at low temperatures for compatibility with printing and other large area manufacturing techniques. We will synthesise and characterise the new materials and test their performance using state of the art semiconductors. We will engage with industrial end-users to ensure that our technology is exploited so contributing to the high-tech economy in an area where the UK is already pre-eminent. We anticipate that our novel insulators will provide monodomain order over large areas to the overlying semiconductor and so will enhance OFET performance and stability. Hence we aim to hasten the commercialisation of Plastic Electronics.  "
	},
	{
		"grant":306,
		"ID": "EP/J001627/1",
		"Title": "Study of semi-polar and non-polar nitride based structures for opto-electronic device applications",
		"PIID": "21646",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2014",
		"Value": "387716",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "93",
		"Investigators":[
		{"ID": "21646", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Over the last 10 years gallium nitride based light emitting diodes (LEDs) have found widespread use as the active light emitting element of various optical displays, ranging from traffic lights to large area displays in, for example, sports stadia. This revolution in display technology has occurred because gallium nitride LEDs have not only the ability to generate blue and green light but also very efficiently, both of these attributes were not previously possible with other types of LED. Despite this revolutionary leap forward in display technology gallium nitride LEDs  offer still further opportunities of developing not only even more efficient displays that can be used in televisions but also very efficient lighting systems, so called Solid State Lighting (SSL). At the heart of most modern televisions is a liquid crystal display unit that is capable of displaying today's high definition programs. The liquid crystal display works by either transmitting or absorbing light when an electrical signal is applied to the crystal. For this to occur the light that is shone from the back of the crystal towards the viewer has to be polarised in a particular direction, i.e. the maxima and minima that make up the light wave light lie in a particular direction. Conventional light sources, including the latest generation of LED, emit unpolarised light so to make the light suitable for use in a liquid crystal based television means that the light has to be passed through a light polariser thus rejecting approximately 50% of the emitted light. Clearly this is an inefficient system and the overall efficiency of television displays would be greatly improved if an efficient light source could emit polarised light. By growing the nitride based LEDs on new forms of template, so-called semi-polar and non-polar crystals, it is possible to fabricate polarised light sources offering us the possibility of significant energy savings. At the moment the fundamental scientific questions that govern not only how well the light is polarised but also efficiency of the light generation process are not understood. In this program we will investigate these issues by making a comprehensive study of both the materials and underlying physics that will enable the fabrication of a new generation of liquid crystal based displays for inclusion in low power consumption televisions. SSL is viewed as the most likely replacement for incandescent light bulbs and the current generation of compact fluorescent lamps. From this application alone the scale of the potential for energy saving can be judged by the following: 'By 2025, SSL could reduce the global amount of electricity used for lighting by 50%. In the US alone this would alleviate the need for 133 new power stations (1000 MW each), eliminate 255 million metric tons of CO2 and save $115 billion of electricity costs.'  (The Promise of Solid State Lighting for General Illumination, US Department of Energy, 2000). The basis for SSL  systems is that white light can produced by either using the combined output of a blue light emitting LED and a yellow light emitting phosphor or be combining the output from blue, green and red light emitting LEDs. The highest light generation efficiency achieved so far is ~70%, while in its self is a remarkably high figure for SSL to be employed in our offices requires efficiencies approaching ~90%. This step forward has so far proved impossible and it is widely believed that this is due to intrinsic reductions in the rate of light emission caused by internal electric fields. These fields can be reduced or eliminated by the growth of LEDs on semi-polar or non-polar templates. The promise of highly efficient LEDs using this methodology remains unfulfilled principally due to the difficulties of growing crystals of the required quality. We anticipate by using novel and improved methods of crystal growth that these problems can be overcome allowing the promise of SSL to be fulfilled. "
	},
	{
		"grant":307,
		"ID": "EP/J002224/2",
		"Title": "Logical Foundations of Resource",
		"PIID": "-95896",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "02/06/2012",
		"EndDate": "01/12/2016",
		"Value": "423410",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-95896", "Role": "Fellow"}
		],
		"Collaborators":[
		],
		"Summary": "*Resource problems* are pervasive in computer science and the real world; indeed, the fundamental concept of computation is inextricably linked with the concept of resource (time, memory, etc.). Logic provides a powerful and convenient method for expressing and reasoning about properties of resource, and various resource-oriented logics have been advanced for this purpose in the past. Arguably the most successful application of logic-based resource reasoning to date is the use of *separation logic* and its relatives, based on *bunched logic*, to verify memory-manipulating and concurrent computer programs. The techniques employed are, however, highly specialised to the many domain-specific properties of the verification problem; thus they do not straightforwardly transfer to other domains.  While the aforementioned advances are significant, we propose that resource-oriented logics  can be used to stage a much more wide-ranging and coherent attack on resource problems in general, in line with the central role of resource in a very broad spectrum of application domains. This will be achieved by providing unifying, foundational resource concepts and using these concepts to develop novel applications.  Our plan is to take resource reasoning in two main new directions. The first direction is to take a much more general view of resources themselves. For example, one can consider resources which *dualise* (e.g. assets and liabilities in a financial portfolio) or which can be assembled in several different ways (much like LEGO construction bricks). The second direction is to consider not just verification but a variety of other practical resource problems, including resource allocation, scheduling, abduction and planning.  These correspond to the way that resource problems arise in a number of fields, but have until now been little addressed by resource logics.  We propose that, using suitable resource logics to express resource properties, all of the resource problems above can in fact be recast essentially as *proof search* problems. Such an approach has the potential to significantly unify these diverse resource problems, and open the way for symbolic approaches to them, which could lead to more scalable solutions (as in, e.g., symbolic model checking). Solving these proof search problems will then require search algorithms of considerable sophistication, since the search space may be far too large to explore exhaustively. We plan to employ techniques from automated theorem proving, and from reinforcement learning as used in agent-oriented computing. By combining these techniques with our symbolic methods based upon resource logics, we aim to develop formal methods that are both powerful and widely transferable.  If this proposal achieves its research aims then we expect a significant impact on the way that resource allocation, planning and other related resource problems are handled. These problems are fundamental not only to computer science and its various subfields (e.g. distributed systems, agent-oriented computing, and artificial intelligence) but also to other fields such as economics, engineering, environmental science and finance, and to UK industries such as software, electronics, utility provision, transportation and manufacturing."
	},
	{
		"grant":308,
		"ID": "EP/J002305/1",
		"Title": "Charge Carrier Dynamics and Molecular Wiring in Hybrid Optoelectronic Devices",
		"PIID": "-253397",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2016",
		"Value": "722816",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-253397", "Role": "Fellow"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Federal Polytechnic School of Lausanne"},
		{"Number": "1", "Name": "Tata Steel UK"},
		{"Number": "2", "Name": "University of Oxford"},
		{"Number": "3", "Name": "Weizmann Institute of Science"}
		],
		"Summary": "Charge Carrier Dynamics and Molecular Wiring in Hybrid Optoelectronic Devices  In the past decade there has been an explosion of interest in electronic devices using alternative materials to silicon, the traditional 'workhorse' of the semiconductor and photovoltaic industry. These alternative materials such as conducting molecules, polymers, metal oxides and metal sulphides are attractive because they can be deposited in a wide range of shapes and sizes from solution or printed onto flexible substrates. This allows low temperature, cheaper and more versatile manufacturing. They can be used to make devices such as next generation sensitised solar cells (SSCs), hybrid organic/inorganic solar cells and light emitting diodes (LEDs). These promise higher performance/cost ratios than conventional semiconductor devices. However the cost and processing advantages typically come at the expense of poor electrical charge transport due to the nature and processing of the materials used. This can lead to relatively low power conversion efficiencies in solar cells and LEDs based on these materials. Additionally variations in the energy of electrons (workfunction) at the interface between phases such as polycrystalline metal oxides and conducting polymers can cause non-ideal device behaviour leading to poor transfer of charge between the components and a corresponding loss in performance.  My proposal will address these problems by exploiting an interesting phenomenon where electrical charge is transferred between neighbouring molecules in a single continuous layer attached to a surface. This concept, known as two dimensional molecular wiring, has recently been applied in new battery technology which contains components with very poor conductivity. It allows more efficient collection of charge from the interface between the two phases in the electrochemical cell, allowing more rapid charging/discharging and more space for the active ingredients.  I will work with the leaders in this field (EPFL in Switzerland) to apply molecular wiring to SSCs and hybrid optoelectronic devices. I have a strong research background related to dye sensitised solar cells (DSSCs, a particular class of SSC), and have recently been involved in a study that demonstrated molecular wiring between dye molecules covering the surface of titanium dioxide nanoparticles in liquid electrolyte DSSCs.  By applying  molecular wiring in solid state SSCs and hybrid cells I hope to increase the separation and collection of charge from regions within the devices which are electrically isolated. This would lead to improved photocurrents and power conversion efficiencies. Additionally molecular wiring at oxide electrode interfaces should help to reduce variation in electron energy at interfaces by allowing neighbouring regions to reach equilibrium. This would be very attractive for the plastic electronics industry.  At Imperial College I have developed a unique series of measurements that will allow me to test the effectiveness of different configurations of molecular wiring in the solar cells. I will also work with Prof. Nelson who has expertise in molecular, charge and energy transport modelling to develop a method to screen suitable molecular wiring candidates to incorporate into the devices. I will compare these theoretical calculations with direct measurements of the conductivity through of layers of the molecules using a conducting atomic force microscope coupled with steady state and transient light sources. The results will help to direct researchers at EPFL and elsewhere towards synthesising new functional molecules for these applications. This strategy will be combined with increases in the charge separation efficiency in SSCs leading to easy to manufacture, higher performance devices."
	},
	{
		"grant":309,
		"ID": "EP/J002356/1",
		"Title": "Coherent detection and manipulation of terahertz quantum cascade lasers",
		"PIID": "-94516",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2016",
		"Value": "695589",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "-94516", "Role": "Fellow"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Rutherford Appleton Laboratory"}
		],
		"Summary": "The terahertz (THz) region of the electromagnetic spectrum spans the frequency range between microwaves and the mid-infrared. Historically, this is the most illusive and least-explored region of the spectrum, predominantly owing to the lack of suitable laboratory sources of THz frequency radiation, particularly high-power, compact, room-temperature solid-state devices. Nevertheless, over the past decade, THz frequency radiation has attracted much interest for the development of new imaging and spectroscopy technologies, owing to its ability to discriminate samples chemically, to identify changes in crystalline structure, and to penetrate dry materials enabling sub-surface or concealed sample investigation.  One of the most significant recent developments within the field of THz photonics has been the THz quantum cascade laser (TQCL). These high-power compact semiconductor sources have opened up a host of new opportunities in the field of THz photonics and have attracted significant research interest world-wide. However, there is the need to develop techniques for measurement of the phase of the radiation field emitted from TQCLs, thereby providing a complementary technology to currently established incoherent detection schemes. Furthermore, there is a need to explore fully the advances that can be made through control and manipulation of the phase of the THz field emitted by TQCLs. My vision is to initiate a range of research programmes with the aim of probing, manipulating and utilising the coherent nature of TQCL radiation. This will lay the foundations for a wealth of research opportunities in THz photonics, as well as facilitating the exploitation of THz technology for fundamental science and also for real-world applications.  I will develop both optical and electronic techniques for coherent detection/measurement of the field emitted by TQCLs. One means of achieving optical coherent detection is through the up-conversion of the phase and amplitude of the THz field into the near-infrared band with an electro-optic (EO) crystal. This approach will also allow the large field amplitudes and narrow line-widths of TQCLs to be exploited, enabling QCL radiation to be sampled using a broad-area EO crystal and a standard optical CCD. This will open up a significant range of opportunities for exploiting well developed visible/near infrared detector and CCD technologies within THz science. In parallel, I will develop coherent detection techniques by down-conversion of the THz field to radio frequencies. I will accomplish this through heterodyne phase-locking the fields from two TQCLs using a Schottky diode.  I will investigate coherent detection using self-mixing in TQCLs. This method relies on sensing junction voltage perturbations induced by feedback of the radiation field into the TQCL cavity, enabling coherent detection of the field using a single TQCL device as both source and detector. Using this approach, linewidth narrowing in TQCLs will be investigated, as well as techniques for three-dimensional 'detector-less' imaging and tomography.  I will also establish a programme concentrating on the radio-frequency control and manipulation of the THz field through the use of dynamic and static gratings, generated and controlled via the interaction of surface acoustic waves (SAWs) with TQCL devices. This approach will be used to provide a non-contact means to apply a potential modulation to TQCL devices, thereby providing a distributed feedback mechanism for the THz wave. As part of this I will develop TQCLs with reduced active regions thicknesses and TQCL mesa structures.  The combination of all these technologies will be combined to demonstrate the first 2D phase-sensitive THz tomography system using QCLs, the first full-field imaging system combining TQCLs and commercial CCD technology, and high-resolution THz gas spectroscopy. "
	},
	{
		"grant":310,
		"ID": "EP/J002526/1",
		"Title": "Deep architectures for statistical speech synthesis",
		"PIID": "-170420",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/11/2011",
		"EndDate": "31/10/2016",
		"Value": "741163",
		"ResearchArea": "Human Communication in ICT",
		"Theme": "Information and Communication Technologies",
		"Department": "Centre for Speech Technology Research",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-170420", "Role": "Fellow"}
		],
		"Collaborators":[
		],
		"Summary": "Speech synthesis is the conversion of written text into speech output. Applications range from telephone dialogue systems to computer games and clinical applications. Current speech synthesis systems have a very limited range of difference voices available. This is because it is complex and expensive to create them.  Unfortunately, that is a big problem for many interesting applications, including one we are focusing on in this proposal: assistive communication aids for people with vocal problems due to Motor Neurone Disease and other conditions. At the moment, these people are forced to use devices with inappropriate voices, very often in the wrong accent and sometimes even of the wrong sex! This is a disincentive for them to communicate, even with their own family, since they do not 'own' the voice and it does not reflect their identity. The voice is an integral part of identity, and we are creating the technology to allow people to communicate in their own voice, when their natural speech has become hard to understand or they can no longer speak at all.  The technology we will develop has a lot of other applications too: it will enable a speech synthesiser to adjust not only the speaker identity but many other properties too. For example, adjusting speaking effort will simulate what human talkers do in noisy conditions to make their speech more intelligible. Our starting point is a technique we have pioneered, called speaker adaptation.  Speaker adaptation has proven to be highly successful in enabling the flexible transformation of the characteristics of a text-to-speech synthesis system, based on a small amount of recorded speech. It can be used for changing the characteristics of the speech to a different speaker or speaking style. However, current methods do not use any deep knowledge about speech and does not generalise across similar situations. This is considerably less natural and flexible than human speech production, in which speech is controlled by human talkers based simply on prior experience. For instance, we effortlessly adapt our speech in noisy environments, compared with quiet environments, in order to increase intelligibility. The current adaptation techniques that we have pioneered are completely automatic, but they do not enable this prior knowledge to be incorporated in a straightforward way.  In some preliminary work, we have developed a model which includes information about the movement of the speech articulators: the tongue, lips and so on. Then, using our knowledge of how humans alter their speech production in the presence of noise (hyper- & hypo-articulation), we have demonstrated that it is possible to improve the intelligibility of synthetic speech in noise.  The current proposal is to extend and generalise this preliminary work, in order to integrate many other types of knowledge about human speech into this model. We will develop a new model which allows us to include more information about how speech is produced, as well as information about how it is perceived and how external factors, such as background noise, affect speech.  One important application of this technology is to create personalised speech synthesis for people with disordered speech (caused by Motor Neurone Disease, for example). Current technology for creating voices does not work for these people, because their speech is usually already disordered. Our technique can actually correct this, and produce speech which sounds like the person, but is more intelligible than their current natural speech. We have already produced a proof-of-concept system demonstrating that this works. The current proposal will make the technology available and affordable to a wide range of people. "
	},
	{
		"grant":311,
		"ID": "EP/J002607/1",
		"Title": "Foundational  Structures for Compositional Meaning",
		"PIID": "-185805",
		"Scheme": "Career Acceleration Fellowship",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2016",
		"Value": "529968",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-185805", "Role": "Fellow"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Google Inc"},
		{"Number": "1", "Name": "University of Cambridge"},
		{"Number": "2", "Name": "University of Utrecht"}
		],
		"Summary": "Words are the building blocks of sentences, yet meaning of a sentence goes well beyond meanings of the words therein.  Indeed, while we do have dictionaries for words, we don't seem to need them to  infer the meaning of a sentence from meanings of its constituents. Discovering the process of meaning assignment in natural languages is one of the most foundational issues in linguistics and computer science, whose findings will increase our understanding of cognition and intelligence and may assist in applications to automating  language-related tasks, such as  document search as done by Google.  To date, the compositional logical  and the distributional probabilistic models  have provided  two complementary partial solutions to the problem of meaning assigning in natural languages.  The logical approach is based on classic ideas from mathematical logic, mainly Frege's principle  that meaning of a sentence can be derived from the relations of the words in it. The distributional model is more recent, it can be related to Wittgenstein's philosophy of `meaning as use', whereby  meanings of  words can be determined from their context.  The logical models have been the champions on the theory side, whereas in practice  their probabilistic rivals have provided the best predictions.  This two-sortedness of defining properties of meaning:  `logical form' versus `contextual use',  has left the question of `what is the foundational structure of meaning?' even more open a question than before.   This project has ambitious and far-reaching goals;  it aims to bring together these two complementary concepts to tackle the question. And it aims to do so  by bridging the fields of linguistics,   computer science, logic, probability theory,  category theory, and even physics. Its scope is foundational,  multi and inter disciplinary, with an eye towards applications.     Meaning assignment is a dynamic interactive process involving grammar and logic as well as meanings of words. Both of the two existing approaches to language miss a crucial aspect of this process: the logical model ignores meanings of  words, the distributional model  ignores the grammar and logic. We aim to model   the entire dynamic process alongside the following three strands of integration, foundations, and applications.   (I) In integration we develop a process of meaning assignment that acts with the compositional forms of the logical model on the contextual word-meaning entities of the distributional model.   (II) In foundations, we go beyond classical logical principles of compositionality  and context-based models of meaning to develop more fundamental processes of meaning assignments based on novel information-flow techniques, mainly from physics,  but also from other linguistic approaches and other models of word meaning,   such as  ontological domains  and conceptual spaces.   (III) In applications, we evaluate our theories against naturally occurring data and apply the results to practical issues based on meaning inference and similarity, e.g. in search.  To be able to work with logical connectives in Google, one needs to re-enter them by hand in the `advanced search' tab, by manually decomposing the logical structure of the sentence and moreover  providing the extra context for their different meanings.   This is fundamentally non-compositional and goes against the spirit of automated search.   It is  exactly here that the lack of compositional methods in meaning assignment causes practical problems and where our  compositional methods become of use. Hence, we aim to put forward  our results to tackle such problems, e.g.  to be able to use our sentence similarity models for paraphrasing, question-answering, and  retrieving documents that have the same meaning and/or are about the same subject.  Our proposed partnership with Google, ensures access to real life data and helps implementation and applicability of our methods in small and large scales."
	},
	{
		"grant":312,
		"ID": "EP/J003247/1",
		"Title": "Real Geometry and Connectedness via Triangular Description",
		"PIID": "3819",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2015",
		"Value": "359554",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "3819", "Role": "Principal Investigator"},
		{"ID": "1795", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Maplesoft"}
		],
		"Summary": "Connectedness, as in 'can we get there from here', is a fundamental concept, both in actual space and in various abstract spaces. Consider a long ladder in a right-angled corridor: can it get round the corner? Calling it a corridor implies that it is connected in actual three-dimensional space. But if we consider the space of configurations of the ladder, this is determined by the position and orientation of the ladder, and the `corridor' is now the requirement that no part of the ladder run into the walls - it is not sufficient that the ends of the ladder be clear of the walls. If the ladder is too long, it may have two feasible positions, one in each arm of the corridor, but there may be no possible way to get from one to the other. In this case we say that the configuration space of the ladder is not connected: we can't get the ladder there from here, even though we can get each end (taken separately, which is physically impossible) from here to there. Connectedness in configuration space is therefore the key to motion planning. These are problems human beings (especially furniture movers, or people trying to park cars in confined spaces) solve intuitively, but find very hard to explain. Note that the ladder is rigid and three-dimensional, hence its position is determined by the coordinates of three points on it, so configuration space is nine-dimensional.  Connectedness in mathematical spaces is also important. The square root of 4 can be either 2 or -2: we have to decide which. Similarly, the square root of 9 can be 3 or -3. But, if 4 is connected to 9 in our problem space (whatever that is), we can't make these choices independently: our choice has to be consistent along the path from 4 to 9. When it is impossible to make such decisions totally consistently, we have what mathematicians call a `branch cut' - the classic example being the International Date Line, because it is impossible to assign `day' consistently round a globe. In previous work, we have shown that several mathematical paradoxes reduce to connectedness questions in an appropriate space divided by the relevant branch cuts. This is an area of mathematics which is notoriously difficult to get right by hand, and mathematicians, and software packages, often have internal inconsistencies when it comes to branch cuts.   The standard computational approach to connectedness, which has been suggested in motion planning since the early 1980s, is via a technique called cylindrical algebraic decomposition. This has historically been computed via a 'bottom-up' approach: we first analyse one direction, say the x-axis, decomposing it into all the critical points and intermediate regions necessary, then we take each (x,y)-cylinder above each critical point or region, and decompose it, then each (x,y,z) above each of these regions, and so on. Not only does this sound tedious, but it is inevitably tedious - the investigators and others have shown that the problem is extremely difficult (doubly exponential in the number of dimensions).  Much of the time, notably in motion planning, we are not actually interested in the lower-dimensional components, since they would correspond to a motion with no degrees of freedom, rather like tightrope-walking. Recent Canadian developments have shown an alternative way of computing such decompositions via so-called triangular decompositions, and a 2010 paper (Moreno Maza in Canada + Davenport) has shown that the highest-dimensional components of a triangular decomposition can be computed in singly-exponential time. This therefore opens up the prospect, which we propose to investigate, of computing the highest-dimensional components of a cylindrical decomposition in singly-exponential time, which would be a major breakthrough in computational geometry."
	},
	{
		"grant":313,
		"ID": "EP/J003263/1",
		"Title": "Spintronic device physics in Si/Ge Heterostructures.",
		"PIID": "48924",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2012",
		"EndDate": "29/02/2016",
		"Value": "698526",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "48924", "Role": "Principal Investigator"},
		{"ID": "114032", "Role": "Co Investigator"},
		{"ID": "83008", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Toshiba Research Europe Ltd"}
		],
		"Summary": "Spin injection and transport in semiconductors is under intense investigation by physicists around the world, motivated by fascinating new insights into condensed matter, aware of considerable potential for novel devices and ensuing technologies.  However, spin injection and its detection pose exceptional challenges.  Much focus has been on technologically important materials: GaAs, where optical properties aid spin detection, and more recently Si for its long spin lifetimes.  Here, we propose a new approach based on germanium.  Ge is compatible with Si technology, has a longer spin life time than GaAs, a higher room temperature hole mobility than GaAs or Si, and better modulation properties than Si due to its higher spin-orbit coupling.  SiGe heterostructure technology also has the potential to increase spin diffusion lengths by virtue of dramatic enhancements in carrier mobility.  We recently carried out optical experiments that demonstrated RT spin transport and extraction through Ge for the first time, based on a structure consisting of Ge grown epitaxially on GaAs and an electrodeposited Ni/Ge Schottky contact [C. Shen et al., Appl. Phys. Lett. 97, 162104 (2010)].  Here, we propose to build upon that work and use the Si-Ge system to its full extent, through delta doping and bandstructure-engineering to maximize spin transparency of the electrical contacts and using strain and low dimensionality to enhance coherent transport in the channel.  The culmination of this project should be the exciting prospect of the elusive two-terminal semiconductor spin valve operating at room temperature and an early demonstration of spin modulation by a gate electrode in such a device.  The programme will combine the complementary expertise of the partners: Warwick in SiGe epitaxy and in carrier transport, Southampton in Schottky barrier research, and Cambridge in semiconductor spin transport by optical and electrical means, together with the facilities of the Southampton Nanofabrication Centre and industrial support from Toshiba Europe Research Ltd.   "
	},
	{
		"grant":314,
		"ID": "EP/J003662/1",
		"Title": "LowPowNoC - Evaluation and reduction of power dissipation in multicore systems based on Networks-on-Chip",
		"PIID": "-249197",
		"Scheme": "First Grant Scheme",
		"StartDate": "26/03/2012",
		"EndDate": "25/12/2013",
		"Value": "99685",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "-249197", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Multicore chips are currently the norm in enterprise, scientific and desktop computing and have already made inroads into mobile and embedded computing. One of the major problems faced by multicore system designers is to achieve maximum performance and functionality while respecting the power dissipation budget, which limits how much energy the system can take from mains or batteries. The power budget should be set as low as possible, because of energy costs, impact on the environment, battery lifetime, cooling costs, among other factors. This project aims to investigate and develop design techniques to optimise multicores in such a way that they can perform the same functionality with the same level of performance, while dissipating less power. The main focus of the techniques addressed by this project is the on-chip communication infrastructure, which was introduced to allow the multiple processing cores to exchange data and can account for up to 30% of the total power dissipated by the chip. To maximise the impact of this research to the UK and European economy, the project will address system-level techniques which are available to local system design and integration companies, rather than technology-specific techniques that require control of the chip fabrication process, which is often outsourced."
	},
	{
		"grant":315,
		"ID": "EP/J003727/1",
		"Title": "Verifying Concurrent Lock-free Algorithms",
		"PIID": "4140",
		"Scheme": "Standard Research",
		"StartDate": "19/04/2012",
		"EndDate": "18/10/2015",
		"Value": "378905",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "4140", "Role": "Principal Investigator"},
		{"ID": "-114860", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Software is becoming increasingly complex, and the demand for increased performance is driven by many diverse applications. As a response to this demand, concurrent software that efficiently exploits multi-core architectures is likely to be the norm in many sectors. However, developing correct concurrent algorithms is a difficult task. This is particularly true for a class of concurrent algorithms that fully exploit the potential concurrency by offering the maximum amount of interleavings of different processes working on a shared memory. There is then a real economic imperative to build techniques that can verify as correct such lock-free algorithms as they are known.  Testing and simulation, while valuable and automated to a degree, are ultimately limited in the guarantees they can offer for correctness, particularly in the case of concurrent algorithms, where multiple thread interleavings act on data structures potentially unbounded in size. Formal verification of this class of algorithm is becoming tractable and there has been a surge of interest in applying such techniques, and a unique opportunity to verify a class of algorithms before their widespread adoption.  This project focusses on lock-free algorithms (also called non-blocking algorithms). Lock-free algorithms have been discovered for many common data structures. Non-blocking algorithms are used extensively at the operating system and JVM level for tasks such as thread and process scheduling. While they are more complicated to implement, they have a number of advantages over lock-based alternatives -- hazards like priority inversion and deadlock are avoided, contention is less expensive, and coordination occurs at a finer level of granularity, enabling a higher degree of parallelism.  This project seeks to develop techniques whereby such algorithms can be proved correct. The techniques are based on the notion of refinement which relates an abstract specification with a more detailed concrete implementation. What we will do here is show that a certain type of refinement between an abstract description and a concurrent algorithm implies that the concurrent algorithm is correct. The notion of correctness here being linearizability.  Part of the novelty of what we propose to do is in the construction of the right sort of refinement relation - it has to be one that will imply linearizability, yet at the same time give rise to proof obligations that are tractable for specific algorithms, that is, actually make the verification of linearizability easier to achieve. Another part of the novelty is that we will mechanize the whole thing - both the proofs for specific algorithms, but also the proof that our technique is correct, thus giving an extra level of assurance that the techniques really are sound - the details are sufficiently complex and subtle that mechanization really is necessary to be 100% sure of correctness.  In addition to these basics we want a proof method that is applicable to a wide range of algorithms, and one that is compositional. To aid applicability we develop two flavours of technique - one based on forward simulation, and another based on backward simulation, as well as specific support for unboundedness and dynamic linearization points. To aid compositionality (so that the proofs can be broken down into smaller local steps rather than undertaking one large global proof) we will develop thread modular simulation conditions, and interference freedom conditions that aid the process.  Our work will be evaluated on a number of 'benchmark' algorithms, such as lock-free implementations of stacks, queues, hashtables etc. Finally, we will investigate how our proof methods can be shown to be complete, that is, every algorithm could potentially be verified by using the method. We will also investigate liveness, ie, showing that a concurrent algorithm guarantees various forms of progression. "
	},
	{
		"grant":316,
		"ID": "EP/J004049/1",
		"Title": "Computational Creativity Theory",
		"PIID": "98328",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2016",
		"Value": "970170",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "98328", "Role": "Fellow"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Brigham Young University"},
		{"Number": "1", "Name": "Complutense University of Madrid"},
		{"Number": "2", "Name": "Metropolitan Autonomous University"},
		{"Number": "3", "Name": "Newcastle University"},
		{"Number": "4", "Name": "Rebellion Developments Ltd"},
		{"Number": "5", "Name": "Royal College of Art"},
		{"Number": "6", "Name": "Sony Computer Science Laboratory Paris"},
		{"Number": "7", "Name": "University College Dublin"},
		{"Number": "8", "Name": "University of Aberdeen"},
		{"Number": "9", "Name": "University of Coimbra"},
		{"Number": "10", "Name": "University of Edinburgh"},
		{"Number": "11", "Name": "University of Sydney"}
		],
		"Summary": "Computational Creativity is the study of how to build software which takes on some of the creative responsibility in arts and science projects. We are at a stage where software can generate pictures, melodies, jokes and poems, can invent new words and discover new and interesting mathematical theorems, and regularly helps scientists to make important discoveries. This kind software can be used autonomously, or in collaboration with creative people. It is also used in cognitive modelling projects, to shed light on aspects of human and animal creativity. In the last decade, Computational Creativity has come of age, as evidenced by special issues of publications such as the Minds and Machines journal and the AI magazine, and the first International Joint Conference on Computational Creativity, which replaced 10 years of successful workshops at major AI conferences.   The proposed Leadership Fellow, Simon Colton, is a recognised expert in Computational Creativity, and has been working in the field since 1996. He is unique in having been involved in successful applications of creative software to four different domains, namely mathematical invention, video game design, graphic design and the visual arts. His mathematical theory formation software, HR, has produced theorems and concepts published in the mathematical literature; his visual art software, The Painting Fool, has produced pictures that have been exhibited and attracted much public attention; and research being done in the Computational Creativity group that he leads at Imperial College is helping video games companies to design the next generation of adaptive, personalised games.  A number of authors, such as Boden, Wiggins and Ritchie, have introduced formalisms which help us to be more precise about the creativity of software. However, there is no agreed upon theory which can describe the behaviour of software with sufficient acuity, coverage and formality that enables accurate comparison of implementations. In short, we have no generic way of saying that software B is more creative than software A. This has held back our field, because with no concrete and formal measures of the creativity of the software we build, it has been hard to put forward falsifiable scientific hypotheses that one approach is more creative than another, hence it has been difficult to progress, and to show progress.   With this Fellowship, we propose to change this situation, by developing Computational Creativity Theory (CCT). This will comprise a series of models, each of which contains some conceptual definitions and some calculations involving those definitions which can be used to compare and contrast the creativity of software. The foundational models will make more precise the notion of a creative act and the impact they can have, and the more acute models will cover aspects of creative behaviour including intentionality, interpretation, imagination, appreciation and affect. To model computer creativity sufficiently well, we generalise past the merely generative and past usual AI notions of value, into new areas where software is expected to invent its own aesthetic and utilitarian measures, and frame its creations by describing its motivations, intentions, methods and innovations and by putting its work into historical and cultural contexts.  The proposed programme of research has the development of CCT at its heart. This is informed by a series of practical projects involving applications to creative language, music, visual arts, mathematics and games, and covering modes of creativity including realtime generation, assistive technologies and creative collaborations. By building and disseminating CCT, we will help to bring Computational Creativity research into a new era, where formal notions of creativity underpin software systems which really enrich our cultural lives. "
	},
	{
		"grant":317,
		"ID": "EP/J004049/2",
		"Title": "Computational Creativity Theory",
		"PIID": "98328",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/05/2013",
		"EndDate": "30/09/2016",
		"Value": "669632",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Department",
		"OrgID": "97",
		"Investigators":[
		{"ID": "98328", "Role": "Fellow"}
		],
		"Collaborators":[
		],
		"Summary": "Computational Creativity is the study of how to build software which takes on some of the creative responsibility in arts and science projects. We are at a stage where software can generate pictures, melodies, jokes and poems, can invent new words and discover new and interesting mathematical theorems, and regularly helps scientists to make important discoveries. This kind software can be used autonomously, or in collaboration with creative people. It is also used in cognitive modelling projects, to shed light on aspects of human and animal creativity. In the last decade, Computational Creativity has come of age, as evidenced by special issues of publications such as the Minds and Machines journal and the AI magazine, and the first International Joint Conference on Computational Creativity, which replaced 10 years of successful workshops at major AI conferences.   The proposed Leadership Fellow, Simon Colton, is a recognised expert in Computational Creativity, and has been working in the field since 1996. He is unique in having been involved in successful applications of creative software to four different domains, namely mathematical invention, video game design, graphic design and the visual arts. His mathematical theory formation software, HR, has produced theorems and concepts published in the mathematical literature; his visual art software, The Painting Fool, has produced pictures that have been exhibited and attracted much public attention; and research being done in the Computational Creativity group that he leads at Imperial College is helping video games companies to design the next generation of adaptive, personalised games.  A number of authors, such as Boden, Wiggins and Ritchie, have introduced formalisms which help us to be more precise about the creativity of software. However, there is no agreed upon theory which can describe the behaviour of software with sufficient acuity, coverage and formality that enables accurate comparison of implementations. In short, we have no generic way of saying that software B is more creative than software A. This has held back our field, because with no concrete and formal measures of the creativity of the software we build, it has been hard to put forward falsifiable scientific hypotheses that one approach is more creative than another, hence it has been difficult to progress, and to show progress.   With this Fellowship, we propose to change this situation, by developing Computational Creativity Theory (CCT). This will comprise a series of models, each of which contains some conceptual definitions and some calculations involving those definitions which can be used to compare and contrast the creativity of software. The foundational models will make more precise the notion of a creative act and the impact they can have, and the more acute models will cover aspects of creative behaviour including intentionality, interpretation, imagination, appreciation and affect. To model computer creativity sufficiently well, we generalise past the merely generative and past usual AI notions of value, into new areas where software is expected to invent its own aesthetic and utilitarian measures, and frame its creations by describing its motivations, intentions, methods and innovations and by putting its work into historical and cultural contexts.  The proposed programme of research has the development of CCT at its heart. This is informed by a series of practical projects involving applications to creative language, music, visual arts, mathematics and games, and covering modes of creativity including realtime generation, assistive technologies and creative collaborations. By building and disseminating CCT, we will help to bring Computational Creativity research into a new era, where formal notions of creativity underpin software systems which really enrich our cultural lives. "
	},
	{
		"grant":318,
		"ID": "EP/J004057/1",
		"Title": "WHole Animal Modelling (WHAM): Toward the integrated understanding of sensory motor control in C. elegans",
		"PIID": "108534",
		"Scheme": "Leadership Fellowships",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2016",
		"Value": "1185968",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "108534", "Role": "Fellow"}
		],
		"Collaborators":[
		],
		"Summary": "Animals are remarkable creatures. No man-made machine even comes close in its ability to navigate complex environments, respond to rich sensory cues, learn and adapt its behaviour when encountering completely novel scenarios and much much more. But even the simplest animals, ruled by even the simplest nervous systems, can achieve this. Simple animals may not be able to play chess or balance bank statements, but there is much we can learn from them about their robust mechanisms for sensory-integration and motor control which may be of use to us in control engineering, bio-robotics and even in future brain-machine interfaces that are being developed for neuro-prosthetic applications.  An excellent starting point for understanding animal behaviour is a tiny, free living 1mm long roundworm, called C. elegans. By comparison to our 100 billion nerve cells, or even a fly's 100 thousand nerve cells, this worm's entire nervous system consists of a mere 302 nerve cells. Unlike our nervous system, the worm's circuitry is hard wired and identical across individuals of the species, making it possible to study rigorously and reproducibly. Due in part to its simplicity, and in part to its ease of manipulation in the lab, this is the only animal for which this entire nervous system has been mapped in exquisite detail (to sub-cellular resolution). But despite its relative simplicity, this worm possesses many of the functions that are attributed to more complex animals, including feeding, mating, complex sensory abilities, memory and learning. It is not surprising, therefore, that the modelling of this worm has captured the imagination of physicists, computer scientists and engineers alike. The integrated modelling of C. elegans has even been proposed as one of the UK's ``Grand Challenges for Computing Research.''  In this Fellowship, I will begin to integrate our understanding of C. elegans sensory motor behaviour in a single computational model. The challenge is to bridge the gap between the effectively static neural circuit architecture and the dynamic neural computation it sustains.  This fellowship will enable me to deliver a step change, not only in our understanding of an important model organism, but also in advancing the science and engineering of complex systems, whether in the context of reverse engineering real world networks, or in the context of designing them. "
	},
	{
		"grant":319,
		"ID": "EP/J004197/1",
		"Title": "Crime, Policing and Citizenship (CPC) - Space-Time Interactions of Dynamic Networks",
		"PIID": "-164856",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2012",
		"EndDate": "31/03/2016",
		"Value": "1400235",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Civil Environmental and Geomatic Eng",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-164856", "Role": "Principal Investigator"},
		{"ID": "-2886", "Role": "Co Investigator"},
		{"ID": "-13840", "Role": "Co Investigator"},
		{"ID": "44068", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Metropolitan Police Service"}
		],
		"Summary": "Crime continues to cast a shadow over citizen well-being in big cities today, while also imposing huge economic and social costs. Prevention, early detection and strategic mitigation are all critical to effective policy intervention, especially in domains where coordinated responses are required. Every day, about 10,000 incidents are reported by citizens, recorded and geo-referenced in the London Metropolitan Police Service Computer Aided Dispatch (CAD) database. Today, impending funding cuts bring new pressures for central accountability and improved efficiency, while community empowerment initiatives bring new opportunities and challenges to policing. Timely understanding of how criminality emerges and how crime patterns evolve is crucial to anticipating crime, dealing with it when it occurs and developing public confidence in the police service. It is widely understood that policing, crime and public trust all have strong spatial and temporal dimensions. An integrated approach to space-time analysis is needed in order to analyse crime patterns, police activity patterns and community characteristics, so as to understand and predict the when, where and what of how criminal activities emerge and are sustained.  This research will consolidate achievements in integrated spatio-temporal data mining and emergent network complexity to uncover patterning in crime, policing and citizen perceptions at a range of spatial and temporal scales. Each dataset of police movement, crime (and disorder) reported in the CAD, and citizens making '999' calls constitutes a spatio-temporal network (STN), which has its own characteristic patterning and behaviour in space-time, and which interacts with the other STNs. The (geotagged) deployment of police manpower in space and time, the spatio-temporal patterning of crime and disorder, and the perceptions of members of the public are likely to be interlinked to differing extents. The first of these purportedly both anticipates and responds to the second, while the third is a lagged response to the first two, giving reason to anticipate that all three networks should be tightly coupled. The project will first analyse spatio-temporal patterns of individual STNs, then associate the patterns among these STNs via integrated spatio-temporal data mining developed using innovative statistical regression and machine learning.  This research will utilise a range of disciplines (crime, geography, geoinformatics, and computer science) to help engineer effective practical solutions to crime problems. It proposes a new method for exploring crime patterns and integrating information on crime and police activity. It systematically addresses a structured programme of analytical issues in spatio-temporal data mining, which are becoming core to Geographical Information Sciences. It will advance the theory, methodology and application of research into network complexity by evaluating the forms and interactions of the networks that characterise crime and other socio-economic phenomena. This will make it possible to not only understand activity networks but also to use them for prediction and decision making.   This addresses the aims of RCUK's Global Uncertainties Programme in crime, terrorism, and ideologies and beliefs. It will extend our appreciation of the subtle interplay of different forms of complex systems, in ways that will contextualise tactical and strategic responses to terrorism and organised crime. It will enable intelligent policing of London Met Police by granting unforeseen levels of prediction. The best practice of individual Metropolitan boroughs can be extended to others in the UK. The methodology developed here will be transferrable to other international cities using similar incident report systems. This will directly benefit people who live, work and visit London and those cities to make them feel safe.  "
	},
	{
		"grant":320,
		"ID": "EP/J004448/1",
		"Title": "Exploration of Ultrasound based haptic interaction on a multi-touch surface",
		"PIID": "-188117",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2012",
		"EndDate": "31/01/2015",
		"Value": "335832",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-188117", "Role": "Principal Investigator"},
		{"ID": "50145", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Microsoft Research Ltd"},
		{"Number": "1", "Name": "XMOS Ltd"}
		],
		"Summary": "Multi-touch tables, such as Microsoft Surface, are now widely available. Users can walk-up and use these systems in hotel lobbies and other public settings with very little instruction and with no need to wear or hold intrusive sensors with their hands or body. The ability to 'walk-up and use' with unadorned hands and fingers removes the barrier between human and technology, encouraging spontaneous use.   One of the primary disadvantages of current interactive surfaces is that users can touch objects, but they are unable to feel them. There are a plethora of applications where it is beneficial for the user to have their touches augmented with 'feel-based' haptic feedback. For example, medical applications, virtual training, and modelling applications require precise control from the user-haptic feedback can aid the user in effectively performing these tasks. These applications demonstrate the benefit of augmenting haptic feedback with visual feedback in an interactive application. Often, the visual space has been disconnected from the force-feedback, requiring a prolonged training period for the user to become accustomed to moving a digital object and watching the interactions a small distance away on a monitor.   In this proposal we will investigate the use of ultrasound transducers to provide 'feelable' feedback through air. The skin on a human hand can feel the ultrasonic pressure wave produced by a carefully calibrated series of transducers, in much the same manner that is apparent from loud sub-woofers on a stereo system. Ultrasound waves are outside the human's range of hearing and so provide silent, through-air haptic feedback. We will use this technology to provide multi-point haptic feedback on the surface of a multi-touch horizontal surface.  The team consists of Dr. Sriram Subramanian, Dr. Mark Marshall and Dr. Jason Alexander from the computer science departments and Prof. Bruce Drinkwater from the Mechanical Engineering department of the University of Bristol and Prof. Stephen Brewster from the Computer Science department of the University of Glasgow. The team is internationally recognised for its research in Human-Computer Interaction (HCI), novel integration of hardware for HCI, and Ultrasonic sensors. Microsoft research (Cambridge) and XMOS will serve as project partners and offer valuable resources and support for the project."
	},
	{
		"grant":321,
		"ID": "EP/J004561/1",
		"Title": "BABEL",
		"PIID": "72333",
		"Scheme": "Standard Research",
		"StartDate": "31/08/2012",
		"EndDate": "30/08/2016",
		"Value": "1202384",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing & Mathematics",
		"OrgID": "109",
		"Investigators":[
		{"ID": "72333", "Role": "Principal Investigator"},
		{"ID": "115485", "Role": "Co Investigator"},
		{"ID": "76520", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Recent advances in behavioural and computational neuroscience, in cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field.  This project aims at advancing the understanding of neural and behavioural mechanisms in word learning, the validation of these principles in neuroanatomically grounded models, and real-time implementations of brain language models within the SpiNNaker neuromorphic architecture that will support comparisons with neuroimaging experiments. The scientific hypotheses and cortical language model will also be validated by implementing a model of embodied active language learning on the humanoid robot iCub. Specifically, in the project we will develop, based on neuroscientific principles, a theory of language learning at the neural circuit level and build a neurocomputational model of the language cortex that implements the learning of words used to speak about objects and actions in large-scale neuronal circuits. This theoretical work will be supported by novel, hypothesis-driven brain imaging investigations using MEG, EEG and fMRI to identify the neural correlates and mechanisms of the learning of words for objects and actions. Imaging results will inform the improvement of the large-scale neuroanatomical models. These models will be implemented on the SpiNNaker software and hardware infrastructure, to implement a scaled-up real-time model of the language cortex using more realistic spiking activity. Finally, the project will translationally apply these neuro-anatomical models and SpiNNaker system as controllers for language and action learning simulations with the humanoid robot iCub, within the embodied and active learning context where the semantics of the language is directly driven by the context of object manipulation tasks. This is a highly interdisciplinary project that integrates essential expertise and methodologies from neuromorphic engineering, computational and experimental neuroscience, and cognitive robotics. The project is based around the unique and strategic partnership of applicants with an international track record in these areas of expertise and with previous collaborative experience. Furthermore, the project will benefit from an International Advisory Board, with both academic and industrial advisors, to foster the international dimension and impact of the project. "
	},
	{
		"grant":322,
		"ID": "EP/J004863/1",
		"Title": "Tapered Semiconductor Fibres for Nonlinear Photonics Applications",
		"PIID": "-162039",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "30/04/2015",
		"Value": "395208",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-162039", "Role": "Principal Investigator"},
		{"ID": "73952", "Role": "Co Investigator"},
		{"ID": "41202", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Semiconductor photonics is a field that is currently revolutionizing the future of modern optoelectronic devices. Although semiconductor materials are more commonly associated with electronic functionality (e.g., popular gadgets that use semiconductor microelectronic processors include cell phones, computers, and digital radios), to date a number of important photonic devices have been demonstrated using planar based substrates on a chip including silicon lasers and germanium photodetectors. More recently, however, the incorporation of semiconductor materials into the core of optical fibres has generated much interest as it provides a unique opportunity to completely integrate this technology with existing silica fibre infrastructures used in data transmission networks. Fiberized semiconductor devices offer some notable advantages over those developed on-chip such as simple, low cost fabrication (i.e., no need for multimillion dollar cleanroom based lithography) and robust and versatile waveguide geometries. Furthermore, the wide (visible to far-infrared) optical transparency of the semiconductor materials that can be incorporated into the fibre geometry ensures that their applications will extend far beyond optical communications to disciplines such as medicine, sensing, spectroscopy and security monitoring.  The work described in this proposal seeks to combine this exciting new semiconductor fibre platform with a key waveguide technology: tapered optical fibres. Conventional tapered silica fibres, which have varying waveguide dimensions along the length, have been exploited for a wealth of applications such as, optical signal processing, supercontinuum generation, remote sensing, as well as for optimized mode coupling between devices. The extension of these structures to incorporate semiconductor materials with rich optoelectronic functionality into the tapered cores will present new degrees of design flexibility for the optimization of semiconductor optical waveguides. The primary goal of this work will thus be to develop the procedures for the fabrication of high quality tapered semiconductor fibre structures and to demonstrate their potential for nonlinear photonic applications. This highly innovative project has the potential to lead to the development of a number of technologically disruptive all-fibre optoelectronic devices, for example, ultra-compact broadband mid-infrared laser sources for healthcare, frequency combs for chemical analysis, and highly nonlinear optical couplers and switches for ultrafast telecommunications. "
	},
	{
		"grant":323,
		"ID": "EP/J004898/1",
		"Title": "High-Power Tunable GaAs Distributed Feedback Lasers",
		"PIID": "-19872",
		"Scheme": "Standard Research",
		"StartDate": "16/04/2012",
		"EndDate": "15/04/2014",
		"Value": "167100",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-19872", "Role": "Principal Investigator"},
		{"ID": "-2868", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Oclaro Technology UK"},
		{"Number": "1", "Name": "Scimus Solutions Ltd"},
		{"Number": "2", "Name": "TOPTICA Photonics AG"}
		],
		"Summary": "Photonics is a key enabling technology with far reaching applications. In this project we will realise a new generation of enhanced-functionality GaAs-based lasers for the 300 to 1300nm wavelength range. These devices, based on a new monolithic integration scheme, will impact upon existing applications in display, printing, absorption spectroscopy and telecoms, whilst enabling new applications in astrophotonics, sensing, biomedical imaging, photodynamic therapy, fluorescence spectroscopy, cosmetic and clinical surgery procedures, process control, agriculture, defence and security. Monolithic integration on GaAs has previously been impeded due to the requirement for overgrowth upon aluminium containing layers. We will capitalise on approaches demonstrated in the investigator's previous EPSRC feasibility study into advanced discrete GaAs components to investigate the potential for true monolithic integration. We will realise a distributed feedback (DFB) laser monolithically integrated with a power amplifier within a continuous buried waveguide. As an extra proof of flexibility, there will be a heater element adjacent to the DFB for both wavelength stabilisation and tuning. This disruptive new technology will enable low cost, highly efficient, high power, wavelength agile lasers, in the range from 600 to 1300nm.  Additionally, inherent high power and spectral purity are favourable for frequency doubling to extend the range down to 300nm.   Specific objectives of this project will be a proof of concept demonstration of these integrated devices, backed up with reliability statistics at 760nm (based on quantum wells) and 1180nm (based on quantum dots), i.e. covering the short and long wavelength regions readily available in this technology. "
	},
	{
		"grant":324,
		"ID": "EP/J005177/1",
		"Title": "Dream Fellowship: Energy-Modulated Computing",
		"PIID": "22017",
		"Scheme": "Standard Research",
		"StartDate": "17/10/2011",
		"EndDate": "16/10/2013",
		"Value": "171080",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "22017", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This is both a unique opportunity and a challenge for me as I set my aims at identifying a coherent set of problems to work on after this Fellowship.  Such challenges are sometimes associated with searching not only for solutions to what we don't know, but also for the things we don't know that we don't know.  My main area has for years been electronic systems engineering. By thinking and engaging creative processes in this area, it is first of all important to see where calls for creativity originate. Obviously, these calls come from real life, from the needs of society and industries where electronics is enabling technology. Focusing further, such calls can be found in the links and synergies between the pivotal areas of ICT, which increasingly share common values and criteria: quality of service, usability, cost-efficiency, performance, dependability, and how they interact with the provision of resources. Semiconductor technology permits formidable concentration of electronic devices and electrical power on small areas of a silicon die. At the same time, engineering processes, involving software and hardware, can no longer sustain this growth; they require the design and test processes to be much more resource-conscious. Green, energy-frugal, power-proportional are qualities of computer systems that people begin to use now. Numerous examples bring up the issue of resource and energy-awareness into computing and electronics. From the energy supply perspective, battery life, energy harvesting, power control and regulation are changing systems engineering practices.  From the energy consumption viewpoint, the high end of the spectrum is occupied by mammoth data plants (e.g. Google plant in Oregon was estimated to require 103MWatt of power, enough to supply every home in Newcastle). In the middle, there are many-core chips, such as Intel's 48-core SCC, consuming between 25-125W. The low end of the spectrum is systems that interface to biological organisms, where power constraints are at the level of microwatts. Over the years system design methodologies developed completely relying on feature scaling and availability of as many resources as needed in order to satisfy their performance appetites. However, architecting systems solely on the principles of hierarchy and object-orientation, without proper account of underlying resources often leads to inefficiency, likewise does the full decentralisation of control and distribution of resources on principles of local optima.   One of the important achievements of this fellowship could be obtaining an evolutionary roadmap for electronic system design which is 'modulated' by the energy aspect.   In working towards this goal, I will think about issues involving energy characterisation of components and devices of different functionality and nature, interplay between energy and dependability, power constraints and quality of service, an idea of 'energetic effort' for design criteria, possible role of game-theoretic approaches in resource-driven computing and various modelling and meta-modelling techniques, as well as design automation issues.   The other two important achievements would be: knowledge-transfer routes for providing industry with new design paradigms, methodologies and tools for energy-frugal systems, as well as mechanisms for enthusing a new generation of your researchers about creativity. "
	},
	{
		"grant":325,
		"ID": "EP/J005223/1",
		"Title": "Rank based spectral estimation",
		"PIID": "49980",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2012",
		"EndDate": "31/08/2016",
		"Value": "465218",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "102",
		"Investigators":[
		{"ID": "49980", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Apple"},
		{"Number": "1", "Name": "Buhler Sortex Ltd"},
		{"Number": "2", "Name": "Datacolor"},
		{"Number": "3", "Name": "Unilever UK Central Resources Ltd"}
		],
		"Summary": "The colours, or RGB pixels, recorded by a digital camera are the result of the interaction of the prevailing light in the scene striking and being reflected by objects and the characteristics of the camera itself. The complexity is such that different cameras see differently and no cameras see the world exactly as we do. You will have noticed this when looking at photos where sometimes the colours don't look right or the pictures captured by one camera look 'better' than another. Moreover, sometimes we see colours change dramatically. We have all probably observed that white clothes can look bluish under ultra violet light (say in a night club). But, in fact the colours we see change subtly, all the time, as we move from one light to another (which is why it is always a good idea to check the colour of your clothes outside the shop). Here, even small changes can lead to poor customer satisfaction or, potentially, in a medical imaging application the wrong diagnosis.  Good pictures, by which we might mean accurate 'colour measurement' are possible if we know the spectral colour characteristics of a camera and/or the spectrum of light in a scene. While we can, in principle, measure these quantities the measurement is not easy to do so and is expensive (not easy as it requires considerable (Physics) lab time and expensive because spectral measurement devices cost many thousands of pounds). When measurement is not feasible, there do in fact exist methods for estimating (say) the spectrum of light in a scene. Yet, these methods only tend work if the camera is accurately calibrated first (a sort of chicken and the egg situation). Our 'Rank Based Spectral Estimation' Project aims to make it much easier to calibrate a camera or measure the illuminant in situ (and as such also make it easier to measure reflectance too)  So, how does our method work. Well suppose we gave you 50 grey tiles all of which appeared to have a different brightness. It would be an easy task for you to rank them from darkest to brightest. But, now suppose we change the colour of the light. Depending on the spectral shape of the grey reflectances, the ranking order can change (sometimes considerably). No problem, it is a simple matter to reorder the tiles. Remarkably, for specially chosen reflectances, the rank order will strongly correlate with the spectral shape of the light. Thus a simple ranking experiment gives us a strong clue to the colour of the light. (And, if we knew the colour of the light we could, for example predict whether the colour of our clothes might change when we go outdoors.)  The Rank Based Spectral Estimation project aims to take this simple ranking idea and provide simple, and accurate, estimation tools for deriving the spectral shape of the prevailing light, the spectral characteristics of a camera and the spectral reflectances of surfaces. At the heart of our method is a specially designed reflectance target containing many reflectances (whose design is part of the proposed research). Ranking these reflectances will allow us to accurately estimate the light spectrum and the spectral attributes of a camera. Accurate spectral estimates are required in many applications from photography, through, visual inspection to forensic imaging and telepresence (e.g. remote diagnosis).  Remarkably, we believe the methods we develop will also prove useful in understanding how we see. Indeed, it is very likely that you see the world a little differently than I do. Yet estimating an individual's spectral response is notoriously difficult. To the extent it can be done at all, it requires many hours of (tedious) detailed visual experiments. Through ranking it will be possible to uncover an observers spectral response (technically called 'colour matching curves') quickly and simply. We simply ask the observer to carry out a simple ranking of the kind mentioned above.  "
	},
	{
		"grant":326,
		"ID": "EP/J005266/1",
		"Title": "The Uncertainty of Identity: Linking Spatiotemporal Information Between Virtual and Real Worlds",
		"PIID": "44068",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2011",
		"EndDate": "31/10/2014",
		"Value": "1218191",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Geography",
		"OrgID": "81",
		"Investigators":[
		{"ID": "44068", "Role": "Principal Investigator"},
		{"ID": "101281", "Role": "Co Investigator"},
		{"ID": "-289131", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This is an interdisciplinary proposal from Computer Science (St. Andrews), Engineering (City University London) and Geography (UCL), in partnership with experts in Visual Analytics at Purdue University in the United States. Our goal is to link information pertaining to human characteristics in 'real' and 'virtual' worlds in order to better manage the uncertainties inherent in establishing human identity and linking it to geographic locations. Our basic premise is that uncertainty in identifying and characterising individuals may be managed and understood by: (a) detecting and exploring spatio-temporal profiles of lifestyles and activity patterns; (b) concatenating and conflating detailed, but under-exploited, datasets in the virtual and real domains; and, more speculatively (c) soliciting and analysing crowd-sourced volunteered data that link physical and virtual identities. Through these actions it will be possible to improve our ability to characterize and validate an individual's identity, to infer more informative profiles of individuals and groups that bridge the real and virtual domains, and to document and manage the uncertainties inherent in these tasks. Aspects of this highly innovative research agenda are inevitably risky and speculative, but following an EPSRC 'WDYTYA' Sandpit we have appraised risk, examined the feasibility of data acquisition and addressed ethical approval issues.   The research will require multiple sources of data about a user's online activities (henceforth 'virtual' sources, such as multiple social networks, commercial information, purchases, etc.) alongside more conventional data (henceforth 'real' sources: population censuses, names registers, telephone directories, social surveys, etc.). Systematic linkage will be used to better resolve the question 'Who do I think you are?' We propose the exploitation of complementary databases and methods in order to relate 'real' and 'virtual' properties, to glean, synergise and cross validate new information and to leverage value from secondary sources. This will be achieved by developing novel methods of data collection, maintenance, exploration, analysis and modelling, that are efficient, effective, scalable, and safe to use.  The work programme will be undertaken through a programme of six inter-linked work packages in the UK and US, viz:  Work Package 1: Data Collection Tools  The development of new and effective tools for virtual data collection  Work Package 2: Text Analytics  Development of text analytics algorithms to describe clusters of concepts, or associations between certain concepts or named entities.   Work Package 3: Data Anonymisation and Privacy Preservation  Achieving a balance between the benefits of enhanced data collection (Work Package 1) and text mining (Work Package 2) versus the imperatives of preserving individual privacy.   Work Package 4: Cybergeodemographics  Use of primary (Work Packages 1 and 2) and secondary data to relate virtual Internet traffic to the probable physical locations from which it emanated; and the development of typologies of social networks that are robust, generalized and related to physical locations.  Work Package 5: Spatio-temporal Network Analysis  Development and application of spatio-temporal network analysis techniques to emerging social and geographic networks of individuals and the systems used by them.   Work package 6: Visual Analytics  Deployment of a range of visual exploratory data analysis techniques to alert users to deviations from trend or average behaviour and profile.  http://worldnames.publicprofiler.org/UncertaintyOfIdentity/index.html"
	},
	{
		"grant":327,
		"ID": "EP/J005371/1",
		"Title": "UK Photonic Systems Outreach Network",
		"PIID": "45632",
		"Scheme": "Network",
		"StartDate": "03/10/2011",
		"EndDate": "02/10/2014",
		"Value": "152497",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30",
		"Investigators":[
		{"ID": "45632", "Role": "Principal Investigator"},
		{"ID": "30827", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Photonics is the technology which underpins the Internet and has fuelled a revolution in communications over the past 20 years. Continued development of new technologies has positioned photonics well to make significant contributions to emerging societal challenges, such as energy efficiency and climate change, healthcare in an ageing society, the digital divide, the knowledge society, and public safety and security. Examples include green communication networks, photonics-related medical diagnosis, autonomous sensor networks, manufacturing and solid-state lighting are significant trends that have been identified.  However this potential will often only be realised by forming cross community projects which bring researchers together from appropriate disciplines to offer 'systems'  and solutions. The research community is committed to achieving this holding the view  that there needs to be more cross ICT working, with activities spanning the current research themes in order to deliver solutions.  It is the function of the Network to contribute to achieving this by bring together the photonics research community with other related communities (e.g. computing, people and interactivity, manufacturing and quality, healthcare, safety and security, and cutting-edge materials and technology) to develop research streams which address all the the aspects needed to deliver solutions."
	},
	{
		"grant":328,
		"ID": "EP/J005371/2",
		"Title": "UK Photonic Systems Outreach Network",
		"PIID": "45632",
		"Scheme": "Network",
		"StartDate": "01/09/2012",
		"EndDate": "30/09/2014",
		"Value": "129006",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "45632", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Photonics is the technology which underpins the Internet and has fuelled a revolution in communications over the past 20 years. Continued development of new technologies has positioned photonics well to make significant contributions to emerging societal challenges, such as energy efficiency and climate change, healthcare in an ageing society, the digital divide, the knowledge society, and public safety and security. Examples include green communication networks, photonics-related medical diagnosis, autonomous sensor networks, manufacturing and solid-state lighting are significant trends that have been identified.  However this potential will often only be realised by forming cross community projects which bring researchers together from appropriate disciplines to offer 'systems'  and solutions. The research community is committed to achieving this holding the view  that there needs to be more cross ICT working, with activities spanning the current research themes in order to deliver solutions.  It is the function of the Network to contribute to achieving this by bring together the photonics research community with other related communities (e.g. computing, people and interactivity, manufacturing and quality, healthcare, safety and security, and cutting-edge materials and technology) to develop research streams which address all the the aspects needed to deliver solutions."
	},
	{
		"grant":329,
		"ID": "EP/J00541X/1",
		"Title": "Phase Change Materials based Tunable NEMS",
		"PIID": "-366390",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/12/2011",
		"EndDate": "31/05/2014",
		"Value": "96400",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46",
		"Investigators":[
		{"ID": "-366390", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Plarion Ltd"}
		],
		"Summary": "Tunable resonators have wide ranging applications such as in positioning systems (eg. portable GPS) and in many satellite systems, as well as in almost all wireless devices such as the mobile phone. Better the resonator, more favorable it is for the application in terms of speed, as well as signal quality. MEMS and NEMS have potential for very high quality factors, which can be taken advantage of for designing such resonators. One of the important characteristics of a resonator is its ability to adjust its frequency quickly, so that it can lock in to a signal speedily. In this project we propose a novel NEMS resonator whose frequency can be tuned rapidly (on the order of a few nanoseconds). In addition, this resonator would use materials that have potentially dual-functionality, to help not only in tuning the frequency, but also in sensing the frequency. Such a resonator would reduce costs, take up very much less space than existing resonators, and would be much simpler to fabricate. Furthermore, this research project would shed light on physical mechanisms such as the piezoresistivity of phase change materials, as well as the mechanical properties of such materials."
	},
	{
		"grant":330,
		"ID": "EP/J00541X/2",
		"Title": "Phase Change Materials based Tunable NEMS",
		"PIID": "-366390",
		"Scheme": "First Grant Scheme",
		"StartDate": "15/01/2013",
		"EndDate": "14/06/2014",
		"Value": "62556",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-366390", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Tunable resonators have wide ranging applications such as in positioning systems (eg. portable GPS) and in many satellite systems, as well as in almost all wireless devices such as the mobile phone. Better the resonator, more favorable it is for the application in terms of speed, as well as signal quality. MEMS and NEMS have potential for very high quality factors, which can be taken advantage of for designing such resonators. One of the important characteristics of a resonator is its ability to adjust its frequency quickly, so that it can lock in to a signal speedily. In this project we propose a novel NEMS resonator whose frequency can be tuned rapidly (on the order of a few nanoseconds). In addition, this resonator would use materials that have potentially dual-functionality, to help not only in tuning the frequency, but also in sensing the frequency. Such a resonator would reduce costs, take up very much less space than existing resonators, and would be much simpler to fabricate. Furthermore, this research project would shed light on physical mechanisms such as the piezoresistivity of phase change materials, as well as the mechanical properties of such materials."
	},
	{
		"grant":331,
		"ID": "EP/J005762/1",
		"Title": "Hybrid Quantum-Classical Communication Networks",
		"PIID": "-258244",
		"Scheme": "First Grant Scheme",
		"StartDate": "30/07/2012",
		"EndDate": "29/01/2014",
		"Value": "98178",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "-258244", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Information security is a necessity of today's complex society. Let it be our personal information, a bank transaction, or some confidential military correspondence, they all rely on cryptographic techniques that guarantee secure communication between the transmitter and the intended receiver. This assurance, however, does not necessarily last forever. Technological advancements have already made many older cryptosystems obsolete, and it is anticipated that future discoveries will make the current secure communication methods unreliable as well. In particular, the development of new computational paradigms based on the laws of quantum mechanics is a threat to the security of the widely used public-key cryptosystems. Fortunately, what quantum mechanics may take by one hand, it gives back with the other. Secure communication, facilitated by the use of quantum key distribution (QKD) protocols, is the most imminent application of the developing field of quantum information. QKD provides unbreakable, future-proof, security safe from the vulnerabilities of most cryptosystems currently in operation. To this point, QKD has been implemented over dedicated channels and between two parties. Before current communication vulnerabilities are exploited, it is essential to facilitate the use of QKD technology for any two public users at any distance, via a network. This unsolved problem lies at the intersection of quantum physics and optical communications engineering, as all known QKD protocols rely on light transmission.   This proposal focuses on the problems that arise when multiple users wish to utilise the same infrastructure, namely, optical fibre, for both classical and quantum communication applications. This is in essence similar to a classical multiple-access problem, such as mobile communication, where multiple users communicate via a shared communication channel. In hybrid quantum-classical networks, this feature must be extended to include QKD applications, where we are dealing with optical signals as weak as a single photon.  In this project, I aim at undertaking a theoretical study of a range of network configurations and different multiple-access techniques for hybrid quantum-classical networks. This project will shed light on the necessary steps that underpin future implementations. I will also look at compatibility issues regarding the integration of present optical communication networks, which solely support classical applications, and future hybrid networks, which will offer both data transmission services as well as QKD-driven secure communications. That will enable long-distance classical-quantum communication at a national scale."
	},
	{
		"grant":332,
		"ID": "EP/J005959/1",
		"Title": "Supportive Automated Feedback for Short Essay Answers (SAFeSEA)",
		"PIID": "45577",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2012",
		"EndDate": "31/08/2014",
		"Value": "247912",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Educational Technology",
		"OrgID": "95",
		"Investigators":[
		{"ID": "45577", "Role": "Principal Investigator"},
		{"ID": "115012", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of this research is to provide an effective automated interactive feedback system that yields an acceptable level of support for university students writing essays in a distance or e-learning context.  Our tool will be based on an existing system supporting the online writing and assessment of essays, OpenComment (Whitelock & Watt, 2008), but this requires several problems to be solved, both in natural language processing, and in educational theory and practice. This proposal is therefore a truly interdisciplinary one.  The natural language processing problems are how to `understand' a student essay well enough to provide accurate and individually targeted feedback, and how to generate that feedback automatically. We intend to use techniques from document classification (measures of semantic similarity), information extraction, document summarisation, and text generation to address these problems. The tools we build need to be flexible enough to support different types and models of feedback.  The educational problem is how to develop and evaluate effective models of feedback. This requires research into the selection of the content, mode of presentation and delivery of the feedback.  The primary aspects of feedback which will be investigated are: summarisation; recognition of positive achievements; location of errors of commission or omission; misconceptions or problems connected with causal relationships; tactical and strategic hints both relating to the specific issues being addressed in the essay and issues connected with `metacognition' (i.e. self awareness of the learning process), and self-regulation of the learning process.  For feedback to be effective, it should assist students not only to manage their current essay-writing task, but it should also lead to further development of their essay-writing skills, the skills associated with efficient self-regulation, and their motivation to complete their course. The methods used to obtain data on effectiveness will be student surveys, interviews with selected participants, analysis of the various versions of the essays submitted for feedback, and the quality of the final essay."
	},
	{
		"grant":333,
		"ID": "EP/J006300/1",
		"Title": "Random walks on computer networks",
		"PIID": "57264",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2012",
		"EndDate": "30/03/2014",
		"Value": "131940",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "78",
		"Investigators":[
		{"ID": "57264", "Role": "Principal Investigator"},
		{"ID": "50943", "Role": "Co Investigator"},
		{"ID": "4597", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Carnegie Mellon University"},
		{"Number": "1", "Name": "Kyushu University"},
		{"Number": "2", "Name": "University of Bordeaux I"},
		{"Number": "3", "Name": "University of Paderborn"}
		],
		"Summary": "The basic theme of this research is to use random walks and interacting particle systems to improve network exploration and structure.   Our aim is to study algorithmic problems in Computer Science modeled by particles (agents, messages, robots) moving more or less randomly on a large network. We suppose such particles may be single or numerous, of various types, and that they may able to interact with each other and with the network. We assume the particles have a purpose either in relation to the network, such as searching the network or modifying its structure; or in relation to other particles, such as passing messages to each other.   Many large networks can be found in modern society, and obtaining information from these networks  is an important problem. Examples of such networks include the World Wide Web,   and social networks such as Twitter and FaceBook. These networks are very large, change  over time and are essentially unknowable or do not need to be known in detail. They are highly interlinked (e.g. URL's embedded in Twitter and FaceBook) and can be viewed as part of a larger whole. New social networks appear frequently, and  the influence of these networks on social, economic and political aspects of everyday life is substantial.   Searching, sampling and indexing the content of such networks is a major application area  a substantial user of computer time, and likely to become more so in the future. The evolving use of these networks is changing social and economic behavior. Improving the ability to search such networks is of value to us all.   Random walks are a simple method of network exploration, and as such, are particularly suitable for searching massive networks. A random walk traverses a network by moving from its current position to a neighboring vertex chosen at random. Decisions about where to go next can be taken locally, and only limited resources are needed for the search. The exploration is carried out in a fully distributed manner, and can adapt easily to dynamic networks. The long run behavior of the walk acts as a distributed voting mechanism, in which the number of visits to a vertex is proportional to its popularity or accessability.   Suppose we could alter the behavior of the random walk to reduce the search time. How can this be done, and at what cost?   Speeding up random walks, to reduce search time, is a fundamental question in the theory of computing. The price of this speed up, is normally some extra work which is performed locally by the walk, or undertaken by the vertices of the graph. Possible ways of speeding up random walks we have identified include biassed transitions, use of previous history and local exploration around current position.   One way to reduce search time is to use several random walks which search simultaneously. In the simplest model the walks are oblivious of each other and do not interact in any way. Search time should be reduced, but at the expense of  using additional walks.  Suppose we could also allow the random walks to interact with each other, or with the underlying network? How should this interaction be designed, in order  to speed up search, and what other applications might it have? Historically, interacting particle systems have only been analyzed on infinite networks, and even then not with computer science applications in mind. Recently, we began to make progress in this direction, and found that many related problems such as distributed voting, to elect a leader for example, could be understood in the framework we developed.   Potential applications of interacting particle systems are many and include: Gossiping and broadcasting among agents moving on a network, Models of epidemics spreading between particles and the graph, Distributed search with intelligent robots, Software agents moving in an intranet. Models of voting and social consensus. Good agents chasing bad agents on a network.  "
	},
	{
		"grant":334,
		"ID": "EP/J00636X/1",
		"Title": "Testing, Verifying, and Generating Software Patches Using Dynamic Symbolic Execution",
		"PIID": "-264123",
		"Scheme": "Standard Research",
		"StartDate": "31/05/2012",
		"EndDate": "30/05/2015",
		"Value": "287184",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-264123", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Maxeler Technologies"}
		],
		"Summary": "A large fraction of the costs of developing and maintaining software is associated with detecting and fixing software errors.  As a result, the last decade has seen a sustained research effort directed toward designing and developing techniques for automatically detecting software errors, with some of these techniques making their way into commercial and open-source tools.  However, detecting an error is only the first step toward fixing it.  In fact, many known errors remain unpatched due to the high cost required to diagnose and repair them, combined with the fear that patches are more likely to introduce failures compared to other types of code changes.  The goal of this research project is to address both of these problems, by devising novel techniques based on dynamic symbolic execution for: (1) automatically testing and verifying the correctness of software patches, and (2) (semi-)automatically generating candidate patches for software bugs.  The strength of dynamic symbolic execution lies in its ability to precisely model the behaviour of program paths using mathematical constraints.   However, the cost associated with this level of precision is poor scalability.  The number of paths in a program is usually exponential in the number of branches, which makes it difficult to scale the analysis to very large programs.  However, by focusing the analysis on the incremental changes introduced by program patches, we hope to significantly reduce the cost of symbolic execution and significantly increase its applicability in practice.  Furthermore, the ability to check software patches opens up the possibility of performing patch generation in an automatic or semi-automatic fashion.  In particular, starting from the mathematical constraints gathered from a buggy execution path -- and with the potential addition of a manually-written patch template -- we plan to design techniques for generating a set of candidate patches resembling the ones that would be generated manually by developers."
	},
	{
		"grant":335,
		"ID": "EP/J007439/1",
		"Title": "Ant Colony Optimisation for the Discovery of Gene-Gene Interactions in Genome-Wide Association Studies",
		"PIID": "124474",
		"Scheme": "First Grant Scheme",
		"StartDate": "12/03/2012",
		"EndDate": "11/09/2013",
		"Value": "99212",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46",
		"Investigators":[
		{"ID": "124474", "Role": "Principal Investigator"},
		{"ID": "-28899", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Genome-wide association studies investigate the small changes in DNA among individuals in a population that lead to variations in traits such as height and the propensity to suffer from diseases. Recent advances in genetic technology allow researchers to measure these small differences in DNA in a population (known as single-nucleotide polymorphisms or SNPs) and have already discovered SNPs that are associated with diseases including the widely publicised 'FTO' gene which has been shown to be highly associated with type 2 diabetes.  However, single SNPs do not account for all of the variation that is suspected to be inherited and researchers are now beginning to investigate the potential for interactions between multiple SNPs to explain this variation.  The number of possible pairs and triplets in the genome though is vast and so a full enumeration search is not possible, meaning that intelligent techniques are required to process the large space of potential interactions.  A method that has shown considerable promise in this area is ant colony optimisation (ACO), a nature-inspired search technique based on the way that insects find the shortest path from a nest to a food source in the wild.  This search algorithm has two unique properties that make it ideal for this task.  The first is that local heuristics can be used to influence the search to find specific gene-gene interactions such as epistasis and the second is that the algorithm creates a pheromone matrix that provides a detailed map of the importance of variables (SNPs) found during the search. This project will investigate the use of ACO to search the space of SNP interactions and their association with a number of diseases including type 2 diabetes and Crohn's disease and also the potential for them to explain human traits such as height.  The discovery of these interactions will advance our knowledge of how disease is inherited and could pave the way for highly personalised and pre-emptive treatment based on an individual's genetic makeup."
	},
	{
		"grant":336,
		"ID": "EP/J007463/1",
		"Title": "Autonomic Software Engineering for online cultural experiences",
		"PIID": "51628",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2011",
		"EndDate": "31/08/2013",
		"Value": "204359",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Department",
		"OrgID": "97",
		"Investigators":[
		{"ID": "51628", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project is about exploiting the predominance of social networking using autonomic software agents to enrich, encourage and enliven engagement with online cultural artefacts such as from a museum or a gallery.  With the current problems in the European financial debt, many cultural institutions are planning to shorten the length that visitors can physically enter. In the UK for example we have heard of plans that the British Museum will close earlier and possibly shut down completely for one day a week because of the massive cuts in funding that were presented in the UK Chancellors speech detailing reduction in money for the cultural sector.  The basic idea is to have your friends, museums, art galleries and theatres all in your pocket through your handheld device.  In this project we will harness the power of autonomic agents that work on behalf of human users in an infrastructure that allows for these agents to communicate and negotiate on behalf of their human users to facilitate a collective and social experience of online cultural visits.  For example, we could imagine a scenario where 4 students are visiting an art museum with the desire to purchase something (a print or a physical copy of an artefact for example) for a friend. They would wish to be able negotiate about what to see or experience online, what additional information they want to consider, what comments from what previous visitors over any commentary they individually or collectively want to leave for others and, eventually, over what they collectively choose to purchase for their friend.  We are concerned with the fundamental question of building autonomic agents that can represent their users needs, argue and negotiate on behalf of their users with other software and human agents, maintain models of the other autonomic agents in the system and proactively develop plans and scenarios for their human counterparts. In order for autonomic agents to interact in open systems such as those we are describing we will use the BDI agent architecture (arguably the most important symbolic agent architecture of the last 20 years) on a well-developed infrastructure (called electronic institutions) that facilitates autonomic agent interaction.  In short we believe BDI architectures represent the stronger and best-developed software engineering device for building autonomic agents, that electronic institutions is the best developed infrastructure for supporting the interaction of autonomous interaction, and that the idea of enabling richer social exploration of cultural artefacts online is a timely and critical case study to address.  "
	},
	{
		"grant":337,
		"ID": "EP/J007498/1",
		"Title": "Formal Representation and Proof for Cooperative Games: A Foundation for Complex Social Behaviour",
		"PIID": "57008",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "30/04/2015",
		"Value": "389557",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "57008", "Role": "Principal Investigator"},
		{"ID": "-219876", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This research applies methods and tools from mathematical knowledge management and theorem proving to theoretical economics, by working with a class of cooperative games called pillage games. Pillage games, introduced by Jordan in 2006, provide a formal way of thinking about the ability of powerful coalitions to take resources from less powerful ones. While their name suggests primitive, violent interactions, pillage games are more applicable to advanced democracies, in which coalitions seek to form governments to alter the distribution of society's resources in their favour. If, for some allocation of society's resources, the coalition preferring another allocation is stronger than that preferring the status quo, the other allocation `dominates' the status quo.  The most conceptually intriguing, and the most computationally intractable solution concept for cooperative games is the `stable set'. A stable set, has two features: no allocation in the set dominates another; each allocation outside the set is dominated by an allocation in the set. For pillage games with three agents under a few additional conditions, we have determined when stable sets exist, that they are unique and contain no more than 15 allocations, and how to determine them for a given power function.  In this research, we first formally represent the mathematical knowledge developed in Jordan's and our work using sTeX, a mathematical knowledge management tool. This allows, e.g., automatic identification of how various results depend on each other.  We then use two modern automated theorem provers (ATPs), Isabelle and Theorema, to formally prove these results. Theorem proving is a hard task and if not provided with domain specific knowledge ATPs have to search through big search spaces in order to find proofs. To increase their reasoning power, we shall seek to identify recurring patterns in proofs, and extract proof tactics, reducing the interactions necessary to prove the theorems interactively. As important results in pillage games can be summarised in pseudo-algorithms, containing both computational and non-computational steps, we shall study such pseudo-algorithms, seeking to push them towards the much more efficient computational steps. Finally, we shall use the identified proof tactics to help the ATPs prove new results in order evaluate their true value.  The research seeks to make a number of contributions. For theorem proving, pillage games form a new set of challenge problems. As the study of pillage games is new, and the canon of applicable knowledge small, this gives an unprecedented opportunity to encode most of it. The research will expand the tractable problem domain for ATPs; and - by identifying successful tactics - increase both the efficiency with which ATPs search for proofs, and - ideally - their ability to establish new results.  For economics, this is the first major application of formal knowledge management and theorem proving techniques. The few previous applications of ATP to economics have formalised isolated results without engaging economists and have thus largely gone unnoticed by the discipline. As cooperative games are a known hard class of economic problems, and pillage games known to be tractable, this research therefore presents a strong `proof of concept' for the use of ATP within economics. Cooperative game theory is formally similar to graph theory, the techniques and insights developed may be applicable to matching problems, network economics, operations research, and combinatorial optimisation more generally. Additionally, the researchers will introduce ATP techniques to the leading PhD summer school in computational economics, and are working in collaboration with economic theorists with strong computational backgrounds. Thus, the research seeks to form a focal point for formal knowledge management and theorem proving efforts in economics. "
	},
	{
		"grant":338,
		"ID": "EP/J007595/1",
		"Title": "Terahertz Gas-Fiber Photonics",
		"PIID": "34855",
		"Scheme": "Standard Research",
		"StartDate": "05/11/2012",
		"EndDate": "04/05/2016",
		"Value": "683530",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "7",
		"Investigators":[
		{"ID": "34855", "Role": "Principal Investigator"},
		{"ID": "45280", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The terahertz (THz) part of the electromagnetic spectrum lies between the domains of microwaves and optics. In the last 20 years there has been enormous growth in research on exploiting this spectral window for materials research and practical applications like security and medical screening. Many applications rely on sources of extremely short pulses of THz radiation and are limited by their presently low peak and average power. The recently discovered phenomenon of terahertz supercontinuum generation during the formation of laser ionized plasmas in free space combines remarkably broad, continuous and controllable bandwidth, stretching from the far to the mid infrared, with pulse energies as high as a micro-Joule. It promises to be an important new source of high peak and average power terahertz radiation for scientific studies of materials in extreme THz fields and for imaging and sensing. Its potential exploitation is however constrained by the low optical to THz conversion efficency, which is limited by diffraction, and the cost and low pulse repetition rate of the high energy laser systems currently needed. Our primary aim is to overcome these limitations by spatially confining the THz generation to the hollow core of a gas filled waveguide based on a type recently developed in Bath known as photonic crystal fiber. The combination of small core area and long interaction length  is expected to reduce the threshold pump energy for THz generation by orders of magnitude. The optical waveguide will be integrated with a terahertz guide. By engineering the velocities of the optical and teraherts waves by composite waveguide design to achive a condition called 'phase matching' we will at the same time increase the conversion efficiency and thus retain the high peak power of the THz radiation.  A secondary aim is to perform sensitive detection in similar composite guides by exploiting an intinsic optical nonlinearitiy of gases, thus creating the essential building blocks of what could be, with the addition of powerful fiber amplifiers currently in commercial development, a cost effective and robust 'all-fiber' platform for THz science and technology with unprecedented power and flexibility. This is our long term vision and ambition.  "
	},
	{
		"grant":339,
		"ID": "EP/J007617/1",
		"Title": "A Population Approach to Ubicomp System Design",
		"PIID": "74172",
		"Scheme": "Programme Grants",
		"StartDate": "01/12/2011",
		"EndDate": "30/11/2016",
		"Value": "3253414",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "74172", "Role": "Principal Investigator"},
		{"ID": "84313", "Role": "Co Investigator"},
		{"ID": "15688", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Edinburgh Festivals"}
		],
		"Summary": "Adding and removing modules (also called plug-ins, add-ins and other names) without technical support or supervision is becoming the norm for everyday software applications such as web browsers, image editors and email tools, e.g. Firefox, Photoshop and Outlook. This same approach is becoming important in mobile phone software, where 'in-app purchase' of modules is becoming more and more popular, and a huge money-spinner for developers.   The consequences are not all good: users often do not know which modules to use or change, to suit their goals, or whether a program will crash after such changes. As a result of modules being from different developers, their combination may never have been tested prior to public use. Evaluators and developers struggle to help, because established approaches to software definition, design and analysis are based on the structure of a program being the same wherever and whenever it is used. In constrast, one would be hard put to define a single software structure that accurately describes what a program like Firefox is. Use is similarly hard to pin down, as individuals make systems fit with their own uses and contexts, and share their innovations with others.  As a modular program becomes complex, the result is often a 'plug-in hell' of broken software dependencies, functions, uses and expectations. If, instead, software structure is kept simple, then design opportunities are lost as developers avoid the difficulty and cost of implementing innovative programs. More generally, software theory and engineering have fallen behind practice. We lack ways to reason predictively in a principled way about real world structure and use. We lack tools for designers, evaluators and users that support adaptation, and we lack principles and techniques that can deal with the scale of human and software interaction.   Our primary objective is to deliver a new science of software structures, with design, theory and tools that reflect software in real world use, and able to tackle the complex problem of how to design to support change and appropriation. The key concept is the 'software population': a statistical model of the variety we see when we look at how the same initial program has been used and adapted by its users. A population model is kept up to date by each instance of a program logging how it is used and changed. The population idea affords a common design vocabulary for users' understanding and adaptation of programs, for evaluators' analysis of programs in use, and for developers' making informed changes to the modules available to users.   As a result, users will have programs that may vary but are more comprehensible, robust and adaptable than is the case today. We will enable each individual user to make a practical decision that only he/she is qualified to make: how to balance the changed robustness and functionality of one's system with changes to the system's support for individual and social interactions. One will have tools built into one's program that makes clear what it consists of, and how its structure relates to the programs and experiences of other users. One can find out about other modules that are compatible with one's program, how it will work after adding in one or more new modules, and therefore which configurations of modules will and will not work.  In order to test whether the approach works at the scale of, for example, typical iPhone applications, we will build and deploy programs among large numbers of users, for weeks or months-mobile games and social networking applications. We will work with industrial partners including the Edinburgh Festivals, as well as using popular sports and events, such as soccer (e.g. the 2014 FIFA World Cup Brazil), athletics (e.g. the London 2012 Olympics and the Glasgow 2014 Commonwealth Games), and popular TV programmes. Overall, We plan for 500,000-1,000,000 users of our systems in the course of the programme.  "
	},
	{
		"grant":340,
		"ID": "EP/J007676/1",
		"Title": "Quantum Cascade amplifiers for high power Terahertz time domain spectrometry",
		"PIID": "-149095",
		"Scheme": "Standard Research",
		"StartDate": "24/02/2012",
		"EndDate": "23/02/2015",
		"Value": "324787",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-149095", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Terahertz (THz) light forms part of the electromagnetic spectrum, between microwaves and infrared. It can penetrate a range of materials - including polymers, ceramics and semiconductors - and shows excellent contrast in their internal microstructures. In addition, intermolecular vibrational modes in solids and hydrogen bonding networks in liquids all have resonances at THz frequencies. These unique properties of THz radiation have in recent years permitted new methods to study the interaction of molecules. However, a major limitation of this technique is the lack of high power broadband sources that can be used for spectroscopic imaging and tomography applications. Our proposed plan addresses precisely this problem.   Currently, most commercial THz spectrometers are time-domain spectrometers (TDS), where THz pulses are generated on antennas by a photocurrent created from a pulsed laser. The detection scheme uses a similar antenna where carriers are generated by the same pulsed laser. The advantage of this apparatus is that the detection scheme is synchronous: the receiver is only 'on' when the THz electric field is incident, and this results in a very high signal-to-noise ratio of approximately 50 dB. The disadvantage is that the THz pulses have only micro-Watts of output power, thus the apparatus will only penetrate thin or transparent materials. The major competing THz technology is that of quantum cascade (QC) lasers, which generate radiation with tens of mWatts of power. However, the power advantage of QC lasers is lost by the lack of sensitive detection techniques, and hence they are not used commercially. Until now researchers have tried to combine the technologies of THz-TDS and QCs but the two geometries have proven very difficult to integrate, with antenna emitters in particular proving incompatible with integration.   However, a new geometry emerged in 2010: the so-called the lateral photo-Dember effect that can be used to generate broadband THz pulses. The effect is quite simple, relying on the different mobilities of holes and electrons in a semiconductor which create a changing dipole under photoexcitation to generate THz pulses. We believe that this effect has great potential because it is flexible and its geometry is compatible with integration and quantum cascade lasers. Using the lateral photo-Dember effect will provide an elegant means of coupling a THz pulse into the QC structure, directly, with great efficiency. We intend to exploit this effect and generate THz pulses directly on the facet of a QC cavity and amplify them in the QC waveguide. Therefore we will combine the high output power of quantum cascade lasers with the detection sensitivity and broadband nature of state-of-the-art time-domain technology. It is a game-changing approach that is, according to all indications, absolutely feasible. It is very rare to propose such a potentially high impact research route, which is at the same time such low risk!   Detailed THz spectroscopic studies of samples in our groups have demonstrated an excellent potential to reveal the microstructure of the materials which is key to its industrial performance; but non-destructive studies of the entire specimen are currently impossible due to limited available power. We will use the high power pulses generated by the QC amplifier to explore non-destructive imaging of samples of key importance to sustainable energy and healthcare research. We will apply it to non-destructive THz imaging and tomography across a range of materials, which it is not possible to achieve with today's instruments due to their inherent lack of power. Our research will make a fundamental contribution to explore novel routes to high power broadband THz devices and we will demonstrate how such technology can advance understanding of materials and processes in the chemical and pharmaceutical industries.  "
	},
	{
		"grant":341,
		"ID": "EP/J007838/1",
		"Title": "Device-Independent Quantum Information Processing",
		"PIID": "79530",
		"Scheme": "Standard Research",
		"StartDate": "29/02/2012",
		"EndDate": "28/02/2015",
		"Value": "131840",
		"ResearchArea": "Quantum Optics and Information",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "79530", "Role": "Principal Investigator"},
		{"ID": "53005", "Role": "Co Investigator"},
		{"ID": "-205047", "Role": "Co Investigator"},
		{"ID": "107720", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Device-Independent Quantum Information Processing represents a new paradigm for quantum information processing: the goal is to design protocols to solve relevant information tasks without relying on any assumption on the devices used in the protocol. For instance, protocols for device-independent key distribution aim at establishing a secret key between two honest users whose security is independent of the devices used in the distribution. Contrary to standard quantum information protocols, which are based on entanglement, the main resource for device-independent quantum information processing is quantum non-locality. Apart from the conceptual interest, device-independent protocols offer important advantages from an implementation point of view: being device-independent, the realizations of these protocols, though technologically challenging, are more robust against device imperfections. Current and near-future technology offer promising perspectives for the implementation of device-independent protocols.  This project explores all these fascinating possibilities. Its main objectives are (i) obtaining a better characterization of non-local quantum correlations from an information perspective, (ii) improve existing and derive new application of this resource for device-independent quantum information processing and (iii) design feasible implementations of device-independent protocols. We plan to tackle these questions with an inter-disciplinary approach combining concepts and tools from Theoretical and Experimental Physics, Computer Science and Information Theory."
	},
	{
		"grant":342,
		"ID": "EP/J007854/1",
		"Title": "Rydberg excited Calcium Ions for Quantum Interactions",
		"PIID": "-240925",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2011",
		"EndDate": "31/10/2014",
		"Value": "225543",
		"ResearchArea": "Quantum Optics and Information",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104",
		"Investigators":[
		{"ID": "-240925", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Trapped cold ions are among the most advanced systems to implement quantum information processing. In current experiments entanglement of the qubits, represented by long lived internal atomic states, is achieved via quantum control of the (collective) motion of the ion crystal. Instead, we propose an unprecedented experimental program supported by theory, where the huge dipole moments associated with Rydberg excited ions are the basis of extremely strong spin-dependent long range interactions, and thus exceptionally fast entangling operations as basic building blocks for quantum computing and quantum simulation. While in the short term the fundamental questions to be explored are the understanding of Rydberg excitation and dynamics of single and multiple ions stored in linear Paul traps, and the various ways of manipulating this dynamics with external electromagnetic fields, the long term promise of this project is a potentially scalable very fast ion trap quantum processor, and in particular also a novel efficient quantum simulator of spin models, for Heisenberg type interactions to exotic matter with topological phases. A main experimental challenge is the requirement of a coherent light source near 122nm for the ion Rydberg excitation. Our consortium is in the remarkable and unique situation where in a single laboratory both these coherent light sources as well as advanced ion quantum computing setups are available, thus allowing us to explore this extremely promising new frontier of Rydberg ion quantum information processing on a comparatively short time scale. The planned experiments will be based on the well established techniques of ion trapping, quantum state detection and manipulation with laser fields. An adapted quantum shelving method is proposed to detect transitions to Rydberg states with unity detection efficiency on individual ions even in large crystals. Initially we will accurately determine energy levels and atomic properties of ion Rydberg states, and then we aim for mutual Rydberg state interactions of adjacent ions. Such gate interactions, Rydberg induced quantum phase transitions and a full tomography of the resulting quantum state benefit from the highly developed schemes in quantum information processing. In the future, beyond the experimental horizon of the three-year project, fast Rydberg ion quantum logic operations could possibly be combined with the conventional gate schemes and modern ion trap technology."
	},
	{
		"grant":343,
		"ID": "EP/J007951/1",
		"Title": "Quantum Information with NV Centres",
		"PIID": "11552",
		"Scheme": "Standard Research",
		"StartDate": "03/10/2011",
		"EndDate": "02/10/2014",
		"Value": "106930",
		"ResearchArea": "Quantum Optics and Information",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "31",
		"Investigators":[
		{"ID": "11552", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of QINVC is to exploit the superior quantum coherence of the spins of the negatively charged Nitrogen-Vacancy (NV) colour centres in diamond, at both room and low temperature, for quantum information processing (QIP). This system is indeed among the best solid-state quantum systems in terms of coherence, ease of manipulation by ESR, and addressability down to the single spin using optical microscopy. Our project first focuses on two hybrid strategies for QIP (WP1). The first one consists in fabricating regular arrays of single NV centres, with gate operation performed using a movable NV spin placed at the apex of a cantilever, coming close enough to get entangled with a NV spin on the lattice using magnetic dipolar interaction. The fabrication method is based on pulsed ion beam implantation through a tiny hole threading the tip of an AFM cantilever, or through the voids of a mask deposited at the surface of the diamond sample. The second strategy implements a hybrid architecture for QIP based on circuit QED and ensembles of NV spins: transmon qubits are coupled to ensembles of NV spins through the resonator in which they are embedded. The NV spins will be used as a long-coherence time quantum memory for the transmon qubits which will be used to process quantum information.  QINVC will investigate in WP2 the optical properties of NV centres at low temperature. The first goal of WP2 is to transfer the optical techniques used at room temperature to control electron and nuclear spins to low-temperatures, in the purpose of applying them to hybrid quantum circuits (WP1). A second goal is to investigate the potential of NV centres ensembles to build a quantum memory for optical photons. These ambitious goals will request the optimisation of NV centre production and their spin properties in synthetic diamond (WP3) using state-of-the-art methods and beyond. This will be done in collaboration with the world industrial leader on the production of synthetic diamond Element6 Ltd. Engineered implantation of single N impurities with nanometer resolution will be performed for WP1, and suitable concentrations will be prepared for both WP1 and WP2. Sample processing will be developed for these two WPs in order to minimize the unwanted defects that cause decoherence. Innovative fabrication techniques will be also developed, such as the preferential alignment of NV centres under uniaxial stress, an appealing possibility. "
	},
	{
		"grant":344,
		"ID": "EP/J00796X/1",
		"Title": "HIPERCOM",
		"PIID": "51433",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2011",
		"EndDate": "30/11/2014",
		"Value": "149506",
		"ResearchArea": "Quantum Optics and Information",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "51433", "Role": "Principal Investigator"},
		{"ID": "-249569", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Coherent optics has been known since the 1960's to be, in principle, the best tool to achieve very high bandwidths and bit rates in optical communication. While the development of fiber optical amplifiers in the 1980's has reduced the need for developing such a technology, the advent of quantum information sciences has triggered a renewed interest in using coherent optics to realize high-rate quantum communication systems. The present project is focused on coherent quantum communication as a way to combine the intrinsically very high rates achievable by homodyne or heterodyne detection with the fundamental benefits of using quantum mechanics such as unconditional security.  Unlike with classical communication systems, an optical amplifier cannot be used as a repeater in a quantum communication setup because it is inherently limited by quantum noise. The thrust of this project is to explore different techniques aiming at circumventing this problem and improving the range of coherent (also called continuous-variable) quantum communication systems, with a special emphasis on today's most developed platform towards practical applications, namely continuous-variable quantum key distribution. Different strategies will be followed in order to attain this goal, ranging from the use of classical coding and other post-processing algorithms, which is the most directly applicable solution in the short term, to more elaborate longer-term techniques relying on specific quantum optical schemes and ultimately on the use of quantum coding. In particular, the potential solutions offered by the heralded noiseless linear amplifier, or other non-Gaussian heralded operations, will be investigated in detail.  The specificity of our consortium is to combine the strength of 5 academic groups having an outstanding track record in the area of coherent (continuous-variable) quantum information science, including 2 theory (Universite Libre de Bruxelles, University of York) and 3 experimental (Institut d'Optique Graduate School, Max Planck Institute for the Science of Light, Telecom Paris Tech) groups, together with 1 industrial partner (SeQureNet) who will naturally orient the research towards the needs of the information society. We envision that this synergy between applied and fundamental - both theoretical and experimental - teams will be highly stimulating and productive, and will reinforce European competitiveness in information technologies."
	},
	{
		"grant":345,
		"ID": "EP/J007994/1",
		"Title": "Solid State Quantum Networks (SSQN)",
		"PIID": "18960",
		"Scheme": "Standard Research",
		"StartDate": "05/12/2011",
		"EndDate": "04/12/2014",
		"Value": "291902",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "18960", "Role": "Principal Investigator"},
		{"ID": "-127839", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Quantum communication, the transfer of quantum superposition states over long distances, is presently limited to about 200km (both in optical fibre and free space) due to unavoidable photon absorption losses. For this reason, theoretical schemes to extend this distance using 'entanglement swapping' and 'teleportation' have been established. By concatenating short entanglement swapping sub-sections it is in principle possible to generate entangled (correlated) bits over very long distances with bit rate only limited by the losses in one short section. If realised this would extend quantum communication applications such as quantum cryptography and quantum teleportation out to distances of thousands of kilometres. In this consortium we propose to work towards such a deterministic quantum network based on semiconductor quantum dot-micropillar cavity systems. We will generate entangled photon sources from the biexciton-exciton cascade of a quantum dot (QD), with a potential fidelity of >90%. Moreover, we will develop a QD-spin micropillar cavity system, which acts as an all-in-one spin-photon-interface and a Bell-state analyser. This component eliminates the need for synchronous arrival of the two photons, and allows a wait-until-success protocol over the timescale of the spin coherence time (microseconds to milliseconds). Further subcomponents will include electro-optically tuneable single photon sources and recently proposed sequentially entangled sources. With this suite of subcomponents we will be able to realise all the functions required for a scalable quantum network including the final entanglement purification steps. This is in contrast to previous experimental demonstrations of entanglement swapping (and teleportation) which were probabilistic and thus unscalable. The project involves collaboration between four partners. We will bring together two world-class groups, LPN and Wrzburg (UWUERZ), working on micropillar cavities producing highly efficient entangld pair sources (LPN), and strongly-coupled QD-spin-cavity systems (UWUERZ), with the aim of addressing the challenging issues of entangled-pair sources and spin-cavity systems. Theoretical support for novel and practical entanglement schemes will be provided by Imperial College (IMP), and the experimental implementation will be performed by Bristol (BRIS) and LPN, who have world-class expertise in quantum optical communication , QD spins and semiconductor microcavity quantum electro-dynamics."
	},
	{
		"grant":346,
		"ID": "EP/J00801X/1",
		"Title": "Plasticity in NEUral Memristive Architectures",
		"PIID": "-220230",
		"Scheme": "Standard Research",
		"StartDate": "05/09/2011",
		"EndDate": "04/09/2014",
		"Value": "482747",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "15956", "Role": "Principal Investigator"},
		{"ID": "-220230", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During the past two decades, philosophers, psychologists, cognitive scientists, clinicians and neuroscientists strived to provide authoritative definitions of consciousness within a neurobiological framework. Engineers have more recently joined this quest by developing neuromorphic VLSI circuits for emulating biological functions. Yet, to date artificial systems have not been able to faithfully recreate natural attributes such as true processing locality (memory and computation) and complexity (10^10 synapses per cm2), preventing the achievement of a long-term goal: the creation of autonomous cognitive systems.  This project aspires to develop experimental platforms capable of perceiving, learning and adapting to stimuli by leveraging on the latest developments of five leading European institutions in neuroscience, nanotechnology, modeling and circuit design. The non-linear dynamics as well as the plasticity of the newly discovered memristor are shown to support Spike-based- and Spike-Timing-Dependent-Plasticity (STDP), making this extremely compact device an excellent candidate for realizing large-scale self-adaptive circuits; a step towards 'autonomous cognitive systems'. The intrinsic properties of real neurons and synapses as well as their organization in forming neural circuits will be exploited for optimising CMOS-based neurons, memristive grids and the integration of the two into realtime biophysically realistic neuromorphic systems. Finally, the platforms would be tested with conventional as well as abstract methods to evaluate the technology and its autonomous capacity."
	},
	{
		"grant":347,
		"ID": "EP/J00801X/2",
		"Title": "Plasticity in NEUral Memristive Architectures",
		"PIID": "-220230",
		"Scheme": "Standard Research",
		"StartDate": "15/04/2013",
		"EndDate": "14/09/2014",
		"Value": "218244",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-220230", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During the past two decades, philosophers, psychologists, cognitive scientists, clinicians and neuroscientists strived to provide authoritative definitions of consciousness within a neurobiological framework. Engineers have more recently joined this quest by developing neuromorphic VLSI circuits for emulating biological functions. Yet, to date artificial systems have not been able to faithfully recreate natural attributes such as true processing locality (memory and computation) and complexity (10^10 synapses per cm2), preventing the achievement of a long-term goal: the creation of autonomous cognitive systems.  This project aspires to develop experimental platforms capable of perceiving, learning and adapting to stimuli by leveraging on the latest developments of five leading European institutions in neuroscience, nanotechnology, modeling and circuit design. The non-linear dynamics as well as the plasticity of the newly discovered memristor are shown to support Spike-based- and Spike-Timing-Dependent-Plasticity (STDP), making this extremely compact device an excellent candidate for realizing large-scale self-adaptive circuits; a step towards 'autonomous cognitive systems'. The intrinsic properties of real neurons and synapses as well as their organization in forming neural circuits will be exploited for optimising CMOS-based neurons, memristive grids and the integration of the two into realtime biophysically realistic neuromorphic systems. Finally, the platforms would be tested with conventional as well as abstract methods to evaluate the technology and its autonomous capacity."
	},
	{
		"grant":348,
		"ID": "EP/J008052/1",
		"Title": "Integrated Photonic Materials and Devices",
		"PIID": "4633",
		"Scheme": "Platform Grants",
		"StartDate": "01/03/2012",
		"EndDate": "29/02/2016",
		"Value": "1135914",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "4633", "Role": "Principal Investigator"},
		{"ID": "30005", "Role": "Co Investigator"},
		{"ID": "59852", "Role": "Co Investigator"},
		{"ID": "57632", "Role": "Co Investigator"},
		{"ID": "110324", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Covesion Ltd"},
		{"Number": "1", "Name": "DSTL Porton Down"},
		{"Number": "2", "Name": "Element Six Ltd (UK)"},
		{"Number": "3", "Name": "Gooch & Housego Plc"},
		{"Number": "4", "Name": "Ocean Optics"},
		{"Number": "5", "Name": "Stratophase Ltd"},
		{"Number": "6", "Name": "US Air Force Research Laboratory"}
		],
		"Summary": "This platform grant will underpin integrated photonics research in advanced laser sources, photonic circuits, and sensors, at the Optoelectronics Research Centre (ORC) at the University of Southampton, leveraging the recent investment of >100M in the new Mountbatten Fabrication Complex.  Photonic materials and device research has been the key driver of many disruptive advances in telecommunications, healthcare, data storage, display and manufacturing, and this platform grant will provide the group with the horizon and stability to build upon its international standing to explore new high-risk, high-reward research avenues.  Integrated photonic materials and devices of the future will play a huge role in the next generation of cheaper, faster, greener, disposable, miniaturised and more versatile systems based on silica and silicon, glasses, crystal and polymer hosts, in both channel and planar geometries.  The broad range of expertise within our group and our access to the unequalled brand-new planar fabrication facilities will allow us to fully explore this diverse research area. Impact will be realised through applications in compact kW-class waveguide lasers (new manufacturing techniques), pollution sensors (monitoring climate change), optical amplifiers and switches (high-speed data control), early threat detection devices (homeland security), and fast universally accessible disease screening (point-of-care medical diagnostics).  Applications for the photonic materials, processes and devices developed during this platform grant will play a key role in fields of interest to society, Industry as well as university-based research and development, and will be pursued in collaboration with both existing and newly-identified partners during the programme."
	},
	{
		"grant":349,
		"ID": "EP/J008133/1",
		"Title": "Trustworthy Ambient Systems: Resource Constrained Ambience",
		"PIID": "77274",
		"Scheme": "Platform Grants",
		"StartDate": "01/02/2012",
		"EndDate": "31/01/2016",
		"Value": "981508",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "77274", "Role": "Principal Investigator"},
		{"ID": "44685", "Role": "Co Investigator"},
		{"ID": "34805", "Role": "Co Investigator"},
		{"ID": "46590", "Role": "Co Investigator"},
		{"ID": "22017", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Advances in communications and networking technology have made it possible to devise 'ambient' systems in which mobile devices and software agents form ad hoc groups, trading data and services. However, the technology needed to engender trust in such complex systems, and their resilience to faults and attacks, is only in its infancy.  The TrAmS platform grant sustained a research group that created new projects on technical foundations, methods and tools to model, design and analyse Trustworthy Ambient Systems. TrAmS-2 is, however, shaped by new factors. First, power provision/consumption of devices, rather than cost, is becoming a limiting factor in the deployment of ambient systems. Second, novel paradigms such as cloud computing offer a new dimension of ambience in that data and programs can be migrated without physical movement of agents. Ambient systems can therefore mix mobile devices with mobile software and services, using resources on demand. This increases the significance of threats such as power loss/limitation, and lack of trust in an on-demand computing infrastructure. These factors mean that traditional assumptions underpinning the engineering of fault-tolerant, dependable systems will be challenged.    TrAmS enabled lines of enquiry on formal engineering methods, proof support, embedded systems design, dynamic coalitions and contract-based 'systems of systems' architectures. These led to 9 EPSRC, EU, industry and other projects with applications in automotive, rail, space, business and other sectors. Concrete outputs included tools and patterns for fault tolerance modelling, advances in proof technology, simulation and evidence to support deployment of formal engineering methods.  In TrAmS-2, the group will focus on the most challenging aspects of resource-limited future ambient systems. This requires skills in other areas besides fault tolerance, so we have augmented the TrAmS team with researchers in systems and microelectronics to create a group with an international profile in dependability, data management and asynchronous systems. TrAmS-2 will provide continuity of research staff, encouraging new, risky, research in areas created by this new mix of expertise.   The design and management of trustworthy ambient systems is necessarily a cooperative, large-scale, and potentially error-prone undertaking, partly because they cannot be designed as a coherent whole. Mobility (physical and virtual) makes them open to malicious and accidental failures that are difficult to predict in design. Decentralisation makes controlled recovery and evolution difficult. Lack of power can crash components, but fault tolerance costs extra power. Complex ambient systems yield verification problems beyond state-of -the-art tools. TrAmS-2 addresses these challenges in four domains:   Foundations: work towards calculi that are rich enough to describe the architectures, functionality and stochastic properties of ambient systems composed of diverse services with multiple users and owners.   Tools: exploring the development of cooperative, cloud-enabled design environments that ease access to analytic services to allow the full range of interactive verification techniques to be applied on demand to ambient system designs.  Tractable Design: work towards making design of trustworthy ambient systems designs more tractable by adding facilities to manage the added complexity of error detection and recovery without losing the underlying system structure.   Energy-Aware Ambient Systems: exploring the interplay between energy-awareness and resilience, and the provision of predictable tolerance of energy-induced threats.   Finally, TrAmS-2 will allow the group to continue taking a strategic view of its research and will help develop the careers of its members by building a group of mentors for the team members at all levels, establishing new links and exchanges, leading to further projects.  "
	},
	{
		"grant":350,
		"ID": "EP/J00815X/1",
		"Title": "The Solid State Quantum Network",
		"PIID": "107816",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2011",
		"EndDate": "30/09/2014",
		"Value": "188894",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "107816", "Role": "Principal Investigator"},
		{"ID": "125260", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Quantum communication, the transfer of quantum superposition states over long  distances, is presently limited to about 200km (both in optical fibre and free space) due to  unavoidable photon absorption losses. This research aims to extend this  distance using 'entanglement swapping' and 'teleportation'. By  concatenating short entanglement swapping sub-sections it is in principle possible to  generate entangled (correlated) bits over very long distances with bit rate only limited by the  losses in one short section. If realised this would extend quantum communication  applications such as quantum cryptography and quantum teleportation out to distances of  thousands of kilometres.      "
	},
	{
		"grant":351,
		"ID": "EP/J008249/1",
		"Title": "Device Independent Quantum Information Processing",
		"PIID": "-188465",
		"Scheme": "Standard Research",
		"StartDate": "29/02/2012",
		"EndDate": "28/02/2015",
		"Value": "91722",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Mathematics",
		"OrgID": "43",
		"Investigators":[
		{"ID": "-188465", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Device-Independent Quantum Information Processing represents a new paradigm for quantum information processing: the goal is to design protocols to solve relevant information tasks without relying on any assumption on the devices used in the protocol. For instance, protocols for device-independent key distribution aim at establishing a secret key between two honest users whose security is independent of the devices used in the distribution. Contrary to standard quantum information protocols, which are based on entanglement, the main resource for device-independent quantum information processing is quantum non-locality. Apart from the conceptual interest, device-independent protocols offer important advantages from an implementation point of view: being device-independent, the realizations of these protocols, though technologically challenging, are more robust against device imperfections. Current and near-future technology offer promising perspectives for the implementation of device-independent protocols.  This project explores all these fascinating possibilities. Its main objectives are (i) obtaining a better characterization of non-local quantum correlations from an information perspective, (ii) improve existing and derive new application of this resource for device-independent quantum information processing and (iii) design feasible implementations of device-independent protocols. We plan to tackle these questions with an inter-disciplinary approach combining concepts and tools from Theoretical and Experimental Physics, Computer Science and Information Theory."
	},
	{
		"grant":352,
		"ID": "EP/J008249/2",
		"Title": "Device Independent Quantum Information Processing",
		"PIID": "-188465",
		"Scheme": "Standard Research",
		"StartDate": "18/02/2013",
		"EndDate": "17/07/2015",
		"Value": "91482",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-188465", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Device-Independent Quantum Information Processing represents a new paradigm for quantum information processing: the goal is to design protocols to solve relevant information tasks without relying on any assumption on the devices used in the protocol. For instance, protocols for device-independent key distribution aim at establishing a secret key between two honest users whose security is independent of the devices used in the distribution. Contrary to standard quantum information protocols, which are based on entanglement, the main resource for device-independent quantum information processing is quantum non-locality. Apart from the conceptual interest, device-independent protocols offer important advantages from an implementation point of view: being device-independent, the realizations of these protocols, though technologically challenging, are more robust against device imperfections. Current and near-future technology offer promising perspectives for the implementation of device-independent protocols.  This project explores all these fascinating possibilities. Its main objectives are (i) obtaining a better characterization of non-local quantum correlations from an information perspective, (ii) improve existing and derive new application of this resource for device-independent quantum information processing and (iii) design feasible implementations of device-independent protocols. We plan to tackle these questions with an inter-disciplinary approach combining concepts and tools from Theoretical and Experimental Physics, Computer Science and Information Theory."
	},
	{
		"grant":353,
		"ID": "EP/J008346/1",
		"Title": "PrOQAW: Probabilistic Ontological Query Answering on the Web",
		"PIID": "-266029",
		"Scheme": "Standard Research",
		"StartDate": "30/04/2012",
		"EndDate": "29/10/2015",
		"Value": "813812",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-266029", "Role": "Principal Investigator"},
		{"ID": "-196479", "Role": "Co Investigator"},
		{"ID": "-106108", "Role": "Co Investigator"},
		{"ID": "-185626", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The next revolution in Web search as one of the key technologies of the Web has just started with the incorporation of ideas from the Semantic Web, aiming at transforming current Web search into some form of semantic search and query answering on the Web, by adding meaning to Web contents and queries in the form of an underlying ontology. This also allows for more complex queries, and for evaluating queries by combining knowledge that is distributed over many Web pages, i.e., by reasoning over the Web.   Realizing such semantic search and query answering on the Web by adding ontological meaning to the current Web conceptually means annotating Web pages and their contents relative to that ontology, i.e., relating Web pages and their contents to and thus also via that ontology. From a practical perspective, one of the most promising ways of realizing this is to perform data extraction from the current Web relative to the underlying ontology, store the extracted data in a knowledge base, and realize semantic search and query answering on this knowledge base. There are recently many strong research activities in this direction.  A major unsolved problem in the above context is the principled handling of uncertainty: In addition to natural uncertainty as an inherent part of Web data, one also has to deal with uncertainty resulting from automatically processing Web data. The former also includes uncertainty due to incompleteness and inconsistency in the case of missing and over-specified information, respectively. The latter includes uncertainty due to, e.g., the automatic annotation of Web pages and their contents, the automatic extraction of knowledge from the Web, matching between different related ontologies, and the integration of distributed Web data sources.  The central goal of the proposed research is to develop a family of probabilistic data models for knowledge bases extracted from the Web relative to an underlying ontology, along with scalable query answering algorithms, which may serve as the backbone for next-generation technologies for semantic search and query answering on the Web. We believe that such probabilistic data models and query answering algorithms can be developed by integrating ontology languages, database technologies, and formalisms for managing probabilistic uncertainty in the context of the Web. The objectives include developing probabilistic data models, developing algorithms for ranking and query answering, identifying useful scalable fragments, and practically evaluating our results."
	},
	{
		"grant":354,
		"ID": "EP/J008427/1",
		"Title": "Language Processing for Literature Based Discovery in Medicine",
		"PIID": "79623",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2015",
		"Value": "293127",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "79623", "Role": "Principal Investigator"},
		{"ID": "-381013", "Role": "Co Investigator"},
		{"ID": "28022", "Role": "Co Investigator"},
		{"ID": "-19702", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The amount of published material in biomedicine has been growing exponentially in recent years, particularly in very productive areas, such as genomics. The knowledge it contains is now so vast and fragmented that it is no longer possible for any individual or research group to keep up with the advances relevant to their area. The research literature is also fragmented and researchers naturally concentrate their attention on their own area of expertise, meaning they may not identify research that is relevant to their own if it does not appear within the literature of their scientific discipline. However, medical research is becoming increasingly interdisciplinary with progress being made by combining outputs from various fields.  Hidden knowledge occurs when a connection can be inferred by combining information from multiple documents, but that connection has not been noticed. Literature Based Discovery (LBD) provides tools that analyse the research literature to identify hidden knowledge automatically. Connections it has been used to identify include treatments for diseases (e.g. that fish oil can be used to treat Raynaud's syndrome) and cases of diseases (e.g. that migraines can be related to magnesium deficiency). Despite these successes, the knowledge that has been discovered has been limited by the relatively simple techniques used to analyse the research literature.   This project will develop new approaches to LBD by applying recent advances in the automatic processing of biomedical literature. This analysis will provide a LBD system with more detailed and accurate information about this literature than has previously been possible. In particular, the project will make use of two language processing technologies, Information Extraction and Word Sense Disambiguation, which can now be applied to the biomedical literature on a large scale. Information Extraction will be used to identify connections between items mentioned in documents and will provide more accurate analysis than the simple techniques used by previous LBD systems. Word Sense Disambiguation will be used to avoid the problems caused by polysemy and synonymy (the suggestion of spurious connections and connections being missed) which can adversely effect LBD performance.  The project will implement a LBD system and test it on two domains: oncology and neuroscience. The effectiveness of the system will be judged by researchers working in these areas with interests in melanoma and Parkinson's disease."
	},
	{
		"grant":355,
		"ID": "EP/J008842/1",
		"Title": "Exploiting the bandwidth potential of multimode optical fibres",
		"PIID": "-79875",
		"Scheme": "Standard Research",
		"StartDate": "25/06/2012",
		"EndDate": "24/06/2016",
		"Value": "527233",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-79875", "Role": "Principal Investigator"},
		{"ID": "-53274", "Role": "Co Investigator"},
		{"ID": "126618", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Gennum UK Ltd"},
		{"Number": "1", "Name": "Oclaro Technology UK"},
		{"Number": "2", "Name": "Rsoft Design Group"}
		],
		"Summary": "Historically the optical fibre was perceived to provide 'unlimited' bandwidth, however, the capacity of current communications systems based on single mode optical fibre technology is very close to the limits (within a factor of 2) imposed by the physical transmission properties of single mode fibres. The major challenge facing optical communication systems is to increase the transmission capacity in order to meet the growing demand (40% increase year-on-year) whilst reducing the cost and energy consumption per bit transmitted. If new technologies are not developed to overcome the capacity limitations inherent in single mode fibres and unlock the fibre bandwidth then the growth in the digital services, applications and the economy that these drive is likely to be curtailed. The need for increased capacity in the core and metro areas of the network and within computing data centres is likely to become even more acute as optical access technologies, providing far greater bandwidths directly to the users, take hold and services such as ubiquitous cloud computing are adopted. Multimode optical fibres (MMF) offer the potential to increase the capacity beyond that of current technologies by exploiting the spatial modes of the MMF as additional transmission paths. To fully exploit this available capacity it is necessary to use coherent optical (CO) reception and multiple-input multiple-output (MIMO) digital signal processing techniques analogous to those already used in wireless communication systems such as WiFi. This project aims to develop the technologies and sub-systems required to implement a CO-MIMO system over MMF that exceeds the capacity of current single mode fibre systems and reduces the cost and energy consumption per bit transmitted. To achieve this goal the project addresses the following key engineering challenges necessary to realise a complete system demonstrator. Engineer the channel: The multimode optical fibre MIMO channel, unlike its wireless counterpart, presents the opportunity to engineer the optical channel to optimise its performance for MIMO operation by designing and fabricating new optical fibres, using proven solid core technology, to maximise the MIMO capacity of the fibre. Dynamically control the channel: The transmission characteristic of the multimode optical fibre channel varies with time. We will exploit both the flexible and fast adaptive nature of digital signal processing, and the less energy intensive and slower adaptation of liquid crystal spatial light modulator based optical signal processing to compensate for the channel variation and recover the spatially multiplexed data channels. Employ energy efficient optical amplification: In order to reduce both the energy consumption and cost per bit and to extend the propagation distance into the hundreds of kilometres region it is essential to develop optical fibre amplification technologies that provide amplification to multiple spatial and wavelength channels and thus share the cost. Coherently detect the optical signal to exploit the wavelength and spatial domains: The developed system will combine spatial multiplexing with existing dense wavelength division multiplexing, polarisation multiplexing and multilevel modulation techniques to maximise the capacity. The key to achieving this is the use of coherent optical detection and digital signal processing, which is essential not only to fully exploit the spatial capacity of the MMF channel, but also facilitates the use of existing multiplexing techniques that are difficult to realise in conventional multimode transmission systems. The technologies and systems developed within this project will find applications, ranging from capacity upgrades of existing MMF data networks in data and computer processing centres, through to the installation of new high capacity metro and long haul fibre transmission systems using the MIMO optimised fibres and technologies developed in this project. "
	},
	{
		"grant":356,
		"ID": "EP/J009016/1",
		"Title": "The Influence of Excited State Physics in Conjugated Polymer Devices",
		"PIID": "43826",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2015",
		"Value": "453359",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics and Astronomy",
		"OrgID": "119",
		"Investigators":[
		{"ID": "43826", "Role": "Principal Investigator"},
		{"ID": "94859", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cambridge Display Technology Ltd"}
		],
		"Summary": "There has been remarkable progress over the last decade in making displays, lighting panels, solar cells and lasers out of flexible, plastic materials. This has a wide range of potential applications, such as roll up TV displays or having power generation, sensors and data communications systems woven into your clothing. The technology of organic LEDs has now matured to the degree that OLED displays are mass produced in consumer products such as mobile phones.   The next generations of plastic electronics products will include OLED lighting, solar cells and lasers. It is now clear however that to deliver the technology for these demanding applications it is necessary to develop a deeper understanding of the basic materials physics. In all of these devices the physics of the excited states of molecules plays a crucial role in performance. In OLEDs the efficiency at high brightness is limited by the absorption due to charge carriers and various interactions that quench the light emission from excited states. In lasers there is a delicate interplay of the excited state physics and laser losses, and so far little is known about how the chemical and structural properties of the materials may be used to control this.   This proposal seeks to develop this understanding by bringing together the expertise of two groups: one who are experts in measuring the optoelectronic performance of these polymers and in their application for photonics, and the other who are experts in the quantum theory of organic materials.   Through a combination of theory and experiment we will aim to understand the complex excited state interactions of organic semiconductors, and uncover new design strategies to control these processes. This would help us to optimise the performance (e.g. efficiency and brightness) of current devices; and enable new generations of photonic devices based on these materials.   We will make optical measurements of the fundamental excited-state processes in the materials and their behaviour under device conditions. Using state-of-the-art techniques in quantum mechanics we can also simulate the microscopic physics which gives rise to these effects. Measuring these interactions in working devices is particularly demanding and to achieve this we will also draw on specific complementary expertise from our project partners at Cambridge Display Technologies and the University of Alicante. We will then apply our new knowledge of excited states to the operation of a range of organic devices including OLEDs, lasers, solar cells and optical amplifiers. We will quantify the significance of the different excited state interactions and develop design strategies that can minimise parasitic processes and optimise operation.  "
	},
	{
		"grant":357,
		"ID": "EP/J009075/1",
		"Title": "New Techniques for Finding and Analysing Information Leaks",
		"PIID": "-136653",
		"Scheme": "First Grant Scheme",
		"StartDate": "02/04/2012",
		"EndDate": "01/08/2013",
		"Value": "88652",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-136653", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We all rely on the security of computer systems in  our daily lives; from using a credit card or ATM, to checking our e-mail, we need the computer systems around us to keep our secrets and to preserve our privacy. It would be misleading to think of these systems as either perfectly secure, or entirely broken and open to abuse. For example, some systems can be broken by brute force but still provide some protection against a casual attacker, whereas other systems might leak a small amount of information that could be exploited by an attacker over time. Understanding and measuring the different levels of security that a system might offer is vital if we are going to develop a safe, efficient digital world.  This project will develop new, effective techniques to find and measure security flaws in computer systems. We will use information theory to measure how much information an attacker can learn about the secret information inside a system by observing its public outputs. The key novelty of our approach is to use a combination of statistics and information theory to measure how secure a system is from trial runs of that system.   We will use concepts, such as network information theory and differential entropy, to develop general definitions of quantitative security. These concepts have never before been applied in the field of computer security, and they will lead to better, more expressive definitions that can be applied in a wide range of situations.  Using our theoretical work, we will develop automatic analysis tools, and we will use these to assess the security of a range of systems, including the Freenet anonymity system and RFID tags. We hope that this will serve as an example to other developers and researchers, showing that our tools make information theory-based analysis methods practical and easy to use. This may potentially lead to improved security of many commercial computer systems and faster, easier ways to find information leaks."
	},
	{
		"grant":358,
		"ID": "EP/J009113/1",
		"Title": "Classical Dependent Type Theories",
		"PIID": "-108007",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2012",
		"EndDate": "31/08/2013",
		"Value": "98545",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "43",
		"Investigators":[
		{"ID": "-108007", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The job of a mathematician is to prove theorems - that is, to provide a rigorous, logical argument that establishes that a mathematical statement is true.  If the proof is correct, then we can be certain that the theorem is true.  But how can we be certain that a proof is correct?  Proof assistants are computer programs that help the user to formalise a mathematical proof, and check that the proof is correct.  They are slowly becoming important for research mathematicians.  They have also become very important in the computing industries, for software verification and hardware verification: a formal proof that a product has the properties it is supposed to have, checked by a proof assistant, allows a high degree of confidence in the product.  When designing a proof assistant, one must first choose a system of logic.  A system of logic consists of a symbolic language in which theorems and proofs may be written, together with a set of rules for deciding which proofs are correct.  The systems of logic known as type theories have proven very successful for use in proof assistants.  Given a system of logic, we may ask: which proofs can be formalised in this system?  Proofs are divided into 'constructive proofs' and 'classical proofs'.  Most computer scientists, and the vast majority of mathematicians, accept classical proofs.  But proof assistants based on type theory have, so far, only been able to formalise constructive proofs; and this is quite a large constraint on their usefulness.  Certain objects, called control operators, may be added to a type theory.  When these are added to some simple type theories, the theories now accept classical proofs; this is a very surprising fact, and still not well understood.  However, when control operators are added in the same way to more complex type theories (dependent type theories), the theories become inconsistent; that is, it is now possible to 'prove' statements that are false.  The problem is that there are several different ways in which control operators may be added to a complex type theory; we have several choices as to where we allow a control operator to appear, and how control operators interact with the other features of a type theory.  The naive choice - allow them everywhere, and allow all possible forms of interaction - leads to inconsistency.  Of the many other possibilities, it is not at all obvious which will be most likely to be fruitful; and investigating their properties one by one would be very time consuming.  Systems of logic known as logic-enriched type theories, or LTTs, have also been developed.  These are closely related to type theories.  They differ in being divided rigidly into two parts: one part - the type-theoretic component - for defining mathematical objects and programs, and one part - the logical component - for stating and proving theorems about the object.  We can change the logical part to make it accept classical proofs, without changing the type-theoretic part.  However, LTTs are still quite new, and their theoretical properties and suitability for use in a proof assistant is not yet well understood.  I believe that work on control operators and work on LTTs can help each other.  If we investigate the properties of type theories and classical LTTs, and translations between the two, then we should be able to reuse results and use the insights from one to shed light on the other.  In particular, we should find the best way to add control operators to a complex type theory, by choosing the way that makes translation to and from LTTs easiest.  I therefore propose to construct several type theories with control operators and several classical LTTs, investigate their theoretical properties and translations between them, and experiment with their use in practice.  My aim is to produce one or more systems of logic that keeps all the advantages of type theories; accepts classical proofs; and is practicable for use in a proof assistan"
	},
	{
		"grant":359,
		"ID": "EP/J009520/1",
		"Title": "Structure-Preserving Pairing-Based Cryptography",
		"PIID": "-187034",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2015",
		"Value": "362032",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-187034", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Pairing-based cryptography has boomed over the last decade since it provides secure solutions to problems where traditional cryptographic methods do not suffice or are less efficient.   Boneh and Franklin in a seminal paper showed how to construct identity-based encryption using pairing-based techniques. This makes it possible to encrypt a message under somebody's identity, for instance their e-mail address, eliminating the need to obtain or manage a public key for each user. In large organizations this simplifies key management and identity-based key-management solutions are now used in several Fortune 500 companies.   Another example arises in the context of pervasive computing systems such as intelligent cars that communicate with each other. In an intelligent car processing hundreds of messages from surrounding vehicles in every 300ms interval it is essential to minimize communication and optimise efficiency. Pairing-based digital signatures can be useful in this scenario because they are smaller than traditional digital signatures and at the same time allow for fast verification of a large batch of signatures at once.   Other proposed applications of pairing-based cryptography include e-cash, searchable encrypted data, broadcast encryption and traitor tracing, delegatable anonymous credentials, and verifying the presence of data stored in a cloud computing facility.   Security is essential in all of these tasks. As our society has become increasingly digitized and networked so have criminals, hackers, industrial spies, enemy states, etc. It is therefore necessary to design secure cryptographic schemes that can be used to build a digital society that is resilient in the presence of malicious adversaries.  Designing cryptographic protocols for complex tasks requires significant effort and expertise since even a small mistake may render the entire system insecure. It is therefore natural to build cryptographic protocols in a modular fashion. This is what structure-preserving pairing-based cryptography allows. The term structure-preservation refers to pairing-based schemes that preserve their underlying mathematical structure. This structure-preserving property makes it easy to compose them with other pairing-based schemes and enables modular design.  We will design structure-preserving pairing-based cryptographic schemes, study the efficiency limits of structure-preserving pairing-based cryptographic schemes and evaluate the security of pairing-based cryptographic schemes.  By designing structure-preserving pairing-based schemes we develop new building blocks for the digital society. Moreover, the techniques we develop for the design of structure-preserving schemes may make it possible to build pairing-based schemes for significantly more complex tasks than is currently possible.  Very recent work has shown that there are limits to how efficient structure-preserving digital signatures can be. It is usually very difficult to find efficiency limitations, researchers just tend to get stuck at some point without knowing why, but because of their unique nature structure-preserving protocols lend themselves to exact efficiency analysis. By finding efficiency limits for structure-preserving pairing-based schemes, we can get an accurate picture of the exact efficiency for a variety of cryptographic tasks.  Security is essential when designing cryptographic protocols. The security of cryptographic schemes relies on hardness assumptions; for instance that it is computationally infeasible to factor large integers in a short amount of time. Unfortunately, pairing-based cryptographic schemes have been based on a large variety of assumptions making it hard to assess how secure they are. We will map out the landscape of assumptions that are used in pairing-based cryptography and make it easier to assess the security of pairing-based cryptographic schemes. "
	},
	{
		"grant":360,
		"ID": "EP/J009709/1",
		"Title": "Wideband Optical Communication Systems Using Phase-Sensitive/Insensitive Fibre Optical Parametric Amplifiers",
		"PIID": "-116900",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2015",
		"Value": "726736",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "125",
		"Investigators":[
		{"ID": "-116900", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Oclaro Technology UK"}
		],
		"Summary": "As communication services applications continue to grow in number (e.g. Twitter, YouTube, Facebook, etc.) and in bandwidth (e.g. HDTV, 3D,...), all parts of the communication systems carrying this traffic must be able to operate at higher and higher speeds. This ever-growing capacity demand can only be handled by continually upgrading the capacity of all parts of the network, including long-haul links between major cities, as well as the 'last mile' distribution networks ending at or near the customer premises.  As part of this upgrading of the physical layer of optical communication systems, there is increasing pressure to provide more optical bandwidth to accommodate more individual wavelength carriers in high-capacity wavelength-division multiplexed (WDM) systems. The current type of optical amplifiers, namely Erbium-doped fibre amplifiers (EDFAs) were introduced in the early 1990s, but have a fixed bandwidth of the order of 35 nm, covering the so-called C-band (1530-1565 nm). This bandwidth is being rapidly exhausted, and so there is a need for introducing novel optical amplifiers with substantially larger bandwidth. In addition, there is also a trend toward using high-spectral efficiency modulation formats (e.g. quadrature amplitude modulation, or QAM). However, such formats require high optical signal to noise ratios (OSNRs).   We propose to investigate the suitability of fibre optical parametric amplifiers (OPAs) to amplify WDM optical communication signals in wideband optical communication systems. We will investigate both phase-insensitive OPAs (PIAs) and phase-sensitive OPAs (PSAs).The latter are particularly attractive because of their potential for noiseless amplification, which cannot be achieved with EDFAs or phase-insensitive OPAs.  The project will consist of three phases with the following objectives: Phase I (12 months). We will first demonstrate phase-insensitive OPAs (PIAs) with an optical bandwidth matching that of EDFAs. These will be tested in a recirculating loop, with a fully-populated WDM signal spectrum, simulating propagation in a long-haul system. Only the signals will be used; the idlers will be discarded after each OPA. It is expected that the reach of the system will be several thousand kilometres. Different modulation formats will be tested, with baud rates up to 43.7 Gb/s. Aggregate throughput will reach several terabits per second. Phase II (12 months). We will then use signals and idlers in an alternating manner in the recirculating loop. This will allow us to exploit the wavelength conversion/phase conjugation aspects of OPAs to combat dispersion as well as some nonlinear effects. Testing will be done with a wider fully populated CWDM spectrum, at a higher aggregate rate.  Phase III (12 months). We will use phase-sensitive OPAs (PSAs), which have the potential for lossless amplification, leading to an increase in system reach. We will investigate the suitability of propagation along principal states of polarization, in order to maintain the states of polarization of signals, idlers, and pump, necessary for optimum PSA operation.  If the project is successful, it will demonstrate that fibre OPAs are indeed a potential contender for providing optical amplification over wavelength ranges exceeding that of EDFAs, and in a nearly noiseless manner, which is compatible with use in either long-haul or distribution optical communication networks. Hence they could in principle provide the next generation of optical amplifiers for future high-capacity optical networks.  "
	},
	{
		"grant":361,
		"ID": "EP/J009709/2",
		"Title": "Wideband Optical Communication Systems Using Phase-Sensitive/Insensitive Fibre Optical Parametric Amplifiers",
		"PIID": "-116900",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2012",
		"EndDate": "30/06/2015",
		"Value": "652589",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12",
		"Investigators":[
		{"ID": "-116900", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "As communication services applications continue to grow in number (e.g. Twitter, YouTube, Facebook, etc.) and in bandwidth (e.g. HDTV, 3D,...), all parts of the communication systems carrying this traffic must be able to operate at higher and higher speeds. This ever-growing capacity demand can only be handled by continually upgrading the capacity of all parts of the network, including long-haul links between major cities, as well as the 'last mile' distribution networks ending at or near the customer premises.  As part of this upgrading of the physical layer of optical communication systems, there is increasing pressure to provide more optical bandwidth to accommodate more individual wavelength carriers in high-capacity wavelength-division multiplexed (WDM) systems. The current type of optical amplifiers, namely Erbium-doped fibre amplifiers (EDFAs) were introduced in the early 1990s, but have a fixed bandwidth of the order of 35 nm, covering the so-called C-band (1530-1565 nm). This bandwidth is being rapidly exhausted, and so there is a need for introducing novel optical amplifiers with substantially larger bandwidth. In addition, there is also a trend toward using high-spectral efficiency modulation formats (e.g. quadrature amplitude modulation, or QAM). However, such formats require high optical signal to noise ratios (OSNRs).   We propose to investigate the suitability of fibre optical parametric amplifiers (OPAs) to amplify WDM optical communication signals in wideband optical communication systems. We will investigate both phase-insensitive OPAs (PIAs) and phase-sensitive OPAs (PSAs).The latter are particularly attractive because of their potential for noiseless amplification, which cannot be achieved with EDFAs or phase-insensitive OPAs.  The project will consist of three phases with the following objectives: Phase I (12 months). We will first demonstrate phase-insensitive OPAs (PIAs) with an optical bandwidth matching that of EDFAs. These will be tested in a recirculating loop, with a fully-populated WDM signal spectrum, simulating propagation in a long-haul system. Only the signals will be used; the idlers will be discarded after each OPA. It is expected that the reach of the system will be several thousand kilometres. Different modulation formats will be tested, with baud rates up to 43.7 Gb/s. Aggregate throughput will reach several terabits per second. Phase II (12 months). We will then use signals and idlers in an alternating manner in the recirculating loop. This will allow us to exploit the wavelength conversion/phase conjugation aspects of OPAs to combat dispersion as well as some nonlinear effects. Testing will be done with a wider fully populated CWDM spectrum, at a higher aggregate rate.  Phase III (12 months). We will use phase-sensitive OPAs (PSAs), which have the potential for lossless amplification, leading to an increase in system reach. We will investigate the suitability of propagation along principal states of polarization, in order to maintain the states of polarization of signals, idlers, and pump, necessary for optimum PSA operation.  If the project is successful, it will demonstrate that fibre OPAs are indeed a potential contender for providing optical amplification over wavelength ranges exceeding that of EDFAs, and in a nearly noiseless manner, which is compatible with use in either long-haul or distribution optical communication networks. Hence they could in principle provide the next generation of optical amplifiers for future high-capacity optical networks.  "
	},
	{
		"grant":362,
		"ID": "EP/J009814/1",
		"Title": "Nanocrystalline diamond for Micro-Electro-Mechanical Systems (MEMS)",
		"PIID": "-382671",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2012",
		"EndDate": "31/01/2014",
		"Value": "100220",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "28",
		"Investigators":[
		{"ID": "-382671", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Polytechnic University of Madrid UPM"}
		],
		"Summary": "This project aims to develop high frequency, high quality factor Micro-Electro-Mechanical Systems (MEMS) from Nanocrystalline Diamond (NCD). NCD offers superior performance to silicon for MEMS due to its extreme Young's modulus, but it is also compatible with silicon CMOS technologies, offering a key advantage over other potential MEMS materials. High performance NCD growth and planarization will be optimised for MEMS applications. The realisation of continuous, smooth, pin hole free NCD over large areas exhibiting bulk diamond properties will enable a multitude of applications at drastically reduced cost to currently available bulk diamond technologies. The resulting material will have applications outside the MEMS field such as in tribology, optical coatings, electrochemical electrodes (when doped with boron), heat spreading etc. The development of clean room processing technologies of NCD will allow the fabrication of new devices and result in new ideas exploiting this novel material."
	},
	{
		"grant":363,
		"ID": "EP/J010375/1",
		"Title": "Semantic Media: a new paradigm for navigable content for the 21st Century",
		"PIID": "22235",
		"Scheme": "Standard Research",
		"StartDate": "16/04/2012",
		"EndDate": "15/04/2015",
		"Value": "572750",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "22235", "Role": "Principal Investigator"},
		{"ID": "-181217", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BBC New Media"},
		{"Number": "1", "Name": "Creative Industries KTN"}
		],
		"Summary": "This proposal stems directly from the EPSRC Workshop held on 20 & 21 October 2010 'ICT Research - The Next Decade'. It seeks to address the challenge of the navigation of time-based media collections and items throughout the content life-cycle, from creation to consumption. It will achieve this by establishing an open network of researchers from across academia and industry, who engage in workshops, sandpits and, most importantly, feasibility or path-finder mini-projects. These mini-projects have the aims not only of performing leading edge, early stage research that will lead on to larger proposals, but also of building a critical mass of researchers, whose expectation is to tackle significant challenges by collaborating. Other elements of this project are to create a Landscape document for the field, develop appropriate ontologies for capturing media semantics, present results through a diverse range of channels and summarise the findings of the project, including a Roadmap.   The research agenda is based on five premises:  1. Content-related metadata is an effective and scalable approach exemplified in this domain and applicable to future large scale, automated and interactive information systems; 2. The point of creation is the best time and place to collect (and compute) metadata;  3. The best way to represent this metadata is one that is amenable to knowledge processing and management, linked data strategies and logical inference; 4. Significant challenges require a cross-disciplinary approach, ranging from fundamental theory to applied research set in the context of a real problem; and  5. The UK is supremely placed with the world-leading skills and experience to be a world-beating authority in an area of intellectual and societal/commercial benefit.  This proposal deliberately does not address related problems of navigation through legacy content, nor of Rights Management, as these are already embedded in the research landscape. Instead, we concentrate on the production of future media items.  The issues raised and investigated by this proposal are pertinent not only to EPSRC, but also to ESRC, AHRC, JISC and TSB, and with particular relevance to the Digital Economy.  The applications of such ideas span all the different time-based media, including music, drama, documentary, film, texts and so on. In order to advance the field, this project will bring together acknowledged experts from across UK academia in a diverse range of disciplines, including Semantic Web experts, Signal Processing experts, Video experts, Performance experts and more. The project aims to form a network and a critical mass of expertise by a series of interventions that will also include industrial collaborators (assisted by the TSB Creative Industries Knowledge Transfer Network). Network activities include workshops and sandpits, as well as collaborative small scale research projects, each typically of 6 months duration with 2 or 3 participant universities.  The outcomes of the project include: Research and Impact Roadmaps; a well-connected community of researchers engineers, creatives, content producers  and funders; commercial and full-fledged research proposals; research publications; and specific impact activities at world leading Broadcast and Media conventions.  "
	},
	{
		"grant":364,
		"ID": "EP/J010383/1",
		"Title": "Robust Incremental Semantic Resources for Dialogue",
		"PIID": "111934",
		"Scheme": "First Grant Scheme",
		"StartDate": "11/01/2012",
		"EndDate": "30/04/2013",
		"Value": "96230",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "111934", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "When humans process language, they do so incrementally, understanding and producing sentences on a word-by-word basis. In conversation, we easily switch roles between speaker and hearer mid-sentence, taking turns speaking and listening to show attention, clarify information or add detail when needed, interactively contributing to a shared, emerging picture of what we mean. If we want human-computer dialogue systems to be natural, efficient and easy to use, they must behave as incrementally as humans do: understanding and reacting interactively on a word-by-word basis rather than insisting on fully-formed sentences.  We would prefer a system which behaves as in (1) below to the more familiar but annoying (2), or even the more patient but less interactive (3):  (1) Usr:I'd like er [pause] . . . Sys:Yes? Usr:a ticket to Paris from, hang on . . .  Sys:Paris, France?  Usr:right, from London please.  Sys:OK, checking for Paris to London.  (2) Usr:I'd like er [pause] . . . Sys:I'm sorry, I don't understand. Please state your destination.  (3) Usr:I'd like er [pause] . . . Usr:a ticket to Paris from, hang on . . .  Usr:from London please.  Sys:OK. Do you mean Paris, France?  Previous research has developed computational models of dialogue which can behave incrementally, allowing the kind of interaction shown in (1); but they currently rely on hand-written rules or statistical models to relate words to actions and concepts. These lack the ability to express the complex meanings that human language is so good at conveying, and are time-consuming to create for any new system, domain or task.  Instead, they need incremental models which deal with semantics, updating some representation of meaning as each word is heard or spoken, and which can be automatically learned from data; but general methods for doing this are currently lacking. This project will bridge this gap, providing a linguistically-based, learnable framework for incremental semantic interpretation and generation, which can be used to improve and extend existing dialogue systems.  The project will start from recent work in theoretical linguistics and dialogue modelling which has produced the incremental semantic processing framework Dynamic Syntax (Kempson et al., 2001). This shows promise in modelling complex incremental dialogue, but is currently under-developed from a practical point of view, needing time-consuming expert hand-crafting, and missing a link between action planning and language generation. This project will address these issues. First, we will develop methods for automatically learning Dynamic Syntax grammars from data, allowing other researchers to easily produce and use their own versions in their own systems. Second, we will develop its methods for generating language so that it can be integrated with the way dialogue systems plan their actions on the fly. These new capabilities will be implemented computationally and evaluated on real data.  Together, they will then be used to build a demonstration dialogue system which can behave incrementally, and will be packaged into a publicly available toolkit for researchers to develop their own incremental, semantic dialogue systems. "
	},
	{
		"grant":365,
		"ID": "EP/J010413/1",
		"Title": "Grating and waveguide plasmonic sensors",
		"PIID": "91905",
		"Scheme": "Standard Research",
		"StartDate": "03/12/2012",
		"EndDate": "02/12/2015",
		"Value": "511724",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12",
		"Investigators":[
		{"ID": "91905", "Role": "Principal Investigator"},
		{"ID": "85241", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Femtosecond lasers produce pulses of light which are extremely short and at the same time extremely powerful. The intensities available when light from such a laser is focussed down are capable of modifying the structure of transparent materials or even ablating material from the surface. We have developed an understanding of the interaction of fs laser pulses with optical glasses so that, depending on the pulse parameters, we can create light waveguides, couplers, bends and grating structures or even machine the surface to alter its topology on a micron scale.  In this project we wish to bring these capabilities together to create a generic plasmonic sensing technology. Surface plasmons are oscillations of the free electrons in a thin metal film and these can be generated using the energy from light travelling in a waveguide close to the metal film. Importantly, the transfer of energy from the light to the plasmon only occurs at a well defined wavelength which depends strongly on the refractive index in a micron thick region above the metal film where the electric field of the plasmon extends. By sending a broad spectrum of light though the waveguide near the metal film and noting which wavelength is absorbed by the device it is possible to measure the refractive index above the metal very accurately.  If chemical or biochemical specific coatings are applied to the metal film then the sensor can detect specific species. In this proposal we plan to investigate the use of aptamers in this regard. Aptamers are oligonucleotide sequences, which can be designed to bind to specific molecules, proteins, DNA sequences or even cells, providing a highly flexible sensing technology.  An additional application for the technology is as a means of monitoring cell movement and growth. Cells contact a surface at specific points and if a cell is placed on the plasmon supporting metal film, light will be scattered from the plasmon field at the points of contact. This light may be viewed using a microscope which will allow the movement of the cells to be tracked over time. Cells respond differently depending on surface topology and the fs laser can be used to modify the sensor surface to enable studies of the effect of different surface topologies on cell movement and growth.  "
	},
	{
		"grant":366,
		"ID": "EP/J010766/1",
		"Title": "Kinetic User Interfaces and Multiuser 3D Virtual Worlds for Older People",
		"PIID": "-181039",
		"Scheme": "First Grant Scheme",
		"StartDate": "27/09/2012",
		"EndDate": "14/12/2013",
		"Value": "97397",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering & Digital Arts",
		"OrgID": "27",
		"Investigators":[
		{"ID": "-181039", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "AgeUK Canterbury"},
		{"Number": "1", "Name": "Silverfit BV"}
		],
		"Summary": "Older people may experience changes with age, such as a decline in physical and cognitive capabilities, which could result in deterioration of their social relationships and quality of life. It is important to help them to maintain independence and delay the need for institutionalised care. It is therefore crucial to find novel ways that motivate them to have a more active life. One area of such possibility is 3D virtual worlds (VW), which provide a creative and social space for users to engage in various activities.   In fact, virtual worlds, such as Second Life, have increasingly showed promise in healthcare. There are for instance a variety of healthcare support groups for topics such as mental health, disabilities, depression and cancer in virtual worlds. Most importantly, it is claimed that virtual worlds could help chronically ill patients and disabled users by allowing them to temporarily leave their disabilities behind and participate in social activities and better express their ideas and creativity through association with groups in virtual worlds. Studies suggest that frequent engagement in creative activity, such as computer game playing, might be helpful in reducing the risk of Alzheimer's disease and combating serious depression among older people.  However, a key obstacle when implementing virtual worlds for healthy ageing is the adoption of conventional user interfaces (such as the keyboard and mouse) which are often not well suited for older people. For instance, navigation (moving around in virtual world) and object manipulation (such as selecting and positioning a virtual object) pose serious usability issues even for younger users who are reasonably familiar with the computer. Another potential issue concerns the design of non-verbal cues for online communication through the use of avatars (graphical virtual representation of the user). Non-verbal cues are important in human communication. In virtual world, users can express themselves through the use of avatar gestures (such as waving, bowing), usually by pressing certain shortcut keys on the keyboard. An important design issue therefore lies in finding a natural way to allow for more expressive social interaction with nonverbal cues.    In recent years, there has been an emergence of a new user interface paradigm known as Natural User Interfaces (NUI). Instead of using conventional devices such as the keyboard and mouse, users interact with their computers using gestures, speech, and thoughts. One type of NUI, known as Kinetic User Interfaces (KUI) allows users to interact with the computer through body motions. Examples of such devices include Microsoft Xbox Kinect and Nintendo Wii. Such relatively natural ways to interact with technologies make it easier to learn, thus lowering the barrier of engagement.   Therefore, the proposed research programme aims to conduct a detailed investigation on how Kinetic User Interfaces (KUI) can be employed effectively to design innovative 3D virtual worlds accessible to older people. Older people's behaviour will be studied in depth to inform to 3D design solutions for this specific population. Prototypes will be developed and evaluated to assess the effectiveness of KUI-based 3D design in virtual world. Addressing these issues, we ultimately aim to make 3D technology more accessible and engaging for older people, thus motivate them to participate in active life style through this technology.  "
	},
	{
		"grant":367,
		"ID": "EP/J010898/1",
		"Title": "Automatic Diagram Generation",
		"PIID": "-79136",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2012",
		"EndDate": "26/09/2013",
		"Value": "99940",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing, Engineering & Maths",
		"OrgID": "19",
		"Investigators":[
		{"ID": "-79136", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Nokia Research Centre"}
		],
		"Summary": "Diagrams are an effective means of conveying a wide variety of different sources of information. The automatic generation of diagrams is essential for tasks such as the presentation of multiple views of large scale data sets (e.g. ontology analysis benefits from viewing information revolving around particular relationships). Enabling multiple significant relationships to be clearly presented greatly enhances the utility of visualisations (e.g. consider social network information presented as a diagram with graph edges used to depict the friends' relationship and containing curves to simultaneously indicate overlapping interest groups). The project aims to develop a unified framework for automatic diagram generation, allowing a mixture of features from different diagram types and enhancing control over layout; these theoretical advances are an essential precursor to use in practice. The project will provide significant academic impact (advances in the field, plus interaction between researchers in distinct fields) and industrial impact (improved support for ontology analysts, thereby reducing costs of product development cycles), and  societal impact (enhanced communication between researchers and the public via improved visualisation capabilities).  The involvement of prominent academic advisors from related fields and the substantial commitment from project partner Nokia will help to ensure the success of the project and assist in achieving long term impact.   A major problem is the Generation Problem (GP): given an abstract specification, decide if there is a suitable drawing satisfying the specification and if so, then to automatically produce one. The drawing conventions (constraints on the diagram syntax that must be satisfied) and drawing rules (constraints that are desirable to be satisfied) may vary according to application domain requirements or user preferences. There is a substantial amount of work on the graph-based GP, and the GP for the region-based Euler diagrams has recently been addressed, but there is a serious gap concerning diagrams that intrinsically contain both graph and region based features. Generation techniques for such mixed diagram types are desirable to enhance views of complex information (thereby assisting analysts) by enabling the visualisation of the grouping of items for emphasis, alongside a graph based visualisation of other ontological relationships. Other application areas include visual languages (e.g. diagrammatic logic proof presentation), information visualisation (e.g.  geographic information systems, network visualisation), graph drawing and knot theory. Thus use of knot theoretic codes within the Euler diagram setting to enhance generation and layout techniques is a novel approach, taken in conjunction with a graph based generation methodology.    The automatic generation of diagrams has considerable potential for use as a means of efficiently and effectively displaying results or data in multiple scientific fields. Providing a single, formal basis will make the generation of diagrams easily accessible to researchers, without the need to reinvent techniques developed in other application areas. In general, communication between different disciplines is often difficult due to subject-specific terminologies and therefore research risks repetition. The development of a framework for automatically generating diagrams, which is applicable in different research areas, is important because it has the potential to stimulate communication and collaboration in different disciplines, as well as between researchers and industrial collaborators, by providing a common language. We will provide the theoretical grounding for software tools, enhancing industrial applicability as well as dissemination possibilities to the wider public and within the scientific community. The long term vision is that the project will set the groundwork for the establishment of a new field of Diagram Generation."
	},
	{
		"grant":368,
		"ID": "EP/J010995/1",
		"Title": "Unifying Theories of Generic Programming",
		"PIID": "-227295",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2012",
		"EndDate": "31/01/2015",
		"Value": "574865",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-227295", "Role": "Principal Investigator"},
		{"ID": "74459", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The world is increasingly dependent on robust, reliable software that meets its specification. At the same time, software has to be delivered at an increasingly faster rate. As a consequence, the software industry, especially in Europe and the UK, is facing a growing tension between productivity and reliability. Generic programming aims at relieving this tension.  The vision of this project is to develop a unifying theory of generic programming that will inform the design of future programming languages. Our goal is to produce a body of work that is in the same vein as the seminal work on Unifying Theories of Programming by Hoare and He: we consider the outcome of this project to be a unified foundation for generic programming that brings together the advantages of previous work into a coherent whole.  From the perspective of increasing programmer productivity, the importance of understanding and applying generic programming has never been so critical. Software engineers are constantly faced with the challenge of adapting to changing specifications and designs, while ensuring that the ensuing refactoring maintains the correctness of the algorithms they have crafted. Our approach to solving this problem - generic programming - exploits the inherent structure that exists in data, where this structure can be used to automatically produce efficient and flexible algorithms that can be adapted to suit different needs. Furthermore, generic programs ensure that the structure of the data itself plays a central role in maintaining the correctness of these algorithms.  The functional programming language Haskell offers rudimentary support for generic programming in the form of the deriving mechanism. Instead of manually coding, for example, equality for a datatype, the Haskell programmer attaches a simple hint to the datatype declaration which instructs the compiler to auto-generate equality and inequality for the datatype. Simple, convenient and robust. If the datatype is changed at a later point in time, equality and inequality are modified accordingly behind the scenes, supporting software evolution and easing software maintenance.  Haskell's support for generic programming is only partial, as the deriving mechanism is limited to a few predefined classes. In particular, one cannot define new derivable classes. This is exactly what generic programming supports. Informally, a derivable or generic function is defined by induction on the structure of types. The generic programmer provides code for a few type constructs, the rest is taken care of automatically. The generic program can then be instantiated many times at different types.  The last two decades have witnessed a number of approaches to generic programming differing in convenience, expressiveness and efficiency. We can roughly distinguish two main approaches: algebraic and type-theoretic ones. Both come with their various strengths and weaknesses. This project seeks to generalise and unify the two approaches, combining their individual strengths. It will do so using methods from a branch of mathematics called Category Theory. Furthermore, the project will explore novel approaches for the specification of generic programs, provide the infrastructure for reasoning about generic programs, and demonstrate that GP has far-reaching and important applications in practice."
	},
	{
		"grant":369,
		"ID": "EP/J011347/1",
		"Title": "Nanoporous artificial materials: confining the THz excited surface plasmon polariton",
		"PIID": "-26308",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/01/2012",
		"EndDate": "30/06/2013",
		"Value": "98930",
		"ResearchArea": "Materials Engineering - Composites",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-26308", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Durham Photonics"}
		],
		"Summary": "Terahertz light can be found between visible light and microwaves in the electromagnetic spectrum. It is relatively unexploited because compact, powerful sources have been particularly difficult to make in this region. Furthermore, it suffers from a shortage of materials which produce efficient (i.e. low loss) devices to guide and manipulate the terahertz light. It is, however, an interesting region because it is energetically similar to many biological processes, so we can study protein behaviour and DNA. It is able to excite intramolecular vibrations which means that drugs and explosives can be readily identified. This proposal builds on our expertise in the design and development of so-called artificial materials. An artificial material has electromagnetic properties which have been engineered. This means that we can control what happens when the terahertz light interacts with the material. A typical approach to tailor the electromagnetic properties involves incorporating small (sub-wavelength) features into and on readily available materials (e.g. metals such as gold and copper). Here we will produce artificial materials which are capable of producing a special type of wave when the terahertz light illuminates the material's surface. This special type of wave, or oscillation of charge, is known as a surface plasmon polariton (SPP), and its properties are highly dependent on the type of material that is in contact with the surface. This makes it suitable for sensing a wide variety of materials with excellent selectivity. The approach will enable us to monitor activity and changes on a very short (picosecond) timescale. Unfortunately, when excited by terahertz light, this wave extends some distance from the surface. This makes it less sensitive to events at the surface (e.g. biomolecules attaching). The aim of this research is improve the confinement of the SPP by introducing a nanoporous layer to our existing artificial material designs (based on copper foils with arrays of microscale apertures). The team includes microfabrication engineers and physicists who will work together to design, fabricate and test the materials."
	},
	{
		"grant":370,
		"ID": "EP/J011525/1",
		"Title": "Towards More Autonomy for Unmanned Vehicles: Situational Awareness and Decision Making under Uncertainty",
		"PIID": "77575",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2012",
		"EndDate": "29/02/2016",
		"Value": "1006188",
		"ResearchArea": "Control Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Aeronautical and Automotive Engineering",
		"OrgID": "90",
		"Investigators":[
		{"ID": "77575", "Role": "Principal Investigator"},
		{"ID": "22032", "Role": "Co Investigator"},
		{"ID": "-204619", "Role": "Co Investigator"},
		{"ID": "-49837", "Role": "Co Investigator"},
		{"ID": "95505", "Role": "Co Investigator"},
		{"ID": "-165898", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "It is anticipated that unmanned vehicles will be widely used within military and civilian operations and have a profound influence in our daily life in near future. Before fully realising the potential that unmanned vehicles bring, it is reasonably expected that to make unmanned vehicles accepted by users, the public and regulatory authorities, they shall achieve a similar level of safety as human operated systems. Among many others, a fundamental requirement for an unmanned vehicle is the capability to respond to internal and external changes in a safe, timely and appropriate manner. Therefore, situational awareness and decision making are two of the most important enabling technologies for safe operation of unmanned vehicles. To a large extent, they determine the level of autonomy and intelligence of an unmanned vehicle. Compared with a human driver or pilot residing in the vehicle, a major safety concern is the inevitable reduction in situational awareness of the unmanned vehicle operator remotely located in a control station.   Unmanned vehicles operate in a dynamic, unpredictable environment with incomplete (or inaccurate) sensory information, which creates many challenges in situational awareness and decision making. Probabilistic and bounded approaches are widely used to represent uncertainty with a known distribution or with a known upper and lower bounds respectively. Situational awareness includes the perception of the objects in the environment within a volume of time and space, the comprehension of their meaning and the projection of their status in the near future. For example, in projection of the near status of moving objects of interest, any initial uncertainty associated with perception and comprehension will expand exponentially with the increase of the projection time span. However, it is possible to significantly reduce the uncertainty by utilising the information in the world model such as the operation environment, the Rules of the Road (or of the Air) and the properties of an identified object. For probabilistic uncertainty, this makes the Gaussian distribution assumption invalid, which is fundamental for most of the current statistical approaches such as Kalman filtering. Under the Gaussian distribution assumption, the estimated state about a moving object can be presented by its mean with a variance, and a symmetric uncertain region can be defined with the mean located at the centre (under a specified confidence level such as 99%). The introduction of knowledge (e.g. constraints due to the roadway layout) makes this not true anymore. To address the challenge of non-Gaussian distributions imposed by making use of information from the world model, a rigorous Bayesian learning framework will be developed for pooling all the knowledge from the world model and measurement data to provide a better estimate of the environment, and to propagate the uncertain regions with projection time. Reachability analysis will be developed for bounded uncertainty for worst case analysis, where the uncertainty will be reduced using constraints from the world model. Hazard analysis will be carried out to identify any potential risk. The key idea is to take a proactive approach to prevent any emergent situation through improving situational awareness reasoning and decision making. The estimates and associated uncertain region provided by the situational awareness will be fed to novel decision making and planning tools. The research activities will be strongly supported and verified by experimental tests on small scale ground and aerial vehicles. This project aims to significantly improve the level of safety of unmanned vehicle operation and to bridge the gap between the development and deployment of unmanned vehicles in real world applications, which is a strategically important area for new business growth."
	},
	{
		"grant":371,
		"ID": "EP/J011541/1",
		"Title": "Bridging Theory and Practice in Key Exchange Protocols",
		"PIID": "-384825",
		"Scheme": "First Grant Scheme",
		"StartDate": "13/06/2012",
		"EndDate": "12/06/2014",
		"Value": "99871",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "-384825", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Mozilla Foundation"}
		],
		"Summary": "Key exchange protocols address a crucial problem in security: how to securely distribute cryptographic keys to remote users. Since the seminal paper by Diffie and Hellman in 1976, this subject has been extensively studied for over thirty years.   Yet, designing a secure key exchange protocol is notoriously difficult. Many proposed schemes have been found with security flaws, including those specified in the international standards. Heuristic designs based on ad-hoc arguments rather than rigorous security proofs are commonly seen as bad practice. However, several 'provably secure' key exchange protocols also turn out to be insecure.  So far, almost all key exchange protocols in the past have sidestepped an important engineering principle, namely the sixth principle -- i.e., 'Do not assume that a message you receive has a particular form unless you can check this' (Anderson & Needham, 1995).  The importance of the sixth principle has been widely acknowledged by the security community for many years, but in reality, key exchange protocol designers have generally abandoned this prudent principle on the grounds of efficiency -- following the sixth principle would require using Zero Knowledge Proof (ZKP), which is considered too computationally expensive. However, discarding ZKP has the serious consequence of degrading the security, as evident by many reported attacks in the past. All these indicate a gap in the field.  In the project, we propose to bridge the gap by combing the sixth principle and the Public Key Juggling (PKJ) technique. The PKJ technique has proved useful in tacking several important security problems in the past. It can be integrated with the sixth principle in a perfect match: the former serves to optimize the protocol efficiency while the latter underpins the protocol robustness.   In the proposed research, we will apply the sixth principle and the juggling technique to design new key exchange protocols that are robust and efficient. We will develop new formal models to capture the sixth principle, which has been largely neglected by existing model specifications. Finally, we will aim to promote robust and efficient key exchange protocols to the international standards. In particular, our J-PAKE key exchange protocol has stood years of cryptanalysis and has been deployed in practical applications. Its standardization is a natural step forward and will benefit the security industry in general."
	},
	{
		"grant":372,
		"ID": "EP/J011770/1",
		"Title": "Reconfigurable Autonomy",
		"PIID": "29766",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2016",
		"Value": "419422",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "29766", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": " As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such 'reconfigurable autonomy' can be achieved in relevant applications. "
	},
	{
		"grant":373,
		"ID": "EP/J011797/1",
		"Title": "Electrical and picosecond optical control of plasmonic nanoantenna hybrid devices",
		"PIID": "-228147",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2015",
		"Value": "484951",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-228147", "Role": "Principal Investigator"},
		{"ID": "99939", "Role": "Co Investigator"},
		{"ID": "-111019", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Miniaturization of optical components for on-chip integration of electronic and photonic functionalities is one of the new frontiers with the promise of enabling a next generation of integrated optoelectronic circuits. A particularly fascinating prospect is the achievement of an optical analogue of the electronic transistor, which forms the building block of our computers. Our approach involves a nanoscale version of a radiowave antenna, the plasmonic nanoantenna. Plasmonic antennas are designed to overcome the diffraction limit of light and to focus light into a nanometer-sized antenna 'feed' gap.   In our first studies supported by EPSRC we have proposed a variety of devices exploiting hybrid interactions of a nanoantenna with an active substrate. Here, we aim to launch a full-scale investigation of such hybrid antenna devices including various geometries and metal oxide substrates, where the plasmonic antenna will be exploited as a nanoscale sensitizer for the active substrate. Integration of a nanoantenna switches with a nanoelectronic transistor will yield a new class of optoelectronic devices: the nanoantenna MOSFET.  The proposed optically and electrically controlled nanoantenna devices are of enormous interest as a bridge for on-chip control of electrical and optical information. In addition, ultrafast active control of local fields and antenna radiation patterns will enable new applications in nonlinear optics, Raman sensors, and optical quantum information technology."
	},
	{
		"grant":374,
		"ID": "EP/J011940/1",
		"Title": "Dynamic pattern matching: Faster Algorithms and New Bounds",
		"PIID": "-150371",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2012",
		"EndDate": "31/12/2014",
		"Value": "289045",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-150371", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project aims to provide the tools necessary to address the challenges that arise from processing massive datasets which are themselves dynamic. That is to say the data changes, perhaps very rapidly, over time. Where traditional ideas have found sophisticated methods for indexing and searching very large sets of static information, much of today's applications, from high frequency finance to the indexing of the world wide web, requires us to be able to cope with dynamic change.  Pattern matching algorithms are central to the modern digital world. From word processing and web search to computational genetics and high finance, the ability to find patterns efficiently and accurately is of fundamental importance to modern industry. However, we are now witnessing a fundamental change not only in the ways data are being processed but also in the sheer size of commonly held datasets. The public genome sequencing projects, for example, have produced hundreds of gigabytes of sequence and related meta data and web search tools must answer billions of queries a day, each within a fraction of a second. Telecommunication companies collect terabytes of data every day which is itself only a tiny proportion of the data that travels over their networks. The types of data that are being stored in massive volumes online are also increasing, from audio to still images, streaming video and entire library collections.  As examples of the challenge we face, in telecommunications not only is the total data size massive, it streams at such a rate that we can only afford to store a small sketch of the input at any time and yet must still answer sophisticated queries of the data as quickly as possible. In web search, as well as the arrival of new pages, any part the existing data may change without notice. In all cases, we must be able handle such changes quickly, efficiently and with minimum cost. This proposal will allow us to develop fast algorithms to perform pattern matching under these new conditions. We will consider a range of scenarios, even taking into account multiple simultaneous streams of data or situations where previously successfully processed data can be modified at any point and at any time.  The second and complementary topic for this project is that of showing time and space lower bounds for the problems we have discussed. Where ever faster and more efficient algorithms are developed by the algorithms community, our understanding of the limits of what can be achieved, even in principle, is currently at a much more basic level. Such lower bounds, were they available, would not only be important for the significant theoretical interest and insight they provide, but also for the practical purposes of preventing fruitless search for algorithms which cannot exist. Within computer science in general, lower bounds have historically proven hard to develop but in the context of streaming and dynamic computation new ideas can now be applied. These results will provide a framework within which future algorithmic research can be carried out, therefore saving many hours of potentially fruitless search for methods we have shown cannot exist. "
	},
	{
		"grant":375,
		"ID": "EP/J011991/1",
		"Title": "Machine Learning and Adaptation of Domain Models to Support Real-Time Planning in Autonomous Systems",
		"PIID": "30887",
		"Scheme": "Standard Research",
		"StartDate": "27/03/2012",
		"EndDate": "26/03/2016",
		"Value": "366420",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing and Engineering",
		"OrgID": "56",
		"Investigators":[
		{"ID": "30887", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Simulating low-level cognitive behaviour, such as reaction to stimuli, has been a major focus of research and development in the autonomous systems (AS) community for many years. Automated assessment of sensor data, and reactive selection of actions in the form of condition-action pairs, is well developed in robotic and control application areas. In contrast, a characteristic of more intelligent behaviour is the ability to reason with self-knowledge: an AS knows about the actions it can perform, the resources it has, the goals it has to achieve, the current state and environment it finds itself in; and it has the ability to reason with all this knowledge in order to synthesise, and carry out, plans to achieve its desired goals. So for example an unmanned vehicle on the Mars surface might be requested to collect a rock sample at some position, or a spacecraft might be required to take a photograph of some star constellation. These tasks require an AS to generate or be given detailed plans to achieve them. Enabling applications involving AS to have the general ability to generate reliable plans in this manner is a great challenge, because of the difficulty of creating plans fast enough in real-time situations, and the problems in representing and keeping up to date the AS's domain knowledge.  Recently researchers who are working on automatically creating plans (automated planning) have made many breakthroughs, so that now such automated planners are capable of reasoning very efficiently and accurately with detailed representations of knowledge. This has resulted in automated planning software being used within a wide range of applications including fire fighting, elevator control, emergency landing, aircraft repair scheduling, workflow generation, narrative generation, and battery load balancing. In Space applications, scientists at NASA have been developing systems with such technology for the control of autonomous vehicles, and have deployed systems which can plan activities for spacecraft, schedule observation movements for the Hubble Telescope, and control underwater vehicles.  While the development of automated planning has been encouraging, a major problem remains in all these applications, which limits their adaptability, and makes them difficult to maintain and validate: much of the AS's high level self-knowledge, that is knowledge of such things as actions, resources, goals, objects, states and environment, has to be programmed or encoded into the system before its operation (this encoded knowledge is often called a `domain model'). Experience has shown that this encoding involves a great deal of expert time and effort. It also means that if the AS's capabilities change, for example if the preconditions or effects of an action change, then new knowledge describing this must be re-entered into the system by human experts.      This research project seeks to lead the way to overcoming this challenge by enabling the AS to learn and adapt its own domain model. It seeks to discover methods for an AS to acquire knowledge initially, and to maintain and evolve that knowledge via feedback sensing after executing actions. While methods for empowering AS to learn basic reactions, or learn how to classify data, are well established, methods for getting an AS to learn and adapt knowledge of structures such as actions, is not so well developed. The proposers will use their recent research results in this area  and research and develop prototype AS which can learn and adapt their domain models. They will demonstrate and evaluate their research using virtual worlds which model real applications. "
	},
	{
		"grant":376,
		"ID": "EP/J012106/1",
		"Title": "3D Intrinsic Shape Recognition Under Deformation and View Changes",
		"PIID": "-386104",
		"Scheme": "First Grant Scheme",
		"StartDate": "20/06/2012",
		"EndDate": "19/06/2014",
		"Value": "97751",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-386104", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Samsung Electronics Research Institute"}
		],
		"Summary": "In the proposal, we tackle the novel visual recognition problem of 3D (three-dimensional) deformable object shape identities or categories. Images of 3D objects undergo large appearance changes due to different object poses (articulation or deformation) as well as camera view-points. We attempt to recognise objects from single images by their 3D shape identities (or intrinsic shapes) regardless of their present poses and camera view-points. Humans can perceive 3D shapes of objects from single images, provided that they have previously seen 3D shapes of similar other objects. The knowledge formerly learnt on 3D shapes is called 3D shape prior. A key idea for fulfilling the proposed task is to learn and exploit the shape priors for object recognition.  The proposed research is well-lined with and goes beyond important topics of computer vision. Whereas much work for view-point invariant object recognition is limited to rigid object classes with bountiful textures, we consider deformable object shapes. In a series of work in the field of single view reconstruction, promising results have been shown for human body shape reconstruction under pose variations. There has also been a notable latest success in 3D human pose recognition. On the top of these results, we go beyond to capture 3D intrinsic shape variations for object recognition. The intended outcomes would benefit the relevant academic fields and their existing markets, and would also lead to potential new applications such as automatic monitoring of public obesity and animal tracking."
	},
	{
		"grant":377,
		"ID": "EP/J012211/1",
		"Title": "Sustained Autonomy through Coupled Plan-based Control and World Modelling with Uncertainty",
		"PIID": "47044",
		"Scheme": "Standard Research",
		"StartDate": "16/07/2012",
		"EndDate": "15/07/2015",
		"Value": "237002",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Informatics",
		"OrgID": "78",
		"Investigators":[
		{"ID": "47044", "Role": "Principal Investigator"},
		{"ID": "46163", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Sustained autonomous behaviour requires a system that is robust to uncertainty, both at the low level of interactions between actuators and sensors and its environment, but also at the intermediate level of sensory perception and interpretation, action dispatch and execution monitoring and, at the highest level of planning, action selection, plan modification and world modelling. In this project we bring together a team of experts with complementary and linked skills and experience, from robotics and sensor data processing, from planning and from reasoning under uncertainty. Our goal is to combine these areas in order to build and demonstrate a robust approach to sustained autonomy, coupling plan-based control to the construction of world models under constraints on resources and under uncertainty.   We plan to demonstrate the approaches in an underwater environment, using Autonomous Underwater Vehicles (AUVs), performing inspection and investigation missions. These missions share many features with space exploration, the use of autonomous Unmanned Aerial Vehicles (UAVs) for track-and-target missions and investigation of terrestrial hazardous sites, such as nuclear waste storage sites. In all of these case, communication between a human supervisor and the autonomous system is often tightly constrained. This is particularly true of deep space missions (for example, Mars missions face transmission delays of about 15 minutes, but windows might consist of a just two 30 minute slots in 24 hours). However, in aerial observation missions involving multiple assets the need for rapid responses in the control of fast moving vehicles reacting to agile targets also leads to bandwidth constraints for a single human controller attempting to manage and coordinate the mission. Hazardous sites, particularly those subject to radiation emissions, often contain communication black-spots where vehicles must operate without human intervention over extended periods. The underwater setting also imposes limits on communication due to the physical difficulties in transmitting control signals over significant distances.   Many of these missions involve multiple assets, often mounting different capabilities. Space missions might combine orbital observing assets, ground-based landers or rovers, possibly aerial vehicles (in some settings) and even astronauts, each offering different subsets of capabilities. Aerial observation might combine slower but more agile vehicles with others that are fast but less manoeuverable, while mounted imaging systems might exploit different wavelengths (visible, infrared, radar) and vehicles might offer other capabilities. We intend to explore the use of multiple assets, including the coordination of AUVs mounting different sensors and actuators.   Uncertainty offers different challenges according to environment. Many space environments have relatively predictable dynamics (although Martian winds are one example of a highly dynamic and uncertain factor), but aerial observation missions operate in highly dynamic and unpredictable environments: both atmospheric conditions and target behaviours can be a source of dynamic uncertainty. The underwater environment is also highly dynamic: phenomena such as currents will act as useful proxies for similar dynamic sources of uncertainty in other execution environments. Other sources of uncertainty arise from the inherent limitations of sensors and actuators and our ability to process and interpret the data that can be recovered from these devices. One of the biggest challenges in achieving robust autonomy is in recognising that uncertainty about the state of the world and the state of execution of a plan is inevitable, but the form of that uncertainty is itself an unknown.  By combining techniques in modelling and reasoning about uncertainty, plan modification and sensor data perception and interpretation, we propose to build a robust approach to autonomous systems control. "
	},
	{
		"grant":378,
		"ID": "EP/J012343/1",
		"Title": "Novel Sensing Networks for Intelligent Monitoring (Newton)",
		"PIID": "74076",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2016",
		"Value": "1175282",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98",
		"Investigators":[
		{"ID": "74076", "Role": "Principal Investigator"},
		{"ID": "-189859", "Role": "Co Investigator"},
		{"ID": "21216", "Role": "Co Investigator"},
		{"ID": "108835", "Role": "Co Investigator"},
		{"ID": "-89125", "Role": "Co Investigator"},
		{"ID": "103323", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cybula Ltd"}
		],
		"Summary": "This proposal seeks funding for a four-year research programme to develop an autonomous, intelligent system to obtain a revolutionary solution for condition/structural health intelligent monitoring, with specific applications in condition monitoring of railways and in-service Non-Destructive Evaluation (NDE) for nuclear applications. This project addresses important goals: low cost and low power consumption sensor networks, sensor exploration, software architectures, autonomous data fusion and intelligent system management, spectrally efficient and reliable communications with novel approaches of radio frequency identification (RFID) based passive sensing networks, non-linear feature extraction and model based fusion, compressed sensing, cloud-based computing and decision making. The research will extend our knowledge in several complementary areas: low cost sensor technologies, wireless sensor network (WSN) for NDE and structural health monitoring (SHM), feature extraction and fusion, robust communication, and software architectures. The work will be undertaken jointly by cross-disciplinary research teams from Newcastle, Sheffield and York Universities, in collaboration with industrial strategic partners."
	},
	{
		"grant":379,
		"ID": "EP/J012521/1",
		"Title": "Human-Autonomous Systems Collective Capability (HASCC)",
		"PIID": "8369",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2012",
		"EndDate": "30/03/2015",
		"Value": "669334",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "8369", "Role": "Principal Investigator"},
		{"ID": "-121833", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Human-Autonomous Systems (HAS) are collections of human and autonomous agencies of great importance to, defence, disaster and emergency response, transport, and energy services (especially in hostile/inhospitable environments). However, current reality is that HAS do not provide the right information at the right time to the right agent (human or autonomous); cause information overload; and produce rigid, inflexible and ineffective rule bound behaviours. The current state-of the art in Human-Autonomous Systems is that they often involve disparate, incompatible, and 'stove-piped' communication and information structures with conflicting technologies. This has resulted in failures, ineffectiveness and inefficiency, costing resources and even lives. Improving the collective capability of human-autonomous systems requires agile and flexible behaviour in the face of complex and rapidly changing situations. Developing the collective capability of HAS requires and leads to improving; i) the levels of local and global awareness and utility of information and knowledge, ii) the quality and trustworthiness of decision-making and consideration of alternatives, iii) the ability to increase the level of 'command by intent' through the development of lightweight but richer reporting and monitoring mechanisms; and iv) the ability to globally exploit and learn from local initiatives. Underlying all of these lies the importance of the, representation, interactive manipulation and communication of information and knowledge.   This 36 month research project will achieve improvements in HAS performance through novel breakthroughs in important areas of Collective Capability for Human-Autonomous Systems (HASCC0. Those breakthroughs will enable improved levels of shared awareness, collective decision-making, agile, responsive command, and collective learning. To achieve this we will develop protocols and technologies for information and knowledge abstraction and representation, argumentation, rationale, command and reporting structures.  Our approach is to develop protocols and technologies to support the interactions and knowledge manipulations needed to enhance HAS collective awareness and decision-making and capable of representing and interacting with;  -the (rich but lightweight) Argumentation, Rationale, Command and Reporting Structures, -which influence local and global and include strategic, tactical and operational decision-making. Enabling HAS collectives to be agile and responsive. Our investigations comprise two cycles corresponding to different application domain scenarios. Each application domain will present different information and decision-making requirements, and will require different strategic, tactical and operational deployments of HAS. In this way we will seek to assess the generality and wider applicability of our research findings. In the first cycle, we will focus on the situation awareness and decision-making required of HAS for 'Multiple Vehicle Cooperative Autonomy'. In the second, we will expand our research to investigate HAS for  'investigation and repair of defective infrastructure'. In each cycle, we will undertake scenario development, modelling, prototyping, evaluation and revision. At the end of each cycle we will produce versions of Protoypes, Models and Principles of HASCC. The research will directly contribute to several EPSRC strategic priority themes by providing science and technology that strengthens critical national infrastructure in:   Global Uncertainties  - Collective Capability to underpin agile, coherent and integrated HAS, in Defence and Disaster Emergency Response Services  Digital Economy - the development of novel Collective Capability Technologies to advance Autonomous Systems,  Energy - Collective Capability to underpin HAS enabling safe and reliable energy provision.  Transport - Collective Capability for HAS to provide reliable, safe and efficient Transport Services.   "
	},
	{
		"grant":380,
		"ID": "EP/J012564/1",
		"Title": "New Foundational Structures for Engineering Verified multi-UAVs",
		"PIID": "-202368",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2015",
		"Value": "636718",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-202368", "Role": "Principal Investigator"},
		{"ID": "-185805", "Role": "Co Investigator"},
		{"ID": "2488", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "George Washington University"},
		{"Number": "1", "Name": "IBM UK Labs Ltd"},
		{"Number": "2", "Name": "McGill University"},
		{"Number": "3", "Name": "University of Liverpool"}
		],
		"Summary": "In March 2011, Japan suffered from its biggest earthquake and devastating tsunami. Severe damage were inflicted on its Fukushima nuclear plants and more than 100,000 people had to be evacuated after the radiation levels became unsafe. Workers were not able to operate on site, preventing them from securing safety at the atomic power plant and averting a major radiation leak. One month after the disaster, in order to assess the severity of the damage to the nuclear plant from above, a small aerial vehicle equipped with cameras was sent to take pictures and videos of the affected areas. The video footage obtained brought valuable information to the rescue teams that could not have been acquired otherwise. But the use of aerial vehicles still remains limited by the fact that they require a remote operator at transmission range to control them. It is also necessary to have an operator to control the camera and interpret the data.  In order to work autonomously, these systems need to be highly intelligent and rational so that they can become reliable: they must have high levels of knowledge to accomplish their AI-complex missions which occur in any other information environment. This implies that they should adapt to any unexpected situations such as recent changes not reflected in prior information on the environment and possible loss of GPS due to obstructing buildings or indoor exploration; reliable operation under such conditions would, for instance, enable them to return safely to their base station. In a multi-UAV setting, they should additionally be able to communicate with each other to simplify their goals, to learn from each other's information, and to update and share their knowledge. Given that any mission is unique in terms of deployment areas, tasks and goals to be achieved, etc., and can be critical in the sense that human lives may be involved, the implementation must be verified to be correct with respect to a formal specification. A famous example of an implementation error and a failure to comply with the specification is the self-destruction of Ariane 5 in 1996 immediately after take-off, caused by a numeric overflow due to an implementation that was not suitable for all possible situations. In 1996, the Lockheed Martin/Boeing Darkstar long-endurance UAV crashed following what the Pentagon called a 'mishap [..] directly traceable to deficiencies in the modelling and simulation of the flight vehicle'.  To achieve the reliability required, we will need to develop a formalism that represents the sets of actions each Unmanned Aerial Vehicle (UAV) can perform while allowing capture of the kinetic constraints of the UAVs. We will then verify that the behaviours of each UAV modelled using this formalism lead to the individual or overall goal of the mission they are to achieve. These need to be extended from individual behaviours to a cooperative level amongst the multiple UAVs. Next, we plan to link the low-level code to high-level abstraction and verify it via advanced model-checking techniques. Finally, logical tools will be used to exhaustively reason about learning as a result of information flow among UAVs and their environment."
	},
	{
		"grant":381,
		"ID": "EP/J012815/1",
		"Title": "Silicon based QD light sources and lasers",
		"PIID": "67555",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2012",
		"EndDate": "30/09/2015",
		"Value": "698434",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Physics and Astronomy",
		"OrgID": "28",
		"Investigators":[
		{"ID": "67555", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Realising efficient electrically-pumped lasers based on Si substrates is the transformative step that enables the unification of III-V based communications technology with Si data processing and memory electronics. We will demonstrate that high performance light emitting devices can be fabricated on Si substrates using an approach based on quantum dots (QDs). The successful outcome will provide the basis for cheaper and better Si-based optoelectronic integrated circuits, a key enabler for the Digital Economy, and provide potential solutions for the impending Si CMOS interconnect challenges (where the physical length and energy requirements of the connections between electronic elements limits processing performance). This project is expected to contribute to improving quality of life for consumers and to wealth creation, for example low-cost and increased complexity Si chips for next-generation computers and higher-capacity communication systems. The problems we will address include the very different crystal lattice size, and the different temperature dependence of the lattice size, of Silicon (the basis for most electronics) and the majority of III-V semiconductors (the basis for most light emitting devices) and the low device power consumption requirements for densely integrated components.  The research will investigate how to manage the lattice mismatch across the silicon to III-V interface, introduce methods to filter out crystal defects that originate from this region of the device and will use an active layer that is relatively intolerant to any remaining defects generated by this interface.  We will investigate how to make devices that require small numbers of electrons and devices that lose a very small number of the photons generated by these electrons using, for example, a wide range of materials such as GaInP for the laser cladding for low optical loss and InGaAsN(Sb) to allow quantum mechanical tunnelling into a small number of lasing states hence minimising electron use. This will make the overall devices very energy efficient which is also necessary to avoid the generation of large amounts of waste heat that is difficult (and energy costly) to dissipate. We will also demonstrate that it is possible to manufacture laser mirrors and waveguides to couple light between the laser and other optical devices, for example amplifiers.  We will liase with leading UK based companies that are ideally placed to exploit the immediate outcomes of our work and also interact with other academic groups, where further research is necessary before our advances can be fully exploited. One example is an optical imaging technique that will benefit from increased data acquisition speed, enhanced portability and reduced price of the devices we will produce to allow early diagnosis of, for example, skin cancer or retinal diseases causing blindness. "
	},
	{
		"grant":382,
		"ID": "EP/J012874/1",
		"Title": "Continuously Tunable Optical Buffer",
		"PIID": "107145",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2015",
		"Value": "259102",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117",
		"Investigators":[
		{"ID": "107145", "Role": "Principal Investigator"},
		{"ID": "28779", "Role": "Co Investigator"},
		{"ID": "-10060", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern society is based to a large extent on the fast and reliable exchange and processing of information. This has led to an explosive growth of the internet, and every year new applications and services are added with ever increasing demands on information transfer capacity - music downloads are replacing high street CD purchases, video downloads have reached 3bn per day on YouTube alone, TV is streamed live on the internet, HD video-on-demand is just round the corner, and cloud computing may mean that data is increasingly stored and processed remotely. The vast majority of these data are transmitted in form of small data packets over a worldwide network of optical fibres. The bottleneck in the capacity is currently formed by the routers, the 'distribution centres' of the internet where packets are switched between optical fibres depending on their destination. This process is done by electronics, and thus at much lower speeds than the capacity of the optical transmission fibres. Moreover, as usage nears the network capacity limits, data congestion at the routers is a serious issue which requires storage of data packets electronically until they can be re-transmitted. Finally, the conversion from optical to electronic to optical is also inefficient and thus consumes significant amounts of energy.  One of the most attractive solutions to this problem is storing data in its optical form until it can be re-transmitted. Such an optical buffer should be fast, allow for arbitrary storage times, and should be broadband, that is, it should work over the whole range of optical wavelengths used for data transmission in fibres. Several optical buffers have been suggested and partially demonstrated so far, but none of them fulfils all these requirements.  Here, we propose a novel type of optical buffer to meet these specifications. It is based on integrated photonics, which will ultimately allow the buffer to be scaled and mass fabricated for the market. In its simplest form, the chip contains two parallel optical waveguides whose separation can be controlled electronically. Light propagating simultaneously through the two waveguides is coupled optically through the separating air gap and the propagation velocity depends on the exact size of that gap. In other words, the speed of light and hence the time the pulse spends on the chip can be controlled by moving the waveguides. We have already shown through simulations that the delay time can be changed by a factor of three using this method. Using our optical buffer in a ring configuration can therefore create any arbitrary time delay. Moreover, the buffer is predicted to work at all wavelengths relevant for optical telecommunications. In practice, the controllable separation of the two waveguides will be achieved using the latest micro-electromechanical technology on a III-V semiconductor platform.  In this project, we will first design and optimise the optical buffer by theoretical analysis and simulations. We will then fabricate the device using III-V deposition, e-beam lithography, and a combination of plasma and wet etching techniques. We will characterise and evaluate the device, and finally demonstrate the optical buffer in an optical telecommunication system.  The project is a collaboration between the University of Southampton and University College London and will bring together their expertise in photonics (UoS) and III-V nanofabrication (UCL) to investigate and fabricate a device which has the potential to become an enabling technology for further acceleration of packet-switched networks and thus for future growth of the internet. "
	},
	{
		"grant":383,
		"ID": "EP/J01303X/1",
		"Title": "Integrated Tunable Flat Lenses (TuneFuL)",
		"PIID": "108792",
		"Scheme": "Standard Research",
		"StartDate": "17/05/2012",
		"EndDate": "16/05/2015",
		"Value": "412176",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "108792", "Role": "Principal Investigator"},
		{"ID": "18960", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal aims to revolutionise one of the fundamental components used in optics : the semiconductor laser. These components are used in a huge array of applications from high speed optical communications - they power the internet, to laser machining, ultra sensitive gas sensors, the defence industry and scientific research. Common to all these applications is the requirement to focus, control or shape the beam of light coming from the laser, this is conventionally done with an external lenses which are often many 10's of times the size of the laser, which is typically a few mm^3 in volume, and often much more expensive. Allied to this problem is that once the beam is focused or controlled it is very difficult and expensive to alter it. Moreover, for many applications such as sensing and defence one may want to scan the laser beam across a field of view to interrogate or acquire information from a wide area. Finally, the spectral purity of the laser is critically important in many of these applications where it essential that there is only one single emitted wavelength. This proposal aims to solve all these problems in a low cost, mass market way by creating a flat, electronically controllable lens which can be patterned onto the emitting facet of the laser. This proposal will apply exciting ideas from the emerging field of optical nanoantennas to create tunable nanoantenna arrays configured as flat lenses. They will be first developed as standalone devices and then integrated with a range of semiconductor lasers to create electronically tunable output beams for focusing and steering applications and simultaneous control of the spectral purity of the laser."
	},
	{
		"grant":384,
		"ID": "EP/J013072/1",
		"Title": "META: Multifield Extension of Topological Analysis",
		"PIID": "-269821",
		"Scheme": "Standard Research",
		"StartDate": "12/08/2012",
		"EndDate": "11/08/2015",
		"Value": "744252",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "-269821", "Role": "Principal Investigator"},
		{"ID": "41482", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Physical scientists, engineers and clinicians rely on visualization to obtain insight into data arising from scans and simulation. Since the 1980s, when an influential US NSF report ushered in the use of graphics to make sense of large volumes of numerical data, visualization techniques have advanced in surges marked by major breakthroughs, including: 'marching cubes' and its derivatives for interpreting scalar fields, volume rendering for inherently volumetric data; and vector field topology for understanding the structure of flow.  However, existing visualization techniques are limited to individual properties of data, temperature, pressure, velocity, vorticity, shear, combustion rate, rainfall, and so on.    Techniques for multivariate (multifield) data do exist in information visualization, where parallel coordinates, spider plots etc are widely used.  But these tools are of little use for scientific datasets, where the interpretation of data is intimately tied to physical space/time, or to the scale of scientific datasets, which is routinely measured in gigabytes or terabytes. The key problem is that, until now, we have lacked any suitable mathematical and computational model for multifield analysis. The mathematics is needed to explain what exactly it means to understand how multiple fields interact; the computational model is needed to explain how this interaction can be mapped into visual representations that can be generated efficiently from large volumes of data. Techniques for multifield analysis would be of enormous benefit right across the diverse range of application domains that rely on (scientific) visualization, including aerospace, materials engineering, climatology and meterology, astrophysics, radiology and surgical planning. It would enable new scientific insight, and provide industry with new tools through which to develop competitive advantage.  Recent work by the applicants has achieved a breakthrough result. The 'Joint Contour Net' is a new abstraction that hold great promise in providing the mathematical and computational machinery needed for multifields. The origins of the JCN are in computational topology, a field that, over the last decade, has made major contributions to visualization through finding and explaining structure within data. Topology provides a rigorous foundation for identifying features and transitions within data, and these are of particular interest to end users in understanding the original problem. Topological models are also essential in simplifying and presenting massive datasets, as our ability to interpret data has to pass through the bottleneck of screen space (typically around 2M pixels) and the gigabyte limits of the human visual system.  The Joint Contour Net provides a first glimpse of how to generalise topological analysis from one field to many fields, and importantly, how to do so efficiently, and in a way that accommodates parallelisation to scale up to processing massive datasets.  This proposal will deliver on the initial promise by developing the mathematical theory for multifield analysis, generating the and the visual abstractions needed to understand multifield behaviour. To achieve this, we will work closely with other international leaders in visualization and computational topology to address specific issues, such as simplification and rendering techniques based on JCNs, and user interfaces for steering analysis that are adapted to the needs of particular applications.  To ensure the research has the maximum possible reach, we will embed our software into the most widely used visualization toolkits, with dedicated effort to ensure that the implementation is robust and maintainable, and courses to train end-users in its application.    "
	},
	{
		"grant":385,
		"ID": "EP/J013293/1",
		"Title": "Learning Highly Structured Sparse Latent Variable Models",
		"PIID": "-149587",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2012",
		"EndDate": "31/12/2013",
		"Value": "99532",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Statistical Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-149587", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Technological advances have brought the ability of collecting and analysing patterns in high-dimensional databases. One particular type of analysis concerns problems where the recorded variables indirectly measure hidden factors that explain away observed associations. For instance, the recent National NHS Staff Survey of 2009, taken by over one hundred thousand staff members, contained several questions on job satisfaction. It is only natural that the patterns of observed answers are the result of some common hidden factors that remain unrecorded. In particular, such answers could arguably be grouped by factors such as perceptions of the quality of work practice, support of colleagues and so on, that are only indirectly measured.  In practice, when making sense out of a high-dimensional data source, it is useful to reduce the observations to a small number of common factors. Since records are affected by sources of variability that are unrelated to the actual factors (think of someone having a bad day, or even typing wrong information by mistake), removing such artifacts is also part of the statistical problem. A model that estimates such transformations is said to perform 'dimensionality reduction' and 'smoothing'.  There are a variety of methods to accomplish such tasks. At one end of the spectrum, there are models that assume the data match some very simple patterns such as bell curves and pre-determined factors. Others are very powerful, allowing for flexible patterns and even an infinite number of factors that are inferred from data under some very mild assumptions. The proposed work tries to bridge these extremes: the shortcomings of the very flexible models are subtle but important. In particular, they can be very sensitive to changes in the data - meaning some very different conclusions about the hidden factors might be achieved if a slightly different set of observations is provided. Moreover there are computational concerns: calculating the desired estimates usually requires an iterative process, a process that needs some initial guess about these estimates. So, even for a fixed dataset, results can vary considerably if such an initial guess is not carefully chosen. Our motivation is that if one does have these concerns, one might as well take the trouble of incorporating domain knowledge about the domain. The upshot: we do not aim to be general, and instead target applications where some reasonable domain knowledge exists. In particular, we focus on problems where the hidden targets of interest are pre-specified, but infinitely many others might exist. While we map our data to a fixed space of hidden variables, we provide an approach that is robust to the presence of an unbounded number of other, implicit, common factors. The proposed models are adaptive: they account for possible extra variability between the given hidden factors that would be missed by the simpler models. At the same time, they are designed to be less sensitive to initial conditions while being less sensitive to small changes in the datasets. "
	},
	{
		"grant":386,
		"ID": "EP/J014222/1",
		"Title": "MACHINE LEARNING COALGEBRAIC AUTOMATED PROOFS",
		"PIID": "-185954",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/03/2012",
		"EndDate": "28/02/2014",
		"Value": "100268",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing",
		"OrgID": "37",
		"Investigators":[
		{"ID": "-185954", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Some steps in formal reasoning may be statistical or inductive in nature. Many attempts to formalise or exploit this inductive or statistical nature of formal reasoning are related to methods of  Neuro-Symbolic Integration, Inductive Logic and Relational Statistical Learning. The proposal is focused on one statistical/inductive aspect of automated theorem proving -- proof-pattern recognition.   Higher-order interactive theorem provers (e.g. HOL or Coq) have been successfully developed into  sophisticated environments for mechanised proofs.  Whether these provers are applied to big industrial tasks in software verification, or to formalisation of  mathematical theories, a programmer may have to tackle thousands of lemmas and theorems of variable sizes and complexities. A proof in such languages is constructed by combining a finite number of tactics. Some proofs may yield the same pattern of tactics,  and can be fully automated, and others may require a user's intervention. In this case, manually found proof for one problematic lemma may serve as a template for several other lemmas needing a manual proof. At present this kind of proof-pattern recognition and recycling is done by hand, and the ML-CAP project will look into methods to automate this.   Another issue is that unsuccessful attempts of proofs  --- in the trial-and-error phase of proof-search, are normally discarded once the proof is found. Conveniently, analysis of both positive and negative examples is inherent in statistical machine learning. And ML-CAP is going to exploit this.  However, applying statistical machine-learning methods to analyse data coming from proof theory is a challenging task for several reasons.  Formulae written in formal language have a precise, rather than a statistical nature.   For example, list(nil) may be a well-formed term, while list(nol) - not; although they may have similar patterns  recognisable by machine learning methods.  Another problem that arises when merging formal logic and statistical machine-learning algorithms is related to their computational complexity. Many essential logic algorithms are P-complete and inherently sequential (e.g., first-order unification), while neural networks and other similar devices  are based on linear algebra and perform parallel computations.  As a solution to the outlined problems, the coalgebraic  approach to automated proofs  may provide the right technique of abstraction allowing to analyse proof-patterns using machine learning methods.  Firstly, coalgebraic computations lend themselves to concurrency,  and this  may be the key to obtaining adequate representation of the outlined problems. Secondly, they are based on the idea of repeating patterns  of potentially infinite computations, rather than outputs of finite computations.  These patterns may be detected by methods of statistical pattern recognition.   ML-CAP is based upon a novel method of using statistical machine learning in analysis of formal proofs. In summary, it provides algorithms for extracting those features from automated proofs that allow to detect proof patterns   using statistical machine learning tools, such as neural networks. As a result, neural networks can be trained to distinguish well-formed proofs from ill-formed; distinguish whether a proof belongs to a given family of proofs,  and even make accurate predictions concerning potential success of a proof-in-progress. All three tasks have serious applications in automated reasoning.  The project will aim to generalise this method and develop it into a sound general technique for automated proofs. It will result in new methods useful  for a range of researchers in different areas, such as AI, Formal Methods, Coalgebra and Cognitive Science.  "
	},
	{
		"grant":387,
		"ID": "EP/J014338/1",
		"Title": "Laymen To The Help Of Experts: Crowdsourcing To Aid The Reassembly Of Ancient Frescoes",
		"PIID": "-223789",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2014",
		"Value": "98209",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-223789", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Reconstruction of fragmented objects is of great interest in archaeology, where artefacts are often found in a fractured state, and the effort of manually putting the broken pieces together represents a significant amount of time and resources. Representative of this problem is our work on documenting and reconstructing fragments of Late-Bronze-Age wall paintings from the site of Akrotiri on the volcanic island of Thera (modern-day Santorini, Greece). As is common in archaeological finds, the wall paintings are excavated as tens of thousands of pieces, and searching for matches is a daunting manual process that dwarfs the available resources.  In a previous project we have contributed methods to acquire object fragments efficiently, identify pairwise matches between fragments, interactively refine automatically suggested matches and automatically assemble them into larger clusters.  However, unavoidable error sources in acquisition and processing, but also physical influences, such a erosion and other damage over the last 3,500 years, lead to uncertainty in identifying matches between fragments. We find that humans are required to sift the automated match suggestions and that without massively parallelising and without introducing redundancy in the manual sifting, there will always be a substantial number of matches go unnoticed.  This work proposes a crowdsourcing solution to obtain this goal. We believe that a key novelty lies in the fact that we are trying to let non-expert solve a problem in the digital domain that normally requires experts to operate in the physical domain. We believe that we are excellently positioned to turn this into a high-impact project with generalisable outcomes. "
	},
	{
		"grant":388,
		"ID": "EP/J014354/1",
		"Title": "WhatIf: Answering 'What if...' questions for Ontology Authoring",
		"PIID": "10744",
		"Scheme": "Standard Research",
		"StartDate": "17/09/2012",
		"EndDate": "16/12/2015",
		"Value": "542078",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Science",
		"OrgID": "0",
		"Investigators":[
		{"ID": "10744", "Role": "Principal Investigator"},
		{"ID": "54662", "Role": "Co Investigator"},
		{"ID": "-119787", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We have a richness of data about numerous aspects of our activities, yet these  data are only any use when we know what they are, agree upon what they are and  how they relate to each other. Semantic descriptions of data, the means by  which we can achieve these aims, are widely used to help exploit data in  industry, academia and at home. One way of providing such meaning or semantics  for data is through 'ontologies', yet these ontologies can be hard to build,  especially for the very people that are expert in the fields whose knowledge is being captured but who are not experienced in the specialised 'modelling' field.   In the 'what if...?' project we look at the problems of creating ontologies using the Web  Ontology Language (OWL). With OWL logical forms, computers can  deduce knowledge that is only  implied within the statements made by the modeller. So any statement made  by a modeller can have a dramatic effect on what is implied.  These implications can be both 'good' and 'bad' in terms of the aims of the modeller. Consequently, a  modeller is always asking themself 'what if...?' questions as they model a field  of interest. Such a question might be 'what happens if I say that a planet must be orbiting a star?' or 'what happens if I add in this date/time  ontology?'.   The aim of the 'what if...?' project is to build a dialogue system allowing a person building an ontology to ask such questions and get meaningful answers. This requires getting the computer to determine what the consequences of a change in the ontology would be and getting it to present these consequences in a meaningful way. To do a good job, the system will have to understand something about what the person is trying to do and what sorts of results will be most interesting to them. For this, we need to understand more about  how ontologists model a domain and interact with tools; be able to model the  dialogues between a human and the authoring system; achieve responsive  auttomated reasoning that can provide the dialogue system with the information  it nees to create that dialogue. "
	},
	{
		"grant":389,
		"ID": "EP/J014591/1",
		"Title": "Haskell Types with Added Value",
		"PIID": "-145924",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/07/2012",
		"EndDate": "30/06/2013",
		"Value": "96582",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer and Information Sciences",
		"OrgID": "48",
		"Investigators":[
		{"ID": "-145924", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Good ideas, like lightning, take the most conductive path to earth. This one-year project takes advantage of fresh technological insights to narrow the spark-gap from theoretical research to the programming mainstream. In the last decade, dependent types --- capturing relative notions of data validity --- have jumped from logics and proof systems to programming.  Prototype languages such as Cayenne, ATS, Agda and our own Epigram teach us how to characterize data precisely, but none has a coherent treatment of interaction in applications. This project will bring the basics of dependent types to application development now, not via a prototype, but with Haskell, a mature functional programming language with growing traction, thanks to the Glasgow Haskell Compiler (GHC), now developed under the Microsoft aegis.  To make this jump, we must give practical answers to theoretical questions about the mathematical structures which underpin interactive and distributed systems. We must take the blackboard to the motherboard.  The tool which enables this project is our GHC preprocessor, the Strathclyde Haskell Enhancement (SHE), which mechanizes a partial translation from 'dependently typed Haskell' to Haskell as it stands. Up and running, SHE has already delivered the basics of our approach, leading to an article accepted in 2011 by the Journal of Functional Programming, and spurring deeper investigation of both the mathematics of dependently typed interaction and the engineering challenge of scaling up. Through theoretical research, library design and case study, we shall deliver progress across this spectrum through papers and open source software.  GHC is adopting our functionality, but we do not need to wait. SHE can sustain low-cost exploration, putting an effective toolkit in users' hands now, as well as informing the future prospectuses both for dependent types in Haskell and for programming interaction in the next generation of functional languages.  Haskellers recognize the need: Microsoft currently funds a PhD at Strathclyde on numerical dependency in Haskell types.  This project is, then, a double fix: it imports dependent types from tomorrow's languages to today's, and it allows us to guide tomorrow's dependently typed languages towards principled approaches to production software. We have proven track records in theoretical research and professional software development, key ideas to change programming for the better, and the skills to deliver world-leading research. "
	},
	{
		"grant":390,
		"ID": "EP/J014699/1",
		"Title": "ULTRA-HIGH-RESOLUTION, ULTRA-SENSITIVE MULTIFUNCTIONAL BALLISTIC NANO SENSORS  FOR THE SIMULTANEOUS DETECTION OF MAGNETIC, ELECTRIC AND OPTICAL FIELDS",
		"PIID": "44683",
		"Scheme": "Standard Research",
		"StartDate": "14/11/2012",
		"EndDate": "13/11/2015",
		"Value": "493258",
		"ResearchArea": "Condensed Matter: Electronic Structure",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "44683", "Role": "Principal Investigator"},
		{"ID": "-88475", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "University of Bath"}
		],
		"Summary": "At macroscopic length scales, charge carriers in semiconductors can be described by diffusive transport properties and sensor concepts such as the Hall Effect are applicable. When materials are fabricated at the nanoscale, new properties emerge and quantum effects dominate. In high-mobility materials such as narrow-gap semiconductors (NGS), charge carriers exhibit ballistic properties, when device length scales are smaller than the electron mean free path. In InSb (and InAs) quantum well structures, room-temperature mobilities exceed 40,000 (and 25,000) cm2/Vs, respectively, yielding electron mean free paths in excess of 500nm, which are accessible in principle using current processing technology. Nevertheless, room-temperature ballistic effects, even in high mobility NGS have remained elusive. As a result there has been no significant exploration and exploitation of this interesting and potentially important transport regime.    As a result of current funding (EPSRC EP/F067216 - EP/F065922 - end date November 2011) we have made significant theoretical and experimental developments, resulting in the demonstration of room temperature ballistic effects in NGS heterostructures.  We have shown that it is possible to create collimated ballistic electrons in a simple cross-structure, which enhances the so called negative bend resistance (NBR).   We have also shown that collimated NBR sensor responsivity (the change in four terminal device resistance to the perturbing field) scales with inverse device size. Remarkably, this means that there is no significant loss in sensor performance as the dimensions shrink, a highly desirable property for nanoscale electronics applications.  The focus of our new proposal is to build on these considerable experimental and theoretical developments. We plan to use the NBR geometry as a natural platform to realise high sensitivity multifunctional NGS ballistic nanosensors operating at room temperature, which utilise the change in electrical resistance that results when the device is exposed to magnetic, electric and/or optical fields. As part of this vision we plan to integrate two other key device concepts that will enable the multifunctional character of the devices and boost sensitivity. The first is our discovery that in the appropriate device geometry, perturbing external fields, such as optical fields, can convert carriers from the ballistic to the diffusive regime. The second is that a metal shunt appropriately placed within the device architecture, provides a low resistance path and access to that path via a Schottky barrier is tunable via external perturbing fields. This latter property has been used to great effect in diffusive devices known as EXX sensors, which were invented by a coinvestigator and visiting academic on our proposal, Prof Stuart Solin. Apart from our own preliminary investigations, the integration of a metallic shunt in the ballistic limit, is a completely new concept and aspects such as plasmonic effects and hot carrier effects will need to be investigated.   Up to this time our achievements are based on InSb heterostructures. The motivation to examine the smallest possible devices, means that the grant activities will include exploration of the properties of InAs quantum well devices. InAs offers similar room temperature mean free paths to InSb, but has attractions including lower overall resistance, the option to be more heavily doped and greater potential for tunability as far as controlling collimation because of negligible side wall depletion.  The study of interface effects in the ballistic regime at room temperature is a field almost completely unexplored and because of the recognised demand for high-resolution high-sensitivity sensors for applications spanning biosensing, point of care diagnostics using magnetic bead detection, ICT, and Security, our work is timely and fits well within the EPSRC research strategic areas. "
	},
	{
		"grant":391,
		"ID": "EP/J014729/1",
		"Title": "Enabler for Next-Generation Mobile Video Applications",
		"PIID": "-190135",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2012",
		"EndDate": "28/02/2014",
		"Value": "99872",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing",
		"OrgID": "107",
		"Investigators":[
		{"ID": "-190135", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "AIMS (INT) Ltd"},
		{"Number": "1", "Name": "Qualcomm"}
		],
		"Summary": "The High Efficiency Video Coding (HEVC) standard under development is the next-generation video compression standard, which is destined to revolutionise the world of video applications thanks to the expected high compression efficiency compared with the current standard. Nevertheless, to turn the vision into reality, substantial research efforts are necessitated since HEVC is still at its early standardisation stage and there is little research on networked HEVC applications. The proposed project targets mobile HEVC video delivery in an emerging and challenging networking paradigm, multihomed mobile networks, which are moving networks with a number of mobile users onboard (e.g., passengers on public transport vehicles). A multihomed mobile network has access to multiple complementary wireless networks such as 3G and beyond, WiFi and WiMAX, each of which provides a different network path to onboard multihomed mobile routers managing the multihoming and the mobility of the whole moving network. Compared with individual multihomed mobile hosts, a distinctive advantage of a multihomed mobile network is that the mobile users' devices do not need being multihomed or mobility aware. Such multihoming and mobility transparency substantially reduces the requirements on users' devices and the costs for deploying and operating such a system.  The proposed research will attempt to enable bi-directional real-time video transmission between the mobile users within a multihomed mobile network and the infrastructure to support various applications and services for business, entertainment, safety, security or other purposes. A typical application scenario is in the public transportation context. In this case, the onboard passengers receive high-quality downlink HEVC-based video streamed from an HEVC media server. Meanwhile, the live onboard scenes are captured by a camera, encoded by an HEVC encoder, and streamed upwards to a monitoring station for public security and incident management. It is expected that the mobile user's quality of experience (QoE) will be significantly improved compared with latest standards H.264 Advanced Video Coding (AVC) and Scalable Video Coding (SVC) through the proposed research.  The proposed research can be easily adopted in a range of application scenarios such as intelligent transportation systems, emergency/urgency management, telemedicine, personal area networks, and home and camping entertainment. Therefore, there is a strong technological and economic need for further research into the theory and application of the proposed topic. Moreover, joint contributions with Qualcomm to the JCT-VC meetings will enable us to set benchmarks in mobile HEVC applications and contribute to the standardisation of HEVC, which can lead to a long-lasting global impact. Dissemination and exploitation plans will promote the research outcome to the academic community as well as companies (such as AIMS (INT) Ltd., another partner of the project) related to video networking applications to exploit potential commercial product development. "
	},
	{
		"grant":392,
		"ID": "EP/J014990/1",
		"Title": "Constant-time wide-area monocular SLAM using absolute depth hinting",
		"PIID": "11374",
		"Scheme": "Standard Research",
		"StartDate": "01/12/2012",
		"EndDate": "31/05/2016",
		"Value": "393464",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "11374", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Understanding the visual environment is key to allowing machines interact with us and the space we occupy, whether the machine is to take fully autonomous action or just provide us with extra information and advice. A core competence is the ability to reconstruct a 3D representation of a moving camera's surroundings and to locate the camera relative to them from moment to moment. Over the last years, enormous practical strides have been made in this problem of visual simultaneous localization and mapping (visual SLAM), to the point now where robust live reconstruction is possible on modest hardware using stereo cameras and, more challengingly, using just a single camera.  We are concerned here with single camera visual SLAM, of particular importance when the payload and power manifest has to be kept small. The state of the art allows reconstructions containing several tens of thousands of 3D point locations imaged from a few hundred viewpoints to be optimized on the fly. Though this sounds large, in practice these numbers restrict the operational scope to modestly-sized environments. Our aim in this proposal is to tackle the two chief impediments to increasing that scope.   First is algorithmic computational complexity. Polynomial complexity in the number of map points and/or the number of camera positions gradually stifles live operation as the environment expands. This means that gains in processor speed do not lead to proportional gains in map size, and we cannot merely wait for cpu development to catch up. Here we will pursue our recent work on constant-time exploration in monocular SLAM, in which only a local region around the camera needs to be re-optimized frame-by-frame.  Second is monocular vision's inherent depth/speed scaling ambiguity. Using image motion alone only relative depth, rather than absolute depth, is observable. As the camera moves around, uncertainty builds up not only in position and orientation, but also in the scale of the surroundings. This leads to added difficulty when returning to a previously visited location: not only do the surroundings appear translated and twisted, but they also appear the wrong size. But a human one-eyed observer does not suffer in the same way: whether deprived of stereo vision or not, we use other visual clues to maintain our sense of scale, clues from objects, object classes, and from low-level image traits. It is these that this project plans to glean to provide partial information about absolute depth, sufficient to remove that extra degree of freedom in the solution.  We aim to produce a SLAM algorithm (i) that functions at video frame-rate; (ii) that functions in on-average constant time quite independently of the size of the map it is constructing, and (ii) that behaves gracefully whatever quality of depth information is provided to it. "
	},
	{
		"grant":393,
		"ID": "EP/J015180/1",
		"Title": "Sensor Signal Processing",
		"PIID": "10612",
		"Scheme": "Platform Grants",
		"StartDate": "01/06/2012",
		"EndDate": "31/05/2016",
		"Value": "1051764",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40",
		"Investigators":[
		{"ID": "10612", "Role": "Principal Investigator"},
		{"ID": "11300", "Role": "Co Investigator"},
		{"ID": "34198", "Role": "Co Investigator"},
		{"ID": "101189", "Role": "Co Investigator"},
		{"ID": "-162130", "Role": "Co Investigator"},
		{"ID": "16440", "Role": "Co Investigator"},
		{"ID": "77863", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The fundamental challenges for signal processing are: how best to sense; how to distribute the processing and communication of the data within the network to maximize performance and minimize cost; how to analyze it to extract the salient information. Signal processing is the glue which holds together much of modern technology. The algorithms underpinning mobile communications, medical imaging, image rendering for games and many other technologies were all developed within the global signal and image processing research community. Today the world is an environment of pervasive interconnected sensing with the associated requirement to extract useful information from the large volumes of data that arise. In applications of defence, homeland security and environmental monitoring there is a need to collect and combine data from a range of sensors of widely differing complexity (e.g. from satellite imaging to ground based motion detectors) to achieve persistent wide area monitoring of a scene of interest. This can assist in the assessment of threats, e.g. the planting of improvised explosive devices, the long-term ecological effects of deforestation, or the monitoring of time critical events such as devastation by fire or flood. On the roads the external monitoring of traffic flow by closed circuit television networks, junction-based pressures sensors and GPS create an opportunity when combined with on-vehicle sensors (e.g. lidar, radar and video) to provide driver assistance and ultimately automatic driving systems. This Platform proposal seeks funding for a foundation for our research team in addressing these challenges."
	},
	{
		"grant":394,
		"ID": "EP/J015377/1",
		"Title": "Querying Graph Structured Data:  Principles and Techniques",
		"PIID": "-117299",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2012",
		"EndDate": "31/10/2016",
		"Value": "637076",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-117299", "Role": "Principal Investigator"},
		{"ID": "120752", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "One of the most challenging problems in information processing is handling large amounts of information (EPSRC, in its priority theme `Towards an intelligent information infrastructure' refers to 'deluge of data' and delivering `understanding from information' as the key problems).  Very often nowadays, these vast amounts of information are associated with new applications. For instance, the popularity of social networks such as LinkedIn, Facebook, and others, results in large amounts of data they accumulate. The Semantic Web effort generates high volumes of data too, as it attempts to facilitate understanding of Web data. Many of these applications have one feature in common: the underlying data model is described by a graph, and in querying such graph-structured data, the topology of the graph is as important as the data itself. Graph data arises in a multitude of other applications, including intelligence analysis and crime detection, biology, cheminformatics, knowledge discovery, and network traffic.  At the same time, foundational aspects of graph data have not yet been adequately studied.  While sharing the same high-level model, applications of graph data are developing largely independently of each other, producing separate - but related - sets of data processing tools.  The main goal of this project is to develop principles and techniques underlying querying of graph data, concentrating on query language design, algorithmic tools for query processing tasks, delivering exact or approximate answers fast from extremely large graph databases, and implementing the algorithmic toolbox in a prototype to be used by our industrial partners.  The main challenges lie in combining data and topology in querying, in the inherent complexity of graph queries, and in the dynamic and distributed nature of graph data. The project will be split into three components. The first will concentrate on query language design for handling data and topology, and on different semantics of query answering for dealing with extremely large data graphs.  The second will provide an algorithmic toolbox for query evaluation techniques, for all possible combinations of the query processing mode (batch vs incremental), for the locality of data (single-site vs distributed), and for the type of query answers (exact vs approximate). The third component concerns with the implementation of a prototype on which we shall test our design decisions and algorithms. "
	},
	{
		"grant":395,
		"ID": "EP/J015520/1",
		"Title": "Channel Decoder Architectures for Energy-Constrained Wireless Communication Systems: Holistic Approach",
		"PIID": "-99841",
		"Scheme": "Standard Research",
		"StartDate": "12/10/2012",
		"EndDate": "11/10/2015",
		"Value": "316040",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-99841", "Role": "Principal Investigator"},
		{"ID": "50450", "Role": "Co Investigator"},
		{"ID": "21686", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "British Telecommunications Plc"},
		{"Number": "1", "Name": "Cambridge Silicon Radio Ltd"},
		{"Number": "2", "Name": "Nokia Siemens Networks"}
		],
		"Summary": "The Machine-To-Machine (M2M) applications of Wireless Sensor Networks (WSNs) and Wireless Body Area Networks (WBANs) are set to offer many new capabilities in the EPSRC themes of 'Healthcare technologies', 'Living with environmental change' and 'Global uncertainties', granting significant societal and economic benefits. These networks comprise a number of geographically-separated sensor nodes, which collect information from their environment and exchange it using wireless transmissions. However, these networks cannot as yet be employed in demanding applications, because current sensor nodes cannot remain powered for a sufficient length of time without employing batteries that are prohibitively large, heavy or expensive. In this work, we aim to achieve an order-of-magnitude extension to the battery charge-time of WSNs and WBANs by facilitating a significant reduction in the main cause of their energy consumption, namely the energy used to transmit information between the sensor nodes. A reduction in the sensor nodes' transmission energy is normally prevented, because this results in corrupted transmitted information owing to noise or interference. However, we will maintain reliable communication when using a low transmit energy by specifically designing channel code implementations that can be employed in the sensor nodes to correct these transmission errors. Although existing channel code implementations can achieve this objective, they themselves may have a high energy consumption, which can erode the transmission energy reduction they afford. Therefore, in this work we will aim for achieving a beneficial step change in the energy consumption of channel code implementations so that their advantages are maintained when employed in energy-constrained wireless communication systems, such as the M2M applications of WSNs and WBANs. We shall achieve this by facilitating a significant reduction in the supply voltage that is used to power the channel code implementations. A reduction in the supply voltage is normally prevented, because this reduces the speed of the implementation and causes the processed information to become corrupted, when its operations can no longer be performed within the allotted time. However, we will maintain reliable operation when using a low supply voltage, by specifically designing the proposed channel code implementations to use their inherent error correction ability to correct not only transmission errors, but also these timing errors. To the best of our knowledge, this novel approach has never been attempted before, despite its significant benefits. Furthermore, we will develop methodologies to allow the designers of WSNs and WBANs to estimate the energy consumption of the proposed channel code implementations, without having to fabricate them. This will allow other researchers to promptly optimise the design of the proposed channel code implementations to suit their energy-constrained wireless communication systems, such as WSNs and WBANs. Using this approach, we will demonstrate how the channel coding algorithm and implementation can be holistically designed, in order to find the most desirable trade-off between complexity and performance."
	},
	{
		"grant":396,
		"ID": "EP/J015563/1",
		"Title": "Yield and reliability enhancement techniques for novel memory devices",
		"PIID": "98871",
		"Scheme": "Standard Research",
		"StartDate": "24/08/2012",
		"EndDate": "23/12/2015",
		"Value": "274137",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "98871", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Hewlett-Packard Company Inc"},
		{"Number": "1", "Name": "Intel Corporation"},
		{"Number": "2", "Name": "STMicroelectronics SrI"}
		],
		"Summary": "Describe the proposed research in simple terms in a way that could be publicised to a general audience [up to 4000 chars]. Note that this summary will be automatically published on EPSRC's website in the event that a grant is awarded.  The recently developed memory architectures based on resistive-variable devices such as Phase Charge Memories, Programmable Metallization Cell or memristors have reliability issues that are drastically different from those affecting CMOS based memories. These novel memories although based on different technologies, they all share the principle of storing information as the resistance value imposed to a resistive-variable devices and consequently also the possible type of faults that may occur. This project proposes to leverage data obtained from experimental results to characterize resistive-variable devices and to exploit both information and architectural redundancies to enhance reliability and yield of these devices.  To face the presence of a massive number of defects suitable spare resources, such as spare row and/or columns will be used combined with suitable error detection methods and efficient readdressing scheme to substitute faulty elements. To leverage the use of spares resources, codes novel models and algorithms to estimate the reliability versus overhead trade-off will be developed, with the aim of obtaining a reliability-aware driven synthesis tool for these memory devices.  "
	},
	{
		"grant":397,
		"ID": "EP/J015849/1",
		"Title": "InAsNSb Dilute Nitride Materials for Mid-infrared Devices & Applications",
		"PIID": "9043",
		"Scheme": "Standard Research",
		"StartDate": "15/08/2012",
		"EndDate": "13/03/2016",
		"Value": "370325",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "63",
		"Investigators":[
		{"ID": "9043", "Role": "Principal Investigator"},
		{"ID": "111427", "Role": "Co Investigator"},
		{"ID": "-253003", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Compound Semiconductor Tech Global Ltd"},
		{"Number": "1", "Name": "Instro Precision Ltd"},
		{"Number": "2", "Name": "SELEX Galileo"}
		],
		"Summary": "We aim to achieve a breakthrough in the performance of 'dilute nitride' semiconductor materials to enable the development of novel light sources and photodetectors which can operate in the mid-infrared spectral range. The 3-5 um wavelength range is technologically important because it is used for applications including; remote gas sensing, range-finding and night vision, bio-medical imaging for diagnosis in healthcare and sensitive detection in optical spectroscopy.  However, the development of instrumentation is limited by the availability of efficient, affordable light sources and photodetectors, which is directly determined by the semiconductor materials which are currently available. By introducing small amounts (~ 1%) of N into InAs(Sb) we have shown that it is possible to access the mid-infrared using a new (dilute nitride) semiconductor  and we are now seeking to engineer its band structure in order to significantly enhance the material's optical properties and increase quantum efficiency for light detection and emission.   To enable the development of new photodetectors we will exploit the sensitivity of the conduction band to the resonant interaction of the N-level with the extended states of the host InAsSb crystal lattice to tailor the photoresponse and create a near ideal situation for electron acceleration and avalanche multiplication, resulting in a much larger detectable signal. To minimise the unwanted processes causing excessive noise and dark current, which compete with the avalanche multiplication and light detection in the detector, we shall arrange for the avalanche multiplication to be initiated by only one carrier type (electrons in our case). Many applications rely on the detection of very weak signals consisting of only a few photons. Conventional photodiodes have a limited sensitivity, especially if high speed detection is needed. In applications which are 'photon starved', avalanche photodiodes (APDs) can provide an effective solution. However, at present effective avalanche multiplication in the mid-infrared spectral range can only be obtained by using exotic CdHgTe (CMT) semiconductor alloys. The resulting detectors require cooling, thus making CMT-based APDs prohibitively expensive for all except military applications. Simpler fabrication, low noise, low operating voltage, inexpensive manufacturing and room temperature operation, together with monopolar electron ionisation are all significant advantages of APDs based on the dilute nitride materials compared to existing technologies. Similarly, we shall enable the development of more efficient mid-infrared light sources. By adjusting the N content within InAsN(Sb) quantum wells and carefully tailoring the residual strain and carrier confinement, we shall be able to defeat competing non-radiative recombination processes whilst simultaneously enhancing the light generation efficiency. These novel quantum wells would then form the basis of the active region from where the light is generated, either within an LED or a diode laser. Currently mid-infrared LED efficiency is low at room temperature, and with the improvements which we shall deliver; we envisage that devices with significantly higher dc output power will be developed following our lead. Mid-infrared diode lasers incorporating our strained dilute nitride quantum wells are also expected to exhibit a reduced threshold current and could offer an affordable alternative to existing technology, especially in the 3-4 um spectral range. We will produce prototype photodetectors and LEDs and use these to demonstrate the above-mentioned avalanche behaviour and quantum efficiency improvements respectively. We shall validate our dilute nitride materials and structures in close collaboration with our collaborators at NPL, SELEX, CST and INSTRO to evaluate performance for use in practical applications and help ensure uptake of our technology. "
	},
	{
		"grant":398,
		"ID": "EP/J016330/1",
		"Title": "DOME: Delaying and Overcoming Microprocessor Errors",
		"PIID": "116096",
		"Scheme": "Standard Research",
		"StartDate": "27/09/2012",
		"EndDate": "26/03/2016",
		"Value": "589625",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "116096", "Role": "Principal Investigator"},
		{"ID": "5628", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern day computer systems have benefited from being designed and manufactured using an ever-increasing budget of transistors with very reliable integrated circuits. However, moving forward such a ''free lunch'' is over and forgotten nightmares faced by computer pioneers are coming back to haunt us. Not so long ago, unreliable valves were the basic building blocks for computers and research focussed on how to successfully compute, overcoming this underlying weakness (e.g. von Neuman, 1956, ''Probabilistic logics and the synthesis of reliable organisms from unreliable components'').  State-of-the-art integrated circuit technologies have now reached the range of 40-22 nanometers, posing significant reliability challenges. Hard or permanent errors can manifest themselves at any point during a processor's lifetime. During manufacturing, errors can render a proportion of a chip incapable of computing, thus decreasing yield and profit. As we move towards smaller and smaller components, transistors take less and less time to wearout, becoming more prone to failure in the field. Traditional reliability solutions involve applying high-cost redundancy to the hardware structures within the processor, providing backup spares for when errors occur. On the application side, solutions also involve redundancy by running multiple copies of each piece of software.  A common criticism of current reliability solutions is that they do not consider how the software and hardware can be co-designed synergistically to tackle this challenge. Redesigning and reimplementing general purpose software applications will incur an unaffordable price tag. Our hypothesis is that virtualization technologies (a layer that transparently hides the underlying platform from the application software) have an important role to play. In particular, managed runtime environments (MREs) have become pervasive for high-productivity software developers and represent a promising vehicle for providing reliability mechanisms.  Within these systems, applications can be monitored and morphed without user intervention.  There are two complementary strands to our proposed research, focused around a co-designed MRE and multicore computer architecture.  Firstly, we will consider wearout mitigation schemes to slow processor ageing and lengthen a chip's lifetime before a hard fault occurs.  Secondly, given that an error will occur at some point during a system's life, we will develop error-tolerance approaches that maintain execution on faulty hardware.  If successful, we believe this project will be seen as a significant milestone in the development of wearout-conscious and error-tolerant multicore architectures over the next decade. This research programme will advance our understanding of the field, tackling the UK Microelectronics Grand Challenge of Moore for Less that has been signposted by EPSRC. It is also important to highlight that this proposal tackles a key aspect of the new EPSRC ICT capability priority on 'Many-core architectures and concurrency in distributed and embedded systems'. "
	},
	{
		"grant":399,
		"ID": "EP/J016748/1",
		"Title": "India-UK Advanced Technology Centre (IU-ATC) in Next Generation Networks Systems and Services (Phase Two Follow-on)",
		"PIID": "12101",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2012",
		"EndDate": "30/06/2014",
		"Value": "595614",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing & Information Eng",
		"OrgID": "9",
		"Investigators":[
		{"ID": "12101", "Role": "Principal Investigator"},
		{"ID": "11216", "Role": "Co Investigator"},
		{"ID": "56878", "Role": "Co Investigator"},
		{"ID": "32051", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BTEXACT"},
		{"Number": "1", "Name": "Lancaster University"},
		{"Number": "2", "Name": "Queen Mary, University of London"},
		{"Number": "3", "Name": "Toshiba Research Europe Ltd"},
		{"Number": "4", "Name": "University College London"},
		{"Number": "5", "Name": "University of Bristol"},
		{"Number": "6", "Name": "University of Cambridge"},
		{"Number": "7", "Name": "University of Southampton"},
		{"Number": "8", "Name": "University of St Andrews"},
		{"Number": "9", "Name": "University of Surrey"}
		],
		"Summary": "This is a follow-on proposal for a Phase Two from the highly successful Phase One under EPSRC funding (GR EP/G051674/1; EP/G049874/1; EP/G049939/1; EP/G050600/1; EP/G05178X/1; EP/G053847/1;EP/G054886/1; EP/G055610/1; EP/F030118/1) of the IU-ATC which was for an initial 30-month period of a 5-year project envisioned by EPSRC and DST.  The IU-ATC project represents the largest collaboration of its kind between UK and India and as such provides a unique and internationally competitive research eco-system to be further leveraged for maximum impact.   As commented by the EPSRC Review Panel that met on 15th August (i)  'The panel were positive about the success of Phase 1 of IU-ATC, commenting that they were impressed with the achievements of the consortium so far in the face of the significant challenge of making a consortium work across numerous institutions and country boundaries.', (ii) 'The panel were clear that there is no question of the huge capacity that the IU-ATC has built over phase 1.'  A summary of our strengths is provided in the Joint 2-page (planning for IU-ATC Phase 2) document submitted to EPSRC-DST on August 5th 2011 (attached).  In summary  there has  been 246 international Conference Papers,  106 Journal Papers ( with 31 papers still under review), Papers under dissemination 31 , 6 Books , and 10  Technical Reports.  Of particular significance are the 15  Patents Submitted, the 8  technical Prototypes built and the 12  Technical Testbeds / Demonstrators that support the work of the team in both countries.   As we plan for Phase 2, we have reflected on our outputs to-date and also the recently published strategic research priorities from EPSRC published in July 2011 on Global Uncertainties , Healthcare, Digital Economy, E-Infrastructure, Intelligent Information Infrastructure, Working Together and DST 11th Plan, DST SAC  respectively. In light of the respective national priorities for ICT Research and Innovation that have been identified by EPSRC-DST, there are a number of directly relevant 'grand challenges' which we highlighted in our 2-page plan for the respective EPSRC-DST Review Committees on 5th August 2011 (attached).    Leveraging the capacity that has been developed in IU-ATC Phase 1, we will  take into consideration some of the respective national priorities areas as listed in 1..7 above and the key  recommendations of the EPSRC-DST review Panels.  As evidenced from the EPSRC Review Panel a specific recommendation was made that whilst we should strive to have commonality of approach between work areas in both countries we should not 'force-fit' all research activities to both countries.  Given this recommendation, we have developed a plan of innovative research that attempts to address global issues, common challenges and respective national priorities.   "
	},
	{
		"grant":400,
		"ID": "EP/J017108/1",
		"Title": "Approximate Matching of Sequences.",
		"PIID": "37926",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "30/11/2013",
		"Value": "156691",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "78",
		"Investigators":[
		{"ID": "37926", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "One of the most ancient, fundamental and still active research field in theoretical computer science concerns Pattern Matching and its many approaches. The classic pattern matching problem consists in recognizing a pattern P as 'matching' or 'not matching' within a given text T and of listing all the occurrence positions, if any. It is straightforward that the text may be whatever kind of input data and the pattern is usually a smaller piece of the same kind of data. One instance of the problem is to suppose that the text is provided before the pattern. Then the text can be preprocessed to build an index data structure that accelerates dramatically further searches. There exist several such data structures. This branch of pattern matching is usually called Text Indexing. Its natural application scenario is when the text does not change or changes slowly with respect to the great amount of pattern search queries, e.g. a sequenced genome, a digital library or a biometric database. The most important variant of Text Indexing concerns 'approximate matching' of the pattern with text segments. It is called Approximate Pattern Matching. Approximate means that an occurrence of the pattern within the text is considered a match even if it presents some errors, but if the distance (resp. similarity) between the pattern and the text segment is under (resp. over) a certain threshold.  Approximate variants of text indexing fit well with scenarios where a mistyping errors may occurs, the data sampling process is prone to introduce errors, the data source is not constant over the time as for biometric data, or when we are just looking for data similarity, e.g. for genes mutations in DNA sequences.  Pattern matching plays a fundamental role in many research and application fields, like information retrieval, data mining, bioinformatics, molecular biology functional and structural analysis, molecular pharmacology, computational musicology, network security, biometric identification, and many others. Among these, bioinformatics and computational molecular biology are the domains that mostly and directly benefit from any enhancement of pattern matching theoretical knowledge and solutions.  While straight text indexing counts many efficient solutions, its approximate versions have not found optimal solutions yet. Several solutions exploit the property that an approximate match is usually composed of some shorter and closer matches, i.e. exact matches or approximate matches with a smaller number of errors. This approach does not provide any breakthrough towards an optimal solution.  Our approach is to design data structures and their related algorithms to get an efficient solution to the problem. We have explored and experimented several solutions that provided remarkable potentialities. They need to be developed further and tailored to some applications. Implementation for external memory and for distributed calculus shall be investigated. As a by-product some theoretical results related to the distance between sequences should be achieved. The complexity of many classical result will be reviewed to highlight their dependence from the maximal-repetition-with-error parameter. All this is expected to provide a substantial step forward in the pattern matching research field.  The technological breakthrough proposed in this research allows to dramatically decrease the computational cost of detecting local similarities within a large family of biological sequences and of performing some approximate pattern matching related tasks. Furthermore, our designed algorithms will extend the range of applications of approximate pattern matching based solutions as they will allow to accomplish more complex investigations on sequences with the same computational resource either at the level of personal computer scale solutions, and of big research centres. "
	},
	{
		"grant":401,
		"ID": "EP/J017159/1",
		"Title": "Efficient Delivery of Interactive Content via Large-Scale Wireless Mesh Networks",
		"PIID": "-251433",
		"Scheme": "First Grant Scheme",
		"StartDate": "02/01/2013",
		"EndDate": "01/01/2015",
		"Value": "76617",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Science & Technology",
		"OrgID": "103",
		"Investigators":[
		{"ID": "-251433", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In the past few years, a global flurry of wireless communications in the rapid roll-out of multimedia (e.g., YouTube, Skype) has consumed a large portion of wireless bandwidth. With the availability of various advanced devices (e.g., smart phones, tablet pc, mobile gaming terminals), the demands of low-cost and large-scale interactive multimedia content delivery will become a key feature of next generation wireless networks. The primary requirement for these services is the sustained transmission of high quality data at high rates across large scale areas, comprising a set of strict criteria in terms of short delays, small delay jitter, and high throughput under changing wireless communication conditions.   Wireless mesh networks (WMNs) are a disruptive technology that will be an alternative to existing mobile wireless architectures for low-cost multimedia communications in large scale outdoor environments. However, the fundamental drawbacks (e.g., limited bandwidth, wireless transmission interference and conflict) inherent in the wireless transmission medium make it a challenge to deploy WMNs to carry high-performance multimedia communications in large-scale areas. Integrated communications that combine the Internet and wireless networks were provided to solve the scalability problem.  Current studies focus on single-source integrated multimedia communications and will cause a set of problems such as heavy overheads, complicated data distribution processes and complexity in maintaining the architectures for multiple communications if being used for interactive multimedia communications, due to multiple concurrent senders of interactive multimedia content generating a heavier and more complicated traffic load for wireless networks to manage.   This project, as a part of the initial work on large-scale and low-cost wireless interactive multimedia communications, will study advanced algorithms/protocols and theorems for optimally integrating the Internet and WMNs which will efficiently meet wired and wireless integration requirements for transmissions from multiple interactive sources.  To tackle this difficult problem progressively, three major tasks are planned. We will first study the extension of performance-guaranteed wireless transmission environments which will avoid the overuse of the Internet and backhaul access resources via mesh gateways and therefore the scalability of integrated communications. The study will be implemented by exploring the advantages provided by multiple transmission rates and multiple transmission channels. We will then study an integrated architecture for multiple interactive sources. This study will be based on the investigation of multimedia transmission performance in multi-rate multi-channel WMNs and the discussion of the impact of mesh gateways on the performance of integrated communications.  Finally, we will study the proposed architecture in theory. We will provide a mathematical model that can reflect the advanced technology arising from this project. We will justify the proposed integrated architecture through solid theoretical performance analysis.   The research proposed in the project is believed to among the first to consider large-scale affordable interactive multimedia communications any time and anywhere for the general public. It is now commonly accepted by most industry players that multimedia communications act like a gateway to a whole new communication world. This project thus will not only aid the UK in keeping its world-leading position in telecommunications but will also be attractive for commercial exploitation in the near future. "
	},
	{
		"grant":402,
		"ID": "EP/J017205/1",
		"Title": "Design Patterns for Inclusive Collaboration (DePIC)",
		"PIID": "109935",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2012",
		"EndDate": "31/10/2015",
		"Value": "787237",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "109935", "Role": "Principal Investigator"},
		{"ID": "60141", "Role": "Co Investigator"},
		{"ID": "44146", "Role": "Co Investigator"},
		{"ID": "-218241", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our interaction with the world around us relies on perception which exploits combinations of the senses we have available to us. Enabling people to use combinations of senses becomes critical in situations where people who have different senses available to them interact with each other. These differences can arise because of temporary or permanent sensory impairment, or due to the technology they are using. However, very little research has examined how people combine and map information from one sense to another, particularly for individuals with sensory impairments, and then used such mappings to inform the design of technology to make collaboration easier. The aim of this multi-disciplinary project is to develop new ways for people to interact with each other using different combinations of senses. This will reduce barriers to collaboration caused by sensory impairment, and improve social and workplace inclusion by optimising the use of available senses. We will combine empirical studies of mappings between senses with participatory design techniques to develop new ideas for inclusive design grounded in Cognitive Psychology. We will capture these design ideas and mappings in the form of Design Patterns and demonstrate their usefulness through the development of interactive systems to support assisted work, living, and leisure. "
	},
	{
		"grant":403,
		"ID": "EP/J017655/1",
		"Title": "Filter Bank Based Multi-Carrier Systems for Future Broadband Wireless Communications",
		"PIID": "132133",
		"Scheme": "First Grant Scheme",
		"StartDate": "03/12/2012",
		"EndDate": "02/04/2014",
		"Value": "99599",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Communications Systems Res CCSR",
		"OrgID": "51",
		"Investigators":[
		{"ID": "132133", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Huawei Technologies Co Limited"}
		],
		"Summary": "Orthogonal Frequency Division Multiplexing (OFDM) technique has gained increasing popularity in both wired and wireless communication systems, mainly due to its immunity to multipath fading, which allows for a significant increase in the transmission rate. By inserting a cyclic prefix (CP) before each transmitted block longer than the length of the channel, OFDM effectively transforms a frequency selective channel into a parallel of flat-fading channels. This greatly simplifies both channel estimation and data recovery at receiver. However, these advantages come at the cost of a loss of 10-25% spectral efficiency due to the insertion of CP, and an increased sensitivity to frequency offset and Doppler spread as well as transmission nonlinearity accentuated by non-constant modulus of OFDM signals. Additionally, due to the time-varying nature of wireless channels, training sequence needs to be transmitted periodically for the purpose of channel estimation. The overhead imposed by training sequence and CP can be up to 50 percent for fast fading channels, causing significant loss of spectral efficiency.  In this proposal we aim to tackle these problems with the filter bank based multi-carrier system employing a special pulse shaping filter called IOTA (isotropic orthogonal transform algorithm) to yield good time and frequency localization properties so that inter-symbol interference (ISI) and inter-carrier interference (ICI) are avoided without the use of CP. We also investigate a linearly precoded IOTA system which facilitates blind channel estimation, resulting in a spectrally efficient multi-carrier system without the transmission of training sequence in addition to the elimination of CP. In order to effectively combat carrier frequency offset and high PAPR problems in the current orthogonal frequency division multiple access (OFDMA) and single carrier frequency division multiple access (SC-FDMA) based uplink communications, we propose a novel multiple access scheme which combines IOTA with low density signature (LDS) technique.  The focus of our work will be on the study and utilization of some special properties of IOTA which have been overlooked by others. We aim to leverage these properties in the equalization, decoding and channel estimation design in order to achieve optimal performance and maximum capacity with affordable computational complexity. Our goal is to provide theoretical references and guidelines for successful implementation of IOTA systems for future wireless communications."
	},
	{
		"grant":404,
		"ID": "EP/J018805/1",
		"Title": "Lexico-syntactic text simplification for improving information access",
		"PIID": "-150560",
		"Scheme": "First Grant Scheme",
		"StartDate": "28/02/2013",
		"EndDate": "27/06/2014",
		"Value": "97365",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Science",
		"OrgID": "0",
		"Investigators":[
		{"ID": "-150560", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Text simplification is the process of reducing the grammatical and lexical complexity of a text, while retaining its information content and meaning. The main goal of simplification is to make information more accessible to the large numbers of people with reduced literacy. The National Literacy Trust (www.literacytrust.org.uk) estimates that one in six adults in the UK have poor literacy skills. There is therefore a need to make information available in simple English, as advocated by organisations such as the Plain English Campaign (www.plainenglish.co.uk). This need for text simplification is likely to become more acute for a variety of reasons; for instance, a growing aging population with language difficulties arising from neurodegeneration and other causes, children accessing information on the internet or lay readers trying to access technical writing online (perhaps, to research an illness or treatment).   One of the most popular information sources online is Wikipedia (www.wikipedia.org), a free-content encyclopedia written collaboratively by internet volunteers. The Simple English Wikipedia (simple.wikipedia.org) initiative to make information more accessible contains over 60,000 articles in Simplified English. However, these are only a fraction of the 3.3 million articles in the main English Wikipedia and further, the simplified articles tend to be very short (often just the first paragraph). Our goals in this proposal are twofold. From a theoretical perspective we want to gain an understanding of the text revisions humans perform to simplify text, and learn rules for simplification from corpora. From an applied perspective, we want to implement a system for automatic text simplification that can perform the wide range of revisions that humans perform. We will make this system available to the Simple English Wikipedia community as a tool to expand the content available in simplified form."
	},
	{
		"grant":405,
		"ID": "EP/J019291/1",
		"Title": "COMPPACT: Compression of Video using Perceptually Optimised Parametric Coding Techniques",
		"PIID": "2212",
		"Scheme": "Standard Research",
		"StartDate": "08/08/2012",
		"EndDate": "07/08/2015",
		"Value": "547101",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "2212", "Role": "Principal Investigator"},
		{"ID": "21241", "Role": "Co Investigator"},
		{"ID": "-88056", "Role": "Co Investigator"},
		{"ID": "72832", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "BBC Research and Development"},
		{"Number": "1", "Name": "Fraunhofer Heinrich Hertz Institute"},
		{"Number": "2", "Name": "Watershed Media Centre"}
		],
		"Summary": "It is currently a very exciting and challenging time for video compression. The predicted growth in demand for bandwidth, especially for mobile services is driven largely by video applications and is probably greater now than it has ever been. There are four reasons for this:   (i) Recently introduced formats such as 3D and multiview, coupled with increasing dynamic range, spatial resolution and framerate, all require increased bit-rate to deliver improved immersion;  (ii) Video-based web traffic continues to grow and dominate the internet;  (iii) User expectations coninue to drive flexibility and quality, with a move from linear to non-linear delivery;  (iv) Finally the emergence of new services, in particular mobile delivery through 4G/LTE to smart phones. While advances in network and physical layer technologies will no doubt contribute to the solution, the role of video compression is also of key importance.  This research project is underpinned by the assumption that, in most cases, the target of video compression is to provide good subjective quality rather than to minimise the error between the original and coded pictures. It is thus possible to conceive of a compression scheme where an analysis/synthesis framework replaces the conventional energy minimisation approach. Such a scheme could offer substantially lower bitrates through reduced residual and motion vector coding.   The approach proposed will model scene content using combinations of waveform coding and texture replacement, using computer graphic models to replace target textures at the decoder. These not only offer the potential for dramatic improvements in performance, but they also provide an inherent content-related parameterisation which will be of use in classification and detection tasks as well as facilitating integration with CGI.   This  has the potential to create a new content-driven framework for video compression. In this context our aim is to shift the video coding paradigm from rate-distortion optimisation to rate-quality modelling, where region-based parameters are combined with perceptual quality metrics to inform and drive the coding and synthesis processes. However it is clear that a huge amount of research needs to be done in order to fully exploit the method's potential and to yield stable and efficient solutions. For example, mean square error is no longer a valid objective function or measure of quality, and new embedded perceptually driven quality metrics are essential. The choice of texture analysis and synthesis models are also important, as is the exploitation of long-term picture dependencies."
	},
	{
		"grant":406,
		"ID": "EP/J019399/1",
		"Title": "Efficiency and Complexity in Congestion Games",
		"PIID": "-298121",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/01/2013",
		"EndDate": "31/12/2014",
		"Value": "99571",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-298121", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Xerox Research Centre Europe"}
		],
		"Summary": "The goal of this project is to study ways to improve the performance of large scale networks, like the Internet, in the presence of selfish entities. This can only be achieved with a better understanding of these environments and their operational points, called Nash equilibria. A Nash equilibrium is a state in which no player improve their utility by changing to another strategy.  In this project we focus on the very fundamental class of congestion games and the related class of potential games.  In a congestion game, we are given a set of resources and each player selects a subset of them (e.g. a path in a network). Each resource has a cost function that depends on the load induced by the players that use it. Each player aspires to minimise the sum of the resources' costs in its strategy given the strategies chosen by the other players.  Such games are expressive enough to capture a number of otherwise unrelated applications - including routing and network design - yet structured enough to permit a useful theory.   In this project, we will push the frontiers of this theory even further. Moreover, in collaboration with XEROX, we will investigate applications of this theory to demand management in transportation systems. Two of these applications  are smart road toll and parking management systems.   Congestion games have attracted lots of research, but many fundamental problems are still open. We have identified three important directions in which we want to extent the current state-of-the-art. These are: (1) evaluation of Nash equilibria (2) computational complexity of Nash equilibria (3) approximation of optimal solutions"
	},
	{
		"grant":407,
		"ID": "EP/J019488/1",
		"Title": "Lodie,Web Scale Information Extraction via Linked Open Data",
		"PIID": "89709",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2012",
		"EndDate": "31/08/2015",
		"Value": "540483",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "89709", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Knowledge Now Ltd"},
		{"Number": "1", "Name": "Yahoo! Research"}
		],
		"Summary": "The World Wide Web provides access to tens of billions of pages. These pages contain information that is largely unstructured and only intended for human readability, however we are reliant on computers 'reading' these pages in order to find the information we need. The proposed research intends to develop technologies to radically improve the billions of searches which are performed every day by fulfilling the initial vision, by Tim Berners-Lee, for a Web where the webpage content is readable by both humans and machines.  Such a vision, disregarded during the initial development of the Web, has now come back in the form of the Web of Data, or Linked Open Data (LOD), where billions of pieces of information are linked together and made available for automated processing. There is however a lack of interconnection between the information in the webpages and that in LOD. A number of initiatives, like RDFa (supported by W3C) or Microformats (used by schema.org and supported by major search engines) are trying to enable machines to make sense of the information contained in human readable pages by providing the ability to annotate webpage content with links into LOD. While the current state of the art in Web Information Extraction (IE) relies on domain specific training data or generic extraction patterns, by leveraging LOD the proposed research aims to develop IE methodologies and technologies providing pervasive, user-driven, Web-scale information extraction where the target of the IE is defined by the user information needs and aimed at the billions of available Web documents covering an unlimited number of domains. In this research we aim to develop models and algorithms to create a continuum between LOD and the human readable Web. The approach will utilise wealth of facts available from LOD and the limited number of pages annotated with RDFa/Microformats to learn to connect unannotated webpage content to the LOD cloud. This will provide the reciprocal advantages of: (i) enabling the search of Web pages via the unambiguous LOD instances and concepts, and (ii) the extension of the LOD with the wealth of information available from webpage content. The key challenge is the development of efficient, Web-scale, semi-supervised, iterative learning methods able to use the initial 'seed' data and annotations, by generating models which exploit: (i) the local and global information regularities (e.g. structured information in tables, as well as pages and site-wide regularities); (ii) the redundancy (or repetition) of information; (iii) any ontological restrictions available in LOD. As the learning methods iterate from known interconnections to infer new connections they must cope with the massive amount of noise generated by the number and variety of documents, domains and facts available. In addition to publishing the research and its findings the IE methods developed will be tested on the task of extracting information relevant to schema.org (a task currently promoted by large search engines companies such as Google and Bing) as well as in international public evaluations. As part of such evaluations the project will generate at least one publicly available, Web-scale IE task (inclusive of corpora, linked resources, etc.) to enable comparison of research results by other researchers. The project aims to impact the fields of Natural Language Processing, Machine Learning, Information Retrieval and Web and Semantic Technologies by exploring the extraction of information in Web-scale, user-driven tasks. Success in the project will enable new ways of both creating/using the LOD and providing a paradigm shift in the way information can be retrieved from the Web; away from a reliance on keywords and towards the search and exploration of the concepts and meaning (semantics) embedded in those words. "
	},
	{
		"grant":408,
		"ID": "EP/J01950X/1",
		"Title": "RefNet: An interdisciplinary network focussing on reference",
		"PIID": "54662",
		"Scheme": "Network",
		"StartDate": "01/11/2012",
		"EndDate": "31/10/2015",
		"Value": "56388",
		"ResearchArea": "Human Communication in ICT",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Science",
		"OrgID": "0",
		"Investigators":[
		{"ID": "54662", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "RefNet proposes an EPSRC research network advancing the interdisciplinary study of reference by linking communities that have so far tended to work in near isolation from each other.   Reference is the process of making sure that a user/receiver can identify an entity - for example a person, thing, place, or event. Reference can be accomplished through language, but also through for example pointing, gaze, highlighting, and combinatons of the above. Reference can be considered the 'anchor' of communication, and as such it is crucial for practical applications,  from robotics and gaming to embodied agents, satellite navigation, and multimodal interfaces.   RefNet will foster collaboration between researchers in Computing Science and researchers in Psycholinguistics, and between researchers and practitioners in industry and the public sector. Through the study of reference, RefNet will build a wider interdisciplinary skills base for research on communication that will be of crucial importance at a time when computing science is becoming closely interlinked with other disciplines."
	},
	{
		"grant":409,
		"ID": "EP/J019534/1",
		"Title": "Green Brain: Computational Modelling of the Honeybee Brain",
		"PIID": "119263",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2013",
		"EndDate": "29/02/2016",
		"Value": "660561",
		"ResearchArea": "Robotics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "119263", "Role": "Principal Investigator"},
		{"ID": "109549", "Role": "Co Investigator"},
		{"ID": "21243", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Building intelligent machines that can perform complex cognitive tasks as well as or better than the human brain is a long-standing challenge of modern science. This quest has seen one of its highlights when IBM's Deep Blue chess computer beat the world champion Kasparov in 1997. Despite its superior performance in chess, this system was however in no way similar to or as powerful and versatile as a brain. More recently the Blue Brain initiative, also partially funded by IBM, set out to build a real-scale model of a cortical column of the human brain, moving us closer to the goal of eventually building an artificial brain that works like its biological counterpart.   In the Green Brain project we propose to build such an artificial brain, but of the smaller brain of the honeybee. We will work with the world-leading research group of Prof. Martin Giurfa in Toulouse, who are experts in all aspects of bee brain anatomy, physiology and bee cognition and behavior. Bees have a surprisingly large cognitive capacity including transfer of learned associations across sensory modalities, e.g. from smells to colors, and learning abstract concepts such as the categories of 'the same' and 'different'. At the same time their brains are much smaller, structured and (proportionally) much better researched than the complex human brain. It is also much easier to perform invasive manipulations to dissect how different parts of the bee brain function.  In the Green Brain project we will build detailed computer models of the two most important sensory systems of the bee, the senses of smell (olfactory system) and of sight (visual system). In doing so, we will incorporate existing data, models and principles and identify further how they give rise to the observed impressive cognitive abilities. We will then combine the sensory systems with learning models and models of sensory integration in close collaboration with the experts in the Giurfa lab to eventually build a full-scale model of the bee brain. This model will be implemented on state-of-the-art massively parallel graphical processing unit (GPU) based super-computers, a new technology spearheaded by NVIDIA Corporation who is supporting this project with GPU hardware donations. Using GPU computing will allow us to simulate our Green Brain model in real time, which will be essential for the final phase of the project when we will put the Green Brain to work as the brain of an autonomous flying robot. This is an important further advance over current work on brain models because it is becoming more and more clear that an essential aspect of brain function is that the brain is not acting in isolation but in constant interaction with the body and the environment. This concept of 'embodiment' and its consequences for cognition are important insights of modern cognitive science and will become equally important for modern neuroscience.   The outputs from the Green Brain project will have impacts in several academic areas. In the neurosciences it will advance the field of large scale brain models and our understanding of how information is processed in the sensory systems of bees. We will also contribute new tools for the use of modern GPU technology for artificial brains and employing them in bio-mimetic robotics. For the cognitive sciences we will contribute to the understanding of embodiment in biologically realistic model systems.  Beyond academia, the development of autonomous flying robots may have applications in environmental exploration, search and rescue and artificial pollination. Developing a better understanding of the mechanisms underlying cognition may ultimately translate into greater insights into human cognition and cognitive disorders. Finally, developing a better understanding of the honeybee may prove to be important in its own right as bees are a key pollinator in most ecologies and hence a 'keystone species' and vital for food security."
	},
	{
		"grant":410,
		"ID": "EP/J019577/1",
		"Title": "Game Semantics for Java Programs",
		"PIID": "127032",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2013",
		"EndDate": "30/03/2015",
		"Value": "209084",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "31",
		"Investigators":[
		{"ID": "127032", "Role": "Principal Investigator"},
		{"ID": "-182055", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": " Although software verification has made great strides in the last few decades, it is well understood that  there is still a significant gap between the need and the supply of verification methods and tools.  In particular, methods which enjoy high accuracy and can scale up to the size of realistic applications  are extremely rare.  In this situation, methods stemming from the field of denotational semantics, which studies mathematical  models of programs using elements of set theory and logic, seem naturally advantageous. The denotational  approach uniquely combines two desirable features: abstraction and compositionality. Abstraction means  that the analysis focusses only on the observable aspects of program behaviour and is therefore bound  to lead to concise models. On the other hand, compositionality allows for large verification tasks to be broken  into constituent parts, which can then be solved separately.  The aim of this project is to use game semantics in order to devise models, prototype methods and tools  for Java programs. Game semantics is a denotational theory describing computation as a game: a dialogue  between the program and its environment which follows their exchange of data during program execution.  Games strike a balance between abstract mathematical models and operational techniques. They precisely  capture observable program behaviour but also provide models which have concrete representations and  are therefore amenable to algorihmic reasoning and automation. As a result, games constitute a natural  candidate for a refined semantics on which to base program verification.  Research in game semantics has recently made an important breakthrough towards the modelling of realistic  programming languages, with the introduction of a novel strand of the theory called Nominal Game Semantics.  Nominal games pinpoint dynamic program behaviours involving the generation of new resources, such as  reference names, objects, channels, etc. Moreover, research in automata over infinite alphabets has produced  machines with the potential of standing as algorithmic counterparts to nominal games. These developments  place us in a powerful position to attack Java, a mainstream language featuring such dynamic generative effects.  Our goal is to make an important step in devising disciplined, systematic and automated methods that bear  the potential to address difficulties not tackled by the state of the art and eventually lead to the development  of better software.  "
	},
	{
		"grant":411,
		"ID": "EP/J019712/1",
		"Title": "Milner Symposium 2012",
		"PIID": "12545",
		"Scheme": "Standard Research",
		"StartDate": "16/04/2012",
		"EndDate": "15/01/2013",
		"Value": "22301",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "12545", "Role": "Principal Investigator"},
		{"ID": "34704", "Role": "Co Investigator"},
		{"ID": "32155", "Role": "Co Investigator"},
		{"ID": "42399", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The purpose of this research grant is to support a unique event where leading computer scientists from around the world come together at a memorial symposium held in Edinburgh in honour of one of the world's leading computer scientists, Robin Milner, who died in 2010.  The speakers at the symposium have strong connections to Robin Milner's work and his legacy.  They cross a broad spectrum of specialisms across artificial intelligence and computer science from automatic theorem proving to programming language design to the theory of concurrent and communicating systems to modelling and understanding complex pervasive and ubiquitous systems.  Many of the speakers are themselves very distinguished and highly regarded researchers in their own right but they are united in having been inspired and influenced by Robin Milner's ideas, his methods, and his work.  Because it is highly unusual to have such a distinguished programme of speakers, the symposium provides a unique opportunity for students, early career researchers, or more senior researchers to learn from, and benefit from, pioneering work of huge significance and impact.  In addition to speakers from research labs and universities, the symposium will also feature industrial participants who have followed a path which has been shaped by Robin Milner's lifelong belief that work in theoretical computer science should provide strong foundations for computing practice, software engineering, and applied practical work in the design, development and maintenance of computer and communication systems.  The industrial speakers at the Milner symposium are leading examples of how theory can be applied in practice and bring unexpected and surprising benefits.  Events such as these are a melting pot for ideas and they both inform and inspire.  Progress in science is made at events such as these where participants come together and discuss and exchange ideas.  Collaborations between previously unrelated researchers can be started at events such as these, leading to new research projects, new discoveries and new breakthroughs.  Different from a conventional conference, where the participants are members of one community and are focused on a particular approach to a particular problem, the Milner symposium crosses many boundaries between communities, as Robin Milner's work did.  In such a setting, serendipitous meetings can take place leading to fresh ideas and remarkable insights. "
	},
	{
		"grant":412,
		"ID": "EP/J019747/1",
		"Title": "Compact MMIC Terahertz Sources in the 0.1 - 1 THz Range",
		"PIID": "109541",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2013",
		"EndDate": "30/03/2016",
		"Value": "583091",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Engineering",
		"OrgID": "49",
		"Investigators":[
		{"ID": "109541", "Role": "Principal Investigator"},
		{"ID": "71158", "Role": "Co Investigator"},
		{"ID": "-258910", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project addresses the bottleneck of Terahertz Science and Technology, where the fabrication of room temperature, continuous wave, compact, tunable and powerful sources (at low cost, if possible) is the prime challenge. Terahertz (THz) radiation, whose frequency range lies between microwaves and infrared light in the electromagnetic spectrum, opens the possibility for a new imaging and spectroscopic technology with a broad range of applications, from medical diagnostic (without the damage produced by ionising radiation such as X-rays), industrial quality control or security-screening tools. Unfortunately terahertz technology suffers from some significant difficulties that requires research to overcome. Bright terahertz sources are difficult to make, so considerable effort is needed to improve what we have at the moment. The sources must be obtained at the limits of electronics from one side and optical systems from the other, resulting in a lack of efficient and practical radiation sources. This project is therefore dedicated to developing a compact high performance solid-state source. The potential for employing resonant tunnelling diodes (RTDs) to realise THz sources is well known and so are the circuit design challenges (unwanted oscillations and low output power) to achieve this task. This grant proposal seeks to develop this potential, by exploring and exploiting novel circuit concepts that allow the use of multiple (and optimally sized) RTDs in single oscillators. We aim to fabricate the RTD sources using the conventional microwave monolithic integrated circuit (MMIC) technology. By the conclusion of this project, we aim to demonstrate a simple single pixel THz imaging system. These developments should pave the way for adoption of this technology by British industry. "
	},
	{
		"grant":413,
		"ID": "EP/J020214/1",
		"Title": "Score!: Scalable and Complete Reasoning with Incomplete Ontology Reasoners",
		"PIID": "-186548",
		"Scheme": "Standard Research",
		"StartDate": "07/01/2013",
		"EndDate": "06/01/2016",
		"Value": "555708",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-186548", "Role": "Principal Investigator"},
		{"ID": "71179", "Role": "Co Investigator"},
		{"ID": "-192991", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Decisions in industry, government and health care increasingly depend on improved access to and processing of digital information. This has led to a pressing demand for more powerful and flexible information systems. New generation information systems will need to efficiently process large data sets, exploit machine-readable domain knowledge, and answer queries by taking into account both knowledge and data.  Ontology-based information systems (OISs) constitute a rapidly maturing technology with the potential to meet these requirements. An ontology provides a vocabulary of terms that are familiar to the user, together with axioms describing the meaning of those terms. OISs can exploit the rich domain knowledge in an ontology to provide a unified view of the data and enrich query answers with implicit information using an automated reasoner.  Several standards for ontology and query languages have been developed, including RDF, OWL, OWL 2, and SPARQL. OWL and its revision OWL 2 provide a powerful and flexible ontology modelling language that can capture features such as class hierarchies, incomplete information, negative information, and so on. OWL ontologies are being used in an increasing range of applications, and are becoming a core technology for accessing, gathering, and sharing knowledge and data.  Applications involving large amounts of data, however, still pose serious challenges to the applicability of OISs. Problems in the applicability of OISs typically originate from conflicting application requirements.  - Modelling complex application domains requires rich ontology languages. - Fine-grained access to information requires powerful query languages. - Answering queries over large data sets requires scalable reasoners. - Critical decisions that depend on access to information require query answers that are either complete, or where the incompleteness is well-understood.  Due to high worst-case complexity of the relevant reasoning problems, scalability is usually in conflict with the use of powerful ontology and query languages, and many applications give up completeness to achieve the desired scalability. As a result, existing OISs fail to meet one or more of these requirements: they support only weak ontology or query languages, they do not scale to the required volumes of data, or they do not provide guarantees as to the completeness of query answers.  Our goal in this project is to lay the foundations for a new generation of OISs that meet all the aforerementioned requirements, thus providing the ideal combination of expressive power, scalability and completeness.  To accomplish such an ambitious goal, we observe that the limitations imposed by the trade-offs between expressivity, scalability and completeness apply at the language level: that is, they involve worst-case complexity bounds for every ontology, query, and data set  expressed in given ontology, query and data modeling languages. The class of ontologies, queries and data sets that are relevant to a particular application is, however, much more restricted. For example, although application data is often unknown or frequently changing, the ontology itself is fixed at design time, or changes infrequently. As a result, a reasoner known to be incomplete in general for given query and ontology languages might yield the same results as a complete reasoner for the application at hand.  Identifying such cases is challenging, but it would have tremendous added value: applications could exploit scalable incomplete reasoners  while still enjoying completeness guarantees, thus achieving 'the best of both worlds'.  We believe that our main goal can be accomplished by designing OISs that are optimised for the ontologies, queries and data sets relevant to the application at hand.  Such OISs would maximise scalability while ensuring completeness of query answers, even for rich ontologies, large-scale data sets, and complex user queries."
	},
	{
		"grant":414,
		"ID": "EP/J020230/1",
		"Title": "Imperfect data: accuracy, impacts and extraction of meaningful information",
		"PIID": "103767",
		"Scheme": "Standard Research",
		"StartDate": "04/05/2012",
		"EndDate": "31/07/2013",
		"Value": "69249",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Geography",
		"OrgID": "104",
		"Investigators":[
		{"ID": "103767", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Meaningful information is a fundamental requirement for informed, logical and reasoned activity. Extracting meaningful information from data can, however, be a challenge, especially given problems that data may, amongst other things, be inaccurate, incomplete, and possibly contradictory as arise from a variety of sources of variable quality and trust level.   Data imperfections are a generic problem in information extraction and decision making and so the work is relevant in many disciplines. Imperfect data are, for example, evident in medical diagnosis (e.g. a patient's test results are typically only an imperfect indicator of a condition), in defining nature reserves for species conservation (e.g. the species distribution maps and models are often highly sensitive to 'absence' data - was the species actually present but not observed?) and in security and defence applications (e.g. sub-pixel target detection algorithms applied to surveillance imagery vary in performance and utility between environments). Some problems with imperfect data were recently highly apparent in relation to the response to the Haiti earthquake of 2010, especially in relation to damage mapping to inform relief activities. Vast amounts of well-intentioned assistance was provided by numerous professional and amateur bodies with unprecedented data rates but the volumes of data and the problems with them were a concerns. Key problems were that maps were inaccurate, inconsistent and sometimes contradictory. As such a major mapping challenges arises in how to work with such data. One key issue is the need for information on the accuracy of data sources and methods to help use imperfect data. This project seeks to contribute to this task. It aims to illustrate the impacts of using imperfect data, explore methods to characterise the quality of the data and methods to combine data sources to yield an enhanced product of known accuracy.  A range of methods will be used but the core focus is on the use of latent class modelling. This type of analysis is based on multiple observations or data from a variety of sources. The relationships between the observers/data sources are used to attempt to explain their quality and suggest how the data could be interpreted to yield information. The approach is a form of statistical modelling and is highly attractive for the specific research proposal because if a model can be formed that fits the observed data, then model's parameters define the accuracy of the data sources and its outputs can be used to form new products of known accuracy. As such the modelling analysis may add value to data by indicating its quality and combining it usefully for extraction of information.  As the problems of imperfect data are generic the proposal has broad potential impacts. For the specific DaISy call there are clear impacts in relation to security and defence. For example methods that enable rapid and qualified information to be derived from sources of variable accuracy, completeness and trust level will increase effectiveness and the quality of decision making. Additionally as a model based approach it removes/reduces the need for reference data to be acquired for validation which could otherwise require deployment of personnel to dangerous locations and so of considerable benefit to health and well-being. "
	},
	{
		"grant":415,
		"ID": "EP/J020257/1",
		"Title": "FISH: Fast Semantic Nearest Neighbour Search",
		"PIID": "80116",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2012",
		"EndDate": "30/09/2013",
		"Value": "95625",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "80116", "Role": "Principal Investigator"},
		{"ID": "97176", "Role": "Co Investigator"},
		{"ID": "91035", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal addresses Theme 1 of the Data Intensive Systems (DaISy) Call 'Extracting meaningful information: Deriving meaning from large, heterogeneous, incomplete, contradictory, noisy and dispersed data sets with many different forms and formats (e.g. text, image and sound files)'. The project will research and develop novel fast semantic nearest neighbour search (FISH) data structure, algorithm and software utilities for rapidly discovering semantically similar neighbours of complex data objects and automatically assigning semantic class labels to them. It will use large scale, high dimensional, heterogeneous, incomplete, and noisy internet image labelling datasets as Case Study for technology development and evaluation. The FISH solutions and utilities can be directly applied to automatically label security surveillance videos and defence reconnaissance imageries to extract semantic meanings to facilitate security and defence intelligence gathering and analysis."
	},
	{
		"grant":416,
		"ID": "EP/J020338/1",
		"Title": "Information Cognition (IC): Enabling the Human Cog in the Information Retrieval Machine",
		"PIID": "-401519",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "31/08/2013",
		"Value": "117053",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Design Manufacture and Engineering Man",
		"OrgID": "48",
		"Investigators":[
		{"ID": "-401519", "Role": "Principal Investigator"},
		{"ID": "3366", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "SUMMARY  This research programme will investigate the feasibility of using visual representations for the secure navigation and search of large, complex, multimedia data sets. It draws upon prior research that shows the human visual system has a powerful ability to recognise and classify objects in 3D environments. To do this the project will create an experimental platform that combines human cognitive abilities with recent advances in design information management, computer graphics and data mining. The resulting system will support the systematic study of how advanced visual interfaces impact on a user's ability to both find individual items and identify patterns, or oddities, within subsets of the data.   The veracity of the project's experimental methodology is ensured through close collaboration with an international research centre supported by a consortium of leading defence contractors (ie Rolls Royce, Boeing etc). This partnership will ensure the project is focused on the challenges faced by the defence industry and can assess its outputs using a realistic bench mark environment. "
	},
	{
		"grant":417,
		"ID": "EP/J020354/1",
		"Title": "Sticky Policy Based Open Source Security APIs for the Cloud",
		"PIID": "2698",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "126939",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "27",
		"Investigators":[
		{"ID": "2698", "Role": "Principal Investigator"},
		{"ID": "-388046", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cisco Systems Inc"},
		{"Number": "1", "Name": "Influential Software"}
		],
		"Summary": "The Internet and telephone are successful because they use open protocols and open interfaces, allowing users to communicate, innovate and share at will. We propose to facilitate this process in cloud computing, by developing a set of open security services, protocols and interfaces (APIs) that will allow cloud resource owners to be able to specify their policies for fine grained access control to their cloud resources, and have these enforced everywhere at all times, regardless of the subsequent location or data processing that has ensued. The ability to securely share data with anyone, anywhere, at any time, will facilitate spontaneous collaborations and ensure confidence in collaborative working. This will be achieved by using 'sticky policies', delegation of authority, federated access and attribute based access controls. Sticky policies are policies which are cryptographically linked or 'stuck' to the data and meta-data they control, so that access to the data is only granted if the policy is honoured.  In order to cater for Internet scale cloud usage, federated access and attribute based access controls are needed. Federated access allows users to identify themselves to a cloud service using their existing credentials, without having to first obtain new ones from the cloud service itself. Attribute based access controls allows access to be specified based on a user's identity attributes rather than simply an identifier, which is typically used today. In order to achieve Internet scale in identifying users and data resources, an ontology is needed that will classify both the data and the users who wish to access it. The authorities who issue identity attributes will also need to be classified. The characteristics of any particular set of data will be held in meta-data that describes or identifies the data, and conforms to the ontology. The meta-data itself will be stuck to the data in a similar way to the sticky policy.  When data is merged or fused with other data, or is split, filtered or reduced, then its meta-data will need to change accordingly, in order to describe the new data. Similarly the sticky policy that controls access to the new data will need to be derived from the original sticky policy(ies).  This project will develop a new algebra and algorithms for deriving the new sticky policy from the old, using the ontology and meta-data as a guide.  (Note that this project will not be performing the actual data merging or splitting, but simply assumes that trustworthy services are available to do this.)  The protocols and APIs specified in this project will be standardised through an organisation already well versed in cloud APIs, such as the Open Grid Forum or OASIS.  In order to ensure the widest take up of the services and APIs specified in this project, pilot implementations will be developed in Python and distributed as part of the OpenStack suite of software. OpenStack is a community project involving over 135 organisations, ranging from multi-nationals such as HP, Cisco and Intel, to specialist SMEs such as Cloudscaling. This project proposes to harness the energies of the OpenStack community by acting in a leading role to facilitate others in contributing to the development effort.  "
	},
	{
		"grant":418,
		"ID": "EP/J020370/1",
		"Title": "CloudFilter: Practical Confinement of Sensitive Data Across Clouds",
		"PIID": "-128335",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "30/04/2013",
		"Value": "135209",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-128335", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Nexor Ltd"}
		],
		"Summary": "Cloud computing aims to revolutionise traditional ways of service delivery. It enables companies, research institutions and government organisations to consolidate services in a shared ICT infrastructure supported by cloud providers. This reduces ownership and management costs, allows services to scale on-demand and improves energy efficiency. Security considerations, however, are a practical obstacle for the adoption of cloud computing. Cloud providers consolidate data from multiple services, which may result in wide-spread data disclosure when their security is compromised.  Strong cloud security is hard to achieve because it requires that the cloud platform cannot be compromised by hosted applications and that applications belonging to different cloud tenants are isolated to prevent data leakage. It is even harder for federated clouds, i.e. when a cloud provider uses another provider for some of its services. This is common in a Software-as-a-Service (SaaS) model, in which a provider offers a high-level service that can be reused by other providers. Both clients and cloud providers have an incentive to control the propagation of sensitive data. Clients are often legally responsible for data protection, and cloud providers want to prevent hosting sensitive data to avoid liability claims after security incidents.  The CloudFilter project explores novel methods for exercising control over sensitive data propagation across multiple cloud providers. The targeted outcome is a practical solution that allows clients and cloud providers to control the sensitivity of data that is transferred across their systems and to prevent user actions that would violate data dissemination policies. Our key idea is to provide application-level proxies that transparently monitor data propagation from clients to cloud providers and between cloud providers. These proxies employ a data labelling scheme inspired by decentralised information flow control (DIFC) models, in which security classes express the sensitivity of transfered data. When crossing domain boundaries, labels are attached to data automatically based on data dissemination policies. Proxies verify labels according to domain policies to detect and prevent unauthorised data propagation between cloud domain domains. "
	},
	{
		"grant":419,
		"ID": "EP/J020389/1",
		"Title": "Game Theoretic Privacy-Preserving Collaborative Data Mining",
		"PIID": "-203954",
		"Scheme": "Standard Research",
		"StartDate": "21/05/2012",
		"EndDate": "20/05/2013",
		"Value": "120189",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90",
		"Investigators":[
		{"ID": "-203954", "Role": "Principal Investigator"},
		{"ID": "36938", "Role": "Co Investigator"},
		{"ID": "69275", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal focuses on the problem setting where coalition parties, each owning a large set of data, desire to discover new knowledge when they collaborate to jointly process all the datasets; while ensuring that each individual dataset is not revealed to the other parties.   Solutions to this problem are key enablers for ensuring smooth cooperation among parties who do not necessarily trust each other fully.  Example situations that reflect this need include coalition forces on military or peace-keeping missions, nations joining forces to detect and prevent terrorism activities, while not willing to reveal their actual intelligence data on national security, and organisations collaboratively analysing consumer behaviour while keeping their customers' profiles private.    The proposed research addresses challenges within the three themes of the DaISy Call, notably those aimed at secure and privacy-preserving collaborative extraction of meaning from data intensive systems comprising different parts of data owned by adaptively changing coalition partners.  We aim to develop novel techniques with guarantees of privacy that will perform data mining even when data are in encrypted form, including the specific tasks of clustering, dependency modelling, classification, regression, and association. The behaviour of such coalition parties involved in performing joint data mining will be analysed using a game theoretic framework, and various innovative collaborative data mining techniques will be developed using this framework while ensuring that privacy is preserved, even when some coalition members may collude.  Our techniques will also be designed to be adaptive to and efficient when coalition membership changes. "
	},
	{
		"grant":420,
		"ID": "EP/J020427/1",
		"Title": "Real-Time Detection of Violence and Extremism from Social Media",
		"PIID": "-196798",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "113921",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Knowledge Media Institute",
		"OrgID": "95",
		"Investigators":[
		{"ID": "-196798", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The explosive use of social media tools in recent years has turned them into a double-edged sword. On one hand, social media is viewed as a positive factor in Middle East revolutions. On the other hand, violence events such as the UK riots occurred in August this year appeared to be driven by the use of social media. The proposed project represents a timely development of intelligent systems for addressing the emerging defense and security issues arising from social media.  The proposed research describes a new approach to detecting trends of violent radicalization and extremism from social media. In particular, it proposes a Bayesian modeling approach which detects violence contents from social media without the use of any labelled data. Words indicating violence, anger, hate, racism, etc. are naturally incorporated as prior knowledge into the model learning process. Efficient online parameter updating and parallel data processing procedures will be investigated. This proposal falls into the area of 'Extracting meaningful information' listed in the original call for proposals. It particularly aims to tackle the technical challenge of real-time processing of large-scale social media data for early detection of violent extremism from text. The results of the research are potentially very important to society as they aim to enable the deployment of the forces of law to prevent violent events."
	},
	{
		"grant":421,
		"ID": "EP/J020435/1",
		"Title": "Integrated Visualization of Multiple Data Streams for Command Control Interfaces (CCI)",
		"PIID": "22018",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "110513",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Oxford e-Research Centre",
		"OrgID": "106",
		"Investigators":[
		{"ID": "22018", "Role": "Principal Investigator"},
		{"ID": "54871", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Visualization is generic enabling technology, but most visualization systems have not been designed to accommodate the complexity of dynamic input data streams in many real world applications. For example, in a typical traffic control centre, there are many different data streams, such as video streams from traffic cameras, partial traffic information collected from GPS devices, accident reports via radio and emergency services, satellite images (e.g., Google earth), traditional maps, and other related information such as concerts and sports events. However, these data streams are usually displayed separately. Traffic controllers have to gather information from different displays simultaneously and build a mental image about the global situation.  Although there are geographical information systems that can display multiple maps using a classic technique called map overlay, the use of such a mechanism in applications such as traffic control centre, is rare. This is partly because that geographical information systems typically rely on a small number of visual channels, such as colour lines, patches, and text labels. It is rather easy for different maps to use similar visual mappings, creating conflicts in the combined visualization. Furthermore, temporal data (such as videos and time series) are becoming common in many applications. The depiction of such data is typically in the form of animation, tracking lines and heatmap (using colour to show the level of changes). Tracking lines and heatmaps are typically in conflict with the underlying geographic maps and satellite images respectively, while animation suffers from a number of perceptual and cognitive shortcomings and is not suitable for continuous monitoring and objective evaluation. For example, watching animation requires time and full attention. If a traffic controller is watching a previously-recorded event unfolding, he/she cannot pay attention to the current situation. So the challenge is how can one visualize 'time' (i.e., temporal data) without using 'time' (i.e., video or animation).  In this project, we will develop two pieces of novel techniques. We will increase the number and types of the visual channels that can be used in multi-stream visualization, while develop a set of conflict diagnostic facilities, which algorithmically measure the quality deficiency due to conflict visual mappings associated with different layers. The  diagnostic facilities can give warnings to the users as the potential risks of confusion and misunderstanding caused of conflict use of visual channels. In addition, we will design a set of new forms of temporal visualization that enable uses to visualize 'time' without using 'time'.  We will develop a demo system, where different data streams can be plugged-in and play. The system will be supported by a dashboard, running on a tablet computer such as iPad. Through the dashboard, the users can activate and deactivate individual data streams, select appropriate visual channels, receive advice from the conflict diagnostic facilities, control the appearance of each layers, and manger the order of different layers. We will evaluate the demo system in two applications, risk visualization in resilience and archeological data visualization."
	},
	{
		"grant":422,
		"ID": "EP/J020443/1",
		"Title": "DIVA: Data Intensive Visual Analytics - Provenance and Uncertainty in Human Terrain Analysis",
		"PIID": "-183749",
		"Scheme": "Standard Research",
		"StartDate": "31/05/2012",
		"EndDate": "30/05/2013",
		"Value": "172333",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Information Science",
		"OrgID": "83",
		"Investigators":[
		{"ID": "-183749", "Role": "Principal Investigator"},
		{"ID": "-122075", "Role": "Co Investigator"},
		{"ID": "-303015", "Role": "Co Investigator"},
		{"ID": "-87130", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Data Intensive Visual Analytics can help address the data deluge by helping decision makers to rapidly reach informed and effective decisions in a range of situations.  This exploratory project will apply DIVA to defence and security applications in close collaboration with DSTL. It will investigate visual methods for effectively utilising the kinds of dynamic and uncertain data that are emerging from multiple and frequently conflicting sources. Methods will be developed to store, communicate and use metadata about (potentially conflicting, uncertain and messy) data origins, quality and analytical process. They will be transferable and apply at operational and strategic levels.  We draw together a team of UK academics with complimentary expertise. Contributors from Middlesex University and City University London have growing international reputations for developing innovative and applied visual analytics solutions and the theoretical work that supports this activity. Contributors from Loughborough University offer experience of information management and analysis in real time, harsh environments and the military context. The team will work closely to establish and evaluate the potential for DIVA in the area of Human Terrain Analysis.  The programme of work is designed to ensure close engagement between academics and DSTL colleagues. Short bursts of concerted activity focusing around a series of participatory design workshops will result in rapid development and evaluation. These intense periods of coordinated co-located activity will stimulate subsequent reflection and respond to feedback involving DSTL in an iterative process. A continuous bridging presence over a 12 month elapsed period (one researcher working at two sites) will support and consolidate this work. These efforts will address critical research issues faced by the emerging academic VA community:    *How can we best support analysts with information about data uncertainty and provenance? These factors underlie analytic approaches in data intensive systems yet many issues remain unresolved.   *How can we capture, annotate and explain the analytic process?  Doing so will enable us to reproduce the analytic process and support communication and collaborative analysis.    *How do VA approaches apply in critical applications areas? Close collaboration with DSTL will ensure that academic developments are grounded in and informed by an applications domain that is vital to national security.  The planned activity will produce schemas, methods and prototypes that address these questions, support analytical work and demonstrate DIVA potential in the military context.  The results are likely to have application impact across MOD and in wider disciplines to which VA is being increasingly applied, including significant data intensive areas in science, industry and government.    Findings will be communicated widely through national and international academic conferences, social media, press releases and at DSTL networking events. Software and functionality developed will be made available through a Creative Commons licence. Along with the knowledge derived through the planned research, this will be used by the UK Visual Analytics Community.  The project offers significant value, using existing skills, equipment and technology, and has low start-up costs. No recruitment is necessary with all participants employed in dynamic and successful research groups at the three participating institutions: City University London (lead), Middlesex University and Loughborough University. The programme of activity involves 24 months of research time over 12 months elapsed time and fits in well with the schedules and workloads of world class researchers operating in the international arena. All are committed to the work plan, which will contribute to institutional objectives in all cases and is supported by the US National Visual Analytics Centre. "
	},
	{
		"grant":423,
		"ID": "EP/J020478/1",
		"Title": "Rogue Virtual Machine Identification in DaISy Clouds",
		"PIID": "55263",
		"Scheme": "Standard Research",
		"StartDate": "15/05/2012",
		"EndDate": "14/05/2013",
		"Value": "119691",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "55263", "Role": "Principal Investigator"},
		{"ID": "6728", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our work in this proposal focuses primarily on the safe and secure cloud computing challenge of the EPSRC DaISy call. In addition, it also addresses closely issues relevant to the extracting meaningful information challenge, specifically extracting meaning from large-scale monitoring information collected in a cloud computing environment, and the ensuring confidence in collaborative working challenge by developing meta-data and methods that enable users to monitor how their digital assets are being used in shared environments.   The proposal builds on the investigators' expertise in building secure cloud computing systems (especially the IC-Cloud system), developing large-scale data analysis systems and developing algorithms and methods for the security analysis of program behaviour to develop develop and evaluate novel methods to detect subtle attacks by adversaries who have already gained access to a VM within a secure cloud system.  Our general methodology for this 1-year project is based on 1) Building on the existing IC-Cloud platform as a test-bed our research. The use of the in-house infrastructure based on the popular XEN Hypervisor enables us to rapidly develop and evaluate monitoring tools, to develop and test different attack scenarios and collect the log data from the real applications. 2) Building on our in-house repertoire of data mining and analysis tools developed over the years for classification, clustering and association analysis and on our recent expertise developed in real-time data mining methods and Bayesian analysis frameworks for modeling and analysis anomalies in large scale sensor data for environmental and security applications. 3)  Close collaboration with partners and collaborators of the Institute and Security Science and Technology to receive ongoing feedback on our methodology, results and methods as we develop them.     "
	},
	{
		"grant":424,
		"ID": "EP/J020494/1",
		"Title": "Trusted Dynamic Coalitions",
		"PIID": "-156057",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "30/10/2013",
		"Value": "99038",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "-156057", "Role": "Principal Investigator"},
		{"ID": "34805", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Businesses are finding more and more ways to take advantage of the internet, and can they can reduce costs and increase their sales by going online. Communication across continents is easier than ever before, and so  they can also use the internet to work together. Smaller specialist companies can now compete with large multinationals by  working together to distribute sub-tasks of the work to companies specialist in that sub-task. They have then created a 'virtual organisation' -- an increasingly common mode of working.   Virtual organisations also form when organisations, military forces and bystanders put aside differences and work together to alleviate the consequences of a major disaster.  One of the characteristics of these virtual organisations is that they are highly fluid -- as the situation on the ground changes, partners and the trust between them changes, structures change and even the purpose of the virtual organisation can change.  In the military sphere, virtual organisations such as these are known as dynamic coalitions.  A problem virtual organisations face is that the members often do not trust one another.  They may be competitors for the most part, and only working together on a single job, and so there will naturally be information that they are not prepared to disclose to potential competitors.   In a virtual organisation, partners need to take action based on the information they receive from other partners. This presents a problem, because the information may not be trustworthy.  To increase their confidence that they are taking the right action, there are some things a partner can do. They can ask the sender for more details about the information itself, such as its source or age (known as the provenance of the information), but these details can often give away organisation or government secrets, and partners are naturally not keen to release them.   They could try to verify the information themselves or via a third party, but these activities take time and in situations such as disaster response, speed is an important factor.  We wish to enable a partner to increase their confidence that they are taking the right action in a virtual organisation.  We will do this by providing methods and tools that help partners to choose the virtual organisation policies so that the desire for provenance data and the desire for secrecy are held in balance.    We will look particularly at the provenance acquisition policy, which states what provenance information is associated with communications within the virtual organisation, and the provenance use policy, which states the action that a partner will take on receipt of information with a certain provenance associated to it.  Choosing the right combination of policies here is critical to the success of a virtual organisation, and this choice must be made and re-made as the partners, trust relationships and goals of the virtual organisation change.  We will build mathematical models of the policies and the virtual organisation itself, and a tool to allow partners to interact with these models (without their needing to understand the models) to predict the impact of changes to these policies in terms both of the secrecy requirements within the virtual organisation and their ability to have confidence in the information they receive. This will help in the choice of provenance policies both at the start of a virtual organisation and throughout its lifetime.   "
	},
	{
		"grant":425,
		"ID": "EP/J020524/1",
		"Title": "Interpreting and integrating mismatched data on the fly",
		"PIID": "2244",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "58667",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "2244", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This project is concerned with the problem of interpreting information and requests which are received during communication: for example, during an emergency response event.  If the data sources of the sender of the information/request and the data source of the receiver are not fully compatible (which is highly likely even within an organisation and almost certain between different organisations) then the received information or request will not be understood by the receiver.  Our matching system is designed to explore the receiver's data source to find one or more terms which appear to be similar to the received term, in order to allow the receiver to interpret this term with respect to its own data.  The implications of this match (for example, that a more general term is being used, that certain attributes of a term are being ignored or introduced, and so on) are explained to a user representing the receiver organisation, who can then approve the proposed match, and interaction will proceed as if the two different terms were equivalent.  The matching system is local to every participant that wishes to make use of it and so it is only able to access the data of the organisation which is using it.  This therefore avoids privacy and security issues by ensuring that only data that has been approved for sharing in the current context can be accessed by others.  The aims of the project are: - to investigate the kinds of mismatches that have been encountered in real emergency response scenarios and within real data sources, which will be provided by our collaborators (in Dstl, the Met Office, Scottish Resilience and Strathclyde Fire&Rescue). - to implement a system which will apply techniques based on existing matching techniques to these data sources, to enable appropriate interpretation of unknown data, and to provide feedback to users so that they can approve these matches (or in situations where the consequences of errors is low, these can proceed automatically without user approval). - to evaluate the system against real data, with feedback from expert users as to how useful and appropriate the suggested matches are.  This project will be exploratory and is designed to evaluate the potential of the approach.  There are several factors which should be addressed before the system is ready to be used in the field that are outside the scope of this project, specifically:  - Carry out further analysis of data, and of mismatches that have actually occurred in emergency response situations.  Performing this will be a key feature of this project, but this can only be an initial study;  - Design and implement new matching techniques to address any possible mismatches which are not addressed by those currently implemented.  Depending on the complexity of the data, it is not anticipated that it will be possible to cover every possible mismatch within this project;  - Provide a carefully designed web interface for user interaction with the system.  This would require specialist design advice and will not be attempted during the project; instead, a simple interface will be provided.  The key output of this project will be an understanding of the potential of this approach, based on the evaluation of the implementation, and a roadmap for how to develop this prototype work into a system which can be used in the field. "
	},
	{
		"grant":426,
		"ID": "EP/J020532/1",
		"Title": "EMOTIVE - Extracting the Meaning Of Terse Information in a geo-Visualisation of Emotion",
		"PIID": "109610",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "88552",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Information Science",
		"OrgID": "90",
		"Investigators":[
		{"ID": "109610", "Role": "Principal Investigator"},
		{"ID": "-9427", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The ability for ordinary people to express and exchange their opinions and feelings has increased beyond all expectations in the past ten years of internet expansion and availability. To the military and national security agencies this has provided both opportunities and challenges. Opportunities have emerged in the sense of readily available awareness of discontent and oppositional movements and initiatives. Recent urban disturbances have illustrated the key role played by social networks in the fast-moving events of Summer 2011. The challenges have escalated due to the sheer number of sources of social interaction and public communication media. This research addresses some of these issues in a bold initiative to combine well established and considered science with the increasingly familiar tools of Web 2.0.  Four of the most popular sources of the public exchange of ideas (email, social networks, such as Facebook, microblogs, such as Twitter and comments to newspaper editorials and high-profile stories) will be selectively monitored. Sensitive words and phrases which may be of concern to the military and national security agencies will be extracted by extending a Natural Language Processing technique already developed for email by the Principal Investigator. The team will develop an ontology (a rule-based linguistic database) in which the extracted words and phrases will be semantically filtered and restricted to a manageable set of agreed terms. An example of how the ontology will work can be illustrated by suggesting the number of ways the word 'looting' might be expressed in, for example, established vocabulary (raiding, pillaging, ransacking, etc.) as well as in urban and regional street language and text speak ( doin' over, scamming, etc.). The ontology will be trained to recognise the words and phrases, make semantic links between them and deliver one or more accepted descriptors to the analysts.   EMOTIVE will monitor the traffic of sensitive words and phrases filtered through the ontology when applied to specific incidents, individuals and groups. Increased activity will be indicated by frequency of occurrence or severity, which will be presented through a concept cloud which uses the size of words as a metaphor for frequency and hence importance.  Further to this, a second ontology will be created in which words and phrases that express emotion will be harnessed and this ontology will process the emotionally charged words and phrases extracted from the four sources described above in a similar way to the first  The output of both ontologies will be linked, so that the monitoring analyst will be presented with a colour-coded indication of the strength of emotion attached to the language-based terms. The final feature in Emotive will be a geo interface to point to the location of the emotionally charged traffic. This interface will be refreshed every 60 seconds with the effect of helping to identify sensitive hot spots of communication and activities. Outputs from the system consisting of effectively presented new knowledge will enable defence and national security agencies both to predict and monitor selected events as they develop and will assist in the formulation of policy.  It can be argued that the general public will be direct beneficiaries of this research in that the defence and national security agencies who act as guardians of public safety and order will be further equipped by this tool to identify, evaluate and ultimately safeguard the public from potentially harmful events.  Defence and national agencies will already be experienced at monitoring these data sources but this tool adds an extra filter of analysis, it will work in almost real time, will amalgamate data from several sources if desired and will provide harmonised output."
	},
	{
		"grant":427,
		"ID": "EP/J020540/1",
		"Title": "Accelerated Real-Time Information Extraction System (ARIES)",
		"PIID": "66486",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2013",
		"Value": "252523",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10",
		"Investigators":[
		{"ID": "66486", "Role": "Principal Investigator"},
		{"ID": "-403838", "Role": "Co Investigator"},
		{"ID": "52947", "Role": "Co Investigator"},
		{"ID": "113229", "Role": "Co Investigator"},
		{"ID": "-403839", "Role": "Co Investigator"},
		{"ID": "-269587", "Role": "Co Investigator"},
		{"ID": "90240", "Role": "Co Investigator"},
		{"ID": "-403907", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Technological advances in CMOS semiconductor technology paved the way for the digital revolution. As predicted by Moore, silicon integration capability has been doubling every 18 months over the past four decades, providing the foundation for low-cost computing and memory technology.  The digitisation of information and communication technologies sparked a number of innovations revolutionising the way we compute and communicate. Ubiquitous high-bandwidth communication, enabled by WiFi and 3G/4G technologies, facilitates on-demand access to a vast amount of application and location specific information including multimedia and broadcast content, video and voice communications, email and SMS/MMS. Furthermore, it has enabled on-demand access to personalised storage and computing resources, providing the foundation for the development of cloud computing infrastructures and a wide range of online web-based services and applications.  With the decreasing cost of communication and storage the Internet has also become the global communication infrastructure for a wide range of autonomous sensor technologies, referred to as the 'Internet of things'. Key application areas include monitoring/surveillance, smart grid, smart homes and smart cities.   Monitoring internet traffic and mining meaningful information from both the online traffic and the stored information has emerged as essential for many critical applications and services. For example resource management, market intelligence, physical and cybercrime investigations and forensics, cyber space policing, situation awareness and the monitoring of malicious behaviour for criminal and terrorist intent. As the scale, diversity and distributed nature of current and emerging data assets increases and as data becomes ever more ubiquitous and critical to decision making, effective real-time mining of useful information becomes essential.   Considering the exponential increase of internet traffic and stored data, traditional software based approaches have become inadequate and unsustainable. Performance gain achieved due to Moore's law does not keep up with the required computing bandwidth of current and near future generated data assets. Internet traffic bandwidth is doubling every 12 months while the emerging content diversity is significantly increasing mining complexity. As the enterprise becomes more data centric, with a significant increase in data assets within the public and private cloud, traditional scaling by increasing the number of computing resources can no longer be sustained due to cost and power dissipation.  Most data mining algorithms are derived by the software community and are optimised for data structures for platforms based upon the Von-Neumann architecture. An effective solution now requires a paradigm shift in the way we process data and also how we extract meaningful information from a large amount of distributed, constantly changing data that is partially stored or in-transit. "
	},
	{
		"grant":428,
		"ID": "EP/J020567/1",
		"Title": "Algorithms for Data Simplicity",
		"PIID": "-178077",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "39695",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Mathematics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-178077", "Role": "Principal Investigator"},
		{"ID": "-302138", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Most of the information we encounter on a day to day basis has some inherent simplicity, from music having a limited number of dominant frequencies at any given time and natural images being composed of smooth regions separated by relatively few abrupt edges.  This inherent simplicity can be leveraged to allow extraction of meaningful information from the vast data and for more efficient information acquisition.  For example, consider consumer grade mega-pixel digital cameras.  The camera has a carefully constructed array of millions of photodiodes, but as soon as the image is taken the camera compresses the data to a much smaller JPG format.  This compression is crucial to allow for efficient storage and transmission of the image, but it is wasteful to have acquired millions of pixels and then to immediately compress and remove the measured image.  Compressed sensing and matrix completion are techniques by which the camera can measure the image at a rate proportional to the size of the final compressed image, dramatically increasing the efficiency of the camera.  Taking advantage of this technology requires the design of computationally efficient algorithms for the recovery of the image from the compressed measurements.  This proposal concerns the design, analysis, and application of fast algorithms able to act on large data sets."
	},
	{
		"grant":429,
		"ID": "EP/J020567/2",
		"Title": "Algorithms for Data Simplicity",
		"PIID": "-178077",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2012",
		"EndDate": "31/05/2013",
		"Value": "31092",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Mathematical Institute",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-178077", "Role": "Principal Investigator"},
		{"ID": "-302138", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Most of the information we encounter on a day to day basis has some inherent simplicity, from music having a limited number of dominant frequencies at any given time and natural images being composed of smooth regions separated by relatively few abrupt edges.  This inherent simplicity can be leveraged to allow extraction of meaningful information from the vast data and for more efficient information acquisition.  For example, consider consumer grade mega-pixel digital cameras.  The camera has a carefully constructed array of millions of photodiodes, but as soon as the image is taken the camera compresses the data to a much smaller JPG format.  This compression is crucial to allow for efficient storage and transmission of the image, but it is wasteful to have acquired millions of pixels and then to immediately compress and remove the measured image.  Compressed sensing and matrix completion are techniques by which the camera can measure the image at a rate proportional to the size of the final compressed image, dramatically increasing the efficiency of the camera.  Taking advantage of this technology requires the design of computationally efficient algorithms for the recovery of the image from the compressed measurements.  This proposal concerns the design, analysis, and application of fast algorithms able to act on large data sets."
	},
	{
		"grant":430,
		"ID": "EP/J020583/1",
		"Title": "RAnDMS (Real time Analysis of Digital Media Streams)",
		"PIID": "89709",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "31/08/2013",
		"Value": "208723",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "89709", "Role": "Principal Investigator"},
		{"ID": "-128498", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "RAnDMS will study, implement and evaluate Real-time Data and Visual Analytic techniques to enable intelligence agencies, the MoD, the police and emergency responders to monitor and make sense of local, regional and global events using web-scale data from social and traditional media streams. The intelligence gathering task will be defined as identifying, correlating, integrating and presenting data and information, in order to understand situations as they arise. Current technology does not provide efficient and effective solutions, as it mainly focuses on detecting trends in the use of keywords and tags. While this is able to spot overall patterns in the data, it just enables the retrieval of relevant documents, without any correlation and integration of the contained information. Moreover, information concerning local situations and events, which may only be discussed within a handful of documents, is ignored. Within RAnDMS data analytics will focus on enabling the capture of information from media streams; illuminating situations at all levels, from global to local. This information will support decision making for the intelligence community, which is expected to increase their ability to monitor events and situations relevant to homeland security and to peace-keeping efforts. The scientific challenge is that data and information in these streams are: (i) high in volume, and constantly increasing, (ii) often duplicated, incomplete, imprecise and incorrect; (iii) written in informal style (i.e. short, unedited and conversational); and (iv) generally concerning the short-term zeitgeist. These characteristics make analysis very hard, especially when considering that major requirements of the intelligence community are that (i) documents must be processed in real-time and (ii) the relevant information may be in the long-tail of the distribution, i.e. it may be mentioned very infrequently.  We will provide highly efficient and effective technologies able to associate each document with its context. A documents context is provided by four dimensions: (who) the author of the document, (when) the time it was sent, (where) the location referred to in the document and (what) other documents with similar content. This information is either provided by the media stream or extracted from the document's content using efficient statistical text-mining techniques. By interpreting documents in terms of these four dimensions we enable: (i) the detection of events, i.e. documents and their content (what) are clustered around a time and place; (ii) the profiling of authors from the content (what and where) of the documents they have created; and (iii) determine information that is missing or ambiguous in document, using information present in the documents within their context. Visual analytics will facilitate the exploration of the information by providing multiple views; enabling focused investigation and trend visualisations across the four dimensions. We will devise methods to (i) suggest the right level of detail (granularity) for the user focus in rapidly changing environments; (ii) alert users to any significant development outside of their current viewpoint; and (iii) enable users to understand how the current state of affairs came into being by browsing along the all information along the time dimension. Methods will en able to see through the irrelevant banter (noise) that often surround events in social media and go directly to the relevant information that can be hidden in the long tail of the distribution.  RAnDMS will be tested on the task of supporting intelligence operators during relevant events happening during 2012/13. We will publish the research and its findings in international journal and conferences. Subject to MoD agreement, we will also create public research resources by generating one publicly available task (inclusive of corpora, resources, etc.) to enable comparison of research results by other researchers.  "
	},
	{
		"grant":431,
		"ID": "EP/J020591/1",
		"Title": "Evaluating the Usability, Security, and Trustworthiness of Ad-hoc Collaborative Environments (EUSTACE)",
		"PIID": "122242",
		"Scheme": "Standard Research",
		"StartDate": "30/05/2012",
		"EndDate": "29/05/2013",
		"Value": "120779",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "122242", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Ensuring confidence in collaborative working is an important concern; Government work is increasingly collaborative in nature and needs to be enabled by advances in ICT supporting the provision of collaborative working environments. Part of this challenge involves managing rapidly changing situations, where prospective collaborators join and existing collaborators leave a coalition, without compromising security requirements.  The aim of the EUSTACE project is to develop a decision-making framework and tool support for rapidly evaluating the security implications of ad-hoc collaborative work.  We propose a framework that reuses existing models in Security, HCI, and Computer Science and makes these amenable to automated analysis and tool support.  The framework describes how formal specifications of implied behaviour are generated from existing usability and system models (such as personas and use cases) and combined with formal specifications of security requirements (derived from existing policies and requirements). A model checker is then used to analyse these specifications for failures and contradictions. These are then visualised in a collaborative work model that captures elements of the system, its users and their activities. The failures and contradictions are then highlighted in this model, providing the means of rapidly evaluating whether a proposed collaboration is likely to create security problems."
	},
	{
		"grant":432,
		"ID": "EP/J020664/1",
		"Title": "CROSS: Real-time Story Detection Across Multiple Massive Streams",
		"PIID": "80552",
		"Scheme": "Standard Research",
		"StartDate": "01/05/2012",
		"EndDate": "30/04/2013",
		"Value": "209736",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "80552", "Role": "Principal Investigator"},
		{"ID": "-99837", "Role": "Co Investigator"},
		{"ID": "97008", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The World is rapidly becoming more and more connected, with people communicating using multiple streams - Social Media, Newswire, Wikipedia etc - on a bewildering range of topics and at a furious rate. Twitter alone receives more than 250 million new posts every day (Tsotsis 2011). This massive interconnection means that content can appear and quickly spread through and across different streams. For example, in the recent London riots, many tweets reported the rioting events as they happened in real-time. However, not all content posted is either of good quality or is factually correct, complicating the job of monitoring such streams for any purpose. An example of this happened when a comedian spread false rumours on Twitter about Osama Bin Laden watching his television show (Lineham 2011). Communication streams are also known to spread rumours, outright misinformation and content with malicious intent. For instance, during the same riots, radicalising posts were spread calling for participation in the so-called 'cyber-jihad' (BBC 2011). Systems that can identify such posts is of paramount importance for security monitoring purposes.  On the other hand, not all information spread on mediums such as Twitter are accurate or interesting. This is compounded with the peculiarities of messages on modern social media (short, jargon, social context, etc.) where biased, incomplete, inaccurate and misleading messages are common. The latter makes it extremely challenging to automatically identify events worth monitoring for security purposes in real-time.  We propose a distributed infrastructure to automatically identify important new events (aka stories) in real-time by combining and comparing multiple message streams. The value of such story detection to many applications is clearly increased the faster this can happen. A security agency using our system would be better prepared when dealing with fast moving events as they unfold. Indeed, in this project, the notion of importance will be defined within a security context. Given the fact that streams typically have possible bias and not everything present can be trusted, a key requirement of the system is minimising false positives (uninteresting stories that are discovered). Moreover, the effective management and efficient processing of multiple streams of real-time data poses new technological and scientific challenges:  Challenge 1:  Identify interesting new stories and not drown in a sea of false positives, yet reduce the effects of bias and rumour. Challenge 2:  Minimise system latency, such that new stories are detected in real-time with low latency.  We tackle the first challenge from the novel perspective of processing multiple streams and exploiting the fact that stories reported multiple times across several streams can cancel-out stream-specific bias and errors. For example, if a story is true, then it is more likely that it manifests in both Twitter and as an update to a Wikipedia article. Alternatively, a story might appear in Twitter and also appear in a governmental cable. The more often a story occurs within and across streams, the more likely it will be interesting. This is the cornerstone of our proposal, which we tackle by building upon modern first story detection techniques, adapted to account for bias and rumours.  In the second challenge, we ensure low-latency story detection by using a distributed real-time data processing architecture (e.g. S4 or Storm), similar to MapReduce but better suited for real-time operations. Real-time architectures for dealing with massive-scale data are in their infancy, hence CROSS will present a first concrete application, with a corresponding development of best practices for such architectures.  "
	},
	{
		"grant":433,
		"ID": "EP/J020915/1",
		"Title": "Transparent Rational Decisions by Argumentation (TRaDAr)",
		"PIID": "78607",
		"Scheme": "Standard Research",
		"StartDate": "20/09/2012",
		"EndDate": "19/09/2015",
		"Value": "583833",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "78607", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Asian Institute of Technology"},
		{"Number": "1", "Name": "Polytechnic University of Madrid UPM"},
		{"Number": "2", "Name": "UCL Hospitals NHS Foundation Trust"}
		],
		"Summary": "Argumentation provides a powerful mechanism for dealing with incomplete, possibly inconsistent information and for the resolution of conflicts and differences of opinion amongst different parties. Further, it is useful for justifying outcomes. Thus, argumentation can support several aspects of decision-making, either by individual entities performing critical thinking (needing to evaluate pros and cons of conflicting decisions) or by multiple entities dialectically engaged to come to mutually agreeable decisions (needing to assess the validity of information the entities become aware of and resolve conflicts), especially when decisions need to be transparently justified (e.g. in medicine).  Because of its potential to support decision-making when transparently justifying decisions is essential, the use of argumentation has been considered in a number of settings, including medicine, law, e-procurement, e-business and design rationale in engineering. Potential users of existing argumentation-based decision-making methods are empowered by transparent methods, afforded by argumentation, but lack either means of formal evaluation sanctioning decisions as (individually or collectively) rational or a computational framework for supporting automation. The combination of these three features (transparency, rationality and computational tools for automation) is essential for argumentation-based decision-making to have a fruitful impact on applications. Indeed, for example, a medical practitioner would not find a 'black-box' recommended decision useful, but he/she would also not trust a fully transparent, dialectically justified decision unless he/she were sure that this is the best one (rational). In addition, the plethora of information doctors need to take into account nowadays to make decisions requires automated support.  TRaDAr aims at providing methods and prototype systems for various kinds of argumentation-based (individual and collaborative) decision-making that generate automatically transparent, rational decisions, while developing case studies in smart electricity and e-health to inform and validate methods and systems.  In this context, TRaDAr's technical objectives are:  (O1) to provide novel argumentation-based formulations of decision problems for individual and collaborative decision-making;  (O2) to study formal properties of the formulations at (O1), sanctioning the rationality of decisions;  (O3) to provide real-world case studies in smart electricity and e-health for (individual and collaborative) decision-making, using the formulations at (O1) and demonstrating the importance of the properties at (O2) as well as the transparent nature of argumentation-based decision-making;  (O4) to define provably correct algorithms for the formulations at (O1), supporting rational and transparent (individual and collaborative) decision-making;  (O5) to implement prototype systems incorporating the computational methods at (O4), and use these systems to demonstrate the methodology at (O1-O2) for the case studies at (O3).  The project intends to develop novel techniques within an existing framework of computational argumentation, termed assumption-based argumentation, towards the achievements of these objectives, and adapting notions and techniques from classical (quantitative) decision theory and mechanism design in economics.   The envisaged TRaDAr's methodology and systems will contribute to a sustainable society supported by the digital economy, and in particular they will support people in making informed choices. The project will focus on demonstrating the proposed techniques in specific case studies (smart electricity and e-health for breast cancer) in two chosen application areas (digital economy and e-health), but its outcomes could be far-reaching into other case studies (e.g. in other areas of medicine) as well as other sectors (e.g. in engineering, for supporting decisions on design choices). "
	},
	{
		"grant":434,
		"ID": "EP/J021458/1",
		"Title": "Learning Models of Handwriting for Structured Texture Synthesis",
		"PIID": "-258048",
		"Scheme": "First Grant Scheme",
		"StartDate": "21/01/2013",
		"EndDate": "20/01/2014",
		"Value": "99142",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-258048", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed research aims to produce images of handwriting using a new generative mathematical model of different people's handwriting styles and the specified text strings to be 'forged.' Our model uses image-fragments of real handwriting samples and learns what fragments are compatible under what deformations.  Our intended analysis/synthesis applications include quantitative authentication of handwriting in legal cases, inpainting of destroyed sections of historical documents, imitation of handwriting for banking purposes, and eventually, writer-specific handwriting recognition and drawn-sketch interpretation.  The specific objectives of this project are to: - Automatically synthesize handwriting of novel text but in a particular person's style. - Measure the likelihood that a given sample was written by a specific individual. - Build a system that suggests what examples of handwriting should be captured next.   The project scope is limited to just handwriting to assess feasibility. At its conclusion, we expect to further pursue the following additional objectives: - Demonstrate the synthesis and analysis approaches on large-scale forgery studies. The goal is to measure our forgery and forgery-detection rates to help transfer these techniques into industrial practice. Abuse of the technology for forgery creation could be limited by filming an individual while writing, to confirm that the synthesis was not automatic. - Build a similar adaptive authoring system to help synthesize cartoon animation. Replace strings of characters used in the present project with a structure based on limb-configurations (2D joint-angles) of a stick-figure. - Explore a unified framework for example-based generative models. In many domains, better synthesis should be possible when multiple examples are available. Findings from this near-term research on handwriting should generalize, so that structured acquisition of training examples steadily improves the quality of synthesized drawings, 3D shapes, and video post-production."
	},
	{
		"grant":435,
		"ID": "EP/J021628/1",
		"Title": "Real World Optimisation with Life-Long Learning",
		"PIID": "84023",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2013",
		"EndDate": "31/12/2015",
		"Value": "238068",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing",
		"OrgID": "42",
		"Investigators":[
		{"ID": "84023", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "INRA"},
		{"Number": "1", "Name": "Optrak Distribution Software Ltd"}
		],
		"Summary": "Many practical problems arising in industrial domains concerned with operating sustainably, meeting demand and minimising costs cannot be solved exactly. Meta-heuristic optimisation techniques have been widely developed in academia to solve such problems with much success reported in the literature. However, there remains a worrying void between scientific research into optimisation techniques and those problems faced by end-users and addressed by commercial optimisation software vendors. From a commercial perspective, the problems addressed by academia are too simplistic compared to those faced in the real-world, failing to embrace many real-world constraints. From the scientific perspective, researchers have also identified a 'lack of advanced metaheuristic techniques in commercial software'' which has been attributed in part to the academic community failing to demonstrate that their solutions are applicable to the needs of the commercial world, and in part to academics failing to impart their message the industrial community.  Meta-heuristic approaches can be costly to develop as they generally require human expertise to integrate specialist knowledge into an algorithm, and expertise in heuristic methods to design and tune algorithms. Recent research has therefore focused on automated algorithm design and configuration which produce tuned solvers that perform well on either individual problems or across suites of problems. One branch of this field is hyper-heuristics, which operates on a space of low-level heuristics, searching for combinations of heuristics which exploit the strength and compensate for the weaknesses of individual known heuristics. The resulting algorithms are cheap to implement, require less human expertise, have robust performance within a problem class, and are portable across problem domains. These features compensate for some reduction in solution quality compared to tailor-made approaches, while still ensuring solutions of acceptable quality. However, most automated  design approaches fail to incorporate or recognise a crucial human competence; human beings continuously learn from experience - by generalising observations and feedback, they are able to update their internal problem-solving models in order to continuously improve them, and adapt to changing circumstances. The failure of computational solvers to exploit previous knowledge both wastes useful knowledge and potentially hinders the discovery of good solutions. Furthermore, if the characteristics of instances of problems in the domain change over time, solvers may need to be completely re-tuned or in the worst case redesigned periodically.  This proposal addresses these dual concerns raised above. We propose a novel lifelong-learning hyper-heuristic system which addresses current deficiencies inherent in current systems: it will exhibit short-term learning,  producing fast and effective solutions to  individual problems and at the same time, long-term learning processes will enable the system to autonomously adapt to new problem characteristics over time. It therefore exploits existing knowledge whilst simultaneously adapting to new  information. Secondly, by working closely with two collaborators, a commercial routing software vendor and a forestry expert, our research will be directly informed by real-world problems, accounting for real constraints and performance criteria, thereby producing economic impact. Future advances in optimisation techniques will be facilitated by the development of a  problem generator and a number of problem suites which reflect real-world priorities and constraints, derived from actual problem data provided through our collaborators and defined in conjunction with metrics which reflect not only economic drivers but also address environmental impact and the reduction of carbon emissions. This information database will be widely disseminated to provide an extensive platform for future research."
	},
	{
		"grant":436,
		"ID": "EP/J021768/1",
		"Title": "Green Heterogeneous Networks",
		"PIID": "-79823",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2016",
		"Value": "624273",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93",
		"Investigators":[
		{"ID": "-79823", "Role": "Principal Investigator"},
		{"ID": "111115", "Role": "Co Investigator"},
		{"ID": "91395", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The tremendous development of mobile networks has enabled ubiquitous communication that has transformed the way people connect with each other. With fast and reliable information exchange, mobile communication is becoming (if not yet) a daily necessity. However, existing mobile networks are not 'green'. Although much efforts has been invested in exploring energy efficient techniques for mobile phones, this is not so for base stations and mobile networks. As of 2008, mobile networks consume around 0.5% of global electricity, which is a significant amount for one particular industry. To make matters worse, this consumption is expected to rise by 200% within the next few years. With increased electricity price, this high energy consumption will increase the operation expenditure for mobile operators, which results in higher cost-per-Mbyte. This will create difficulties for mobile operators, and will also impact on pricing for consumers. On the other hand, the increased energy consumption will lead to higher CO2 emission. Given the threats of global climate change, it is highly desirable for future mobile networks to be more energy efficient and thus more environmental friendly.  Equally important, the need for higher data rate mobile communication is ever increasing. First of all, even with the extensive development of third generation (3G) mobile network, a recent study by the BBC shows that 25% of the time users will only able to access the much slower second generation (2G) network. This slow connection is a bottleneck for true ubiquitous communication. Therefore, to bridge the digital divide, there is a strong desire by the government, operator, and user to have high data rate connections everywhere. Secondly, the remarkable growth of high quality multimedia and social networks has pushed mobile data rates to the limit, and thus faster networks for the future are required. Although the 3GPP LTE (Long Term Evolution) and 802.16j (WiMax) will ease some of these pressures, advanced network topology and techniques are needed. Research work has already been started on IMT Advanced, which is the next generation mobile network. It aims to achieve a peak data rate of 1Gbps.   The most promising approach for IMT Advanced is the Heterogeneous Network (HetNet). It is an advanced network topology that cooperates between multiple tiers of base stations, i.e., macro, micro, pico, femto and relay base stations. By intelligent interference management, HetNet exploits frequency reuse to its maximum, and provides high data rate coverage everywhere. However, existing research on HetNet has focused mostly on the high data rate aspects, but rarely on the energy efficiency aspects. In this project, both goals on high energy and spectral efficiency will be targeted jointly. In particular, we will develop innovative techniques to provide radical improvement, instead of incremental enhancement. Adaptive network topology that can optimise these factors will be devised. Smart interference management and exploitation techniques will be developed to exploit the potential of HetNet. Network MIMO, cognitive radio, and 3 dimensional beamforming techniques will also be developed to achieve a green HetNet that is suitable for the future. The goal is to develop new technologies that can reduce energy consumption while providing the required data rate increase.   "
	},
	{
		"grant":437,
		"ID": "EP/J021814/1",
		"Title": "Probabilistic Rounding Algorithms for Mathematical Programming",
		"PIID": "-402956",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2012",
		"EndDate": "31/10/2015",
		"Value": "360972",
		"ResearchArea": "Logic and Combinatorics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-402956", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "IBM Almaden Research Center"},
		{"Number": "1", "Name": "IBM Watson Research Centre"}
		],
		"Summary": "This proposal falls into the general area of design and analysis of algorithms for discrete optimization problems. Such problems arise in Business Analytics, Management and Computer Sciences and in all Engineering subfields. The variety of models and problems arising in this area is astonishing. Nevertheless the method of choice to solve such problems in practice is some combination of mathematical programming solver (CPLEX, Gurobi, IPOPT) of a relaxed problem where some of the problem constraints (like integrality of decision variables) are relaxed or dropped and some rounding algorithm that converts a relaxed solution into a solution of the original problem. In many cases such practical algorithms work in multiple stages by slowly transforming the relaxed solution into an unrelaxed one while constantly monitoring the quality of the current solution.  On the other hand it was long recognized in the Theoretical Computer Science, Mathematical Programming and Operations Research communities that understanding the performance of various  methods to transform an optimal or near-optimal solution of an 'easy' optimization problem into a high quality solution of a 'hard' optimization problem  is the key to understanding the performance of practical heuristics and design new algorithms to solve hard optimization problems. Such methods are usually called  rounding algorithms since they usually transform a fractional solution into an integral one. In this project we would like to apply the modern methods of Probability Theory, Matroid and Polyhedral Theories to explain why such algorithms perform well in practice. We also would like to design new algorithms for transforming solutions of  relaxed practically relevant optimization problems into solutions of original hard optimization problems. Along the way we would like to design new concentration inequalities of random processes associated with our probabilistic rounding algorithms. Such concentration inequalities are useful in explaining the quality of randomized rounding procedures and can lead to design of new rounding algorithms."
	},
	{
		"grant":438,
		"ID": "EP/J021857/1",
		"Title": "A Multi-Functional Organic Charge Coupled Device",
		"PIID": "15524",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2012",
		"EndDate": "31/10/2014",
		"Value": "231548",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronics",
		"OrgID": "6",
		"Investigators":[
		{"ID": "15524", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Cambridge Display Technology Ltd"},
		{"Number": "1", "Name": "SmartKem Ltd"}
		],
		"Summary": "The charge injection device (CID) and the charge coupled device (CCD) fabricated on silicon have a wide range of applications, the most common being in image capture for example in digital cameras and sensitive image arrays in astronomy.  These devices are composed of arrays of two-layer capacitors fabricated adjacent to each other. One layer is a semiconductor, the other an insulator. In these devices charges are induced at the semiconductor/interface either electrically or optically. By applying suitable voltages across successive capacitors these charges can be moved through the array from one capacitor to the next. The CID is composed simply of two capacitors but is extremely useful for studying the generation and transfer efficiency of charges from one capacitor to the other. The CCD is a multicapacitor array. Until our recent publication of an organic CID no laboratories had previously reported organic versions of these two devices. This project is directed at demonstrating a multi-functional organic charge coupled device (CCD). Initially the programme will characterise fully our groundbreaking demonstration of an organic CID which is, in essence, a single-stage CCD. We plan to utilise the optical response of the CID to investigate the interface physics of the device. We will study specifically chosen insulator/semiconductor combinations in order to obtain the essential information on the dynamics of charge transport and trapping at the insulator/semiconductor interface. This information will allow us to design and fabricate the optimum test structure and identify the appropriate test conditions for demonstrating a multi-functional organic CCD. The device offers opportunities for exploitation in signal processing and imaging applications that are compatible with printed plastic electronic systems, a key investment area for the UK Government."
	},
	{
		"grant":439,
		"ID": "EP/J021962/1",
		"Title": "Hybrid Colloidal Quantum Dot Lasers for Conformable Photonics",
		"PIID": "-80756",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2014",
		"Value": "98975",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Photonics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "-80756", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We propose an innovative hybrid technology for mechanically-flexible lasers based on semiconductor colloidal quantum dots. These lasers will be suitable for integration in conformable photonic systems and have identified applications in (bio)-sensing.  Photonics on mechanically-flexible platforms (so called flexible or conformable photonics) is an exciting and potentially disruptive technology and applications in which it already plays a role include bendable displays, electronic paper and solar cells. It is an integral part of the field of plastic electronics, whose worldwide market will be worth several hundred billion pounds in 2025 according to expert analysts. In order to create the flexible photonic systems of the future and unleash the associated benefits, the availability of a laser technology compatible for embodiment in flexible formats is critical. Suitable solution-processed organic semiconductor (OS) and inorganic colloidal quantum dot (CQD) lasers have been the subject of intense research although only a minority rigorously studied mechanical flexibility. OS are carbon-based materials and CQDs are minuscule inorganic crystals with typical diameters below 10 nm, which benefit from a narrow, shape and size-dependent emission colour. Both OS and CQD can be processed from solution, are compatible with a wide range of materials including plastics and provide gain across the visible spectrum. Despite these attractive attributes, the deployment of these materials in practical laser systems has been hampered up to now due to specific material limitations.   In this context, we will create a mechanically-flexible laser technology based on hybrid CQD gain media that address these limitations. The approach combines the best of both OS and CQD materials into a new functional system with added light harvesting capabilities analogous to biological and biomimetic nanoantenna complexes.   More specifically our approach will build on the advantages of energy transfer effects, local field phenomena and enhanced material processability that nanocomposites confer. For this, different types of CQDs and OS will be blended or hybridised and optionally incorporated into transparent matrices. All-inorganic mixes of CQDs with heterogeneous sizes will also be studied. As opposed to existing soft laser gain media, these hybrid materials will relax tolerances on the excitation process and will enable a wide wavelength emission coverage with no compromise on the overall efficiency. Furthermore, by collecting and concentrating the excitation energy where it is needed, they will also lead to much improved CQD laser thresholds. In turn, the resulting nanocomposites will be patterned at the nanoscale to assemble low-threshold CQD lasers in a mechanically-flexible format.  The proposed lasers will be compatible with other categories of emerging conformable devices therefore paving the way for truly integrated conformable flexible systems. Ultimately they will be electrically-driven or more simply incorporated on top of flexible blue-emitting GaN lasers or light-emitting diodes (LEDs), yielding optically-pumped hybrid flexible lasers. Blue and violet emitting GaN devices on flexible platforms are the subject of recent intensive research in groups including our own and should be available in the near future. We note that such inorganic GaN sources emitting efficiently above 515 nm (blue-green) are challenging to develop due to inherent material constraints (the so-called 'green gap'). Henceforth, a hybrid integration approach may be regarded as essential for full visible region coverage as already demonstrated for rigid device formats. The flexible hybrid laser chips that we envision will find extensive applications. In particular, the structures are ideally suited for wearable (bio)-sensor systems, and we will take initial steps to demonstrate this capability."
	},
	{
		"grant":440,
		"ID": "EP/J02211X/1",
		"Title": "Robust and Sensitive Methods for Non-rigid and Partial 3D model Retrieval",
		"PIID": "-193862",
		"Scheme": "Standard Research",
		"StartDate": "22/04/2013",
		"EndDate": "21/04/2016",
		"Value": "309947",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "28",
		"Investigators":[
		{"ID": "-193862", "Role": "Principal Investigator"},
		{"ID": "10226", "Role": "Co Investigator"},
		{"ID": "52643", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "3D models have a broad range of applications in many different areas such as engineering, biology, chemistry, medicine, entertainment and cultural heritage. Many 3D models are available from the Internet and other sources, resulting in a problem of how to effectively and efficiently find required 3D models (i.e., 3D model retrieval). Current research on 3D model retrieval mainly focuses on global rigid 3D model retrieval, and algorithms for solving this problem are not effective for non-rigid and partial 3D model retrieval. Because many 3D models of interest are non-rigid (such as humans, and mechanisms), and because it is often important to consider just parts of a 3D model (e.g. find a model with a particular connector), finding an efficient way to retrieve non-rigid and partial 3D models is a pressing and challenging problem. This project intends to develop robust and sensitive algorithms for non-rigid and partial 3D model retrieval.  A typical shape-based 3D model retrieval algorithm consists of three main steps: model preprocessing, feature/shape descriptor extraction, and feature/shape indexing and matching. This project will investigate all three steps and develop new non-rigid and partial 3D model retrieval algorithms based on novel techniques from other research areas. Set-membership estimation from control theory will be introduced into model preprocessing and feature/shape descriptor extraction. New machine learning methods, such as affinity propagation, manifold learning and ranking, will be explored for extracting features/shape descriptors, and for feature/shape indexing and matching. The N-gram model from natural language processing will be adapted to feature/shape indexing and matching. Other new techniques from image processing and computer vision will be investigated regarding their effectiveness for non-rigid and partial 3D model retrieval.  This project will also consider potential applications of the newly developed techniques. The 3D model retrieval algorithms will be evaluated jointly with Delcam plc with a view to commercial exploitation. A practical non-rigid and partial 3D model search engine will be developed and deployed on the Internet for public use."
	},
	{
		"grant":441,
		"ID": "EP/J022128/1",
		"Title": "DIverse DatabasE ReplicatiOn - Performance Comparison (DIDERO-PC)",
		"PIID": "-250741",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2014",
		"Value": "97386",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Centre for Software Reliability",
		"OrgID": "87",
		"Investigators":[
		{"ID": "-250741", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal is about experimental evaluation of diverse database replication protocols. It aims at comparing the performance of several database replication protocols, which protect against non-crash failures and are based on use of diverse, i.e. different by design, database management systems (DBMSs). These DBMSs are developed by different vendors, and can be either commercial (e.g. Microsoft SQL Server, Oracle), or open-source products (e.g. MySQL, PostgreSQL). The experimental evaluation will be conducted using two approaches: a model-based approach and a direct experimentation on a testbed with 'real' systems. Diverse database replication protocols have a potential of providing higher dependability assurance than traditional approaches that are based on the use of the DBMSs from the same vendor (research from the Centre for Software Reliability (CSR) at City University London has contributed significantly to that finding). However, besides achieving appropriate dependability level, system designers and other DBMS stakeholders are interested in the performance characteristics offered by alternative solutions. For many applications performance is, in fact, the dominant criterion in the selection process. A fair and thorough performance comparison between database replication protocols, including the ones studied in this proposal, is of great benefit to system designers when making decisions about which one is most suitable to integrate in their system. This is the main motivation of the proposal. The project aims at comparing the performance of three database replication protocols which guard against non-crash failures and are based on use of diverse DBMSs: our DivRep (the reference point), SES and Byzantium. There are two main objectives of the particular performance evaluation to be achieved: - Objective 1 - evaluation based on using 'real' systems on an experimental testbed, i.e. using respective implementations of the replication protocols together with the real DBMSs. - Objective 2 - model-based evaluation, whereby each replication protocol, as well as DBMS resources, are represented with a suitable model (implemented in a particular tool of choice) and the protocol performance measured by simulation of the respective model. Performance comparison of DBMSs and related replication protocols is too often based only on the comparison that uses (synthetic) benchmarks, which in most cases do not reflect the operational profile of the system under test. In addition, there are limits to what one can achieve with performance comparison based only on experimental evaluation with 'real' systems: one can feasibly explore only a limited range of parameter values (such as number of replicas, ratio between read and write operations, etc). In contrast, model-based approach offers greater flexibility by providing means for evaluation with a wide range of configuration parameters in a practical manner. Therefore, the proposal aims at satisfying the following objective: - Objective 3 - provision of a general-purpose methodology for model-based performance evaluation of database replication protocols."
	},
	{
		"grant":442,
		"ID": "EP/J500173/1",
		"Title": "Wirelessly gathered road vehicle data for traffic control and other applications.",
		"PIID": "94747",
		"Scheme": "Technology Programme",
		"StartDate": "06/06/2011",
		"EndDate": "05/12/2012",
		"Value": "172835",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Engineering & the Environment",
		"OrgID": "117",
		"Investigators":[
		{"ID": "94747", "Role": "Principal Investigator"},
		{"ID": "81373", "Role": "Co Investigator"},
		{"ID": "94744", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "It is understood that wireless communication links between road vehicles and infrastructure will be commonplace in the near future. Such links will lead to the availability of a rich new data source potentially containing real time position and speed data (among others) for some or all vehicles on the UK road network. This is known as localization probe data. The availability of this new data source creates business opportunities to develop tools that manage, store, and process these data for applications. An application of particular interest is Urban Traffic Control (UTC). There are a large number of existing data sources that currently provide useful information for UTC. These include direct measurement data, for example from inductive loops and cameras. They also include data that are informative of the expected levels of demand on the system, for example shipping timetables and event schedules. Currently these data sources are not fully unified and it would be desirable to fuse them. Similarly there are data sources, such as smart phone data, that are present but not fully utilised. The aim in exploiting these multiple data sources is to create a single coherent image of the state of the network. It is important that the new localization probe data does not become an additional disparate source of information, but is combined with already available data sources to generate an image of the network state that can be employed in UTC and other applications. The technical objective of this research project is to create a city centre sized microscopic traffic simulation model, where wireless communication between vehicles and infrastructure is modelled and thus simulated localization probe data are generated. Simulated data from existing sources will also be generated and validated against the available historical data. The simulated data will then be used in prototype model Urban Traffic Control systems that will implement control directly in the simulation. Thus the simulator becomes a proving ground for the development of techniques to process and analyse these data and for novel traffic control methods using these data. Also the simulator will be used to analyse the efficacy of prototype traffic control systems and compare them with existing methods, thus enabling a quantitative analysis of the economic value of this new data source and these new techniques in the transport sector. An important aspect of the solution to the UTC problem is to develop software tools for processing, refining and visualizing the data to present human traffic controllers with a single coherent image of the state of the network. The software development aspect of the project will be extended to develop some general processing and visualisation tools for applications other than UTC. These applications would include research applications where, for example the data could be used for pollution monitoring. They would also include legislative monitoring where key performance indicators can be extracted from the data. They would also include value added services to the general public, such as real-time travel information. The principal outputs of the project will be new methods and techniques for collecting and processing new localization  Proposal original proforma document  Page 3 of 10 Date printed: 08/12/2010 12:09:52  TS/I003642/1 Date saved: 08/12/2010 12:09:35  data, fusing it with existing data and employing it in UTC. This will include patents for both data management technologies and new UTC technologies. Also output will be software tools to analyse process and visualise these data. Because there is a market for these data in areas other than signal control, general software for data refinement and visualisation will also be produced for marketing to third parties."
	},
	{
		"grant":443,
		"ID": "EP/K000330/1",
		"Title": "Models and Measures of Findability",
		"PIID": "-172924",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/11/2012",
		"EndDate": "21/01/2014",
		"Value": "97376",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "-172924", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Designing and organizing a website or any other information space is a complex process where the goal is to create structure around content that enables users to complete their tasks (such as finding the relevant information and performing associated transactions) in a seamless and efficient manner. Typically, Information Architecture techniques are applied in an attempt to improve a site's usability and the overall user experience by optimizing ``the structural design of an information space to facilitate task completion and intuitive access to content'' Although there are numerous principles and heuristics that have been developed, Information Architecture as a discipline, lacks formal models for evaluating or predicting whether such techniques will improve the usability of a website. In this project, we aim to develop formal models to measure, analyze and evaluate how easily a site can be navigated and the information within it can be retrieved. Such a model would provide a way to objectively measure what is colloquially  termed, ``Ambient Findability'', i.e. the ease with which a given information object can be found. If the structure of a particular website precludes users from intuitively and easily locating key information, then in competitive online environments users are likely to abandon the site in favour of alternative sites that provide competing services or information. For example, online retailers need to ensure that users can quickly and easily find and locate the products and services offered along with related information such as product reviews, help files, and other supporting information. Consequently, the ability to determine or predict the findability of information objects such as key web pages is vital.  While various measures have been developed to objectively quantify how easily a user can navigate a website, or how easily a user can retrieve a particular page, =these techniques are often very coarse grained in nature. They ignore the needs or goals of users in their calculations which directly influence how easily a user could locate an object. They are agnostic to design in every sense, and thus ignore key issues such as layout, location and the visibility of links within web pages and the user's broader task context. Moreover, they assume each user is equally likely to select a given link (i.e. acting as a random surfer), and ignore well-known patterns and strategies of user behavior. Thus, they provide little value to practicing Information Architects who require more sophisticated, behavioral measures that accommodate these dimensions. To this end, this project will attempt to model more accurately the interaction of users with webpages and sites to provide an estimate of the probability of a particular page being found by different users groups with particular information needs/intentions.  The development of concrete mathematical and probabilistic models of findability and measures that capture the complex interactions of users, systems and information resources will be a significant achievement that will advance research and development into understanding behaviors, improving components and maximizing the findability of information. To determine the utility of these measurements, this project will take the theory out of the lab environment and evaluate the measures in operational settings with practicing Information Architects. This evaluation will provide external verification and validation of the theory and models developed to demonstrate how the measures can be deployed and utilized in practice. "
	},
	{
		"grant":444,
		"ID": "EP/K00042X/1",
		"Title": "Ultra-parallel visible light communications (UP-VLC)",
		"PIID": "45306",
		"Scheme": "Programme Grants",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2016",
		"Value": "4595366",
		"ResearchArea": "Optical Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Photonics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "45306", "Role": "Principal Investigator"},
		{"ID": "61164", "Role": "Co Investigator"},
		{"ID": "109313", "Role": "Co Investigator"},
		{"ID": "12308", "Role": "Co Investigator"},
		{"ID": "27588", "Role": "Co Investigator"},
		{"ID": "44903", "Role": "Co Investigator"},
		{"ID": "94859", "Role": "Co Investigator"},
		{"ID": "-200461", "Role": "Co Investigator"},
		{"ID": "16932", "Role": "Co Investigator"},
		{"ID": "43826", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Avago Technologies"},
		{"Number": "1", "Name": "BAE Systems"},
		{"Number": "2", "Name": "Bell Labs Ireland"},
		{"Number": "3", "Name": "Compound Semiconductor Tech Global Ltd"},
		{"Number": "4", "Name": "EV Group Inc"},
		{"Number": "5", "Name": "Micro Resist Technology GmbH"},
		{"Number": "6", "Name": "NEC Telecom MODUS Ltd"},
		{"Number": "7", "Name": "Osram Opto Semiconductors GmbH"},
		{"Number": "8", "Name": "STMicroelectronics (R&D) Ltd."},
		{"Number": "9", "Name": "Thorn Lighting Ltd"}
		],
		"Summary": "We are on the verge of a global revolution in lighting, as efficient and robust light emitting diode (LED) based 'solid state lighting' (SSL) progressively replaces traditional incandescent and even fluorescent lamps and finds its way into new areas including signage, illumination, signalling, consumer electronics, building infrastructure, displays, clothing, avionics, automotive, sub-marine applications, medical prosthetics and so on. This technology has tended to be viewed, so far, primarily as a way to improve energy- and spectral-efficiency, but what has been relatively little studied or appreciated is its profound implications for the future of communications.  We envisage the tremendous prospect of an entirely new form of high bandwidth communications infrastructure to complement, enhance and in some cases supercede existing systems. This LED-based technology will utilise the visible spectrum, largely unused for communications at present and more than 10,000 broader than the entire microwave spectrum. This promises to help address the 'looming spectral crisis' in RF wireless communications and to permit deployment in situations where RF is either not applicable (e.g. in underwater applications) or undesirable (e.g. aircraft, ships, hospital surgeries), but the implications are more fundamental even than that. The key point, in our view, is that lighting, display, communications and sensing functions can be combined, leading to new concepts of 'data through illumination' and 'data through displays'. Imagine, for example, a 'smart room', where 'universal illuminators' provide high-bandwidth communications, sensors monitoring the environment and people within it, provide positioning information and display functions, and monitor the quality of the light. Imagine novel forms of personal communications system that combine display functions and video with multiple, high-bandwidth communications channels. These could be through mobile personal communicators (developments of mobile phones or personal digital assistants) or even wearable and mechanically flexible displays.  Our ambitious programme seeks to explore this transformative view of communications in an imaginative and foresighted way. The vision is built on the unique capabilities of gallium nitride (GaN) optoelectronics to combine optical communications with lighting functions, and especially on the capability of the technology to implement new forms of spatial multiplexing, where individual elements in high-density arrays of LEDs provide independent communications channels, but can combine as displays. We envisage ultra-high data density - potentially Tb/s/mm2 - arrays of LEDs in compact and versatile forms, and will develop novel transceiver technology on this basis on both mechancially rigid and mechanically flexible substrates. We will explore the implications of this approach for multi-channel waveguide and free-space optical communications, establishing guidelines and fundamental assessments of performance which will be of long-term significance to this new form of communications. "
	},
	{
		"grant":445,
		"ID": "EP/K000519/1",
		"Title": "SEQuence-Analysis Based Hyperheuristics (SEQAH) for Real-World Optimisation Problems",
		"PIID": "124474",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2015",
		"Value": "256486",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46",
		"Investigators":[
		{"ID": "124474", "Role": "Principal Investigator"},
		{"ID": "41055", "Role": "Co Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Mouchel Group PLC"}
		],
		"Summary": "Selective hyperheuristics are a set of optimisation techniques that effectively optimise the search algorithm during an optimisation run by selecting combinations of lower level heuristic operations (e.g. mutation, crossover & replication).  They operate at the level above metaheuristics (e.g. evolutionary algorithms) and are thus able to react to changes in the search space by modifying the heuristics that are applied to the search problem.  Traditional selective hyperheuristics consider single heuristics and heuristic pair performance when determining the heuristic to select next.  This project will develop  new methods known as a sequence analysis based hyperheuristics (SEQAH) and will investigate the use of sequence analysis techniques, taken from other computational domains such as bioinformatics and natural-language processing, to determine heuristic selection.  SEQAH methods will record the search process as a sequence of pairs of heuristic application and performance, and will process this information to inform the selection of the next heuristic to apply in the optimisation.  This will allow the technique to automatically select the best heuristics to apply for a given problem - effectively tuning the algorithm to new optimisation problem types, regardless of the underlying application area.  By selecting from a set of heuristics, the SEQAH techniques can combine ordinary heuristic operations (e.g. mutation and crossover) with more problem-specific heuristics such as human-designed 'rules-of-thumb' into one coherent algorithm that is able to generate near-optimal solutions in less computational time.  The developed techniques will be tested on problems from the literature and a suite of real-world problems in water distribution optimisation including the design, rehabilitation and operation of large-scale water systems.  The optimisation of these systems has the potential to offer improved services in terms of reliability and water quality and to reduce the future cost and environmental impact of providing clean, safe drinking water to homes across the country.  The SEQAH technique also has the potential to extend beyond the water industry and should be applicable to any number of optimisation problems in many application areas due to its ability to adapt to new problem spaces online."
	},
	{
		"grant":446,
		"ID": "EP/K000683/1",
		"Title": "Verification and correctness of service-oriented systems",
		"PIID": "107748",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "18/03/2013",
		"EndDate": "17/06/2013",
		"Value": "5360",
		"ResearchArea": "Verification and Correctness",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "43",
		"Investigators":[
		{"ID": "107748", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Service-oriented computing (SOC) is the paradigm that underlies the infrastructure on which the Digital Economy is being built.  It offers the ability for businesses and organisations to operate on a globalised, platform-independent market of services, and for individuals to interact with businesses and organisations in a flexible and seamless way that responds to individual needs.  The specific research that we propose to develop addresses fundamental questions raised by SOC on the computational theory and the methods and techniques that are needed to make this infrastructure reliable. Although research has been funded on global ubiquitous computing in general, and Web services in particular, only very little effort has been directed to two of the distinguishing aspects of SOC as a computational paradigm: the asynchronous nature of the interactions that SOC presupposes (the large majority of models that have been proposed are based on synchronous communication, which prevents the kind of loose coupling that is required for open-world software), and the fact that the configurations of the systems that operate on global computers (like the Internet) are intrinsically dynamic, thus defeating the traditional engineering methods that rely on design-time integration of software applications."
	},
	{
		"grant":447,
		"ID": "EP/K000802/1",
		"Title": "Hardware Acceleration of Simulations of Extreme Weather Events",
		"PIID": "117271",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "15/07/2012",
		"EndDate": "14/10/2012",
		"Value": "25365",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "117271", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "Kyoto University"}
		],
		"Summary": "This is a proposal for a two-month research visit to the Disaster Prevention Research Institute of the University of Kyoto in Japan, to work with Prof. Tetsuya Takemi of the Atmospheric and Hydrospheric Disasters Division on hardware acceleration of simulations of extreme weather events. Hardware acceleration means the use of special hardware -- in our case General-Purpose Graphics Processing Units (GPGPUs) and Field-Programmable Gate Arrays (FPGAs) to speed up a computational task. In particular, our aim is to reduce the simulation times for models using the Weather Research and Forecasting (WRF) model, the leading next-generation model for weather and climate simulations. This will allow the models to be run at higher precision, which is essential in the prediction of extreme weather events. The main purpose of this research visit is to lay the groundwork for a long-term collaboration including research groups in Japan and the UK, with the aim of accelerating the full Weather Research and Forecasting (WRF) model and adding support for FPGA acceleration.     "
	},
	{
		"grant":448,
		"ID": "EP/K000810/1",
		"Title": "Resilient and Testable Energy-Efficient Digital Hardware",
		"PIID": "50450",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2013",
		"EndDate": "29/02/2016",
		"Value": "449117",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "50450", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		{"Number": "0", "Name": "ARM Ltd"},
		{"Number": "1", "Name": "University of Southampton"}
		],
		"Summary": "The UK is home to some world-leading electronic companies including semiconductor IP supplier of low-power microprocessors (ARM), multimedia and communications cores (Imagination Technologies); which are at the heart of today's and future consumer electronics, and home entertainment. Power management is an essential enabling technology in such electronics and will become more prominent in future electronic systems. The downside of power management is that it decreases the reliability and increase the testability cost of energy-efficient hardware as demonstrated by recent academic and industrial research including that reported by the investigation team. This is because energy-efficient hardware often have no provision for tolerating run-time soft errors (unless for safety critical applications); and current methods for testing such hardware for manufacturing defects don't explicitly target power management circuitry. There are currently no fault models or test methods for power distribution networks and power management circuitry and no on-line soft error monitoring and correction methods for power management hardware. This grant application is focused on developing new fault models, methods, circuits and their validation (simulation, FPGA and AISC) to quantify and improve the resilience and testability of energy-efficient digital hardware. Particular emphasis is placed upon cost-effectiveness through joint consideration of reliability, and test and re-using on-chip hardware to minimise silicon area, power consumption and impact on functional performance. This is a three-year project involving two post-doctoral researchers (one for three years and the other for two years), and ARM (Cambridge) as an industrial partner. The project will be carried out in collaboration with Prof. F. Kurdahi (Uni. of California, Irvine) and Prof. M. Tehranipoor (Uni. of Connecticut). Both acknowledged world experts in the proposed research.    This project will significantly advance the present state-of-the-art in reliable and testable energy-efficient hardware and will lead to the following research deliverables: 1. New fault models for power management circuitry and power distribution network (PDMC) to underpin their logic and timing behaviour due to soft errors and manufacturing defects;   2. New methods and circuits and their practical validation for improving testability and diagnosis (against manufacturing defects) and reliability (against soft errors) through online monitoring and correction.  3. A design automation methodology for embedding automatically into an energy-efficient design the required circuitry to enable enhanced reliability and testability using existing EDA tools. "
	},
	{
		"grant":449,
		"ID": "EP/K001469/1",
		"Title": "Next generation avalanche photodiodes: realising new potentials using nm wide avalanche regions",
		"PIID": "120483",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2013",
		"EndDate": "30/09/2016",
		"Value": "549432",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "120483", "Role": "Principal Investigator"},
		{"ID": "-19880", "Role": "Co Investigator"},
		{"ID": "3826", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The internet data rate of Mb/s is currently available to UK homes thanks to installation of fibre network. Recently Fujitsu, a major telecom company, outlined their plan to lay Gb/s fibre network in UK, which can increase the data rate to 10 Gb/s and beyond. Therefore optical fibre will play an ever increasing importance in our life and hence there is a clear need to carry out research in ultrafast optical components such as photodiodes, used to convert optical signal to electrical signal. In photodiodes the energy from light is used to release an electron from an atom and a detectable current is generated when the electron is swept by an electric field. In a specially designed avalanche photodiode (APD) the electric field is increased such that a single electron generated by the photoelectric effect can produce an avalanche of electrons and holes. Consequently a much larger signal is produced, leading to a better signal to noise ratio. Unfortunately current commercial APD can only work up to 10 Gb/s and is therefore not future proof.   In this proposal, we will develop extremely thin 10-50 nm semiconductor layer to achieve the avalanche effect at ps time scale such that our APDs can operate at bit rates of Tb/s. The new semiconductor materials that will be developed in this project are AlAsSb and AlGaPSb since they have great potential to withstand extremely high electric field while maintaining low dark current (essential to minimise errors in digital signal). Crucially since our materials are only nm thick, we can engineer the electric field in APD to impose some degree of coherence in the electron and hole behaviours so that the avalanche effect occurs with minimal noise. We believe our APDs can be designed to approach the performance of an ideal noiseless APD with high bandwidth for optical communications.    We recently demonstrated that the avalanche effect in thin AlAsSb is relatively immune to temperature change. Therefore in addition to ultra high speed optical communication, our proposed nm scaled AlAsSb and AlGaPSb avalanche layers are envisaged to work as an ultra fast photon counter with high immunity to ambient temperature fluctuation. Since a photon is the basic unit of light, the 'ultimate' light sensor is achieved by increasing the avalanche gain to approximately a million so that the APD works as a photon counter.  Our thin avalanche layer has the potential to register a photon count in a few ps, which is at least an order of magnitude faster than current APD photon counters. If successful one of the major impacts of our photon counter will be to improve the data encryption technique called quantum key distribution in which the data is encrypted using a single photon.  This is believed to be the most secure encryption technology. Any unauthorised detection of the photon will cause a significant error rate, and hence alerting the sender of the attempted hacking. Therefore the high thermal stability and fast response time of our APDs will enhance the robustness of future quantum cryptography systems.   We also believe our new technology will bring significant improvement to medical X-ray imaging as the APD can improve the signal to noise ratio of X-ray detection system. Typically the avalanche effect increases the electrical signal, induced by the X-ray absorption, to above the electronic circuit noise and hence enhancing the image quality. Our recent work showed that having a thin avalanche layer is essential for high performance X-ray APD. Hence our work will enable a new generation of X-ray APDs for imaging applications.  To achieve the goals discussed above we will carry out very systematic development of AlAsSb and AlGaPSb APDs via advanced growth of the semiconductor crystals and optimised chemical etching process as well as meticulous measurements to extract key material properties for design of high performance APDs utilising nm avalanche regions.      "
	},
	{
		"grant":450,
		"ID": "EP/K001698/1",
		"Title": "UNderstanding COmplex system eVolution through structurEd behaviouRs (UNCOVER)",
		"PIID": "9032",
		"Scheme": "Standard Research",
		"StartDate": "14/01/2013",
		"EndDate": "13/01/2016",
		"Value": "559122",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "9032", "Role": "Principal Investigator"},
		{"ID": "22017", "Role": "Co Investigator"},
		{"ID": "12985", "Role": "Co Investigator"},
		{"ID": "109749", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The project aims to develop theories and implement prototype software tools for the formal verification, synthesis and analysis of *complex evolving systems*. Such systems may involve hardware, software and human organizations; though very common in practice, as yet such systems lack robust scientific and engineering support. The project will do this by developing a rigorous methodology supported by a toolkit based on structured behavioural representations (structured occurrence nets). The effective use of such representations greatly reduces the cognitive complexity of, and the storage and computational resources involved in the modelling and manipulation, of large systems. Moreover, it offers a solution to the difficult problem of representing and analyzing the behaviour of systems that are evolving, e.g. through being subject to modification (by other systems). The power and generality of the new formalism of structured occurrence nets, and the potential of our planned toolkit, will be demonstrated using three case studies: (i) the verification of asynchronous VLSI circuits, (ii) on-line deadlock detection in networks-on-a-chip, and (iii) in partnership with a leading commercial developer of such systems, a major crime investigation support system."
	},
	{
		"grant":451,
		"ID": "EP/K004832/1",
		"Title": "Establish collaboration with CSIRO for research projects.",
		"PIID": "69020",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "01/09/2012",
		"EndDate": "30/11/2012",
		"Value": "4526",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "69020", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The WSN group in the E&EE department at the University of Leeds has published techniques to progress and enhance the performance of localization systems.  These techniques have been published in top quality international journals. The group would like to extend these ideas and realize these techniques to offer enhancement to all localization systems.  The Australia China Research Centre for Wireless communications has also been very active on the world stage in developing novel positioning and localization solutions.  Building a collaboration between the two groups will benefit UK, European and Austrialian industries and enhance academic standing.  The theoretical ideas will be demonstrated and optimised for practical systems."
	},
	{
		"grant":452,
		"ID": "EP/K005111/1",
		"Title": "Radio Resource Allocation for Multi User Spectrum Sharing with Parameter Uncertainty",
		"PIID": "-394342",
		"Scheme": "First Grant Scheme",
		"StartDate": "30/06/2013",
		"EndDate": "29/12/2014",
		"Value": "99973",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "-394342", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Spectrum sharing (SpS) technology has continued to develop since its introduction as an alternative spectrum management policy by the US's Federal Communication Commission (FCC) in 2002. Radio resource allocation (RRA) is a crucial function in SpS technology which allocates the available radio spectrum to the secondary users and adjusts the transmission parameters such as secondary user transmit power and rate in an efficient way so that the system performance is maximised. RRM is implemented in the secondary network and needs to address the following significant technical challenges. Input parameters to RRA algorithms include spectrum availability and channel gains. Spectrum availability is evaluated using a spectrum sensing mechanism with a certain level of accuracy. The required channel gains are also evaluated using signal processing techniques with a given level of estimation errors. One of the main challenges is that in most cases there is either no or very limited signalling between the primary and secondary networks. This makes parameter estimation a very complicated task with a limited level of accuracy, particularly for those parameters which needed to be measured/estimated in the primary network. Uncertain parameters may result in either decreasing the efficiency, or infeasibility of an RRA algorithm that is designed assuming perfect parameters.  It is also required to address efficient and adaptive radio resource allocation among multiple secondary users. Other challenges include distributed implementations as well as developing analytical methods for performance evaluations of RRA algorithms. Performance analysis of RRA function sheds light on the fundamental performance bounds such as maximum achievable spectrum utilization, scaling property regarding the number of secondary users as well as other system trade-offs. Addressing these challenges is required for efficient adoption of SpS technology in full scale commercial wireless communication systems.  The applicant's proposal aims at the theoretical investigation of the impact of parameter uncertainty on the efficiency of the radio resource allocation in multi user secondary service spectrum sharing systems. This proposal also includes development of novel RRA techniques based on the theoretical investigations that differs from many conventional studies, in that practical concerns including robustness to uncertain input parameters, minimizing the level of inter and intra system signalling overhead and impact of multiple secondary service users are also taken into consideration.   The overall aims of this research proposal are: i) to develop an analytical framework which incorporates physical and higher layer functionalities into the system model in a multi secondary user environment and is able to model the impact of uncertainty in the parameters on the network performance,  ii) to develop practical RRA algorithms based on this analytical framework to manage the impact of parameter uncertainty, and iii) to obtain fundamental performance limits for the multiple secondary service with uncertain parameters. The applicant's ultimate goal is to make SpS technology available to general public. "
	},
	{
		"grant":453,
		"ID": "EP/K005162/1",
		"Title": "Parameterized Algorithmics for the Analysis and Verification of Constrained Workflow Systems",
		"PIID": "54046",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2016",
		"Value": "602797",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "44",
		"Investigators":[
		{"ID": "54046", "Role": "Principal Investigator"},
		{"ID": "3132", "Role": "Co Investigator"},
		{"ID": "112700", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Many business processes (or workflows) can be modeled as a set of computational tasks or steps. Some of these steps may be performed by a human agent (a 'user') or a software agent operating under the control of a user. There may be some flexibility in the order in which those steps may be completed: one step, for example, can be performed in parallel with some other step(s), while another step may be required to be performed before some other step(s). In addition to such 'control flow' constraints on a business process, we may wish to impose restrictions on the users who perform the steps in the workflow. Some steps, for example, may be commercially sensitive and must be performed by a senior member of the user population. Requirements of this nature can be captured in an authorization policy stating which users are allowed to perform which steps. It is comparatively easy to ensure that such policies are enforced. However, we may wish to impose more complex rules on the business process. We might require, for example, that the same user does not perform a particularly sensitive combination of steps; this is sometimes known as the 'two-man rule' for 'four-eyes rule'.  The combined effect of all these constraints on a business process is that it may not be possible to allocate users to steps in such a way that all constraints are satisfied simultaneously. It is known that determining whether a workflow specification (defining the control flow and authorization constraints) is satisfiable is a hard computational problem. Moreover, it is clearly important to determine whether a workflow specification is satisfiable; there is no point in defining a workflow specification that is not satisfiable. An algorithm for deciding the satisfiability of a workflow specification (a static analysis) must run in a reasonable amount of time.  The development of efficient algorithms to determine workflow satisfiability will be one of the objectives of the proposed research. In particular, we will examine the workflow satisfiability problem (WSP) from the perspective of fixed-parameter tractability. A hard computational problem, such as the WSP, admits no algorithm with run-time polynomial in the size of the input to the problem. Informally, fixed-parameter tractable (FPT) problems are hard problems for which there exist algorithms whose run-time is exponential in only one of the parameters that comprise the input to the problem. If that parameter is small in practice and the exponential function of the parameter is not growing too fast, then an FPT algorithm will be efficient. Wang and Li (2010) have shown that a restricted form of WSP is FPT, but their algorithm is too slow to be of practical value. We will develop faster FPT algorithms that can be used in practice.  Many workflow specifications of practical relevance do not have the form studied by Wang and Li. Thus, another objective of this project is to understand what other forms of WSP are FPT. This objective includes the study of richer types of constraints and more complex control flow patterns. Once we have an understanding of what forms of WSP are FPT we will then evaluate the performance of FPT algorithms against existing solutions.  A novel aspect of our proposal is to view WSP as a type of constraint satisfaction problem (CSP). Despite the fact that WSP exhibits features that make it unusual as a CSP, we expect that complexity results, techniques and algorithms for CSP will still provide new tools for tackling WSP. We believe that the investigators' expertise and experience in workflow modelling, algorithmics, parameterized complexity and constraint satisfaction will yield fascinating insights into difficult theoretical problems and provide practical and relevant tools that could be deployed in commercial business information systems. "
	},
	{
		"grant":454,
		"ID": "EP/K005952/1",
		"Title": "Human Vision: Relationship to Three-Dimensional Surface Statistics of Natural Scenes",
		"PIID": "-64990",
		"Scheme": "Standard Research",
		"StartDate": "30/06/2013",
		"EndDate": "29/06/2016",
		"Value": "505830",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Psychology",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-64990", "Role": "Principal Investigator"},
		{"ID": "-117281", "Role": "Co Investigator"},
		{"ID": "-295687", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The human visual system has been fine-tuned over generations of evolution to operate effectively in our particular environment, allowing us to form rich 3D representations of the objects around us. The scenes that we encounter on a daily basis produce 2D retinal images that are complex and ambiguous. From this input, how does the visual system achieve the immensely difficult goal of recovering our surroundings, in such an impressively fast and robust way?   To achieve this feat, humans must use two types of information about their environment. First, we must learn the probabilistic relationships between 3D natural scene properties and the 2D image cues these produce.  Second, we must learn which scene structures (shapes, distances, orientations) are most common, or probable in our 3D environment. This statistical knowledge about natural 3D scenes and their projected images allows us to maximize our perceptual performance. To better understand 3D perception, therefore, we must study the environment that we have evolved to process. A key goal of our research is to catalogue and evaluate the statistical structure of the environment that guides human depth perception. We will sample the range of scenes that humans frequently encounter (indoor and outdoor environments over different seasons and lighting conditions).  For each scene, state-of-the-art ground based Light Detection and Ranging (LiDAR) technology will be used to measure the physical distance to all objects (trees, ground, etc.) from a single location - a 3D map of the scene. We will also take High Dynamic Range (HDR) photographs of the same scene, from the same vantage point. By collating this paired 3D and 2D data across numerous scenes we will create a comprehensive database of our environment, and the 2D images that it produces. By making the database publicly available it will facilitate not just our own work, but research by human and computer vision scientists around the world who are interested in a range of pure and applied visual processes. There is great potential for computer vision to learn from the expert processor that is the human visual system: computer vision algorithms are easily out-performed by humans for a range of tasks, particularly when images correspond to more complex, realistic scenes. We are still far from understanding how the human visual system handles the kind of complex natural imagery that defeats computer vision algorithms. However, the robustness of the human visual system appears to hinge on: 1) exploiting the full range of available depth cues and 2) incorporating statistical 'priors': information about typical scene configurations. We will employ psychophysical experiments, guided by our analyses of natural scenes and their images, to develop valid and comprehensive computational models of human depth perception. We will concentrate our analysis and experimentation on key tasks in the process of recovering scene structure - estimating the location, orientation and curvature of surface segments across the environment. Our project addresses the need for more complex and ecologically valid models of human perception by studying how the brain implicitly encodes and interprets depth information to guide 3D perception. Virtual 3D environments are now used in a range of settings, such as flight simulation and training systems, rehabilitation technologies, gaming, 3D movies and special effects. Perceptual biases are particularly influential when visual input is degraded, as they are in some of these simulated environments. To evaluate and improve these technologies we require a better understanding of 3D perception. In addition, the statistical models and inferential algorithms developed in the project will facilitate the development of computer vision algorithms for automatic estimation of depth structure in natural scenes. These algorithms have many applications, such as 2D to 3D film conversion, visual surveillance and biometrics."
	},
	{
		"grant":455,
		"ID": "EP/K006193/1",
		"Title": "Trustworthy Robotic Assistants",
		"PIID": "29766",
		"Scheme": "Standard Research",
		"StartDate": "01/03/2013",
		"EndDate": "31/08/2016",
		"Value": "378892",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "29766", "Role": "Principal Investigator"},
		{"ID": "53674", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The development of robotic assistants is being held back by the lack of a coherent and credible safety framework.  Consequently, robotic assistant applications are confined either to research labs or, in practice, to scenarios where physical interaction with humans is purposely limited, e.g. surveillance, transport or entertainment (e.g. museums).  In the domestic/personal domain, however, interactions take place in an informal, unstructured, and typically highly complex way. Even in more constrained industrial settings, the need for reduced manufacturing costs is motivating the creation of robots capable of much greater flexibility and intelligence. These robots need to work near to, be taught by, and perhaps even interact physically with, human co-workers.  So, how can we enhance robots so that they can participate in sophisticated interactions with humans in a safe and trustworthy manner?  This is a fundamental research question that must be addressed before the traditional physical safety barrier between the robot and the human can be removed, which is essential for close-proximity human-robot interactions.  How, then, might we establish such safety arguments? Intrinsically safe robots must incorporate safety at all levels (mechanical; control; and human interaction). There has been some work on safety at lower, mechanical, levels to severely restrict movements near humans, without regard to whether the movements are 'safe' or not.  Crucially, no one has yet tackled the high-level behaviours of robotic assistants during interaction with humans, i.e.  not only whether the robot makes safe moves, but whether it knowingly or deliberately makes unsafe moves. This is the focus of our project.  Formal verification exhaustively analyses all of the robot's possible choices, but uses a vastly simplified environmental model. Simulation-based testing of robot-human interactions can be carried out in a fast, directed way and involves a much more realistic environmental model, but is essentially selective and does not take into account true human interaction.  Formative user evaluation provides exactly this validation, constructing a comprehensive analysis from the human participant's point of view.  It is the aim of our project to bring these three approaches together to tackle the holistic analysis of safety in human-robot interactions. This will require significant research in enhancing each of the, very distinct, approaches so they can work together and subsequently be applied in realistic human-robot scenarios. This has not previously been achieved. Developing strong links between the techniques, for example through formal assertions and interaction hypotheses, together with extension of the basic techniques to cope with practical robotics, is the core part of our research.  Though non-trivial to achieve, this combined approach will be very powerful. Not only will analysis from one technique stimulate new explorations for the others, but each distinct technique actually remedies some of the deficiencies of another. Thus, this combination provides a new, strong, comprehensive, end-to-end verification and validation method for assessing safety in human-robot interactions. "
	},
	{
		"grant":456,
		"ID": "EP/K007432/1",
		"Title": "Realistic Shape from Shading",
		"PIID": "10226",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2016",
		"Value": "318944",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "28",
		"Investigators":[
		{"ID": "10226", "Role": "Principal Investigator"},
		{"ID": "52643", "Role": "Co Investigator"},
		{"ID": "-193862", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Shape-from-shading (SFS) is a classic problem in computer vision. It aims to estimate 3D surface shape from the variations in shading in a single photographic image. The fact that it recovers shape using only a single image makes SFS attractive to a wide range of applications, especially when other 3D imaging techniques such as stereo or depth scanners are difficult to apply. Example applications can be found in topography analysis of SAR (synthetic aperture radar) images, reconstruction of medical images, inspection of microelectronics, CAD systems, and the entertainment industry. However, despite over four decades research, SFS still remains a challenging problem which is underused in real world problems, due to a lack of robustness, and sometimes implausible results. A good solution is pressing and challenging. This project intends to develop a robust and practical SFS algorithm for accurate shape recovery from real-world images.  The reasons for SFS's current poor performance on real-world images have several underlying causes. The first is that the classic assumptions of orthographic projection, Lambertian reflection, and simple lighting models are inaccurate for real-world surfaces. The second reason is that SFS is an underconstrained problem: the human visual system recovers shape not only from shading, but also from outlines, shadows, and prior experience. In computer vision, little work has considered the combination of shape from shading with other visual cues and human interactions. The third and largely overlooked reason is that many real surfaces are not smooth, and have detailed features. Most existing SFS algorithms only apply to images of smooth surfaces, and tend to over-smooth any features.  Based on these observations, this project will integrate techniques from such areas as feature-aware image filtering, shape from line drawing, and user interaction, to achieve more accurate shape recovery from sophisticated real-world images. An interactive platform for SFS will be developed for realistic applications. The outcome of the research will be tested on various applications in CAD and computer vision: specifically, as part of the project, we will explore the applications to bas-relief generation, and face recognition."
	},
	{
		"grant":457,
		"ID": "EP/K007491/1",
		"Title": "Multisource audio-visual production from user-generated content",
		"PIID": "-17667",
		"Scheme": "Standard Research",
		"StartDate": "08/02/2013",
		"EndDate": "07/08/2015",
		"Value": "362889",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "-17667", "Role": "Principal Investigator"},
		{"ID": "-26550", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": " The pervasiveness of amateur media recorders either embedded in smartphones or as stand-alone devices is revolutionizing the way events are captured and reported. The aim of this project is to devise intelligent editing and production algorithms based on new signal processing techniques for processing multi-view user-generated content.   The explosion of shared video content offers the opportunity for new ways of not only analysing but also timely reporting stories, ranging from disaster scenes and protests to music concerts and sports events. However, the large amount of data increasingly available and their varying quality makes the selection and editing of appropriate multimedia items in a timely manner very difficult thus strongly limiting the opportunity to harvest this data for security, cultural and entertainment applications. There is an urgent need to investigate and develop new ways to help or replace what used to be the role of a producer/director in this rapidly changing landscape. In particular, there is the need to automate production tasks and to generate new and high-quality content from multiple views.   The key aspect of the project is the integration of audio and visual inputs that support each other in reaching objectives that would otherwise be impossible using only one modality.  We will focus on a set of relevant event-types: sports, music shows and crowd scenes.  We will devise novel multisource processing techniques to improve audiovisual production and to enable synchronisation processing. This will in turn allow generation of novel and higher quality audio-visual rendering of captured events."
	},
	{
		"grant":458,
		"ID": "EP/K00767X/1",
		"Title": "Advanced MIMO Radar Development for Geophysical Imaging Applications",
		"PIID": "33271",
		"Scheme": "Standard Research",
		"StartDate": "04/03/2013",
		"EndDate": "03/03/2016",
		"Value": "305269",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "33271", "Role": "Principal Investigator"},
		{"ID": "-216343", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Mass movement flows are a significant natural hazard throughout the world and yet our ability to predict their behaviour and plan for their effects is limited, in part, by our lack of understanding of their flow dynamics and lack of detailed experimental data, particularly of sub-surface movements. Similarly, polar ice shelf depletion, particularly in Antarctica, is a significant and increasing contributor to climate change and sea level rise, but is difficult to readily measure with conventional techniques.  This research, involving collaboration between electronic engineers at UCL and geophysicists at the University of Sheffield, aims to develop and deploy a sophisticated, versatile, modular phased array radar system that is able to form detailed 2 or 3 dimensional images of the dense flow in snow avalanches, by penetrating the powder cloud, which is not possible using optical instruments. This technology can also perform mm-precision cross-sectional imaging through antarctic ice shelves to monitor changes, in the basal layer depth in particular, over periods of seasons or years. This will provide invaluable new experimental data to inform the flow laws in the case of mass movement flows and models governing ocean circulation and its impact on ice shelf depletion in the case of ice shelf monitoring.  At present in snow avalanches, opto-electronic instruments can provide flow information at a single point in the dense flow only. For other flows such as pyroclastic density currents there are almost no data available. Prior to our recent collaboration, Doppler radar for snow avalanches and Strombolian eruptions provided crude images of the flow speed, averaged over 50 m and only giving an overall measure of the velocity magnitude (with no information on direction). Our instrument reduces the averaging distance to only 1 m so that, for the first time, information on individual blocks in the flow can be obtained and assessed in relation to their significance for the overall flow dynamics.  In addition, a MIMO phased array approach will be adopted that will provide high azimuth resolution, of the order of 1 degree, to yield both range and azimuth resolution of these flows, providing a wealth of new data for researchers in this area. For Antarctic ice shelf imaging, satellite radar imaging is unable to offer high precision estimates of ice shelf depth due to the large stand-off distance, and no precision portable ground penetrating radar instruments are currently available This is addressed by means of a phase-sensitive FMCW processing technique recently developed at UCL will be adopted to provide mm-resolution of features within the ice shelf, including the basal layer, and with a 2D cross-sectional imaging capability. This will produce data of unrivalled clarity and precision in this application.  The new experimental data provided by these instruments will lead to improved models for these processes by constraining the coefficients to reasonable values and perhaps rejecting some proposed laws outright, resulting in more accurate modelling of geophysical mass flows and of the influence of ocean circulation in ice shelf depletion. In turn, this will improve risk analyses and the effect and design of defensive structures. The expected outcome of this study will considerably improve our understanding of flow movement and polar ice shelf depletion and sustain and boost the status of UK research in these areas to internationally-leading standards.  "
	},
	{
		"grant":459,
		"ID": "EP/K008161/1",
		"Title": "Predictive Reliability Modelling and Characterization of Silicon Carbide Power MOS-Transistors in Grid-Connected Voltage Source Converters",
		"PIID": "-408327",
		"Scheme": "First Grant Scheme",
		"StartDate": "17/04/2013",
		"EndDate": "16/04/2015",
		"Value": "99765",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "31",
		"Investigators":[
		{"ID": "-408327", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern society's reliance on electrical energy is almost as critical as its reliance on food and water. In the UK, majority of the electrical energy is generated by electrical machines powered by fossil fuels. The principles of sustainability require that the energy consumption pattern changes since fuel reserves are finite. Furthermore, shifting away from fossil fuels is integral to the de-carbonisation of the economy which is critical for tackling global warming. To this end, substantial progress has been made on harnessing wind, solar and other renewable energy sources. However, change is also required in the manner in which electricity is transmitted and distributed through the grid. Renewable energy is usually intermittent and unpredictable, characteristics which make it unsuitable for direct connection with the electric grid. Renewable sources like wind and solar energy can only interface with the electrical grid through power electronics. Power electronics is required for the processing and conditioning of electrical energy so as to make it complaint with the grid. At the heart of power electronics, we have power semiconductor devices which have traditionally been fabricated out of silicon bipolar technology. However, silicon is reaching its fundamental limits in terms of energy density, hence, moving to advanced power materials like silicon carbide can give added impetus to the field of power electronics. Silicon carbide is a wide bandgap semiconductor with a higher critical electric field and higher thermal conductivity. In this project, the reliability of power converters implemented in Silicon-Carbide MOS-Transistor technology is investigated. These power converters will typically be used in off-shore wind-farms for power conversion in high voltage DC transmission (HVDC) systems. The converters can also be used in flexible AC transmission systems like STATCOMS (Static Compensators). The overall objective is to characterize the reliability of power converters implemented in silicon-carbide MOS transistors.   "
	},
	{
		"grant":460,
		"ID": "EP/K008250/1",
		"Title": "An Active Learning Approach to Network Inference",
		"PIID": "88227",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2016",
		"Value": "268375",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering Mathematics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "88227", "Role": "Principal Investigator"},
		{"ID": "-21859", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The understanding of pathway structures is crucial to our understanding of the functional organisation of genes and proteins. Abnormally functioning pathways underlie many human diseases. Given the extent of noise in biological datasets, and limited amounts of data available, unsupervised determination of network topology is a substantially under-determined inference problem. Consequently, in this project, we focus on network completion, which is a supervised learning task involving multiple types of data. Starting from a set of known links and non-links we construct a classifier which predicts and ranks further possible functionmal links for experimental validation. There will be a parallel experimental programme following an active learning approach to network inference,  that is, predictions of functional links will be investigated which will, in turn, be used to improve the predictor. To provide a focus for the accompanying laboratory work, by the medical researchers associated with the project, our principal aim will be the discovery and validation of pathway structures associated with hypertension. Hypertension is the most common cause of preventable disease in the developed world."
	},
	{
		"grant":461,
		"ID": "EP/K008412/1",
		"Title": "Heat induced phase change exchange coupled composite media (HIP-ECC)",
		"PIID": "46162",
		"Scheme": "Standard Research",
		"StartDate": "31/03/2013",
		"EndDate": "30/03/2016",
		"Value": "320286",
		"ResearchArea": "Condensed Matter: Magnetism and Magnetic Materials",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "46162", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Today the data storage market is dominated by magnetic hard disk drives (HDDs) due to their cost effectiveness and utility compared with competitor technologies (eg. solid state flash drives). The basic layout of a HDD has remained the same since they were invented more than 50 years ago, but the technology of the components has changed beyond imagination and this has led to a 200 million-fold increase in data storage capacity since the first hard disk drives. Today's information society would not be possible without this extraordinary accomplishment which has occurred as a result of scientific advancement and engineering prowess working hand in hand. As an example, the discovery of the giant magnetoresistance (GMR) effect used in HDD data readers for which the Nobel Prize in Physics was awarded in 2007. This project aims to explore new ideas for magnetic disk media to allow a continuation of the phenomenal growth in data storage capacities that is required for societal progress in the future.  The success of HDDs has been built on the scientific and technological progress that has allowed each of the components to be scaled to ever decreasing size. The materials used in conventional magnetic recording media are nanoscale (~8nm) granular magnets where a single bit is stored on multiple grains using an electromagnet designed to fly a few nanometres above the surface of the disk. These grains cannot be scaled down in size indefinitely and as the volume of the grain is limited by the super-paramagnetic effect, where individual magnetic grains may reverse due to thermal excitations, results in data loss and device failure. Recent research has focussed on circumventing this problem.  In this joint project between the University of Manchester and the University of Sheffield we propose a new design for a tuneable exchange coupled composite (ECC) medium for heat assisted magnetic recording (HAMR); a heat induced phase change ECC medium (HIP-ECC). An exchange coupled composite medium typically consists of several nanometre thick layers of magnetically hard and soft materials in intimate contact. Magnetic switching of the hard layer is assisted by coupling with the soft layer, resulting in a lower overall switching field and a higher thermal stability based on the properties of the hard layer. The proposed tuneable ECC medium has an intermediate layer between the soft and hard layer that will allow control of the exchange energy/coupling between both layers using a change in temperature. This thermal switch will allows us to dramatically reduce the heat requirements for recording, thereby avoiding many of the difficulties of more conventional approaches to HAMR. The key advantage of this design is that an extremely thermally stable material can be used to store the data with no loss in writeability.  HAMR is the leading technological candidate for achieving higher data storage densities in magnetic recording. This technology has the advantage that it can be used with both existing and future data recording technologies i.e. conventional magnetic media and bit patterned media (the magnetic material is patterned into individual nanometre-scale islands, each recording a single bit of data). HAMR makes use of the reduction in the magnetic field required to switch a ferromagnet at elevated temperatures. This phenomenon allows the use of the highest magneto-crystalline anisotropy materials such as highly ordered FePt and CoPt alloys to maintain long term stability. Magneto-crystalline anisotropy is an internal property of the material that determines its magnetic thermal stability.  Through this project we aim to deliver scientific progress that will result in clear applications in magnetic data storage, enabling the next generation of HDD products to be produced. Using this technology data storage density can theoretically be increased to 20Tbit/in2, 40 times larger than current commercial disk drives."
	},
	{
		"grant":462,
		"ID": "EP/K008781/1",
		"Title": "Space weather effects on airline communications in the high latitude regions",
		"PIID": "34584",
		"Scheme": "Standard Research",
		"StartDate": "15/05/2013",
		"EndDate": "14/05/2016",
		"Value": "347135",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "66",
		"Investigators":[
		{"ID": "34584", "Role": "Principal Investigator"},
		{"ID": "51634", "Role": "Co Investigator"},
		{"ID": "-80054", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Efficient air traffic management depends on reliable communications between aircraft and the air traffic control centres.  However there is a lack of ground infrastructure in the Arctic to support communications via the standard VHF links (and over the Arctic Ocean such links are impossible) and communication via geostationary satellites is not possible above about 82 degrees latitude because of the curvature of the Earth. Thus for the high latitude flights it is necessary to use high frequency (HF) radio for communication. HF radio relies on reflections from the ionosphere to achieve long distance communication round the curve of the Earth. Unfortunately the high latitude ionosphere is affected by space weather disturbances that can disrupt communications. These disturbances originate with events on the Sun such as solar flares and coronal mass ejections that send out particles that are guided by the Earth's magnetic field into the regions around the poles. During such events HF radio communication can be severely disrupted and aircraft are forced to use longer low latitude routes with consequent increased flight time, fuel consumption and cost. Often, the necessity to land and refuel for these longer routes further increases the fuel consumption. The work described in this proposal cannot prevent the space weather disturbances and their effects on radio communication, but by developing a detailed understanding of the phenomena and using this to provide space weather information services the disruption to flight operations can be minimised.   The occurrence of ionospheric disturbances and disruption of radio communication follows the 11-year cycle in solar activity. During the last peak in solar activity a number of events caused disruption of trans-Atlantic air routes. Disruptions to radio communications in recent years have been less frequent as we were at the low phase of the solar cycle. However, in the next few years there will be an upswing in solar activity that will produce a consequent increase in radio communications problems. The increased use of trans-polar routes and the requirement to handle greater traffic density on trans-Atlantic routes both mean that maintaining reliable high latitude communications will be even more important in the future.    "
	},
	{
		"grant":463,
		"ID": "EP/K009362/1",
		"Title": "Bayesian Inference for Big Data with Stochastic Gradient Markov Chain Monte Carlo",
		"PIID": "-197015",
		"Scheme": "Standard Research",
		"StartDate": "01/08/2013",
		"EndDate": "31/07/2016",
		"Value": "200070",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Statistics",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-197015", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We are in the midst of an information revolution, where advances in science and technology, as well as the day-to-day operation of successful organisations and businesses, are increasingly reliant on the analyses of data. Driving these advances is a deluge of data, which is far outstripping the increase in computational power available. The importance of managing, analysing, and deriving useful understanding from such large scale data is highlighted by high-profile reports by McKinsey and The Economist as well as other outlets, and by the EPSRC's recent ICT priority of 'Towards an Intelligent Information Infrastructure'.  Bayesian analysis is one of the most successful family of methods for analysing data, and one now widely adopted in the statistical sciences as well as in AI technologies like machine learning. The Bayesian approach offers a number of attractive advantages over other methods: flexibility in constructing complex models from simple parts; fully coherent inferences from data; natural incorporation of prior knowledge; explicit modelling assumptions; precise reasoning of uncertainties over model order and parameters; and protection against overfitting.   On the other hand, there is a general perception that they can be too slow to be practically useful on big data sets. This is because exact Bayesian computations are typically intractable, so a range of more practical approximate algorithms are needed, including variational approximations, sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). MCMC methods arguably form the most popular class of Bayesian computational techniques, due to their flexibility, general applicability and asymptotic exactness.  Unfortunately, MCMC methods do not scale well to big data sets, since they require many iterations to reduce Monte Carlo noise, and each iteration already involves an expensive sweep through the whole data set.  In this project we propose to develop the theoretical foundations for a new class of MCMC inference procedures that can scale to billions of data items, thus unlocking the strengths of Bayesian methods for big data.  The basic idea is to use a small subset of the data during each parameter update iteration of the algorithm, so that many iterations can be performed cheaply.  This introduces excess stochasticity in the algorithm, which can be controlled by annealing the update step sizes towards zero as the number of iterations increases.  The resulting algorithm is a cross between an MCMC and a stochastic optimization algorithm.  An initial exploration of this procedure, which we call stochastic gradient Langevin dynamics (SGLD), was initiated by us recently (Welling and Teh, ICML 2011).    Our proposal is to lay the mathematical foundations for understanding the theoretical properties of such stochastic MCMC algorithms, and to build on these foundations to develop more sophisticated algorithms.  We aim to understand the conditions under which the algorithm is guaranteed to converge, and the type and speed of convergence.  Using this understanding, we aim to develop algorithmic extensions and generalizations with better convergence properties, including preconditioning, adaptive and Riemannian methods, Hamiltonian Monte Carlo methods, Online Bayesian learning methods, and approximate methods with large step sizes.  These algorithms will be empirically validated on real world problems, including large scale data analysis problems for text processing and collaborative filtering which are standard problems in machine learning, and large scale data from ID Analytics, a partner company interested in detecting identity theft and fraud."
	},
	{
		"grant":464,
		"ID": "EP/K009788/1",
		"Title": "Network on Computational Statistics and Machine Learning",
		"PIID": "84313",
		"Scheme": "Network",
		"StartDate": "24/06/2013",
		"EndDate": "23/06/2016",
		"Value": "104530",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Statistical Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "84313", "Role": "Principal Investigator"},
		{"ID": "-2886", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of this network is to establish the UK as the world leading authority in the joint area of Computational Statistics and Machine Learning (CompStat & ML) by advancing communication, interchange and collaboration within the UK between the disciplines of Computational Statistics (CompStat) and Machine Learning (ML).  The UK has tremendous research strength and depth that is widely acknowledged as world leading in both the individual areas of Computational Statistics and Machine Learning. Despite each of these fields of research developing, largely, independently and having their own separate journals, international societies, conferences and curricula both areas of investigation share a common theoretical foundation based on the underlying formal principles of mathematical statistics and statistical inference. As such there is a natural diffusion of concepts, research and individuals between both disciplines. This network will seek to formalise as well as enhance this interchange and in the process capitalise on important synergies that will emerge from the combined and shared research agendas of CompStat & ML."
	},
	{
		"grant":465,
		"ID": "EP/K009923/1",
		"Title": "Empirical Modelling of Business Process Patterns with Ontologies",
		"PIID": "-86462",
		"Scheme": "Standard Research",
		"StartDate": "18/06/2013",
		"EndDate": "17/09/2016",
		"Value": "318135",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Information Systems Computing and Maths",
		"OrgID": "129",
		"Investigators":[
		{"ID": "-86462", "Role": "Principal Investigator"},
		{"ID": "70008", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A fundamental problem in the way business process modelling (BPM) is carried out today is the lack of explicit and systematic reuse of previously developed models. Although all business processes possess unique characteristics, they also do share many common properties making it possible to classify business processes into generally recognised patterns of organisational behaviour. Patterns, or general solutions to recurring problems, have become a widely accepted architectural technique in software engineering, however their use in business process modelling is quite limited. Given the documented benefits that patterns have produced in software engineering (for example, increased productivity and acceleration of the learning curve), it can be assumed that their adoption in BPM could yield similar advantages.  However, the systematic adoption of patterns in BPM cannot be a simple transposition of the experience acquired by the design patterns community in software engineering. This is due to some essential differences between business modelling and software design. While the latter involves the representation of an engineered artefact (i.e., software), the former concerns the representation of behaviour of a real world system (i.e., the business organisation) that grows in an emergent manner. Therefore, while software design patterns are normally based on engineering experience, the discovery of generalised business behaviour should be preferably conducted in a more empirical manner via the analysis of organisational process data in all its forms.  Empiricism is currently not the basis for the discovery of patterns for BPM and no systematic methodology for collecting and analysing process models of business organisations currently exists. This project aims at developing such a methodology. Moreover, given the real world nature of organisations, ontology is adopted as the principal driver of the methodology so as to interpret business process data, discover recurrent behaviour and model the generalised patterns found.   This project is called Empirical Modelling of Business Process Patterns with Ontologies (EMBO). The assumption underpinning the project is that business organisations will be capable of more flexibly adapting themselves to changing operational practices thanks to the generalised nature and semantic expressiveness of ontology-based business process patterns. "
	},
	{
		"grant":466,
		"ID": "EP/K009982/1",
		"Title": "Advanced Controllable Raman Lasers",
		"PIID": "-80240",
		"Scheme": "First Grant Scheme",
		"StartDate": "03/12/2012",
		"EndDate": "02/12/2014",
		"Value": "100626",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "48",
		"Investigators":[
		{"ID": "-80240", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Over the last decade, solid-state lasers have been the subject of great interest with a wide range of applications thanks mostly to their superior output-power-to-wall-plug efficiency. However, the limited availability of colours (wavelengths) produced by these lasers forces the user to compromise by utilising a wavelength that is not ideal for the targeted application. Laser systems based on a nonlinear process, called Stimulated Raman Scattering, offer a simple solution to this need. However, the performance and usability of these so-called Raman lasers have traditionally been limited by thermal distortions inherent to the nonlinear process.  This project will, for the first time, investigate the implementation of the adaptive control techniques - typically used in astronomy- inside Raman lasers to significantly alleviate this thermal issue. In this way, the behaviour and performance of the laser can be remotely controlled and optimised resulting in superior performance in terms of output power, beam quality and usability. This offers the prospect of several genuine breakthroughs including a range of world-firsts and world-records as well as the transfer of these laboratory-based systems into an engineering context.  These significantly enhanced systems will address a wide range of applications including astronomy, environmental monitoring, cosmetics and medicine. For instance, the treatment of a variety of skin diseases such as psoriasis or port wine stain removal will strongly benefit from this project.  Finally, knowledge transfer is an important feature of this project with a full strand of activity dedicated to it. The transfer of this technology will be performed within two high profile research groups at Macquarie University, Australia and at the University of Strathclyde. An industrial collaboration with M Squared Lasers will also take place, particularly targeting commercialisation of the final demonstrator. "
	},
	{
		"grant":467,
		"ID": "EP/K01000X/1",
		"Title": "Efficient Algorithms for Mechanism Design Without Monetary Transfer",
		"PIID": "-194260",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2013",
		"EndDate": "31/05/2016",
		"Value": "329764",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-194260", "Role": "Principal Investigator"},
		{"ID": "59559", "Role": "Co Investigator"},
		{"ID": "-416286", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Matching problems involve assigning agents to commodities, such as junior doctors to hospitals, or kidney patients to donors, based on preference lists expressed by the agents. They have important large-scale practical applications in centralised matching schemes that perform allocations of these types in a number of countries.    When designing mechanisms for these matching problems, two issues present research challenges going forward: (i) optimality (based on the social welfare of the agents involved) and (ii) truthfulness.    These two desirable properties may be conflicting. This led Procaccia and Tennenholtz to introduce the idea of approximate mechanism design without money - this involves the design of truthful mechanisms which may lead to a loss of optimality in the constructed solutions, but such loss can be theoretically bounded.    In the above practical applications, monetary transfer is not appropriate. Two further applications that motivate optimal truthful mechanisms, where monetary transfer is not allowed, involve combinatorial auctions and facility location problems.    This proposal lies in the area of Algorithmic Mechanism Design (AMD), part of the wider field of Computational Social Choice Theory.  We are interested in particular in mechanism design without money.    The famous Gibbard-Satterthwaite Theorem roughly states that, in environments without money, the only truthful mechanism is (the trivial) dictatorship. However, it does not apply whenever the domain of preferences of the individuals over the possible outcomes is restricted. That is the case in most real-world applications, where indeed more interesting mechanisms occur.    We plan to advance the area of AMD without payments by tackling combinatorial auctions, junior doctor placement and reviewer assignment, kidney exchange and facility location problems. We will develop new truthful mechanisms whilst measuring their degradation in performance as compared to previous (non-truthful mechanisms). In particular, we will investigate the trade-off between truthfulness and optimality when considering approximate mechanism design.    New software will arise from this work and we have routes for exploiting it through existing University of Glasgow collaborations with the NHS (concerning the assignment of junior doctors to hospitals as part of the Scottish Foundation Allocation Scheme, and the assignment of kidney donors to patients via the National Living Donor Kidney Sharing Schemes)."
	},
	{
		"grant":468,
		"ID": "EP/K011383/1",
		"Title": "3-Dimensional Wearable Patch Antennas with Improved Bandwidth and Efficiency for Athlete, Patient, Firefighter and Soldier Applications",
		"PIID": "-128441",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/05/2013",
		"EndDate": "31/01/2015",
		"Value": "99290",
		"ResearchArea": "RF & Microwave Communications",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90",
		"Investigators":[
		{"ID": "-128441", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We live in a wireless world where we demand the ability to communicate wherever we are. This level of convenience can only be achieved by replacing wires with antennas capable of reliably transmitting signals through the air. To continue to meet this demand for wireless, discrete and robust communication systems, designers and commercial pioneers are conceptualising new applications that will integrate antennas into clothing. These concepts are of particular benefit for certain groups who have specific requirements for communication systems including the emergency services, military, elite athletes, patients and fashion innovators. Realising these concepts will expand an exciting new manufacturing sector tying together innovative textiles and advanced electronics manufacturing.  This research has the potential to transform modern communications. Communication devices that are integrated into clothing will be light, immediately accessible, easy to use, robust and impossible to leave behind. In safety critical scenarios, such as search and rescue or battlefield situations, the risk of communication breakdown between protagonists is reduced and the ability to locate people at risk is greatly increased. In less immediately critical scenarios, such as long term care of dementia patients, integrated antennas can be used to track patients should they wander away from the safety of their own home environments. Similarly, sufferers of chronic diseases such as heart disease could conceivably wear transmitting monitoring devices in their home environments to apprise their supervising physicians of their ongoing condition.   Currently, wearable antennas are not commonly found in mainstream markets due to concerns around user comfort; antenna efficiency; the antenna detuning when in proximity to the human body and risks around the absorption of electromagnetic energy into the body. This project addresses these concerns by reducing antenna volume or improving the bandwidth and/or the communication range and will lead towards making the practical integration of fabric antennas a viable commercial reality.  Typically, reducing antenna size can compromise electromagnetic performance (range and bandwidth). This also applies when reducing the height of planar antennas. Convention has decreed that patch antennas are planar; however, the electric fields underneath an antenna are not uniform. The hypothesis of this project is that increasing the height of the antenna in certain locations will particularly benefit antenna performance. Therefore, an optimised 3-D antenna can be designed to maximise the bandwidth and efficiency to volume ratio.   This research project will improve the electromagnetic performance of wearable antennas using optimised 3-D structures in conjunction with high performance materials. A strong consortium of commercial Project Partners ensures the work has impact for UK manufacturing from the outset and that the benefits to the military, sporting and health sectors can be realised.  Loughborough University is home to two of the UK's leading groups in antennas and electromagnetics and enjoys an international reputation for excellence in sports and sports technology. Dr. Whittow, who has more than ten years of experience in antenna design and electromagnetic interactions with the human body, will manage this 21 month project. "
	},
	{
		"grant":469,
		"ID": "EP/K011499/1",
		"Title": "Scalable Automatic Verification of GPU Kernels",
		"PIID": "-215718",
		"Scheme": "First Grant Scheme",
		"StartDate": "30/06/2013",
		"EndDate": "29/06/2014",
		"Value": "100057",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-215718", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Until relatively recently the processing speed of computer systems increased at an exponential rate. Each year it was possible to use computers to solve computational problems that the previous year were out of reach. Around 2005, physical limits stopped this trend: it became infeasible to increase the clock rate of a processor without consuming an exorbitant amount of energy. To counter this, processor manufacturers have since aimed to provide increased performance by designing 'multicore' processors, which consist of two or more processing units on a chip. Many computational tasks are parallelisable, in which case they can be distributed across the cores of a multicore processor.  Recently there has been a trend towards 'many-core' processors, with hundreds or thousands of processing elements. For highly parallel applications, many-core processors can offer a massive speedup. The most readily available many-core processors are graphics processing units (GPUs) from companies such as NVIDIA, AMD, Intel, ARM and Imagination Technologies. GPUs were originally designed to accelerate graphical computations, but have become sufficiently general purpose for accelerating computational tasks in a variety of domains, including financial analysis, medical imaging, media processing and simulation.  GPUs are programmed by writing a 'kernel' function, a program which will be executed by hundreds or thousands of threads running in parallel on the GPU. Parallel threads can communicate using shared memory, and synchronise using barrier statements. Writing correct GPU kernels is more challenging than writing sequential software due to two main problems: data races and barrier divergence. A data race occurs when two GPU threads access a shared memory location, at least one of the threads modifies this location, and no barrier statement separates the accesses. Data races almost always signify bugs in the kernel and lead to nondeterministic behaviour. Barrier divergence occurs when distinct threads reach different barrier statements in the kernel, and leads to deadlock.  Because GPUs are becoming widely used for general purpose software development, there is an urgent need for analysis techniques to help GPU programmers write correct code. Techniques to analyse GPU kernels with respect to data races and barrier divergence would significantly speed up the GPU software development process, leading to shorter time-to-market for GPU-accelerated applications.  In this project we plan to design formal techniques for verifying race- and divergence-freedom for GPU kernels. To be adopted and trusted by industrial practitioners our techniques must be highly automatic, scalable, and based on rigorous semantic foundations.  We plan to achieve these aims by developing a rigorous GPU memory model specification, and a formal semantics for GPU kernel execution that makes no assumptions about the structure of the kernel to be analysed. Based on these semantic foundations, we will design a verification technique that aims to prove absence of data races and barrier divergence by generating a 'contract' for the kernel: a machine-checkable proof that kernel execution cannot lead to these defects. Contract-based verification is modular - each kernel procedure is analysed separately - and thus scalable. We will design a template-based contract generation method that captures domain-specific knowledge about common GPU programming idioms. This will allow efficient verification of GPU kernels that use typical data access patterns. For more intricate kernels that implement highly optimised algorithms, we will design a method based on Craig interpolation. This method will construct a proof of race- and divergence-freedom up to a bounded execution depth, and then attempt to extract a general contract from this proof.  Throughout, we will evaluate our methods using open source and industrial GPU kernels, including kernels provided by our industrial collaborators."
	},
	{
		"grant":470,
		"ID": "EP/K011626/1",
		"Title": "Mixed Criticality Embedded Systems on Many-Core Platforms",
		"PIID": "2307",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "30/09/2016",
		"Value": "652126",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "2307", "Role": "Principal Investigator"},
		{"ID": "83354", "Role": "Co Investigator"},
		{"ID": "-249197", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "An increasingly important trend in the design of real-time and embedded systems is the integration of applications with different levels of criticality onto a common hardware platform. At the same time, these platforms are migrating from single cores to multi-cores and, in the future, many-core architectures. Criticality is a designation of the level of assurance against failure needed for a system component. A mixed criticality system (MCS) is one that has two or more distinct levels. A number of application domains, such as automotive and avionics, and EU initiatives (for example Horizon2020) have identified Mixed Criticality as a key issue in future systems.  The fundamental research question underlying these initiatives is: how, in a disciplined way, to reconcile the conflicting requirements of 'partitioning' for (safety) assurance and 'sharing' for efficient resource usage. This question gives rise to theoretical problems in modelling and verification, and systems problems relating to the design and implementation of the necessary hardware and software run-time controls. This project addresses both the theoretical and related systems questions.  A many-core platform with a scheduled communications medium is the designated platform on which multiple applications (perhaps composed of what are often called 'system of systems') are to be hosted. The isolation of components with different criticality levels is crucial, but the processor interconnects must be shared and be able to transmit messages with different criticality levels. Moreover, applications with different criticality levels must be able to exchange data in a demonstrably safe way.  A defining property of MCS is that the different means of assurance (for each criticality level) give rise to different values for the component's key parameters such as worst-case execution times and worst-case transmission times. In general, the higher the criticality level, the more conservative are the assumptions made about these values. Hence the context (system criticality level) will determine the parameters that must be used to verify (via scheduling analysis) that each core and each inter-connect will perform as required by the temporal constraints of each application. The development of criticality-aware analysis is needed for these systems.  Although total isolation with rigid time-triggered global scheduling is a possible architectural structure, significantly greater resource utilisation and hence reduced power consumption is possible if trade-offs are made between the overall system criticality level and assumptions about each component's run-time behaviour. For example, we require that: in a dual-criticality systems all applications will meet their timing constraints if all components are constrained by (rely on) their low criticality assumptions, but all high-criticality applications must also meet their deadlines if any component exhibits high-criticality behaviour (i.e. the low criticality assumptions can no longer be relied upon).  Previous work (in York and in a number of other international research centres) has explored this trade-off for single processor systems. This project will focus on many-core platforms to: (i) develop the appropriate scheduling schemes (on the cores and interconnects), (ii) derive verification procedures for MCSs, (iii) explore the theoretical bounds of the developed schemes (to show to what extent resource usage and power consumption are improved over a full partitioned system), (iv) develop the necessary run-time controls (to manage the sharing of communication media between the criticality levels), and (v) demonstrate the developed theory via simulations, a FPGA test-bed and an industrially relevant case study.   "
	},
	{
		"grant":471,
		"ID": "EP/K011693/1",
		"Title": "Cognitive Resource Scheduling Designs Towards Green Wireless Systems",
		"PIID": "-177354",
		"Scheme": "Standard Research",
		"StartDate": "07/01/2013",
		"EndDate": "06/01/2016",
		"Value": "300568",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "-177354", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "It is reported that the total energy consumed by the ICT infrastructure of wireless and wired networks takes up over 3 percent of the worldwide electric energy consumption that generated 2 percent of the worldwide CO2 emissions nowadays. It is predicted that in the future a major portion of expanding traffic volumes will be in wireless side. Furthermore, future wireless network systems (e.g., 4G/B4G) are increasingly demanded as broadband and high-speed tailored to support reliable Quality of Service (QoS) for numerous multimedia applications. With explosive growth of high-rate multimedia applications (e.g. HDTV and 3DTV), more and more energy will be consumed in wireless networks to meet the QoS requirements. Specifically, it is predicted that footprint of mobile wireless communications could almost triple from 2007 to 2020 corresponding to more than one-third of the present annual emissions of the whole UK. Therefore, energy-efficient green wireless communications are paid increasing attention given the limited energy resources and environment-friendly transmission requirements globally.  The aim of this project is to improve the joint spectrum and energy efficiency of future wireless network systems using cognitive radio technology along with innovative game-theoretic resource scheduling methods, efficient cross-layer designs and contemporary clinical findings. We plan to consider the health and environmental concerns to introduce power-efficient resource scheduling designs that intelligently exploit the available wireless resources in next-generation systems. Our efforts will leverage applications of cognitive radio techniques to situational awareness of the communications system with adaptive power control and dynamic spectrum allocation. This project will underpin the UK green communication technology by designing environment-friendly joint power and spectrum efficient wireless communication systems."
	},
	{
		"grant":472,
		"ID": "EP/K011766/1",
		"Title": "Testing view-based and 3D models of human navigation and spatial perception",
		"PIID": "-1912",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2013",
		"EndDate": "31/07/2016",
		"Value": "419878",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Psychology and Clinical Lang Sci",
		"OrgID": "113",
		"Investigators":[
		{"ID": "-1912", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The way that animals use visual information to move around and interact with objects involves a highly complex interaction between visual processing, neural representation and motor control. Understanding the mechanisms involved is of interest not only to neuroscientists but also to engineers who must solve similar problems when designing control systems for autonomous mobile robots and other visually guided devices.  Traditionally, neuroscientists have assumed that the representation delivered by the visual system and used by the motor system is something like a 3D model of the outside world, even if the reconstruction is a distorted version of reality. Recently, evidence against such a hypothesis has been mounting and an alternative type of theory has emerged. 'View-based' models propose that the brain stores and organises a large number of sensory contexts for potential actions. Instead of storing the 3D coordinates of objects, the brain creates a visual representation of a scene using 2D image parameters, such as widths or angles, and information about the way that these change as the observer moves. This project examines the human representation of three-dimensional scenes to help distinguish between these two opposing hypotheses.  To do this, we will use immersive virtual reality with freely-moving observers to test the predictions of the 3D reconstruction and 'view-based' models. Head-tracked virtual reality allows us to control the scene the observer sees and to track their movements accurately. Certain spatial abilities have been taken as evidence that the observer must create a 3D reconstruction of the scene in the brain. For example, people are able to view a scene, remember where objects are, walk to a new location and then point back to one of the objects they had seen originally even if it is no longer visible (i.e. people can update the visual direction of objects as they move). However, this capacity does not necessarily require that the brain generate a 3D model of the scene and, as evidence, we will extend view-based models to include this pointing task and others like it. We will then test the predictions of both view-based and 3D reconstruction models against the performance of human participants carrying out the same tasks.   As well as predicting the pattern of errors in simple navigation and pointing tasks, we will also measure the effect of two types of stimulus change. 3D reconstruction uses 'corresponding points' which are points in an image that arise, for example, from the same physical object (or part of an object) as a camera or person moves around it.  Using a novel stimulus, we will keep all of these 'corresponding points' in a scene constant yet, at the same time, changing the scene so that the images alter radically when the observer moves. This manipulation should have a dramatic effect on a view-based scheme but no effect at all on any system based only on corresponding points.  Overall, we will have a tight coupling between experimental observations and quantitative predictions of performance under two types of model. This will allow us to determine which of the two models most accurately reflects human behaviour in a 3D environment. One potential outcome of the project is that view-based models will provide a convincing account of performance in tasks that have previously been considered to require 3D reconstruction, opening up the possibility that a wide range of tasks can be explained within a view-based framework.   "
	},
	{
		"grant":473,
		"ID": "EP/K011979/1",
		"Title": "SAVVIE: Staying alive in variable, intermittent, low-power environments",
		"PIID": "-118653",
		"Scheme": "Standard Research",
		"StartDate": "20/02/2013",
		"EndDate": "19/08/2016",
		"Value": "331489",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-118653", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Today's low-power electronic systems are designed to handle a high variability in the power demand, for example during transmissions from miniature wireless sensors. However these systems cannot cope with a highly variable power supply. If they are powered by an ambient energy harvester in an environment where the available power is low and sporadic, the system dies once the energy storage becomes depleted or damaged, with start-up being impossible if the power is not increased to a higher steady level. With an increasing number of potential applications of microelectronic systems calling for remote, embedded and miniaturized solutions, sporadic and low power supply and unpredictable energy storage needs to be addressed.  This project researches how to design robust and reliable electronics for situations where there is a variable, unreliable source of energy. A number of situations, or states, have been defined, according to the level of depletion of on-board energy storage, and to how variable the power supply is. In the most challenging states, for example where the input power is sporadic and spread over a wide range from nW to mW, modern electronics fails. We call this the 'survival zone' and are investigating a combination of techniques from the areas of power electronics and asynchronous microelectronics design to allow devices to operate in this zone. Techniques include control circuits that are able to ride through variable voltages, the detection of states, and reconfigurable hardware resources and control algorithms to suit sporadic and sub-microwatt input power. The chief aim of this project is to produce survival zone design methods for the microelectronics design community. "
	},
	{
		"grant":474,
		"ID": "EP/K012029/1",
		"Title": "Long wavelength single photon sources and dotonic molecules",
		"PIID": "11377",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "31/03/2016",
		"Value": "533478",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "11377", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "If a quantum computer can be realised over the next couple of decades it would adversely affect secure communications which are vital in the financial, military and  diplomatic sectors, since current encryption methods will be vulnerable to hacking. Totally secure systems, based on quantum mechanics and in particular the properties of single photons, have been investigated for some years and a few working systems demonstrated. These systems operate at wavelengths which took advantage of available single photon detectors but unfortunately coincide with moderately high absorption in conventional optical fibres. If such quantum key distribution systems based on single photon sources are to be commercially successful then the operating wavelength needs to move to the 1300nm or 1500nm range. This is certainly possible using quantum dots as the photon emitter and recent advances in the quality of single photon detectors at these wavelengths means that detection of the single photons should no longer be an impediment. A major problem is efficient extraction of the single photons produced by the dot. Placing the dot in a photonic crystal cavity (PhC) structure overcomes this difficulty allowing only photons that coincide with an optical mode of the PhC to be emitted along the normal to the surface of the device. This is far from trivial since the dot must be located at the centre of the PhC (to within a few nanometers) and the dot photons must be nearly identical in wavelength to the mode of the cavity or at least be able to be tuned to the same wavelength. Here we use methods familiar to the electronics industry to identify the dot position with metal film markers and use these to fabricate the PhC by drilling a particular pattern of holes in the sample. When this is done correctly the dot and cavity are said to be strongly coupled, meaning that a photon emitted by the dot will always be emitted in the normal direction and can subsequently be collected by an optical fibre. Single photon sources are also crucial components of quantum information networks. Here photons are used like electrons in electrical circuits and are guided along waveguides (which can easily be incorporated into the PhC structures mentioned above) and used to perform logic operations. There are many possible schemes for such networks but in this proposal we hope to demonstrate some basic manipulations of photons using only a few PhC cavities connected by a waveguide and strongly coupled to one or perhaps two dots. This would represent the very first steps towards a possible photonics network. Drawing an analogy with electrical circuits again this would represent the very early stages of an integrated circuit."
	},
	{
		"grant":475,
		"ID": "EP/K014293/1",
		"Title": "Learning the structure and dynamics of human environments to support intelligent mobile robot behaviour",
		"PIID": "-25791",
		"Scheme": "First Grant Scheme",
		"StartDate": "29/04/2013",
		"EndDate": "28/04/2014",
		"Value": "94316",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-25791", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Advances in mobile robot technology over the last 10 years has made the use of service robots (i.e. robots performing tasks for, or with, humans) in the workplace increasingly possible. The problem of building maps of environments that do not change over time has been solved for many application environments, enabling robots to move around in these places for ever-increasing durations. However, real environments do change over time as their inhabitants move around and move furniture and objects as they do so. The resulting changes to a robot's world make it difficult for it to run reliably, and thus there is a danger that we will not be able to create service robots that are able to perform useful tasks in realistic situations.   The proposed research treats the fact that a robot's environment changes regularly as an opportunity rather than a challenge. We will develop systems that are able to extract reliable, significant patterns from the changes observed by an existing intelligent robot (the Dora the Explorer robot from the CogX project). In particular we will develop two approaches. The first will capture how easy or difficult it is for a robot to move through particular parts of its map at particular times of day (e.g. due to humans getting in the way). The second approach will capture how the positions of objects in rooms  change over time (e.g. the desk in my office never moves, but my chair tends to move around in front of the desk, but never near the door). Taken together these approaches will allow a robot to improve its performance on typical service robot tasks such as searching for an object in a building, whilst avoiding certain areas at certain times of day (e.g the corridor by a canteen during lunchtime), all in dynamic environments.  Our research will be informed by an advisory board of experts in reasoning about space and robot learning, and also by the security company G4S who are interested in using mobile robots to assist security guards. Our results could help them by allowing robots to choose better patrol routes through buildings, and to learn how the objects in a room are typically arranged (allowing them to spot when things change due to a burglary or other incidents). As well as presenting our results through the usual scientific channels, we will also demonstrate our finished robot system at the Thinktank science museum in Birmingham, giving the public a chance to learn more about state-of-the-art robots and AI, whilst also testing our systems in a challenging environment."
	},
	{
		"grant":476,
		"ID": "EP/K015206/1",
		"Title": "Natural Language Processing Working Together with Arabic and Islamic Studies",
		"PIID": "602",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "31/03/2015",
		"Value": "336632",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "602", "Role": "Principal Investigator"},
		{"ID": "-372619", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Summary   This is an interdisciplinary project which addresses the ICT call of  'working together' by aligning ICT expertise and research interests from Computational and Corpus Linguistics, with Humanities research streams in Arabic and Islamic Studies, focusing on the Qur'an as a core text. It is also an international collaboration between the Universities of Leeds and Jordan, and further addresses the 'working together' call via incoming and outgoing mobility in the form of Visiting Researcher placements in the School of Computing at Leeds (incoming) and the Centre for the Study of Islam in the Contemporary World at Jordan (outgoing). This agreement is proactive and novel, and has high impact, ensuring knowledge transfer from different methodological perspectives and cultures.  The study of Tajwid or Qur'anic recitation is a sub-field and taught module* in Islamic Studies programmes at both universities and elsewhere, and the original insight informing this project is to view Tajwid mark-up in the Qur'an as additional text-based data for computational analysis. This mark-up is already incorporated into Qur'anic Arabic script, and identifies prosodic-syntactic phrase boundaries of different strengths, plus gradations of prosodic and semantic salience through colour-coded highlighting of pitch accented syllables, and hence prosodically and semantically salient words.   The Computational Linguistics Module in Year 1 entails development and evaluation of software for generating a phonetically-transcribed, stressed and syllabified version of the entire text of the Qur'an, using the International Phonetic Alphabet (IPA). This canonical pronunciation tier for Classical Arabic will be informed and evaluated by Arabic linguists, Tajwid scholars, and phoneticians, and published in an updated version of the open-source Boundary-Annotated Qur'an Corpus [1], [2], preferably for LREC2 2014. The software will also be re-usable for Natural Language Engineering applications for Modern Standard Arabic, and for constructing dictionaries for Arabic language learners.   The Text Analytics Module in Year 2 implements statistical techniques such as keyword extraction3 to explore semiotic relationships between sound and meaning in the Qur'an,  invoking a Saussurean-type view of the sign as '...a bi-unity of expression and content...' [5]. Our investigation entails: (i) text data mining for statistically significant phonemes, syllables, words, and correlates of rhythmic juncture [6], [7]; and (ii) interpretation of results from interdisciplinary perspectives: Corpus Linguistics (ICT); Tajwid science, plus Tafsir or Qur'anic exegesis (Islamic Studies); Arabic (Language and Literature); and Phonetics and Phonology (Linguistics).   In terms of ICT applications, the team will collaborate with stakeholders and beneficiaries to develop an associated or follow-on funding proposal for the UK Research Councils, to include publication of project software as an advanced corpus-query and visualization tool for Islamic Studies and Humanities scholars, plus Arabic language learners. This again represents an extension of the 'working together' theme.   Finally, our approach is interdisciplinary and pioneers stylistic analysis of sound and rhythm encoded in writing as a semiotic system for religious and other literary texts. As such it is entirely novel and has direct implications for research-led teaching in both partner institutions plus a broad cross-section of research groups and user communities, namely: Natural Language Processing and Artificial Intelligence; Qur'anic and Islamic Studies; Arabic Language and Literature; Linguistics and Phonetics; Digital Humanities; and Psychology.   All references appear in Case for Support. "
	},
	{
		"grant":477,
		"ID": "EP/K015214/1",
		"Title": "A higher-order approach to codesign - 27659",
		"PIID": "-79586",
		"Scheme": "Standard Research",
		"StartDate": "17/06/2013",
		"EndDate": "16/12/2016",
		"Value": "274300",
		"ResearchArea": "Microelectronics Design",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-79586", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Current computing architectures are moving increasingly towards heterogeneity, as physical limitations of scale demand a bifurcation of optimisation techniques between reducing latency and increasing throughput. In the past, the amount of heterogeneity in devices tended to be limited, therefore RTL were largely suitable for for hardware design. However, recently there has been a surge of industrial interest in hybrid programmable hardware-software systems. For example, Xilinx has recently launched the Zynq platform which combines a multicore ARM processor with programmable logic. This development thrusts the question of design for such hybrid systems, requiring different programming methodologies, into the foreground.  In general, as architectures move away from the conventional CPU and RAM (von Neumann) model, the task of devising programming models for them falls mostly with their designers, who are usually engineers rather than programming language experts. As a result we are witnessing a flurry of architecture-specific languages (ASL), reminiscent of the early days of computation when each computer came with its own operating system, programming language, etc. Whereas ASLs are unavoidable, good programming methodology recommends that they should be used primarily for the development of system-level infrastructure. The application-level, algorithmic programming should happen as much as possible in portable, machine-independent languages. These lessons are very well known in the programming community and this knowledge can be profitably used in electronic design. We aim to address this problem.  The challenge of heterogeneous codesign is both quantitative and qualitative. A program has components that must be compiled into the FPGA fabric and others to be compiled for the CPU. The reasons are either efficiency (certain architectures are better at running certain types or code) or physical constraints (interactions with other components in a complex design, availability of IP cores, drivers or libraries). But a program may also have components which can be compiled to either architecture or both. A choice must be made and it is reasonable for this choice to be also motivated by efficiency.  In this proposal we will combine and unify the way type systems in higher-level languages specify and solve qualitative (hard) constraints with quantitative optimisation techniques. Specifically, we will investigate the optimization of memory subsystems to support parallel access, by combining knowledge of memory access patterns from the code, resulting in highly efficient programmable memory controllers. In addition, we will optimize the allocation of precision within a combined hardware/software system in order to achieve an accuracy specification while taking into account the capability and cost implications of the programmable hardware and native data types supported in the software. This combined approach can work both at the source code level (program transformations motivated by underlying cost models) and at the synthesisedmachine code and HDL level (pipelined optimisations, etc). We will use a 'game semantic' model, already successfully applied to hardware synthesis from higher order languages, to establish the correctness properties of the type system and to drive the compilation process. "
	},
	{
		"grant":478,
		"ID": "EP/K015664/1",
		"Title": "ENGAGE : Interactive Machine Learning Accelerating Progress in Science, An Emerging Theme of ICT Research",
		"PIID": "84313",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2013",
		"EndDate": "02/06/2016",
		"Value": "674580",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Statistical Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "84313", "Role": "Principal Investigator"},
		{"ID": "-90064", "Role": "Co Investigator"},
		{"ID": "-258048", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Our vision is to establish and lead a new theme in ICT research based on Interactive Machine Learning (IML). Our expansion of IML will give scientists and non-ICT specialists unprecedented access to cutting-edge Machine Learning algorithms by providing a human-computer interface by which they can directly interact with large scale data and computing resources in an intuitive visual environment. In addition, the outcome of this particular project will have a direct transformative impact on the sciences by making it possible for non-programming individuals (scientists), to create systems that semi-automatically detect objects and events in vast quantities of A) audio and B) visual data. By working together across two parallel, highly interconnected streams of ICT research, we will develop the foundations of statistical methodology, algorithms and systems for IML. As an exemplar, this project partners with world leading scientists grappling with the challenge of analysing enormous quantities of heterogeneous data being generated in Biodiversity Science."
	},
	{
		"grant":479,
		"ID": "EP/K015680/1",
		"Title": "Interconnection Networks: Practice unites with Theory (INPUT)",
		"PIID": "15129",
		"Scheme": "Standard Research",
		"StartDate": "23/09/2013",
		"EndDate": "22/09/2016",
		"Value": "353575",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "15129", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "An interconnection network is a mechanism by which different components of a (usually large) computer system communicate.  The design of interconnection networks is not straightforward as there are many issues to take into account, such as: the topology (that is, the basic pattern of connectivity of the components); the routing algorithms (that are used in order to transfer messages around the network); the methods of flow-control (that are used in order to deal with congestion when different network packets, for example, request limited hardward resources); and the methods of switching (the way in which once a route for a message has been selected, the message is physically transferred from component to component throughout the network).  The whole area is an incredible mix of hardware, software and mathematics, and employs principles from both computer science and engineering.  The field of interconnection networks covers a wide variety of different communications subsystems, from relatively small, very local on-chip networks, through supercomputers and clusters, and on to vast, remote and evolving networks such as those implemented in grid and cloud computing (upon which so much of the ubiquitous computing in modern society depends). Although many interconnection network principles apply universally, the varying domain characteristics and intended applications lead to a number of differences. The full extent of these differences is impossible to cover here but one is the scale of the interconnection network. On-chip networks are relatively small - currently tens of nodes (though there are efforts to scale up to a thousand nodes), whilst the number of nodes used in data centre networks or supercomputers can be hundreds of thousands.  The research in this proposal aims to improve the design of interconnection networks for large-scale systems such as those employed in supercomputers, clusters and data centres by developing closer links between the mathematics behind interconnection networks and the practical construction of interconnection networks.  The practical construction of, for example, a supercomputer that might fill a large room is immensely complex, with a multitude of wires, cables, boards, chips, racks and cabinets all conjoined so that all of the computational power of such a system can be employed to yield efficient solutions to problems on massive data sets.  Of course, such a supercomputer has to be programmed so that each of its computational elements knows exactly what to do and when to do it and so that the individual computational results can be rapidly compiled into a solution of the underlying problem.  The design of such a hardware and software system is an incredible feat of engineering.  Mathematicians abstract the essential interconnection network within such a supercomputer as a graph; that is, as a set of vertices, pairs of which are joined by edges.  Whilst this may seem an imprecise abstraction, one can use graph-theoretic properties in order to design interconnection network topologies which possess many properties one would wish of an interconnection network.  Graph properties relating to, for example, symmetry, shortest-paths, connectivity, Hamiltonicity, recursive decomposability and embeddings prove to be extremely important in securing good practical properties for interconnection networks. However, up until now there has been a considerable gap between the mathematical theory on the one hand and practical interconnection network performance on the other.  Our research proposal aims to narrow this gap by providing a closer link between the theory and practice of interconnection networks, with the ultimate goal being techniques by which we can theoretically design an interconnection network and be sure of its resulting practical properties when built and used."
	},
	{
		"grant":480,
		"ID": "EP/K015745/1",
		"Title": "Working Together: Constraint Programming and Cloud Computing",
		"PIID": "107856",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "31/03/2016",
		"Value": "630232",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "119",
		"Investigators":[
		{"ID": "107856", "Role": "Principal Investigator"},
		{"ID": "-6151", "Role": "Co Investigator"},
		{"ID": "-320139", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "This proposal combines two active, important research streams in cloud computing and constraint programming, both of which will realise significant and sustained benefits from working in concert. Constraint programming is a proven technology for solving complex combinatorial problems. However, the inherent difficulty of these problems means that performance can be variable, often requiring tuning by an expert to obtain best results. One approach to obtaining more robust performance is to employ a portfolio of solvers with complementary strengths. The scalable resource offered by the cloud is perfectly suited to the deployment of such portfolios and presents the opportunity to employ large solver portfolios to tackle challenge problems of exceptional difficulty. Conversely, a major concern in cloud computing is how to deploy an application on the available infrastructure so as to maximise performance and minimise operating costs. Added complexity arises when dealing with Big Data scenarios where it is important to run computation as closely (in terms of network distance) as possible to the data, in order to minimise network latency and maximise the performance of an application. This is a difficult combinatorial problem with a large set of variables including: public cloud provider, cloud configuration, geographical region, pricing etc. to which constraint programming is ideally suited.  Our two primary research streams in ICT will interact and work together with a third in astronomy to deliver a solution to a major challenge application: scheduling telescope observations to measure the abundance of planets throughout the Milky Way. If successful, the benefit to astronomy is clear, but our two primary streams will also benefit greatly from a major evaluation of their ability to work together to solve a large, complex problem. "
	},
	{
		"grant":481,
		"ID": "EP/K015796/1",
		"Title": "SAMS - Software Architecture for Mental health Self management",
		"PIID": "13969",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "31/03/2016",
		"Value": "248497",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "13969", "Role": "Principal Investigator"},
		{"ID": "-1970", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "SAMS is a proposed 3-year project to that will investigate the potential for novel data and text mining techniques for detecting subtle signs of Cognitive Dysfunction that may indicate the early stages of Alzheimer's disease. Promoting self-awareness of change in cognitive function is will investigate the potential for novel data and text mining techniques for detecting subtle signs of change in cognition that may indicate the early stages of Alzheimer's disease. Promoting self-awareness of change in cognitive function is a key step in encouraging people to self-refer for clinical evaluation. A key motivation for SAMS, therefore, is to provide a non-invasive tool that helps develop such self-awareness. An increasing number of older people, the group most at risk of cognitive dysfunction and dementia, regularly use the Internet to keep in touch with their families, particularly grandchildren. This Internet activity presents an opportunity to harness rich, routinely available information that may contain indications of changes in the linguistic, executive and motor speed abilities in older people. Development work is needed to develop the software to harness this opportunity, to establish the optimal thresholds for flagging up important changes in cognition and the optimal methods for feeding this information back to individuals. SAMS will validate thresholds by examining changes in performance in people with established cognitive dysfunction and mild Alzheimer's disease and begin to explore the potential for technology-enhanced detection of early cognitive dysfunction. Patterns of computer use and content analysis of e-mails, such as forgetting topics, expressions of concern, emotion, etc., will be analysed and coupled to feedback mechanisms to enhance users' cognitive self awareness, empowering them self administer follow up tests and decide when to self refer themselves for expert medical advice.  Tackling cognitive change detection requires the novel pooling of knowledge and integration of techniques from different sub-disciplines within a Computer Science. In addition to developing techniques for MCI detection and supporting self-referral, an explicit goal of the research is to develop a generic sense making and user-centred feedback architecture. This could be applied to a wide range of problems where interpreting computer use may be appropriate, e.g. mental health, social loneliness, privacy and social exploitation.   "
	},
	{
		"grant":482,
		"ID": "EP/K015850/1",
		"Title": "The SHARC Project: Investigating Technology Support for the Shared Curation of Local History in a Rural Community",
		"PIID": "90168",
		"Scheme": "Standard Research",
		"StartDate": "22/04/2013",
		"EndDate": "21/08/2016",
		"Value": "284378",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "90168", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Shared sense of history is one of the keystones of sense of community (McMillan & Chavis, 1986) and supporting sense of community is a key societal challenge.  In 2006, began work with the rural Wray village community to co-design and then deploy the Wray Photo display (Taylor and Cheverst, 2009). The Photo display proved very popular with residents who uploaded over 1500 photos to the system across a range of categories, with the most popular being hitoric photos. Over the last six years the photo displays have been situated in key settings within the village including: the post-office; the village hall, the village pub (called the George and Dragon) and the village cafe. Feedback from design workshops (and a comments book placed next to the displays) have demonstrated the potential of situated displays and community generated content for supporting sense of community. However, feedback from the community also highlighted the need for tools to provide both greater inclusivity (e.g. for some of the more elderly members of the community) and also for enabling the community to share and co-curate digital narratives (the delightful 'The first chips in Wray' story for example) and accounts/collections of local history relating to the village from a range of sources and perspectives.   The central aim of the proposed research is to co-design these tools with the community and, through longitudinal study, to explore their adoption and appropriation by the community.  It is important that the design of these tools is done in a participatory fashion to help ensure both their appropriateness to the requirements posed by the broad community (given the range of technical abilities for example) and also to foster a greater sense of ownership on behalf of the community.   Furthermore, a 'one-size-fits-all' tool is unlikely to be appropriate given the range of contexts of use, e.g. curating content for consumption by established members of the village community vs. curating content for consumption by residents new to the village vs. curating content for consumption by visitors. Some tools may be mobile applications that support the capture of content in-situ, whilst others may involve the tailoring of existing technologies within the village, for example modifying the colour photocopier in the village Post Office in order to support the simple scanning of historic village newspaper articles.  In terms of tools to support the consumption of these narratives we envisage that situated displays provide suitable properties and affordances (based on our positive experiences with the Wray Photo Displays). Our experiences with situated displays have taught us that their placement is crucial (e.g. siting the display where the audience has due time to interact) and again needs to be done in collaboration with the community in order to promote sense of ownership and avoid inappropriate placements. But we also consider mobile tools as candidate technologies for supporting the consumption of locative media, e.g. enabling a member of the village, out on a ramble, to learn about the historic places she approaches. This could be through a push or pull based approach and involve viewing or listening to the narratives relating to the place. It is anticipated that while consuming such narratives, users will also wish to be able to respond, e.g. contributing stories/narratives of their own or by suggesting links with another related place or narrative.   McMillan, D. and Chavis, D. (1986) Sense of community: A Definition and Theory, Journal of Community Psychology, Special Issue: Psychological Sense of Community, I: Theory and Concepts. Volume 14, Issue 1, pp. 6-23.  Taylor, N. and Cheverst, K. (2009). Social Interaction around a Rural Community Photo Display. International Journal of Human-Computer Studies, 67(12), pp. 1037-1047. "
	},
	{
		"grant":483,
		"ID": "EP/K015893/1",
		"Title": "RC3: Robust Cognitive Cooperative Communications",
		"PIID": "-53274",
		"Scheme": "Standard Research",
		"StartDate": "15/07/2013",
		"EndDate": "14/07/2016",
		"Value": "315750",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-53274", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "According to the Ofcom's Digital Dividend report in 2007, spectrum is limited only because they are seriously underutilised due to rigid and inefficient management. It was reported that over 90% of locations could have around 100MHz spectrum available for other services. These underutilised or unused spectrum holes, also known as white spaces, are mainly due to the interleaved spectrum for the digital TV band. Given the fact that the entire 3G spectrum is only 75MHz, this is an unacceptable wastage. At the same time, however, this gives golden opportunities for mobile operators, broadband service providers and users in the UK to improve the QoE for personal communications using cognitive transmission in the spectrum TV white spaces.   In this context, BT is committed to exploiting the TV spectrum white spaces for providing wireless broadband access for homes in rural areas using cognitive radio (CR) technologies. According to BT, there are 2.75 million customers in rural areas, known as 'not-spots' where, as ridiculous as it sounds, have no 3G coverage and Internet service is pretty much limited to dial-up access over residential or business telephone lines. In the 'not-spots', the service is less than 2M bps but the TV spectrum white spaces, if utilised properly, can potentially cover more than 25% of the 'not-spots' for improved services. The opportunity is that homes in a neighbourhood can share their antennas and signal processing capability to deliver much higher QoE using the spectrum white spaces at no extra cost.  This project takes a novel perspective of enhancing the energy and spectrum efficiencies of wireless communications via user cooperation (e.g., multiple homes cooperation), which offers the possibility to improve the channel by sharing the resources between users. This exceptionally challenging objective has the potential to redefine the architecture of wireless networks, provide a novel system solution for extending the coverage and enhancing the QoE of broadband communications.   In this project, the PI and BT (as the industrial partner) will join force to address the optimisation problem for cognitive cooperation. Our aim is to tackle the fundamental technical challenges specific to a cooperative MIMO channel. For instance, the required optimisation will need to take into account of individual users' requirements, constraints and fairness issues. Also, the proposed cooperative solution is also required to be robust to imperfect channel state information (CSI) and asynchronousity of the cooperating nodes, and be realised in a distributed manner. BT will be a key partner to provide invaluable inputs on the practical level to ensure that the project deliverables are exploitable. The final outcome of the proposed project will be the technologies for self-optimising cooperative antenna systems which can be used to provide broadband coverage for 'not-spot' areas over wireless in the TV spectrum white spaces."
	},
	{
		"grant":484,
		"ID": "EP/K015966/1",
		"Title": "Classifying Images Regardless of Depictive Style",
		"PIID": "51118",
		"Scheme": "Standard Research",
		"StartDate": "24/06/2013",
		"EndDate": "23/06/2016",
		"Value": "231928",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "51118", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computer's today can recognise objects in photographs. This ability has formed the basis of many familiar applications such as Facebook tagging, Google Image Search, Google Goggles, and automated passport checking at UK borders.  Yet a significant restriction remains: computers can only recognise objects in photographs. At least, their ability to recognise objects in drawings and paintings - in artwork of any kind - is strictly limited. If this limitation can be overcome then many more applications will become possible.  One is a new way to search the web for images in which a drawing (say) is dragged from the desktop into a search bar, and paintings and photographs are given back to the user (at the moment a user gets the same sort of image back as was dragged into the search bar).  Another is the automated production of catalogues for taxonomy - which is important to scientists faces with tens of thousands of microscopic creatures; species catalogues are hand-drawn right now so automation would be a significant advance for them.  The output of the programme would also allow ordinary photographs to be converted to icons. This is not as dry as it sounds, but could help the visually impaired to gain access to photographic content. If photographs and drawings can be linked in the way this project has in mind, then objects in photographs could be turned into icons rendered by a set of raised pins. So there would be a symbol for car, say, not unlike that which might be drawn by a child - and in fact this is very close to the icons blind artists draw. This would allow the visually impaired to read photographs in newspapers, or in text books, and allow them to share the holiday snaps of family and friends.  This proposal is about building the basic technology that underpins these applications, and quite possibly others too. Key to it is lifting the barrier that computers of today face - allowing them to recognise objects no matter how they are depicted."
	},
	{
		"grant":485,
		"ID": "EP/K016423/1",
		"Title": "Algorithms for Perfect Graph and Other Hereditary Graph Classes",
		"PIID": "85034",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2013",
		"EndDate": "31/12/2015",
		"Value": "134323",
		"ResearchArea": "Mathematical Aspects of Operational Research",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "64",
		"Investigators":[
		{"ID": "85034", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Developing efficient algorithms for solving optimization problems is of great importance to the modern technological society. Many problems arising in diverse areas such as transportation, telecommunication, molecular biology, industrial engineering, etc., when modeled by graphs reduce to problems such as finding the size of a largest clique (which is a set of nodes that are all pairwise adjacent), or stable set (which is a set of nodes none of which are pairwise adjacent), or the coloring problem (i.e. using the minimum number of colors to color the vertices of a graph so that no two adjacent vertices receive the same color). These fundamental optimization problems are unfortunately NP-hard to solve in general, which means that it is highly unlikely that there will ever be an efficient way to solve them by a computer (i.e. it is unlikely that polynomial time algorithms exist for these problems). They become polynomially solvable when restricted to special graph classes, but also remain difficult even when seemingly quite a lot of structure is imposed on an input graph. Understanding structural reasons that enable efficient algorithms for such optimization problems is the primary interest of this proposal.  In the past few decades a number of important results were obtained through the use of decomposition theory, where one gains an understanding of a complex structure by breaking it down into simpler parts. For example, the famous Strong Perfect Graph Conjecture (that characterizes perfect graphs, a class that emerged from the study of communication theory, by excluded induced subgraphs) was proved by a decomposition theorem. Also it is known how to use this decomposition theorem to construct a polynomial time recognition algorithm for perfect graphs. What is not known is how to make use of it for construction of related optimization problems.  This project will focus on developing techniques for turning such decomposition theorems into efficient optimization algorithms. This is particularly difficult to do when dealing with complex hereditary graphs classes, such as perfect graphs, because very strong cutsets are needed for their decomposition and it is not clear how to use them in the desired algorithms.  "
	},
	{
		"grant":486,
		"ID": "EP/K016857/1",
		"Title": "Electronic nanodevices for energy harvesting: a novel approach to thermal-energy conversion",
		"PIID": "-421255",
		"Scheme": "First Grant Scheme",
		"StartDate": "26/06/2013",
		"EndDate": "25/06/2014",
		"Value": "98113",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-421255", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Every hot body emits a large power in the form of infrared radiation. Electronic equipment, appliances, car exhausts and high-temperature industrial processes, can thus provide an untapped -and currently wasted- energy reservoir. The amount of power is startling: an object at a temperature of 600 C emits 33 kW/m2, while an adult human body approximately 100 W. The proposed work explores the application of a rectenna (RECtifier + antENNA) for harvesting this thermal radiation and converting it into useable electrical power, with potential to develop a disruptive technology. Rectennas consists of a microantenna that captures infrared thermal radiation connected to rectifier, which converts the radiation to DC electrical power. The concept is not new, and it was already investigated by Brown in the 1960s at microwave frequency to transmit power wirelessly, with efficiencies above 80%. However, thermal radiation is located at much higher frequencies in the electromagnetic spectrum, tens of terahertz (THz), which means that the rectifiers must be more than ten thousand times faster. The rectifier proposed in this work is based on the self-switching nanodiode (SSD), a novel semiconductor nanodevice consisting of an asymmetric nanochannel, whose room-temperature operation in the THz range was pioneered by Dr Balocco. Planar microantennas will be integrated with an array of approximately 1000 SSDs connected in parallel, in order to reduce the internal resistance of the rectenna, and hence increase their output power. Because of their compact structures, slim form factor and absence of moving parts, electrical generators based on these new devices will find application in industrial waste-heat scavenging, and where space is at premium, such as smart sensors, mobile and combined heat-power (CHP) systems. For industrial applications, where space might not been a concern, the use of rectennas enables the recovery of the low-grade energy lost during high-temperature processes. The solid-state nature of these devices and their robustness requires little maintenance, and are easy to install. Smart sensors operating in an environment where hot bodies, or other sources of infrared radiation, are available, can take advantage of rectennas' compactness.  Even if a low electrical power is converted, this is sufficient to intermittently power the sensor. Combined heat-power systems and microgeneration in general, can couple rectennas to recover the lost thermal power radiated by the gas burner assembly. A more ambitious application is indirect solar-energy conversion, where the heat stored in a medium during the day (such as molten salt at ~500 C) is used during the night to produce electrical power. Here, rectennas may provide an alternative solid-state approach to the steam-turbine technology currently in use."
	},
	{
		"grant":487,
		"ID": "EP/K016873/1",
		"Title": "SwiTching And tRansmission (STAR)",
		"PIID": "37134",
		"Scheme": "Standard - NR1",
		"StartDate": "01/01/2013",
		"EndDate": "31/12/2015",
		"Value": "356078",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64",
		"Investigators":[
		{"ID": "37134", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Internet power consumption has continued to increase over the last decade as a result of a bandwidth growth of at least 50 to 100 times. Further bandwidth growth between 40% and 300% is predicted in the next 3 years as a result of the growing popularity of bandwidth intensive applications. Energy efficiency is therefore increasingly becoming a key priority for ICT organizations given the obvious ecological and economic drivers. In this project we adopt the GreenTouch energy saving target of a factor of a 100 for Core Switching and Routing and believe this ambitious target is achievable should the research in this proposal prove successful. A key observation in core networks is that most of the power is consumed in the IP layer while optical transmission and optical switching are power efficient in comparison, hence the inspiration for this project. Therefore we will introduce energy efficient optimum physical network topologies that encourage optical transmission and optical switching at the expense of IP routing whenever possible. Initial studies by the applicants show that physical topology choices in networks have the potential to significantly reduce the power consumption, however network optimization and the consideration of traffic and the opportunities afforded by large, low power photonic switch architectures will lead to further power savings. We will investigate a large, high speed photonic switch architecture in this project, minimize its power consumption and determine optimum network physical topologies that exploit this switch to minimize power consumption. We will design new large photonic switch fabrics, based on hybrid semiconductor optical amplifiers (SOA) / Mach Zehnder interferometers as gating elements to minimise the switching energy per bit, and plan to optimize the network architecture making use of these new switch architectures and introduce (photonic) chip power monitoring to inform higher layer decisions.  Networks are typically over provisioned at present to maintain quality of service. We will study optimum resource allocation to reduce the overprovisioning factor while maintaining the quality of service. Protection is currently provided in networks through the allocation of redundant paths and resources, and for full protection there is a protection route for every working route. We will optimize our networks to minimize power wastage due to protection. The power savings due to optimum physical topology design, optimum resource allocation, optical switching instead of IP routing, more efficient photonic switches and energy efficient protection can be combined and therefore the investigators and their industrial collaborators BT, Alcatel Lucent and Telekomunikacja Polska, believe that an ambitious factor of 100 power saving in core networks can be realised through this project with significant potential for resulting impact on how core photonic networks are designed and implemented.  "
	},
	{
		"grant":488,
		"ID": "EP/K017330/1",
		"Title": "Energy harvesting Communication netwoRks: OPtimization and demonStration",
		"PIID": "100470",
		"Scheme": "Standard - NR1",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2016",
		"Value": "391320",
		"ResearchArea": "Energy Efficiency (End Use Energy Demand)",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "100470", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Technology evangelists use the catch-phrase 'Anytime, anywhere, anything' when they promise untethered wireless data flow not only among people, but also among devices of any imaginable sort.   Wireless networks offer a solution to the solve wiring problem, but powering such systems requires increasing amounts of energy, and also introduces the constraint of changing and recharging millions of batteries.  In order to achieve the vision of a seamlessly integrated data and knowledge centric world we must solve the issue of a sustainable replenishment of power to the digital world that can be addressed both by reducing the power consumption of the devices that we use and of harvesting energy if possible on board the devices themselves through energy harvesting.  This project will develop a principled understanding of communication networks which live on energy, so that system designs can be optimised by considering energy harvesting in conjunction with data processing and communications. We will develop methods for system design that allows adaptationion of network to changes in the available energy, as well as to the distribution of the energy within the network itself. The project will also develop novel vibration and thermal energy harvesters, and appropriate storage units.  All these aspects will be integrated into an overall practical wireless  network test-bed to demonstrate and test the concepts studied in the project."
	},
	{
		"grant":489,
		"ID": "EP/K017438/1",
		"Title": "The Limits of Decidability: Counting, Transitivity, Equivalence",
		"PIID": "32082",
		"Scheme": "Standard Research",
		"StartDate": "14/03/2013",
		"EndDate": "13/03/2015",
		"Value": "71959",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "32082", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "First-order logic is a formal language for describing structured ensembles of objects and data. The use of first-order logic to specify, query and manipulate such structured data is now firmly embedded in the theory and practice of a wide range of academic disciplines.  Automating the process of deductive reasoning in first-order logic is therefore a central challenge in Computer Science.  However, reasoning with first-order logic is known to be undecidable: it is in principle impossible to write a computer program that can reliably determine all logical consequences expressible using this formalism. On the other hand, it has long been understood that, by restricting the language of first-order logic in various ways, we obtain 'fragments' of logic in which decidability is restored. Furthermore, we observe a trade-off between expressive power and computational manageability: the smaller a fragment is---i.e. the less expressive it is---the easier it is to reason in. The research proposed here will investigate several very expressive fragments of first-order logic---those near the upper limit of decidability---and determine their decidability/computational complexity.  We take as our point of departure three fragments of first-order logic which are known to be decidable: the 'two-variable fragment', the 'guarded fragment' and the 'fluted fragment'. We investigate the effect of extending the expressive power of these logics in three ways (severally, or in combination). The first extension we consider involves numerical quantifiers, which allow us to place (upper or lower) bounds on how many objects satisfy some given property.  The second extension we consider involves the use of transitive relations such as 'is taller than' or `earns more money then'. (Such relations have special logical properties that need to be taken into account in reasoning problems.)  The third extension we consider involves the use of equivalence relations such as 'is the same height as' or 'has the same tax code as'. In this way, we obtain a collection of fragments of first-order logic for which it is currently open whether reasoning is decidable. We propose to resolve these open problems.  For those fragments which we show to be decidable, we shall obtain (as a by-product of our proof) an algorithm for reasoning within the fragments in question; for those shown to be undecidable, we know that no such algorithm exists. Moreover, for the decidable fragments, we can quantify the difficulty of reasoning within them, and even identify the specific kinds of formulas that are responsible for the difficult cases. Thus, our research represents a contribution to the enterprise of using first-order logic to describe, query or manipulate structured data."
	},
	{
		"grant":490,
		"ID": "EP/K017500/1",
		"Title": "MAPTRAITS: MACHINE ANALYSIS OF PERSONALITY TRAITS IN HUMAN VIRTUAL AGENT INTERACTIONS",
		"PIID": "-252393",
		"Scheme": "First Grant Scheme",
		"StartDate": "23/09/2013",
		"EndDate": "22/09/2014",
		"Value": "98427",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "-252393", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Research findings suggest that personality traits such as extraversion, agreeableness, and openness to experience, are tightly coupled with human abilities and behaviour encountered in daily lives: emotional expression, linguistic production, success in interpersonal tasks, leadership ability, general job performance, teacher effectiveness, academic ability, as well as interaction with technology. In fact, human users tend to anthropomorphise computers and virtual agents, treating them as social beings, and interpreting their behaviour similarly to daily human-human interactions.  The problem of assessing people's personality is very important for multiple research and business domains such as computer-mediated staff assessment and training, human-computer and human-robot interaction. Despite a growing interest and emphasis on personality traits and their effects on human life in general, and recent advances in machine analysis of human behavioural signals (e.g., vocal expressions, and physiological reactions), pioneering efforts focusing on machine analysis of personality traits have started to emerge only recently: (i) there exist a small number of efforts based on unimodal cues such as written texts/ audio/ speech/ static facial features, (ii) despite tentative efforts on multimodal personality trait analysis, the dynamics (duration, speed, etc.) of multiple cues, which have been shown to be important in human judgments of personalities, have mostly been neglected, (iii) although personality analysis research suggests that a trait exists in all people to a greater or lesser degree (i.e. a person can be anywhere on a continuum ranging from introversion to extraversion), none of the proposed efforts have attempted to assess personality traits continuously in time and space  (i.e., how a person can be rated along the multiple trait dimensions at a given interaction time and context), and (iv) how machine (automatic) traits analysis can be utilised for personalised, social and adaptive human - virtual agent interaction has not been investigated.  Overall, both the common everyday technology (e.g., personal PCs, smart phones) and the more sophisticated systems people use nowadays (e.g., computer games, assistive technologies, embodied virtual agents, etc.) lack the capability of understanding their human users' personality and behaviour, and of providing socially intelligent, adaptive and engaging human - computer interaction.  To address these issues and limitations, MAPTRAITS project will bring around a set of audio-visual tools that can analyse and predict human personality traits dynamically from multiple nonverbal cues and channels (i.e., upper body, head, face, voice and their dynamics) in continuous time and trait space. There is no prospect of building a perfect system for automatic analysis of personality traits that can be used in all possible application domains in 12 months' time. Therefore, as a proof-of-concept, the MAPTRAITS technology will be developed for automatic matching of virtual agent and user personalities, to automatically model what type of users would like to engage with what type of virtual agents to the aim of user engagement enhancement. The motivation for choosing this application area lies in  its significance: (i) Research has shown that people's attitudes toward machines and conversational agents is based on the perceived personality of the agent, and their own personality, and (ii) humans are social beings, and currently their everyday life revolves around interacting with computers, virtual agents and robots that are getting increasingly popular as companions, coaches, user interfaces to smart homes, or household robots."
	},
	{
		"grant":491,
		"ID": "EP/K017594/1",
		"Title": "GEMSCLAIM: GreenEr Mobile Systems by Cross LAyer Integrated energy Management",
		"PIID": "-421253",
		"Scheme": "Standard - NR1",
		"StartDate": "10/01/2013",
		"EndDate": "09/01/2016",
		"Value": "349507",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10",
		"Investigators":[
		{"ID": "-421253", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Personal computing currently faces a rapid trend from desktop machines towards mobile services, accessed via tablets, smartphones and similar terminal devices. With respect to computing power, today 's handheld devices are similar to Cray-2 supercomputers from the 1980s. Due to higher computational load (e.g. via multimedia apps) and the variety of radio interfaces (such as WiFi, 3G, and LTE), modern terminals are getting increasingly energy hungry. For instance, a single UMTS upload or a video recording process on today 's smartphones may consume as much as 1.5 Watts, i.e. roughly 50% of the maximal device power. In the near future, higher data rates and traffic, advanced media codecs, and graphics applications will ask for even more energy than the battery can deliver. At the same time, the power density limit might lead to a significant share of 'Dark Silicon' at 22nm CMOS and below. Obviously, disruptive energy optimizations are required that go well beyond traditional technologies like DVFS (dynamic voltage and frequency scaling) and power-down of temporarily unused components. The GEMSCLAIM project aims at introducing novel approaches for reducing this 'greed for energy', thereby improving the user experience and enabling new opportunities for mobile computing. The focus is on three novel approaches: (1) cross layer energy optimization, ranging from the compiler over the operating system down to the target HW platform, (2) efficient programming support for energy-optimized heterogeneous Multicore platforms based on energy-aware service level agreements (SLAs) and energy-sensitive tunable parameters, and (3) introducing energy awareness into Virtual Platforms for the purpose of dynamically customizing the HW architecture for energy optimization and online energy monitoring and accounting. GEMSCLAIM will provide new methodologies and tools in these domains and will quantify the potential energy savings via benchmarks and a HW platform prototype."
	},
	{
		"grant":492,
		"ID": "EP/K017845/1",
		"Title": "Readers: Evaluation and Development of Reading Systems",
		"PIID": "102151",
		"Scheme": "Standard - NR1",
		"StartDate": "01/03/2013",
		"EndDate": "29/02/2016",
		"Value": "296868",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "102151", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Machine reading aims to extract knowledge from unstructured text with little human effort. It has been a major goal of AI since its early days. The ever growing amounts of textual data available over the internet further increase the importance and urgency of computer-based methods for knowledge extraction. The success of machine reading will not only help breach the knowledge acquisition bottleneck in AI, but also revolutionize Web search, information extraction, and the automatic construction of resources such as Wikipedia.   In the past, there has been a lot of progress in automating many substasks of machine reading using standard NLP technology such as tagging and parsing. However, end-to-end solutions are still rare, and existing systems typically require substantial human effort in manual engineering and/or labeling examples. As a result, they often target restricted domains and only extract limited types of knowledge (e.g., a pre-specified relation).   In this project we aim to develop an end-to-end system that operates over raw text, extracts knowledge and is able to answer  questions and support other end tasks.  A key insight in our approach is the use of unsupervised methods that do not rely on large amounts of hand annotation for the acquisition of background knowledge, its linking to existing knowledge bases, and the creation of new ones. Our approach will acquire knowledge at Web-scale, be open to arbitrary domains, genres, and languages.It will constantly integrate new information sources (e.g., new text documents) and learn from user questions and feedback (e.g., via performing end tasks)."
	},
	{
		"grant":493,
		"ID": "EP/K017896/1",
		"Title": "uComp: Embedded Human Computation for Knowledge Extraction and Evaluation",
		"PIID": "-421970",
		"Scheme": "Standard - NR1",
		"StartDate": "15/11/2012",
		"EndDate": "14/11/2015",
		"Value": "375621",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-421970", "Role": "Principal Investigator"},
		{"ID": "32412", "Role": "Co Investigator"},
		{"ID": "-19751", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The rapid growth and fragmented character of social media and publicly available structured data challenges established approaches to knowledge extraction. Many algorithms fail when they encounter noisy, multilingual and often contradictory input. Efforts to increase the reliability and scalability of these algorithms face a lack of suitable training data and gold standards. Given that humans excel at interpreting contradictory and context-dependent language data, the uComp project will address the above mentioned shortcomings by merging collective human intelligence and automated methods in a symbiotic fashion. The project will build upon the emerging field of Human Computation (HC) in the tradition of games with a purpose and crowdsourcing marketplaces. It will advance the field of Web Science by developing a scalable and generic HC framework for knowledge extraction and evaluation, delegating the most challenging tasks to large communities of users and continuously learning from their input to optimise automated methods as part of an iterative process. A major contribution is the proposed foundational research on Embedded Human Computation (EHC), which will advance and integrate the currently fragmented research on human and machine computation. EHC goes beyond mere data collection and embeds the HC paradigm into adaptive knowledge extraction workflows. An open evaluation campaign will validate the accuracy and scalability of EHC to acquire factual and affective knowledge. In addition to novel evaluation methods, uComp will also provide shared datasets and benchmark the EHC approach against established knowledge processing algorithms.  While the methods of uComp will be held generic to be evaluated across domains, climate change was chosen as the main use case for its challenging nature, subject to changing and conflicting interpretations. Active collaboration with international organisations (EEA, NOAA, NASA) will increase the project's visibility and promote the adoption of the EHC paradigm among a wide range of stakeholders. "
	},
	{
		"grant":494,
		"ID": "EP/K017950/1",
		"Title": "SMARTER: Smart Multifunctional ARchitecture & Technology for Energy aware wireless sensoRs",
		"PIID": "124327",
		"Scheme": "Standard - NR1",
		"StartDate": "01/10/2012",
		"EndDate": "30/09/2015",
		"Value": "464687",
		"ResearchArea": "CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Applied Sciences",
		"OrgID": "8",
		"Investigators":[
		{"ID": "124327", "Role": "Principal Investigator"},
		{"ID": "69656", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The overall vision of the project is to develop comprehensive knowledge and an innovative methodology in the areas of energy autonomous wireless systems from a global system perspective, enabling self-powered, battery-free wireless sensing nodes to meet a wide range of structural health monitoring (SHM) applications. The research vision builds on the project partners' complementary skills and strengths in the area of 'towards zero -power ICT' with the potential to lead to multiple scientific and technical breakthroughs. The first breakthrough is to make use of the SHM sensing device itself to implement a single multifunctional device providing both structural health data and electrical energy harvested from mechanical vibrations. Another breakthrough will be to store the harvested energy in a fully integrated smart storage device, which adapts its storage capacity, according to the available energy in the environment and to the power consumption of the load. This adaptability will provide a constantly optimized matching between storage device and energy harvester to foster energy transfer. The energy storage itself will be a micro-ultracapacitor, so will have the desirable features of high specific energy, short time response, long lifetime and safe operation. This micro -ultracapacitor will be implemented in a silicon compatible technology so as to facilitate co-integration with other functions. Moreover, to drastically reduce the power consumption of the communication module, the proposed strategy is based on using impulse radio UWB (ultra-wideband) and dark silicon design approaches. A final innovation will be the co-location of the different devices (harvesting, sensing, storage, processing, data transmission) on the same flexible substrate, in order to enable conformal attachment of the device, a characteristic highly desirable in a SHM context wher e the surfaces to be monitored are seldom planar. Additionally, by this means the issue of the anisotropy of vibration harvesters is settled, the harvester being, by nature, properly oriented. More globally, the project aims at producing a device in which co-integration, co-location of functions, versatility of applications and energy autonomy are pushed to a maximum."
	},
	{
		"grant":495,
		"ID": "EP/K018728/1",
		"Title": "REFRAME: Rethinking the Essence, Flexibility and Reusability of Advanced Model Exploitation",
		"PIID": "72414",
		"Scheme": "Standard - NR1",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2016",
		"Value": "381274",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "72414", "Role": "Principal Investigator"},
		{"ID": "-107268", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "See the attached case for support."
	},
	{
		"grant":496,
		"ID": "EP/K018868/1",
		"Title": "Efficient and Natural Proof Systems",
		"PIID": "-117490",
		"Scheme": "Standard Research",
		"StartDate": "01/02/2013",
		"EndDate": "31/01/2016",
		"Value": "556849",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "-117490", "Role": "Principal Investigator"},
		{"ID": "69169", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We are all familiar with the language of classical logic, which is normally used for both mathematical and informal arguments, but there are other important and useful logics. Some nonclassical logics, for example, can be associated with programming languages to help control the behaviour of their programs, for instance via type systems.  In order to define the proofs of a logic we need a proof system consisting of a formal language and some inference rules. We normally design proof systems following the prescriptions of some known formalism that ensures that we obtain desirable mathematical properties. In any case, we must make sure that proofs can be checked for validity with a computational effort that does not exceed certain limits. In other words, we want checking correctness to be relatively easy, also because this property facilitates the design of algorithms for the automatic discovery of proofs.  However, there is a tension between the ease by which proofs can be checked and their size. If a proof is too small, checking it is difficult. Conversely, formalisms that make it very easy to check and to search for proofs create big bureaucratic unnatural proofs. All traditional proof systems suffer to various extents from this problem, because of the rigidity of all traditional formalisms, which impose an excess of structure on proof systems.  We intend to design a formalism, provisionally called Formalism B, in which arbitrary logics can be defined and their proofs described in a way that is at the same time efficient and natural. Formalism B will ideally lie at the boundary between the class of proof systems and that of systems containing proto-proofs that are small and natural, but are too difficult to check. In other words, we want to maximise naturality by retaining as much efficiency as possible in proof representation. A driving force in this effort will be the use of existing algebraic theories that seem to capture some of the structure needed by the new formalism.  There are two main reasons for doing this. One is theoretical: the problem is compelling, and tackling it fits well into a research effort in the theory of computation that tries to define proofs as more abstract mathematical objects than just intricate pieces of syntax. Suffice to say that we are at present unable to decide by an algorithm when two seemingly different proofs of the same statement use the same ideas and so are equivalent, or not. This is a problem that dates back to Hilbert and that requires more abstract ways to describe proofs than traditional syntax provides.  The second reason is practical: we need formal proofs to verify the correctness of complex computer systems. The more powerful computer systems become, the more we need to ensure that they do what they are supposed to do. Formal verification is increasingly adopted as a viable instrument for this, but still much needs to be done in order to make it an everyday tool. We need to invent proof systems that simplify the interaction with proof assistants, and that could represent in some canonical way proofs that are essentially the same, so that no duplication occurs in the search for and storing of proofs in proof libraries.  This project intends to contribute by exploiting proof-theoretic advances of the past ten years. We have developed a new design method for proof systems that reduces the size of inference steps to their minimal terms, in a theory called `deep inference'. The finer scale of rules allows us to associate proofs with certain purely geometric objects that faithfully abstract away proof structure, and that are natural guides for the design of proof systems whose proofs would not suffer from several forms of bureaucracy.  In short, after a decade in which we broke proofs into their smallest pieces, by retaining their properties, we are now reshaping them in such a way that they still retain all the features we need but do not keep the undesirable ones."
	},
	{
		"grant":497,
		"ID": "EP/K018884/1",
		"Title": "ZnO MESFETs for application to Intelligent Windows",
		"PIID": "6605",
		"Scheme": "Standard Research",
		"StartDate": "27/05/2013",
		"EndDate": "31/05/2016",
		"Value": "598860",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68",
		"Investigators":[
		{"ID": "6605", "Role": "Principal Investigator"},
		{"ID": "-219198", "Role": "Co Investigator"},
		{"ID": "28488", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The central aim of the research is to produce electronic circuits into glass windows for visual display, monitoring and control purposes: 'transparent electronics'. The circuits are made from a very thin semiconductor-like material (ZnO: zinc oxide) and thin metal contacts.  The materials are so thin (10's nm) that most of the light passes straight through.  Thus 'intelligence' can be built into the window.  The material used also offers optical advantages in that is 'low emissivity'; its refractive index serves to enhance the capture of solar energy and reduces the heat loss from the building. The added-value of introducing electronics capability into the glass coatings can open up a new generation of glass windows in buildings, cars and aeroplanes; especially for 'heads-up' displays. The last application is currently available using films containing indium.  This is a very expensive material so its replacement with ZnO will bring down the cost and so open up the availabilty of such products. ZnO also has low-toxicity.   The devices we will use are called metal-semiconductor field effect transistor (MESFET). These devices are chosen because they are very simple and easy to fabricate. They also operate at lower voltages than the more commonly used metal-oxide field effect transistor (MOSFET), which is found in computers and phones etc. The use of MESFETs avoids the need to develop a gate oxide technology which is very challenging technologically. The downside is that the digital circuits need to more complicated and so take up more space. However, the footprint of the circuits is less critical that for Si based chips, as the electronics is on large area panels. The project is then concerned with the establishment of a MESFET fabrication process, device design for the intended circuitry and finally design and realisation of basic circuit blocks and gate arrays.   "
	},
	{
		"grant":498,
		"ID": "EP/K018930/1",
		"Title": "Engineered resonant tunnelling nanostructures for the Terahertz realm",
		"PIID": "-219198",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/04/2013",
		"EndDate": "31/03/2015",
		"Value": "95672",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-219198", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The engineering of the metal/insulator nanostructures capable of harnessing THz energy is the central aim of this project. The challenge lies in tuning the barrier heights at the metal/insulator interfaces for optimal terahertz energy conversion. Instrumental in achieving this ambitious goal is thorough understanding of interfacial barrier formation and correlation of physical and electrical properties of proposed nanostructures. The nanostructures will be fabricated using atomic layer deposition (ALD). The ALD deposition system enables a controlled microstructure, leading to better uniformity and control of the tunnelling barriers' composition and thickness, as well as interface integrity and stability. The target is addressed in three coupled work phases, which are strongly linked, and entail voluminous theoretical and experimental study with a wide range of characterization techniques.   The successful outcome of the project will facilitate an emerging technology and complement research efforts at Manchester University and Imperial College in the UK to bring about new efficient electronic devices for terahertz energy harvesting in infrared and visible domain. "
	},
	{
		"grant":499,
		"ID": "EP/K01904X/1",
		"Title": "Visual Sense. Tagging visual data with semantic descriptions",
		"PIID": "-115276",
		"Scheme": "Standard - NR1",
		"StartDate": "08/02/2013",
		"EndDate": "07/02/2016",
		"Value": "331317",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51",
		"Investigators":[
		{"ID": "-115276", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Recent years have witnessed an unprecedented growth in the number of image and video collections, partially due to the increased popularity of photo and video sharing websites. One such website alone (Flickr) stores billions of images. And this is not the only way in which visual content is present on the Web: in fact most web pages contain some form of visual content.  However, while most traditional tools for search and retrieval can successfully handle textual content, they are not prepared to handle heterogeneous documents. This new type of content demands the development of new efficient tools for search and retrieval.   The large number of readily accessible multi-media data-collections pose both an opportunity and a challenge. The opportunity lies in the potential to mine this data to automatically discover mappings between visual and textual content. The challenge is to develop tools to classify, filter, browse and search such heterogeneous data. In brief, the data is available, but the tools to make sense of it are missing.  The Visual Sense project aims to automatically mine the semantic content of visual data to enable 'machine reading' of images. In recent years, we have witnessed significant advances in the automatic recognition of  visual concepts. These advances allowed for the creation of systems that can automatically generate keyword-based image annotations. However, these annotations, e.g. 'man' and 'pot', fall far short of the sort of more meaningful descriptive captions necessary for indexing and retrieval of images, for example,'Man cooking in kitchen'. The goal of this project is to move a step forward and predict semantic image representations that can be used to generate more informative sentence-based image annotations, thus facilitating search and browsing of large multi-modal collections. It will address the following key open research challenges:  1) Develop methods that can derive a semantic representation of visual content. Such representations must go beyond the detection of objects and scenes and also include a wide range of object relations. 2) Extend state-of-the-art natural language techniques to the tasks of mining large collections of multi-modal documents and generating image captions using both semantic representations of visual content and object/scene type models derived from semantic representations of the textual component of multi-modal documents. 3) Develop learning algorithms that can exploit available multi-modal data to discover mappings between visual and textual content. These algorithms should be able to leverage 'weakly' annotated data and be robust to large amounts of noise.  Thus, the main focus of the Visual Sense project is the development of machine learning methods for knowledge and information extraction from large collections of visual and textual content and for the fusion of this information across modalities. The tools and techniques developed in this project will have a variety of applications. To demonstrate them, we will address three case studies: 1) evaluation of generated descriptive image captions in established international image annotation benchmarks, 2)  re-ranking for improved image search and 3) automatic illustration of articles with images.  To address these broad challenges, the project will build on expertise from multiple disciplines, including computer vision, machine learning and natural language processing (NLP). It brings together four research groups from University of Surrey (Surrey, UK),  Institut de Robotica i Informatica Industrial (IRI, Spain), Ecole Centrale de Lyon (ECL, France),  and University of Sheffield (Sheffield, UK) having each well established and complementary expertise in their respective areas of research."
	},
	{
		"grant":500,
		"ID": "EP/K019392/1",
		"Title": "GALE: Global Accessibility to Local Experience",
		"PIID": "93129",
		"Scheme": "Standard Research",
		"StartDate": "01/04/2013",
		"EndDate": "30/09/2015",
		"Value": "283957",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "93129", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Recommender Systems have been generated in the past 15 years with the aim to suggest to individual users opportunities arising in the virtual space of the Internet on the basis of the individual profile of the user, her/his past history as a customer/web-user and even her/his friends' community in social networks.  Further, the rise of online social networks such as Facebook has allowed for a new source of information to be exploited by recommendation systems: the user social network.  Internet access is now becoming increasingly mobile and smart phones are changing the way people interact with places and with each other in an increasingly complex manner. Smart phones are starting to impact the way users access information on the go and receive suggestions. More specifically, innovative recommender systems are currently being developed to exploit GPS-based or other location-sensitive information, associated on-the-go to individual users through smartphones. This second generation of recommender systems, by being location-based, pose an entirely different set of problems which not only have to do with the knowledge of the user (her or his 'profile'), but also with that of the places. Knowledge of places can be achieved by means of guides, textbooks and journey reports, or by direct experience. These ways are quite different in nature. The former is globally accessible (everybody can get it from afar) and relatively fast to obtain, especially in the age of the Internet. The latter is only locally accessible (one needs to be in the place to access it) and, being generated by those living in the place through personal local interaction, it becomes accessible only after long-term interactions and the construction of personal relationship of mutual trust. When visiting a new place, you would necessarily rely only on global information to navigate the place and access its resources. Conversely, if you are a local, your knowledge of the place is mostly constructed through your personal long-term exchange with what all your neighbors are doing every day and with their favorite places in the neighborhood; as a result, you not only would rely on local knowledge, but you would also contribute - by interacting locally - to the formation and continuous re-shaping of the information used by your neighbors too in their interaction. If we name the long-term, locally generated knowledge of the place 'neighborhood knowledge', we can say that what people locally do in places is in one way or another dependent on the extent to which they have access to the neighborhood knowledge. The second generation of recommender systems allows 'global' place-users, i.e. people visiting a place who are not experienced with the place itself, to access 'globally' available information. However, a good deal of information is still not exploited in these systems, as the geographic and the social only 'meet' in a superficial way: in other words, the system does not take advantage of any information about the particular use of the place that local 'communities' have done in the past and do 'at the moment'.  As neighborhood knowledge information is now becoming increasingly available through the viral expansion of location-based social networks such as foursquare or Gowalla, it is now possible to explore a third generation of recommender systems, where knowledge about how the place had been used in the past (historical use) or is used at the moment of the inquiry (real-time use) by communities of users is the key element of the system. The main motivation behind the GALE project is to pioneer such third generation recommender systems which would make it possible for the rapidly growing population of 'global' city users to access a level of information, that of the neighborhoods knowledge, which is inherently inaccessible to global repositories, and to do that in real time."
	},
	{
		"grant":501,
		"ID": "EP/K019589/1",
		"Title": "COMIT: Active Content Management at Internet Scale",
		"PIID": "62131",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2016",
		"Value": "486939",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "62131", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Internet is currently passively pushing bits between end-host machines, be it servers, fixed or mobile user devices, or sensors. The network does not 'understand' what is being transferred, i.e., it is not content-aware. This agnostic mode of operation affects several of its key functionalities, for example, efficient content distribution and content-aware traffic engineering. As a result, the network is not able to cope well with the exponentially increasing amounts of multimedia content access which constitute the major mode of use in recent years. Fixed and mobile network providers keep continuously upgrading their infrastructures but the situation has become unsustainable due to their eroding profit margins. There is an urgent need to rethink traffic management under the umbrella of active content management, rather than passive content transfer, allowing ISPs to control traffic better and achieve a sustainable model for the long-term evolution of their networks. New approaches that maximise traffic localisation are essential for long-term global network sustainability.  In this context, Information-Centric Networking (ICN) has emerged as an alternative to the current host-to-host communication paradigm and proposes direct communication between user applications and the content itself, putting the actual information or content in the forefront and disregarding location. In ICN, the network transfers individual, identifiable content chunks, instead of data containers, i.e. packets, with opaque data. Contents are identified by name and relevant packets contain a part of a content chunk; the latter can be retrieved from the hosting server or from an in-network router cache, given that in-network caching is a key aspect of the ICN paradigm. Popular content tends to stay longer in network caches and 'anycast routing' based on content names retrieves the closest copy to the user. This increases dramatically traffic localisation, avoids flash crowd effects and gives to network providers control over the information transferred, allowing them to engineer their networks based on the actual demand for named content.  Despite the considerable amount of effort that has been invested to date by the research community in location-independent routing based on content names, a widely acceptable and scalable solution is yet to be found. Any naming scheme would have to be able to accommodate 10**12 or more objects and content resolution and routing based solely on content names raises serious scalability concerns. In addition, the current IP-based Internet represents a massive infrastructure that cannot be easily replaced by a new, clean slate design. Having this in mind, and given our considerable research experience in the ICN area, we believe it is possible to achieve the ICN benefits of traffic localisation and sustainable network evolution without radical ICN approaches but by introducing, in an evolutionary manner, a 'content layer' in the Internet architecture which will operate above the current network layer and below the transport layer, i.e. layer 3.5.  This layer will intercept communication, will produce unique location-independent names for requested content and will store the latter within the network according to sophisticated caching policies. Content will be accessed in an anycast fashion using ICN style of operation but overlaid over IP, exploiting the existence of scalable IP-based routing, maintaining full backwards compatibility and protecting current investment. In addition, congestion control will be dealt with in a hop-by-hop rather than an end-to-end basis within the content layer, maintaining at the same time compatibility with current end-to-end operation while maximizing the use of available network resources, increasing user quality of experience and paving the way for future Internet applications with stringent real-time requirements."
	},
	{
		"grant":502,
		"ID": "EP/K019694/1",
		"Title": "Characterising electromagnetic fields of integrated electronic systems in enclosures -  a ray-wave approach",
		"PIID": "71851",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "582976",
		"ResearchArea": "Electrical Motors and Drive / Electromagnetics",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Mathematical Sciences",
		"OrgID": "104",
		"Investigators":[
		{"ID": "71851", "Role": "Principal Investigator"},
		{"ID": "15671", "Role": "Co Investigator"},
		{"ID": "81026", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Electronic consumer goods and internet-enabled smart infrastructures require highly integrated miniature electronic systems.  One of the main problem with this miniaturisation is that unwanted interactions can arise between different components. Depending on the rate of change of currents within electronic components, these components radiate electromagnetic (EM) waves which can couple into other parts of the structure and can cause interferences. Controlling electromagnetic interferences within electronic devices is becoming an increasingly important challenge. Digital clock speeds are relentlessly increasing already exceeding 10 GHz in high-performance systems and expected to reach 20 GHz by 2020. This is within range of highly sensitive radio frequencies where analogue blocks and chip-sized components become efficient radiators and receivers. In addition, increasing circuit density and decreasing voltage supplies will result in decreased immunity levels.  Future design processes of integrated electronic systems will therefore have to include a much more detailed electromagnetic compatibility (EMC) characterisation than is done at present. Carrying out EMC studies for complex multi-signal components within a device in a fast and efficient way will simplify design decisions in industry enormously and will help to bring down costs.   The challenges of delivering fast and reliable EMC modelling tools at high frequencies are enormous; determining EM fields in a complex multi-source environment and in the GHz range including multiple-reflections, diffraction and interferences is a hard task already. For realistic electronic devices, the underlying source fields depend in addition on the (a-priori unknown) mode of operation and are thus aperiodic and time dependent; they act in many ways like stochastic, uncorrelated input signals. Indeed, no EMC methodology for modelling transient signals inside and outside of electronic devices originating from decorrelated, noisy sources exists today.   This proposal sets out to meet this challenge head-on by developing an efficient numerical method and accompanying measurement techniques for the modelling of radiated transient EM fields inside and outside of multifunction electronic devices. The new numerical method is based on ideas from wave chaos theory using Wigner-Weyl transformation and phase-space propagation techniques. It makes use of the connections between wave correlation functions and phase space densities. Methods for efficiently propagating these densities have been developed recently by members of the project team.  In this way, we can work directly in terms of statistical measures such as averages and field correlation functions appropriate for stochastic fields. This innovative approach demands input data from measurements which require a rethink of standard measurement techniques. In particular, correlated two-probe near-field measurements of electronic components become necessary which will be developed and tested as part of the project.   The proposed way of approaching EMC issues is completely new and becomes possible only due to the unique mix of expertise available at the University of Nottingham both from the Mathematical Sciences and the Electrical Engineering side Support provided by two industrial partners, inuTech and Computer Simulation Technology (CST), will be vital throughout. This fresh way of thinking will provide the necessary leap within EMC research to satisfy the demands of the electronics industry; it will enhance the applicability of existing EMC protocols and provide the tools to meet the challenges of the future.   "
	},
	{
		"grant":503,
		"ID": "EP/K020218/1",
		"Title": "A Theory of Least Change for Bidirectional Transformations",
		"PIID": "54148",
		"Scheme": "Standard Research",
		"StartDate": "15/03/2013",
		"EndDate": "14/03/2016",
		"Value": "395170",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "54148", "Role": "Principal Investigator"},
		{"ID": "-189931", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A bidirectional transformation is a means of maintaining consistency between multiple information sources: when one source is edited, the others may need updating to restore consistency. There are many domains in which bidirectional transformations are needed. Currently, the best-known of these is model-driven development. Model-driven development is the approach to developing software in which models - specialised descriptions of only certain aspects of the software to be built - are important artefacts in the development. Some or all code may be generated from models, rather than written by hand. Different experts work on different models, adapted to their needs, each recording their decisions in their own model. In order to end up with a correct system, the models need to be kept consistent; when one developer changes one model, another model may need to be changed to match, and vice versa. Another current application is the 'view update' problem in databases (updating source tables after edits to view tables). Potential applications include integrating different electronic health records, maintaining consistency between a high-level systems biology model and a more detailed model of some aspect, providing user-friendly access to machine-oriented data representation, automating the coevolution of a program with its proof of correctness, etc.  A bidirectional transformation can be implemented in terms of several unidirectional restoring functions, one per source; but this duplicates information (for example, information about the structure of the models being related appears in every function), wasting effort and risking inconsistencies. Bidirectional transformation languages allow one to describe the consistency relationship and the restoring functions with a single declarative specification. A good bidirectional transformation language should support the developer of the transformation by not allowing the developer to write nonsense: when a transformation is considered correct according to the language, it should obey basic sanity properties.  Various natural properties of bidirectional transformations are now well understood; in particular, 'correctness' (that the bidirectional transformation does actually restore consistency) and 'hippocraticness' (that if an edited source remains consistent, then the bidirectional transformation makes no changes). What is much less well understood, and what we will study in this project, is the 'principle of least change' (that a bidirectional transformation should not make unnecessary changes when it enforces consistency).  Our hypotheses are: (i) that this Principle of Least Change can be precisely and productively captured; (ii) that our understanding of the Principle will benefit from work on 'provenance' in databases - provenance involves building and using a record of which outputs depend on which inputs, and how they do so; and (iii) that this will lead to a theory of 'alignment' - matching of parts - for non-free datatypes such as associative lists and graphs. Graphs are an especially important datatype, because they are ubiquitous in software engineering. In particular, the models in model-driven development are often types of graph, such as class diagrams and statecharts.  This project will work on the mathematical foundations of bidirectional transformations. The reason for doing this is in order that a better understanding of the Principle of Least Change will, in future, contribute to more usable and more reliable model-driven engineering tools, supporting efficient and correct software development, even in the face of continuous change. It will thus enable software developers to combine the advantages of model-driven development with those of agile methods. By enabling better languages and tools to be developed, it will also make practical the use of bidirectional transformations in a wide range of other application areas."
	},
	{
		"grant":504,
		"ID": "EP/K021788/1",
		"Title": "Enriching, repairing and merging taxonomies by inducing qualitative spatial representations from the web",
		"PIID": "-420555",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/05/2013",
		"EndDate": "30/06/2014",
		"Value": "98966",
		"ResearchArea": "Information Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "28",
		"Investigators":[
		{"ID": "-420555", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Taxonomies encode how different terms or concepts from a given domain are related to each other. They are used to standardise vocabularies (e.g. biologists use taxonomies to organise species into broader categories such as family and order), and to categorise content such that it can be more easily searched (e.g. librarians assigning categories from a taxonomy to books). While taxonomies are traditionally the result of a careful and time-consuming manual process, recent developments in the world wide web have led to a proliferation of taxonomies of a more informal nature. Online retailers such as Amazon, for instance, organise their products using an ad hoc taxonomy, which reflects how customers use their website, rather than any commitment on the semantics of the underlying product categories. Similarly, applications such as Foursquare allow users to contribute to a taxonomy of place types.  While these informal taxonomies are useful to organise online content (e.g. products on Amazon, or venues on Foursquare), they are often of poor quality, and difficult to reuse among different applications. Moreover, like traditional taxonomies, they focus on a very limited set of semantic relations; usually only the relation 'is a sub-category of' is considered. In contrast, in practice the semantic relationship between two categories may not be so clear-cut, among others because of the existence of borderline cases (e.g. should a pub which serves food be categorised as a restaurant?). Nonetheless, the widespread availability of taxonomies is of potentially great interest, provided that they can be improved using automated methods.   The goal of this project is to study how such an improvement can be realised, by statistically analysing meta-data that is available on the web, and in particular from so-called Web 2.0 websites such as Flickr, where users describe photos using short textual annotations called tags.  The proposed approach is built on the idea of discovering semantic relationships between categories by statistically analysing such meta-data. On the one hand, these relations will encode information about typicality and similarity. To see why such relations are useful, consider an application which allows a user to search for restaurants in Cardiff. The search engine may rank venues of type 'restaurant' by taking into account features such as distance to the city centre and average ratings (if available). However, as another criterion, one would also want to see 'normal' restaurants before venues such as breakfast places, coffee houses, or pubs, which may be considered as restaurants, broadly speaking, but are not what users would typically be interested in when querying about restaurants. Similarly, when the user's query asks about 'Sichuan restaurants in Cardiff', and no such restaurants are known, instances of the most similar categories may be shown instead (e.g. Cantonese restaurants).   On the other hand, the relations that are discovered will also encode information that can help us to pinpoint likely errors in existing taxonomies and that can help us to merge different taxonomies to get a single coherent view of a given domain. In particular, these relations will allow us to detect irregularities in existing taxonomies. For example, given the assumption that similar categories usually have similar properties, and the knowledge that Cantonese and Sichuan restaurants are very similar, a taxonomy in which Cantonese and Sichuan restaurants are both sub-categories of Chinese restaurants will be considered more regular than a taxonomy in which they have different super-categories.  Our approach is unique in its data-driven approach to enrich taxonomies with semantic relations for common-sense reasoning, as well as in the proposed methods for repairing and merging existing taxonomies. Regarding applications, the results of this project will form a crucial stepping-stone towards more intelligent search engines. "
	},
	{
		"grant":505,
		"ID": "EP/K022407/1",
		"Title": "Unlocking the potential of fibre-based single-photon sources",
		"PIID": "-413742",
		"Scheme": "First Grant Scheme",
		"StartDate": "17/05/2013",
		"EndDate": "16/05/2015",
		"Value": "104486",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "7",
		"Investigators":[
		{"ID": "-413742", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Huge computational power. 100% secure communications. Ultra-precise measurements. All this (and more) can be yours - and all you require is the ability to make photons one-by-one! What could be simpler?  Photons are fundamental particles of light, and there are lots of them about. Billions are entering your eyes every second as you read this. However, in recent years, we have begun to develop the tools and techniques required to generate and manipulate single photons one at a time. Not only has this enabled experiments that demonstrate the counter-intuitive nature of the hidden quantum world underlying the one we inhabit, but also it has heralded a revolution in the way in which we process information and communicate with one another. Unlike the bits of information that are processed by a normal computer or transmitted through the fibre optic networks that make up the internet, single photons have the capability to carry quantum information: a single photon can exist in a superposition state that is both 1 and 0 simultaneously, and two single photons in remote locations can be closely linked through quantum entanglement. These additional capabilities can be applied in three critical areas: computation can be sped up so that tasks that are intractable even for a supercomputer, for example the simulation of complex systems such as new pharmaceuticals, could be carried out easily by a sufficient number of single photons; communications using single photons allows provably secure information transmission and the detection of any attempted eavesdropping; and measurements made with entangled states of light composed of single photons can increase precision beyond that possible with conventional light sources such as lasers.  Significant progress has been made towards achieving these objectives in research laboratories worldwide. However, generating the single photons needed for quantum information processing is a difficult task in itself. Current implementations of small-scale photonic quantum processors are based on single photon sources that are very unreliable - typically these sources function correctly only approximately 1% of the time. This is a significant problem when trying to scale experiments up, something that must be done now in order to run useful algorithms or transmit quantum information at high data rates. These tasks require many single-photon sources each to produce one photon at the same time - this is not possible with the current generation of single-photon sources as the probability of at least one source failing increases exponentially with the number of sources. Hence current state-of-the-art experiments using just four sources must run for days at a time to capture only a few hundred occasions when all four have fired together.  The aim of this project is to demonstrate a method for dramatically improving the performance of single-photon sources and go some way towards solving this scalability problem. We will build an optical fibre network of individual single-photon sources and combine their outputs using a fast optical switch to create one 'multiplexed' single-photon source. The multiplexed source will have significantly enhanced performance relative to an individual source while retaining all its favourable characteristics such as room-temperature operation and ease of use. Furthermore, the multiplexed source will form the building-blocks of a future ultra-high-performance source: theoretical studies have shown that a few of these blocks together provide the required resource for almost perfect single-photon source operation."
	},
	{
		"grant":506,
		"ID": "EP/K022601/1",
		"Title": "Algorithmic and practical foundations of mobile networking",
		"PIID": "3618",
		"Scheme": "Standard Research",
		"StartDate": "14/08/2013",
		"EndDate": "13/08/2014",
		"Value": "35622",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Laboratory",
		"OrgID": "24",
		"Investigators":[
		{"ID": "3618", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In recent years wireless network technology has gained tremendous importance. It not replaces only more and more so far 'wired' network installations but also opens new dimensions in the availability of high network connectivity in various scenarios. In many application areas the integration of wireless communication in particular together with autonomous sensing devices leads to an improved quality of service due to the immediate availability of measurements and data about the current mode of operation. The goal of this proposal is to investigate different approaches that can improve the performance of ad hoc networks. A wireless ad hoc network consists of several transceivers (nodes) located in the plane, communicating by radio. Unlike wired networks, in which the link topology is fixed at the time the network is deployed, wireless ad-hoc networks have no fixed underlying topology. In addition, the relational disposition of wireless nodes is constantly changing. The temporary physical topology of the network is determined by the distribution of the wireless nodes, as well as the transmission range of each node. The ranges determine a directed communication graph, in which the nodes correspond to the transceivers and the edges correspond to the communication links. Topology control is to allow each node in the network to adjust its transmitting power so that a good network topology can be formed. There are many possible metrics to measure the efficiency of the constructed topology. To increase the longevity of such networks, an important requirement of topology control algorithms is to achieve the desired topology by using minimum energy consumption. To speed up the performance of routing algorithm we aim to produce a communication graph of bounded diameter and low interference levels. On the other hand, from the user's perspective, for already determined network topology, we want to identify the ``important'' places in the network in terms of robustness (failure of some node may lead to disconnected network), energy efficiency (some parts of network might be wasteful in terms of energy), and scheduling (nodes may have many common neighbors which causes many interferences). The main goal of this proposal is to design efficient solutions for topology construction of wireless ad hoc networks and to recognize important topology parts, taking into account various parameters altogether: energy, transport, schedule length, lifetime, hop-diameter and interference levels."
	},
	{
		"grant":507,
		"ID": "EP/K022660/1",
		"Title": "Algorithmic Aspects of Intersection Graph Models",
		"PIID": "-403442",
		"Scheme": "First Grant Scheme",
		"StartDate": "09/10/2013",
		"EndDate": "08/10/2015",
		"Value": "96802",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-403442", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The design and analysis of algorithms on graphs is a major sub-discipline of Computer Science. Graphs (composed of vertices and edges) are ubiquitous not only in Computer Science and Mathematics but across the whole spectrum of Science and Engineering. The vast range of applications of graphs result in a whole host of different properties of interest. This basic fact has motivated the extensive study of structured graph classes, i.e. of families of graphs that all share some common structural property. For example, when graphs are used to model networks-on-chips, it is necessary that such graphs are planar for they need to be laid out on the plane so that none of their edges cross. However, no matter which standard data structures we use to represent graphs, most important structural properties are complex enough to be 'well hidden' within these basic representations. Fortunately, more sophisticated representations exist for many graphs that model practical applications. In particular, a graph is called an intersection graph of a family of sets, if we can bijectively assign a set of this family to a vertex of the graph, such that adjacencies between pairs of vertices in the graph correspond bijectively to non-empty intersections of the corresponding pairs of sets. Such a family of sets is then called the intersection model of the graph.  It turns out that many important graph classes can be described as intersection graphs of set families that are derived from some kind of geometric configuration. Probably the most prominent example of this kind is that of interval graphs, i.e. the intersection graphs of intervals on the real line. The applications of intersection graphs of geometric objects straddle several practical fields, such as biology and bioinformatics (e.g. the physical mapping of DNA and the genome reconstruction), mobile computing and sensor networks, map labeling, etc. Specific intersection models provide a natural and intuitive understanding of the inherent structure of a class of graphs, and turn out to be extremely helpful in delivering efficient algorithms for hard optimization problems, as well as in proving hardness results. Consequently, it is of great importance to establish such intersection models that characterize certain families of graphs. Within the proposed research we plan to explore the various intersection models by which many important families of graphs can be represented, as well as to more deeply understand their underlying combinatorial structure. Moreover, we plan to devise new intersection models for important graph classes, for which no intersection model is known so far. Revealing the inherent properties of intersection models for graphs will essentially help us in understanding the boundaries of efficient computation, in both the traditional sense (polynomial vs. NP-hard) and in the sense of parameterized complexity."
	},
	{
		"grant":508,
		"ID": "EP/K022741/1",
		"Title": "Future filesystems: mechanized specification, validation, implementation and verification of filesystems",
		"PIID": "-390041",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/05/2013",
		"EndDate": "30/04/2015",
		"Value": "91730",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "66",
		"Investigators":[
		{"ID": "-390041", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Filesystems are extremely important. Users depend on filesystems to store their files whenever they hit 'save'. Businesses rely on databases to store their data safely, and these databases in turn rely on the filesystem.  Modern filesystems are designed to satisfy many complicated requirements. As a result, implementations are beset with problems. The implementation code is extremely complex, and almost inevitably contains bugs. These bugs can and do lead to data corruption and loss. Development time is very lengthy. Testing is also very lengthy and costly, and does not guarantee to eliminate all bugs. It is often unclear to application developers what guarantees a filesystem provides, so that it becomes extremely difficult to write correct applications for a given filesystem, let alone applications that are portable across different filesystems.  Even NASA was adversely affected by poor filesystem design. On January 21 2004, mission control lost contact with the Mars Exploration Rover (MER) Spirit. Fortunately Spirit had been equipped with a 'crippled mode capability' to cope with unforeseen errors, and together with extensive efforts by the operations team, Spirit was returned to operational status 16 days later on February 6. The official report states: 'the root cause of the anomaly was a design error in the software module that provides file system services'. Without crippled mode capability, 'the MER mission may well have been lost'. The cost of the mission was estimated at over US$ 800 million.  What can be done to tackle these problems? For many decades, computer scientists have been developing techniques for producing dependable systems. One approach that is receiving much current interest is verification and correctness i.e. mathematical proof that software is correct according to its specification. The widespread need in the digital economy for reliable and dependable systems means that research in this area is increasingly important.  In 2003, Tony Hoare proposed a verification grand challenge for the following 15 years: a verified compiler. Following the NASA MER anomaly, two NASA research scientists, Rajeev Joshi and Gerard Holzmann, proposed a more tractable challenge: to build a verifiable filesystem. We will take up this challenge, and specify and build a verified filesystem.  This work is foundational in nature, but tackles the complexities of real-world systems. We will use very sophisticated and high-level logical techniques combined with state-of-the-art software development techniques to address the full complexity of real-world systems. The verified filesystem has the potential to impact both technologically highly sophisticated organizations such as NASA and internet giants such as Google, Amazon and Facebook, whilst also delivering benefits to end users. A verified filesystem can have a significant impact on many areas of the digital economy. For example, an ultra-reliable filesystem is a key part of the quest for ultra-reliable computing infrastructure, which in turn contributes to the goal of delivering information technology as a utility. Finally, in addition to real-world impact, this research will provide a solid mathematical foundation for further research and verification of critical applications that make use of filesystem functionality, such as databases and persistent message queues. "
	},
	{
		"grant":509,
		"ID": "EP/K023063/1",
		"Title": "Integrated Orbital Angular Momentum Quantum Photonics",
		"PIID": "-95250",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2012",
		"EndDate": "31/03/2014",
		"Value": "245528",
		"ResearchArea": "Light Matter Interaction and Optical Phenomena",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-95250", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Harnessing the quantum mechanical properties of light is an appealing approach to developing future quantum technologies for applications in communications, sensing, simulation and computation. Integrated quantum photonics waveguide circuits and orbital angular momentum are two promising but disparate fields of quantum photonic research that hold great potential for the development of future quantum photonic technologies. Integrated quantum photonic (IQP) waveguide circuits control and manipulate light propagating within mm-sized waveguide circuits, whereas the orbital angular moment (OAM) degree-of-freedom of the photon enables efficient communication of quantum information through the generation of high-dimensional entangled qu-dits propagating in free-space.  This research project aims to merge these two seemingly incompatible technologies to realised integrated quantum photonic waveguide circuits that can generate, manipulate and detect OAM states of light for chip-to-chip transfer of quantum information. This new integration technology will harness the advantages of both IQP and OAM to develop new applications in quantum communications, distributed quantum information processing, generation of high-dimensional entanglement and hyper-entanglement, quantum key distribution and the investigation of fundamental science.  "
	},
	{
		"grant":510,
		"ID": "EP/K023195/1",
		"Title": "Ultimate Control in Semiconductor Lasers",
		"PIID": "-2868",
		"Scheme": "Standard Research",
		"StartDate": "17/09/2013",
		"EndDate": "16/09/2016",
		"Value": "702566",
		"ResearchArea": "Optoelectronic Devices and Circuits",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-2868", "Role": "Principal Investigator"},
		{"ID": "-19872", "Role": "Co Investigator"},
		{"ID": "124428", "Role": "Co Investigator"},
		{"ID": "77840", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Current applications for semiconductor lasers are wide ranging and pervade every aspect of life. Indeed, in the developed world, most people already own several lasers and gain the benefit of many more. With every new technology, this proliferation is set to continue. Most importantly, the laser enables the internet age since all data transmitted around the globe is carried as flashes of laser light. As a consequence most people in the developed world have come to depend on many lasers during a typical day. The reduction in their cost of ownership is therefore of critical importance to the extension of these benefits to the developing world and also bringing new benefits to us all.  The potential future applications of photonics are seemingly unlimited, with new technologies and applications continuing to emerge. The key advantage of a semiconductor laser is that if an application has sufficiently large volume, the cost of the semiconductor laser is very low. The DVD player is a good example -with the laser costing a few pence each. The semiconductor laser therefore enables new technologies, devices and processes to be commercialized. However, semiconductor lasers must be able to generate the required 'flavour' of light; i.e. the correct wavelength, spectral width, power, polarization, beam shape, etc.  Some of the fundamental parameters of a semiconductor laser may be controlled by the design and choice of materials, e.g. wavelength, spectral purity (line-width). However, using current technologies the polarization and beam profile are generally fixed at manufacture and may only be subsequently altered by extrinsic optical components. This introduces additional cost (increasing the environmental impact) and reduces the overall efficiency and usefulness of the device. For future engineers and scientists it would be ideal if there were complete control of the output from a semiconductor laser, providing unlimited possibilities in terms of future applications.   The alteration of matter on the scale of the wavelength of light is known to allow the control of the optical properties of a material. Even the laser in something as simple as a mouse incorporates a number of such technologies. We will develop novel nano-scale semiconductor fabrication to modify light-matter interaction and engineer the control of the polarization and form of a laser beam. Our work will realise a volume manufacturable photonic crystal surface emitting laser (PCSEL) for the first time. The nano-scale photonic crystal is responsible for controlling the properties of the laser. It is simply a periodic pattern similar in size to the light itself, a natural example of this periodic patterning produces the blue colour in some butterfly wings, or the iridescence of opal. In our case, every detail of the photonic crystal will be modeled, understood and optimized to control the properties of the laser to meet a range of needs. Lasers will be designed to exhibit almost zero divergence and will also allow, for the first time, the electronic control of divergence and polarization and allow the direct creation of custom engineered beam profiles and patterns. The realization of high efficiency, area scalable high power lasers with ideal beam profiles will contribute to reduced energy consumption in the manufacture of laser devices, and in their cost of ownership. The technologies developed will allow the ultimate in design control of future optical sources, hopefully limiting laser applications only to the imagination.   Once successful, such devices will displace existing lasers in established commercial photonics and enable many more emerging application areas. This will be made possible by introducing both new functionality to laser devices and reducing the cost of existing products. We will develop this technology alongside physical understanding and device engineering, liaising closely with world-leaders in the volume manufacturer of such devices. "
	},
	{
		"grant":511,
		"ID": "EP/K02325X/1",
		"Title": "Accelerated Coordinate Descent Methods for Big Data Problems",
		"PIID": "-302138",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/11/2013",
		"EndDate": "31/10/2015",
		"Value": "100679",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Mathematics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-302138", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Much of modern society and economy, in the United Kingdom and elsewhere, is moving in the direction of digitization and computation. Humankind is now able to collect and store enormous quantities of digital data coming from sources such as health records (e.g., IBM ``Watson'' project, MRI/CT scans),  government databases (e.g., e-Government, GORS: government operational research service), social networks (e.g., Facebook, Linked-IN, delicious), online news (e.g., New York Times article database), corporate databases (e.g., bank records, Amazon.com) and the internet. Global society is, as a consequence, facing many unprecedented challenges and opportunities. One of the biggest of these has to do with the ability (or rather, lack thereof) to distill, understand and utilize in an optimal way the information contained within these gigantic data sources. The main technology for this is to 'form an optimization problem'' and then solve it using a well-chosen optimization algorithm in a suitable computing environment (e.g., a multicore workstation, GPU-enabled machine, cloud).  In this project we aim to contribute to a breakthrough in our ability to solve optimization problems arising from big data domains via developing, analyzing and implementing new accelerated parallel coordinate descent (CD) methods. Since in big data problems the data is typically highly structured, well-designed CD methods can have very low memory requirements and arithmetic cost per iteration---often much smaller than the dimension of the problem. This is in sharp contrast with standard  methods whose arithmetic complexity of a single iteration depends on the dimension at least quadratically.  Our research objectives are:  1. Acceleration Theory. We will analyze the iteration complexity (i.e., give  bounds on the number of iterations/steps needed to achieve a prescribed level of accuracy) of new parallel coordinate descent methods accelerated using the following 4 strategies: a) nonuniformity (of the frequency with which individual coordinates are updated), b) asynchronicity (of updates and computation), c) distribution (of data and computation to nodes of a cluster) and d) inexactness (of certain operations and computations the algorithm depends on).  2. Stochastic Gradient Descent. We will analyze theoretically and test numerically the relationship between parallel coordinate descent (CD) methods and parallel stochastic gradient descent (SGD) methods.  3. ACDC Code. We will implement the accelerated algorithms in a code which we will make publicly available."
	},
	{
		"grant":512,
		"ID": "EP/K02339X/1",
		"Title": "Acquiring Complete and Editable Outdoor Models from Video and Images",
		"PIID": "51118",
		"Scheme": "Standard Research",
		"StartDate": "23/10/2013",
		"EndDate": "22/10/2016",
		"Value": "959781",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "51118", "Role": "Principal Investigator"},
		{"ID": "17222", "Role": "Co Investigator"},
		{"ID": "-107082", "Role": "Co Investigator"},
		{"ID": "-372746", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Imagine being able to take a camera out of doors and use it to capture 3D models of the world around you. The landscape at large, including valleys and hills replete with trees, rivers, waterfalls, fields of grass, clouds; seasides with waves rolling onto shore here and crashing onto rocks over there; urban environments complete with incidentals such as lamposts, balconies, and the detritus of modern life. Imagine models that look and move like the real thing. Models that you can use with to make up new scenes of your own, which you can control as you please, and render in how you like. You can zoom into to see details, and out to get a wide impression.  This is an impressive vision, and one that is well beyond current know-how. Our plan is to take a major step towards meeting this vision. We will enable users to use video and images to capture large scale scenes of selected types and populate them with models trees, fountains, street furniture and such like, again carefully selecting the types of objects. We will provide software that recognises the sort of environment the camera is in, and objects in that environment, so that 3D moving models can be automatically created.  This will prove very useful to our intended user group, which is the creative industries in the UK: films, games, broadcast. Modelling outdoor scenes is expensive and time consuming, and the industry recognises that video and images are excellent sources for making models they can use. To help them further we will develop software that makes use of their current practice of acquiring survey shots of scenes, so that all data is used at many levels of detail. Finally we will wrap all of our developments into a single system that shows the acquisition, editing and control of complete outdoor environments is one step closer."
	},
	{
		"grant":513,
		"ID": "EP/K024043/1",
		"Title": "Statistical Natural Language Processing Methods for Computer Program Source Code",
		"PIID": "-283417",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "375602",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Informatics",
		"OrgID": "41",
		"Investigators":[
		{"ID": "-283417", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Complex software systems involve many components and make use of many external libraries.  Programmers who work on such software must remember the protocols for using all of those components correctly, and the process of learning to use a new component can be time consuming and a source of bugs.  We believe that there is a major untapped resource that can help address this problem.  Billions of lines of code are readily available on the Internet, much of which are of professional quality.  Hidden within this code is a large amount of knowledge about good coding practices, for example, about avoiding error-prone constructs or about the best protocol for using a particular library.  We envision a new type of programming tool, which could be called data-driven development tools, that aggregate knowledge about programming from a large corpus of mature software projects, for presentation within the development environment.  Just as the current generation of IDEs helps developers to manage their code, the next generation of IDEs will help developers to learn how to write better code.  Fortunately, there is a research field that has already developed a large body of sophisticated tools for analyzing large amounts of text: namely, statistical natural language processing.  The long-term strategic goal of this project is to develop new natural language processing techniques aimed at analyzing computer program source code, in order to help programmers learn coding techniques from the code of others. There is a large area for research here that has been almost completely unexplored.  As a first step in this research area, in this project we will focus on automatically identifying short code fragments, which we call idioms, that occur repeatedly across different software projects.  An example of an idiom is the typical construct for iterating over an array in Java.  Although they are ubiquitous in source code, idioms of this form have not to our knowledge been systematically studied, and we are unaware of any techniques for automatically identifying idioms.  The main objective of this project is to develop new statistical NLP methods with the goal of automatically identifying idioms from a corpus of source code text. We call this research problem idiom mining, and it is to our knowledge a new research problem.  This is an interdisciplinary project that draws from statistical NLP, machine learning, and software engineering. The research work of this project is primarily in statistical NLP and machine learning, and will involve developing new statistical methods for finding idioms in programming language text. "
	},
	{
		"grant":514,
		"ID": "EP/K024272/1",
		"Title": "Modelling Discourse in Statistical Machine Translation",
		"PIID": "-295955",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "31/03/2015",
		"Value": "99127",
		"ResearchArea": "Natural Language Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-295955", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Automatic translation of human languages is an increasing necessity in our global society: large amounts of text are constantly produced in various languages and fast, cheap and accurate translation into a number of other languages is required to foster business and communication within and across nations. This high demand for translations cannot be fulfilled by human translators because of its sheer volume, cost and the lack of skilled professionals.   Different Machine Translation (MT) approaches have been proposed to automate translation. The most widely adopted approach is Statistical MT (SMT): the broad availability of free, open source SMT systems, along with significant improvements in their quality in recent years, has made SMT a very promising technology. This is evidenced by the many commercially successful SMT systems, such as those developed by Google, Microsoft and IBM.   Despite its recent success, SMT systems are still far from producing translations that reach human quality levels. A major limitation is that they translate sentences one by one, in isolation, without resorting to any information about the context in which such sentences appear. This leads to systems that are computationally feasible; however, more advanced approaches that overcome this limitation are needed to improve SMT quality and make it a de facto translation technology. The context surrounding a sentence -- its discourse -- contains information about dependencies connecting words or expressions across sentences. Neglecting such connections can lead to incoherent and inconsistent translations:  -- Humans use different words to refer to the same concepts in different sentences. If the links between these words are not identified, sentences can be incoherently translated. E.g.: in 'The man bought a leather bag' and 'It was soft', Bing Translator misses the connection between 'it' and 'bag'. It produces for Portuguese '[...]. *Ele *foi *suave', rendering a completely inadequate meaning: 'He went smooth'.  -- The same text can appear in different sentences. If the links between these occurrences are not identified, they can be translated inconsistently. E.g.: in 'He took cash from the bank' and 'The bank was far away', only the first sentence has enough information about the correct meaning of 'bank', and thus the second occurrence gets translated as '*margem' in Portuguese (river bank).  SMT is a young area and researchers have so far focused on overcoming issues within sentence boundaries. Most of these issues have been addressed to a large extent in recent years and it is now time to turn to discourse-level challenges. Very few attempts to deal with these challenges have been proposed. These are limited to pre- or post-processing strategies.   This project aims at explicitly modelling discourse level relationships across sentences in SMT at translation time without compromising the scalability of existing approaches. The proposed approach includes (i) a novel framework to model discourse level relationships by learning valid transitions across sentences based on rich linguistic information for both source and target languages and (ii) a constraint-based inference algorithm to use these relationships to guide the translation process while keeping it tractable. By decoupling model learning and inference, a basic SMT model will augmented at inference time with document-wide constraints representing expected discourse relationships that are too expensive or unavailable at model learning time."
	},
	{
		"grant":515,
		"ID": "EP/K024345/1",
		"Title": "Novel High Thermal Conductivity Substrates for GaN Electronics: Thermal Innovation",
		"PIID": "56547",
		"Scheme": "Standard Research",
		"StartDate": "08/07/2013",
		"EndDate": "07/07/2016",
		"Value": "393217",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "56547", "Role": "Principal Investigator"},
		{"ID": "2851", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "AlGaN/GaN high electron mobility transistors (HEMT) are a key enabling technology for future power conditioning applications in the low carbon economy, and for high efficiency military and civilian, microwave and RF systems. Although the performance of AlGaN/GaN HEMTs presently reaches RF powers up to 40W/mm, at frequencies exceeding 300 GHz, their long-term reliability, often thermally limited, is still a serious issue, in the UK & Europe, but also in the USA & Japan.  Corresponding challenges exist for power conditioning applications. To mitigate the present thermal device challenges, the aim of this proposal is innovation and step change in thermal management of AlGaN/GaN HEMT devices by developing novel substrates, in particular (1) high value substrates that have higher heat extraction capability than high cost SiC substrates commonly used for GaN RF applications, and (2) low cost substrates that have improved heat extraction capability to GaN-on-Si substrates for more cost sensitive power electronics markets. The resulting step-change in improvement in heat spreading will improve reliability, circuit efficiency and ease system constraints of GaN electronics. To enable the optimization of the thermal substrate properties key enabling new thermal analysis technologies will be developed. The UK has roadmaps for employing RF and microwave GaN electronics in defence as well as satellite communication. The key UK industrial players in this field include Selex, MBDA, Astrium & others, all requiring reliable and efficient GaN RF and microwave electronics, which the proposed work will advance and enable via the new heat extracting substrate technologies and improved methods of thermal characterisation, furthermore with opportunities for IQE UK, supporter of this proposal, of being a key component in the supply chain for RF GaN applications. The corresponding roadmap for power electronics requires cost-effective GaN presently on Si substrates power devices with UK based manufacture at NXP, supporter of this project, and International Rectifier (IR) which the outcome of this proposed work can innovate. Further business opportunities will emerge with the substrate development itself, such as via Element-6, at IQE through the developments of III-Nitride epitaxial growth for best heat extraction, or spin-out companies. Dissemination of results and insights from this project will be via publications in internationally leading journals, via conferences, via the UK Nitrides Consortium, i.e., established dissemination routes will be used to transfer knowledge into academia, and directly with the industrial supporters of this project, as well as other companies Bristol and Bath have links to (e.g. Selex, MBDA). The CDTR in Bristol and the III-Nitride group in Bath have both a strong track record in being successful using these dissemination routes, in particular with companies. The field of thermal management of semiconductor devices is an important academic research field, and is especially topical and useful at the current stage of implementation of this genuinely disruptive technology. It not only trains UK workforce for industry, but also it is essential to help maintain the present high level of device physics and engineering in the UK. It provides stimulus for an efficient interaction between universities and industry to maximize benefit of EPSRC research investment. This includes in this project interaction with UK industry, in particular, IQE, NXP, and Plessey in this project."
	},
	{
		"grant":516,
		"ID": "EP/K024914/1",
		"Title": "CD-GAIN: Content Delivery Using Graph-based Analysis of Interest Networks",
		"PIID": "-401877",
		"Scheme": "First Grant Scheme",
		"StartDate": "15/08/2013",
		"EndDate": "14/10/2014",
		"Value": "100272",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Telecommunications",
		"OrgID": "78",
		"Investigators":[
		{"ID": "-401877", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Recent years have seen a sea change in the Internet traffic mix, with the Web moving from  primarily text-based content towards rich-media such as video and audio streaming. This has imposed significant additional costs both on content providers as well as on Internet Service Providers (ISPs).     This project proposes a novel approach that saves costs by identifying and actively enhancing ISP-local content availability. The core idea is that requests for content items can be served from ISP-local copies, where one is available. ISP-local access creates a synergy, with content providers saving on streaming costs, ISPs decreasing their cross-border transit traffic,  and users obtaining local copies, isolating them from effects beyond the ISP such as packet losses, route failures or congestion in the network core.  Interest-based social networks, which are increasingly available on many content provider sites, provide an ideal framework to engineer the availability of ISP-local copies. Using the interest-based network, communities of users who are interested in similar items can be identified within each ISP. Such communities can serve as repositories of ISP-local copies of items they are interested in, and can support different access rates by pushing additional copies of items among themselves as required. Because members of the community have a high affinity for items they are responsible for, they might have a local copy available, having accessed the item in the past. Alternately, they might be likely to access the item in the future, so if a copy is pushed to them, the expected overhead of the push could be balanced against the future access.  Methods will be developed to identify high quality communities useful for sharing content, by weighting or ranking links based on shared content consumption, predicting additional ISP-local links that have not yet been self-identified by users, and adapting community detection methods to create multi-resolution communities with 'core' and 'peripheral' members, to support fewer or additional ISP-local copies, as required, to sustain different rates of simultaneous content access from the ISP's userbase.  The project will take a data-driven approach, using extensive real-world traces from leading content providers both to derive new patterns in the interest network that can be exploited for content delivery, and also to evaluate the benefits of proposed content delivery architectures under realistic workloads. In analysing the traces, the project will develop new characterisations of interest-based social  networks using content consumption histories and chart ISP-level content availability from a content provider's viewpoint, both of which would be of  independent academic and commercial value."
	},
	{
		"grant":517,
		"ID": "EP/K025090/1",
		"Title": "Detecting Induced Graph Patterns",
		"PIID": "-26269",
		"Scheme": "Standard Research",
		"StartDate": "25/09/2013",
		"EndDate": "24/09/2016",
		"Value": "363442",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-26269", "Role": "Principal Investigator"},
		{"ID": "15129", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A graph is a network consisting of nodes called vertices and links between those nodes called edges representing some relationship such as individuals that are connected to other individuals in a social network. Graphs are ubiquitous, not only in science and engineering but also in real life, as is the study of whether a given graph H appears as a pattern within another given graph G so that G can be transformed to H via a sequence of operations. For instance, can a social network G be compressed to a smaller (and easier to analyze) network H without destroying too much information? This example shows that the notion of appearing as a pattern depends upon the operations allowed when transforming G into H. We consider the following four elementary graph operations:   1. vertex deletions 2. edge deletions 3. edge contractions 4. vertex dissolutions.  A vertex deletion removes a vertex (and its adjacent edges) from the graph. An edge deletion removes an edge from the graph. An edge contraction removes the vertices u and v of the edge (u,v) from the graph and replaces them by a new vertex that is made adjacent to precisely those remaining vertices to which u or v was previously adjacent. A vertex dissolution is the removal of a vertex v from the graph with exactly two neighbours u and w followed by the inclusion of the edge (u,w).   Combining these four graph operations leads to ten essential graph containment relations. For example, a graph H is called a minor of a graph G if H can be obtained from G by a sequence of vertex deletions, edge deletions and edge contractions (and so also vertex dissolutions), whereas H is an induced minor of G if we do allow vertex deletions and edge contractions but no edge deletions. Each graph containment relation corresponds to a decision problem:   subject to the specified containment relation, does a graph G contain some graph H?  In order to answer this question we must design a so-called algorithm, which can be seen as a set of instructions, like a recipe for preparing a meal, but with the purpose to turn it into a computer program to solve the problem automatically. A crucial aspect is the running time, i.e., the time it will take the computer to solve the problem. However, it may well be possible that the problem falls into the category of discrete optimization problems for which no reasonably fast algorithm is known, and for which the existence of such an algorithm is even considered to be unlikely.   One of the most important and fundamental achievements of Theoretical Computer Science and Discrete Mathematics is Robertson and Seymour's Graph Minor Project. They have provided a structural characterization of graphs without a forbidden minor and have designed an algorithm that solves any problem H-Minor in cubic time; the latter problem is to decide whether a given graph contains some fixed graph H (i.e., that is not part of the input) as a minor. Their theory has led to deep results across Computer Science and Mathematics. An important consequence of their theory is that any containment problem allowing edge deletions can be efficiently solved.   Our over-arching aim is to develop a theory, similar to that of Robertson and Seymour, but on induced containment relations, i.e., when edge deletions are not permitted graph operations.  As techniques that are useful for their non-induced counterparts can no longer be applied, a basic theory for induced containment relations, similar to the Graph Minor Project of Robertson and Seymour, is largely absent. Our research proposal aims to change this."
	},
	{
		"grant":518,
		"ID": "EP/K025643/1",
		"Title": "Multiscale Signal Processing for Next Generation Electroencephalography",
		"PIID": "87220",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2013",
		"EndDate": "31/05/2016",
		"Value": "401722",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "87220", "Role": "Principal Investigator"},
		{"ID": "-28425", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": " This proposal seeks to develop a fundamentally new multiscale framework for data-adaptive exploratory analysis of multivariate real-world processes. This will be achieved through a rigorous treatment of both within- and cross-channel intrinsic signal features, spanning time, space, frequency and entropy. Particular emphasis will be on approaches that are free of statistical assumptions and mathematical artefacts, and match the time-varying oscillatory modes inherent in multivariate data. This will help bypass the mathematical obstacles associated with currently used techniques (Fourier, wavelet), which rely on fixed basis functions and integral transforms, thus colouring the representation, limiting their accuracy, and restricting their applicability in problems involving real-world drifting and noisy information.  For multiscale data current statistical and information theoretic measures are inadequate, as they will indicate high correlation for two data channels that share common noises, but do not contain the same useful signal. The proposed data-adaptive analysis framework will resolve such issues, and will create natural 'intrinsic' data association measures (intrinsic multi-correlation, intrinsic multi-information). While current univariate data-adaptive approaches have enormous potential, they are not suitable for direct application to multivariate or heterogeneous sources, as they are bound to create a different number of basis functions for every data channel.  Wearable systems, such as bodysensor networks, strive to find a balance between performance and user benefits (low cost, ease of use), and require next-generation signal processing tools to establish the extent to which they can produce valuable information. The thrust of this proposal is on developing rigorous, data-adaptive, compact, and physically meaningful signal processing solutions in order to provide an algorithmic support for progress in multi-sensor and wearable technologies.  Our own initial multivariate data-adaptive solutions show great promise; they need to be further developed and comprehensively tested for data exhibiting rotation-dependent (noncircular) distributions, power imbalance, uncertainty, and noise. With the aid of nonlinear optimisation in the algorithmic design and insights from dynamical complexity science and multiresolution information theory, our approach promises a quantum step forward in multivariate data analysis, and a significant long-term impact.  The successful outcomes of this proposal will open radically new possibilities for advances in areas that depend on multi-sensor data, and a new front of research in applications dealing with uncertainty, noncircularity, complexity, and nonstationarity in multi-channel recordings. To maximise the short- to medium-term impact of this work and for cost effectiveness, we consider applications in emerging wearable technologies for brain monitoring, in collaboration with the Royal Brompton Sleep Clinic in London and Aarhus University in Denmark.  "
	},
	{
		"grant":519,
		"ID": "EP/K026232/1",
		"Title": "GaN Electronics: RF Reliability and Degradation Mechanisms",
		"PIID": "56547",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "540317",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "22",
		"Investigators":[
		{"ID": "56547", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "AlGaN/GaN high electron mobility transistors (HEMT) are a key enabling technology for future high efficiency military and civilian microwave systems. The aim of this proposal is to provide transformative insight into the underlying physical processes that cause degradation in GaN RF power amplifiers (PA).  This is of strategic importance for the UK given its strong RF electronics base, due to the fact that GaN RF power electronics delivers a disruptive step change in systems capability through power densities as high as 40W/mm and frequencies exceeding 300GHz. The UK has internationally leading academic research groups in this field, including Bristol and Cardiff. The key issue addressed in this proposal is that device degradation under RF stress is distinctly different than under DC stress, often resulting in a large increase in source resistance, something that never occurs under DC stress and is not explicable by conventional models. This observation implies that a device in RF operation applies voltage/current stresses, which are inaccessible under static conditions, making it imperative to understand the interaction between the RF operating mode and the degradation mechanism.  Bristol has provided seminal contributions to the international effort to understand DC GaN transistor degradation, where an understanding is slowly emerging that includes oxygen related reactions and diffusion processes, and dislocation linked breakdown in GaN transistors. This includes electroluminescence imaging for detection of leakage pathways, dynamic transconductance and transient analysis to detect trapping states, and the simulation of the effect of pulsed operation on bulk and surface traps. Over the last 15 years, Cardiff has established a world leading capability in RF PA design and measurement. In particular waveform engineering systems enable RF current/voltage waveforms to not only be measured directly but also to be manipulated almost at will. This manipulation of the waveform has allowed Cardiff to make seminal contributions to the understanding of high efficiency RF PA operation. In this project, the unique capability to 'tune' RF operation into extremely well defined states to enable 'controlled' RF stressing will be used to gain the step change understanding of RF device degradation. Reverse engineering of failed devices, electrical and electro-optical measurement before/after and during the RF stress, combined with physical device simulation, will be used to determine the RF specific degradation mechanisms. This capability to predict, engineer and measure the RF waveforms is key to achieving an understanding of the RF stresses that devices undergo during PA operation, and then to determine and specify the safe-operating-area for HEMTs.  This project utilises a partnership with state-of-the-art foundries in Germany and the USA, allowing the project to use production quality devices, essential for the relevance of the work. The project will be guided in terms of its relevance through guidance and interaction with Selex for systems level issues and IQE for the materials. The key synergy of Bristol and Cardiff will address a vitally important issue for the uptake of this disruptive technology, the identification of the RF degradation mechanisms. This will enable the impact of different modes of RF operation to be predicted, and a novel robust RF reliability test methodology to be developed, thus delivering large UK benefit and international impact. "
	},
	{
		"grant":520,
		"ID": "EP/K026992/1",
		"Title": "Modelling Human Brain Development",
		"PIID": "-108516",
		"Scheme": "Standard Research",
		"StartDate": "23/09/2013",
		"EndDate": "22/09/2016",
		"Value": "465494",
		"ResearchArea": "Biological Informatics",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Sciences",
		"OrgID": "98",
		"Investigators":[
		{"ID": "-108516", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The neural network of the human brain is arguably the most complex biological pattern; however, the mechanisms forming such neural systems are unclear. Neural systems show structurally emergent properties in terms of their topology and functionally emergent properties concerning information processing. Structural properties are the rise of modular and hierarchical connectivity, of long-distance connections, and of highly connected nodes. Functional properties are the distribution and integration of information, the formation of specialized modules dealing with different tasks, and a rapid reaction time due to parallel processing.   Recent advances in neuroimaging, using diffusion tensor imaging, allow us to observe how the human brain network differs over ages ranging from the embryonic to the adult stage (age of 20 years). This project will analyse how the human brain network arises during development by combining data analysis with simulations of brain development. Objectives are to develop a simulation of human brain development, to analyse network features of human brains at different developmental stages, and to compare simulations with real data to discover the underlying mechanisms for brain network development. Simulations are crucial to study the role of different developmental parameters on the final brain network as well as on intermediate networks during development. Understanding how parameters lead to (adult) network features will help to evaluate the contribution of these parameters to healthy and pathological development. Once understanding the time course of development, we should also be able to predict the probabilities of future stages of development. This will be crucial for giving a prognosis for the progression of developmental diseases. In addition to understanding the formation of human cognitive systems, these results will inform the design and update of artificial information processing systems.      Identifying these key developmental mechanisms will greatly improve our understanding of emergence in biological systems. In addition, it might lead to several predictions about the rise of brain disorders such as schizophrenia, epilepsy, and autism that often originate during development and are linked to changes in hub organization.  Beyond biological pattern formation, such 'algorithms' for human brain development could inform us how to build artificial intelligent systems. Rather than constructing artificial brains in a top-down manner, applying identified mechanisms for neural network development will allow the emergence of intelligent information processing systems leading to systems that are more adaptable. In summary, the formation of human brains is a fundamental question that touches on the emergence of natural and man-made information processing systems.  "
	},
	{
		"grant":521,
		"ID": "EP/K029746/1",
		"Title": "Gyrotron Travelling Wave Amplifier for high field Electron Paramagnetic Resonance and high frequency Dynamic Nuclear Polarisation",
		"PIID": "68356",
		"Scheme": "Standard Research",
		"StartDate": "01/06/2013",
		"EndDate": "31/05/2016",
		"Value": "411720",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Physics",
		"OrgID": "48",
		"Investigators":[
		{"ID": "68356", "Role": "Principal Investigator"},
		{"ID": "58233", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of this program is to develop a stable, high power, wideband vacuum tube amplifier and sub-millimetre wave solid state driver, and demonstrate their use for a wide range of applications that cover the scientific spectrum from biochemistry through physics, electrical engineering and energy studies. In the proposed gyrotron amplifer, electrons are made to travel along a helical path in a magnetic field. The design of the device is such that the electrons' kinetic energy is transferred to electromagnetic radiation confined to the same region, thereby amplifying the latter. We will investigate a novel concept which uses a 5-fold helical corrugation on the inside surface of a 'cylindrical' waveguide to radically modify the electromagnetic wave dispersion giving eigenmodes with finite, constant group velocity in the region of near infinite phase velocity. This novel dispersion opens up for the first time the potential for a high power (100 W), broadband (~10 %), high gain (>40 dB) gyrotron amplifier in the 360 to 395 GHz frequency range.   We have performed a preliminary experiment at W-band, 92 to 98 GHz, frequencies and will build on our lead to create an amplifier in the 360 to 395 GHz frequency range based on the best understanding of this new concept and perform precision measurements of its gain, bandwidth, efficiency and stability against oscillations. This development would represent a generic technology with major commercial and scientific applications. These include improving NMR sensitivity through Dynamic Nuclear Polarisation techniques, high field pulse Electron Paramagnetic Resonance spectroscopy, materials processing, fusion diagnostics and long range, high bandwidth, line of sight communications. The proposal is a collaboration between two of the UK's leading millimetre wave groups: the Atoms, Beams and Plasmas Group at the University of Strathclyde and the Millimetre Wave Technology Group at the STFC Rutherford Appleton Laboratory (RAL). Collectively these groups have decades of experience and strong international reputations in the development of high power mm-wave sources, instrumentation and components, with a strong track record in commercialisation, links with industry and delivering on project objectives. The proposed work is in a core area that is likely to lead to UK leadership in advanced high frequency, high power millimetre wave amplifiers and solid state sources that will impact on studies on high field EPR/DNP spectroscopy in years to come.  In the course of the work, Strathclyde staff will design the gyrotron amplifier and pass the designs to RAL for precision manufacture of the fine features, which will be cut into a helix with the diameter of the pencil lead using state of the art computer controlled milling machines. After preliminary testing of their waveguiding properties at RAL, the components will be passed to Strathclyde for final testing and integration with the custom built electron source and vacuum enclosure. RAL will also design and build the frequency multiplier cascade required to drive the novel amplifier: they will use new configurations of their GaAs Schottky diodes to make these components. The frequency coverage, gain and maximum output power of the new amplifiers are such that world class sources are needed to provide the high input powers necessary to exploit fully the amplifier properties."
	},
	{
		"grant":522,
		"ID": "EP/K030353/1",
		"Title": "Explorative Test Oracle Generation",
		"PIID": "-417031",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "31/03/2015",
		"Value": "92718",
		"ResearchArea": "Software Engineering",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-417031", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Testing is a crucial part of any software development process. Testing is also very expensive: Common estimations list the effort of software testing at 50% of the average budget. Our society increasingly depends on a working information infrastructure for more and more aspects of civic, commercial, and social life, while software at the same time becomes ever more complex. For example, a modern car has up to 100 million lines of software code, and software errors can easily lead to fatal consequences. Improving techniques to identify errors in software is therefore of utmost importance.  Manual testing is common practice in software development. As manually testing a program is a laborious and error prone task, automation is desirable. However, automation requires the user to specify the correct behaviour up-front in terms of a specification, or later by adding test oracles to automatically generated tests - both alternatives are difficult. This problem is obliterated as test quality is usually measured with oracle-agnostic code coverage metrics. In truth, however, a test without a good oracle cannot find software bugs. This is the oracle problem, one of the longest standing and greatest remaining challenges in software testing.  As both writing specifications and writing test oracles is difficult and needs to be done manually, this proposal aims to push automation further by exploring the middle ground: The novel concept of an oracle template allows to specify what should be tested and checked, but crucially, it does not require specifying the expected behaviour. Instead, automated test generation instantiates user-specified oracle templates to concrete tests with oracles, and the developer decides case by case about correctness. Thus, programs can be tested without the developer needing to write a specification or having to suffer through seemingly purposeless generated tests. Because test generation is driven by oracles, all tests have a purpose and the essential oracles required to be effective at finding software bugs.  The novel concept of oracle templates requires extension of the current state of the art in test generation, as current techniques either assume the existence of an automated oracle (e.g. a specification) or focus exclusively on the code. This creates three challenges, which will be addressed in this project:  -- Existing code-based testing techniques focus on reaching points in the code. This project will define the concept of oracle templates, and will explore test generation based on oracle templates as a search problem. Given an oracle template, search-based testing techniques will automatically create instances, which are test cases with oracles.  -- Systematic testing is traditionally driven by the idea that a good test set covers all the code, which completely ignores the test oracle problem. This project will define systematic criteria and corresponding search-based test generation techniques to thoroughly test programs based on oracle templates. These criteria will ensure coverage of oracle templates, but will also ensure that the code is executed and checked by oracles (e.g. by applying mutation and data-flow analysis).  -- It is impossible to take the human out of the software testing loop completely. Oracle templates are an attempt at minimizing the human effort, but the task of writing oracle templates still requires manual effort. Therefore, this project will explore strategies to automatically synthesise oracle templates based on standard testing patterns and usage examples. Ultimately, a developer would have all tests and oracles generated automatically on the click of a button, leaving only the task of confirming correctness of the produced examples.  The success in addressing these challenges will be measured using automated experiments, controlled studies with student subjects, and industrial case studies at Google and Microsoft.  "
	},
	{
		"grant":523,
		"ID": "EP/K030469/1",
		"Title": "Learning to learn how to design drugs",
		"PIID": "49796",
		"Scheme": "Standard Research",
		"StartDate": "31/10/2013",
		"EndDate": "30/10/2015",
		"Value": "401412",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "93",
		"Investigators":[
		{"ID": "49796", "Role": "Principal Investigator"},
		{"ID": "-203864", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A key step in developing a new drug is to learn quantitative structure activity relationships (QSARs).  These are mathematical functions that predict how well chemical compounds will act as drugs. QSARs are used to guide the synthesis of new drugs.  The current situation is: 1) There is a vast range of approaches to learning QSARs. 2) It is clear from theory and practice that the best QSAR approach depends on the type of problem. 3) Currently the QSAR scientist has little to guide her/him on which QSAR approach to choose for a specific problem.    We therefore propose to make a step-change in QSAR research.  We will utilise newly available public domain chemoinformatic databases, and in-house datasets, to systematically run extensive comparative QSAR experiments.  We will then generalise these results to learn which target-type/ compound-type/ compound-representation /learning-method combinations work best together.   We do not propose to develop any new QSAR method.  Rather, we will learn how to better apply existing QSAR methods.  This approach is called 'meta-learning', using machine learning to learn about QSAR leaning.   We will make the knowledge we learn publically available to guide and improve future QSAR learning.  "
	},
	{
		"grant":524,
		"ID": "EP/K031864/1",
		"Title": "COALGEBRAIC LOGIC PROGRAMMING FOR TYPE INFERENCE: Parallelism and Corecursion for New Generation of Programming Languages",
		"PIID": "-185954",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "280590",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing",
		"OrgID": "37",
		"Investigators":[
		{"ID": "-185954", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The main goal of typing is to prevent the occurrence of execution errors during the running of a program.  Milner formalised the idea, showing that ``well-typed programs cannot go wrong''.  In practice, type structures provide a fundamental technique of reducing programmer errors. At their strongest, they cover most of the properties of interest to the verification community.  A major trend in the development of functional languages is improvement in expressiveness of the underlying type system, e.g., in terms of Dependent Types, Type Classes, Generalised Algebraic Types (GADTs), Dependent Type Classes and Canonical Structures. Milner-style decidable type inference does not always suffice for such extensions (e.g. the principal type may no longer exist), and deciding well-typedness sometimes requires computation additional to compile-time type inference.  Implementations of new type inference algorithms include a variety of first-order decision procedures, notably Unification and Logic Programming (LP), Constraint LP, LP embedded into interactive tactics (Coq's eauto), and LP supplemented by rewriting.  Recently, a strong claim has been made by Gonthier et al that, for richer type systems, LP-style type inference is more efficient and natural than traditional tactic-driven proof development.  A second major trend is parallelism: the absence of side-effects makes it easy to evaluate sub-expressions in parallel. Powerful abstraction mechanisms of function composition and higher-order   functions play important roles in parallelisation. Three major parallel languages are Eden (explicit parallelism) Parallel ML (implicit parallelism) and Glasgow parallel Haskell (semi-explicit parallelism).  Control parallelism in particular distinguishes functional languages.  Type inference and parallelism are rarely considered together in the literature. As type inference becomes more sophisticated and takes a bigger role in the overall program development, sequential type inference is bound to become a bottle-neck for language parallelisation.  Our new Coalgebraic Logic Programming (CoALP) offers both extra expressiveness (corecursion) and parallelism in one algorithm. We propose to use CoALP in place of LP tools currently used in type inference.  With the mentioned major developments in Corecursion, Parallelism, and Typeful (functional) programming it has become vital for these disjoint communities to combine their efforts: enriched type theories rely more and more on the new generation of LP languages; coalgebraic semantics has become influential in language design; and parallel dialects of languages have huge potential in applying common techniques across the FP/LP programming paradigm.  This project is unique in bringing together local and international collaborators working in the three communities.  The number of supporters the project has speaks better than words about the timeliness of our agenda.   The project will impact on two streams of EPSRC's strategic plan: 'Programming Languages and Compilers' and  'Verification and Correctness'.  The project is novel in aspects of Theory (coalgebraic study of (co)recursive computations arising in automated proof-search); Practice (implementation of the new language CoALP and its embedding in type-inference tools); and Methodology (Mixed corecursion and parallelism)."
	},
	{
		"grant":525,
		"ID": "EP/K032968/1",
		"Title": "NaaS: Network-as-a-Service in the Cloud",
		"PIID": "-128335",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "666149",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-128335", "Role": "Principal Investigator"},
		{"ID": "-200592", "Role": "Co Investigator"},
		{"ID": "-369623", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Cloud computing has significantly changed the IT landscape. Today it is possible for small companies or even single individuals to access virtually unlimited resources in large data centres (DCs) for running computationally demanding tasks. This has triggered the rise of 'big data' applications, which operate on large amounts of data. These include traditional batch-oriented applications, such as data mining, data indexing, log collection and analysis, and scientific applications, as well as real-time stream processing, web search and advertising.  To support big data applications, parallel processing systems, such as MapReduce, adopt a partition/aggregate model: a large input data set is distributed over many servers, and each server processes a share of the data. Locally generated intermediate results must then be aggregated to obtain the final result.  An open challenge of the partition/aggregate model is that it results in high contention for network resources in DCs when a large amount of data traffic is exchanged between servers. Facebook reports that, for 26% of processing tasks, network transfers are responsible for more than 50% of the execution time. This is consistent with other studies, showing that the network is often the bottleneck in big data applications.  Improving the performance of such network-bound applications in DCs has attracted much interest from the research community. A class of solutions focuses on reducing bandwidth usage by employing overlay networks to distribute data and to perform partial aggregation. However, this requires applications to reverse-engineer the physical network topology to optimise the layout of overlay networks. Even with perfect knowledge of the physical topology, there are still fundamental inefficiencies: e.g. any logical topology with a server fan-out higher than one cannot be mapped optimally to the physical network if servers have only a single network interface.   Other proposals increase network bandwidth through more complex topologies or higher-capacity networks. New topologies and network over-provisioning, however, increase the DC operational and capital expenditures-up to 5 times according to some estimates-which directly impacts tenant costs. For example, Amazon AWS recently introduced Cluster Compute instances with full-bisection 10 Gbps bandwidth, with an hourly cost of 16 times the default.   In contrast, we argue that the problem can be solved more effectively by providing DC tenants with efficient, easy and safe control of network operations. Instead of over-provisioning, we focus on optimising network traffic by exploiting application-specific knowledge. We term this approach 'network-as-a-service' (NaaS) because it allows tenants to customise the service that they receive from the network.  NaaS-enabled tenants can deploy custom routing protocols, including multicast services or anycast/incast protocols, as well as more sophisticated mechanisms, such as content-based routing and content-centric networking.  By modifying the content of packets on-path, they can efficiently implement advanced, application-specific network services, such as in-network data aggregation and smart caching. Parallel processing systems such as MapReduce would greatly benefit because data can be aggregated on-path, thus reducing execution times. Key-value stores (e.g. memcached) can improve their performance by caching popular keys within the network, which decreases latency and bandwidth usage compared to end-host-only deployments.  The NaaS model has the potential to revolutionise current cloud computing offerings by increasing the performance of tenants' applications -through efficient in-network processing- while reducing development complexity. It aims to combine distributed computation and network communication in a single, coherent abstraction, providing a significant step towards the vision of 'the DC is the computer'."
	},
	{
		"grant":526,
		"ID": "EP/K032984/1",
		"Title": "Investigating the Power Density of Power Electronics",
		"PIID": "-393162",
		"Scheme": "First Grant Scheme",
		"StartDate": "30/09/2013",
		"EndDate": "29/09/2015",
		"Value": "100395",
		"ResearchArea": "Non CMOS Device Technology",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Engineering",
		"OrgID": "28",
		"Investigators":[
		{"ID": "-393162", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The power density of power electronics is defined by the power processed in a given volume or mass of converter (volumetric and gravimetric power densities respectively) where the converter is normally defined as the combination of power semiconductor devices, the input and output filters, the thermal management system and control electronics that together provide an interface between two or more electrical systems.   The power density of power electronics is an important consideration in modern engineering systems as in many applications it has become attractive to use electrical power distribution and actuation in place of mechanical, hydraulic or pneumatic systems to reduce overall system mass and complexity, along with improving efficiency and reliability. An understanding of the future capabilities of power electronics as well as the bottlenecks limiting increases in performance of power electronic systems will enable the UK to play a leading role in the development and application of this key technology.   Designing a converter is a complex engineering task that balances many interacting pressures: A designer must select from a range of possible designs and technologies, finding the optimal allocation of mass and volume for each component and within each component whilst meeting electrical and thermal specifications.  It the hypothesis of this proposal that in order to reach a true power density optimum, the design process must consider system-wide electrical, mechanical and thermal problems simultaneously: It is not enough to design each component individually based on their electrical specification alone.   This research project will produce software that will optimise the power density of a converter subject to a particular electrical and thermal specification by considering all major aspects of the design under a global electrical, mechanical and thermal optimisation framework. By formulating a set of strict rules governing the design and placement of each component within the converter, the effect of basic design decisions and fundamental material properties on the overall power density of a converter can be explored. The project will provide industry and academia with reliable and justified figures for best-in-class power density achievable now and into the future, identify technology bottlenecks into which application of additional resources bring large improvements in achievable power density, illustrate explicitly important trade-offs in modern converter design and provide converter optimisation tools that can be used and extended by academia and industry.   "
	},
	{
		"grant":527,
		"ID": "EP/K033166/1",
		"Title": "Efficient Energy Management in Energy Harvesting Wireless Sensor Networks: An Approach Based on Distributed Compressive Sensing",
		"PIID": "-418850",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "587661",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-418850", "Role": "Principal Investigator"},
		{"ID": "-169872", "Role": "Co Investigator"},
		{"ID": "98704", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Future deployments of wireless sensor network (WSN) infrastructures for environmental, industrial or event monitoring are expected to be equipped with energy harvesters (e.g. piezoelectric, thermal or photovoltaic) in order to substantially increase their autonomy and lifetime.  However, it is also widely recognized that the existing gap between the sensors' energy availability and the sensors' energy consumption requirements is not likely to close in the near future due to limitations in current energy harvesting (EH) technology, together with the surge in demand for more data-intensive applications. Hence, perpetually operating WSNs are currently impossible to realize for data-intensive applications, as significant (and costly) human intervention is required to replace batteries.  With the continuous improvement of energy efficiency representing a major drive in WSN research, the major objective of this research project is to develop transformative sensing mechanisms, which can be used in conjunction with current or upcoming EH capabilities, in order to enable the deployment of energy neutral or nearly energy neutral WSNs with practical network lifetime and data gathering rates up to two orders of magnitude higher than the current state-of-the-art.  The theoretical foundations of the proposed research are the emerging paradigms of compressive sensing (CS) and distributed compressive sensing (DCS) as well as energy- and information-optimal data acquisition and transmission protocols. These elements offer the means to tightly couple the energy consumption process to the random nature of the energy harvesting process in a WSN in order to achieve the breakthroughs in network lifetime and data gathering rates.  The proposed project brings together a team of theoreticians and experimentalists working in areas of the EPSRC ICT portfolio that have been identified for expansion. This team is well placed to be able to develop, implement and evaluate the novel WSN technology. The consortium also comprises a number of established and early stage companies that clearly view the project as one that will impact their medium and long term product developments and also strengthen their strategic links with world class academic institutions.  We anticipate that a successful demonstration of the novel WSN technology will generate significant interest in the machine-to-machine (M2M) and Internet of Things (IoT) industries both in the UK and abroad. "
	},
	{
		"grant":528,
		"ID": "EP/K033840/1",
		"Title": "Large-area electronics based on two-dimensional atomically thin materials",
		"PIID": "-420725",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2014",
		"Value": "103072",
		"ResearchArea": "Graphene and Carbon Nanotechnology",
		"Theme": "Information and Communication Technologies",
		"Department": "Materials",
		"OrgID": "77",
		"Investigators":[
		{"ID": "-420725", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": " Novel forefront technological products such as, paper like displays, stretchable sensor skin, electronic textiles, and robotic sensors require high speed processing on unconventional form factor substrates, which can be bendable, flexible, stretchable and that they can assume different geometries. Therefore, field effect transistors with performances on a par with the wafer-based conventional electronics and at the same time flexible, compatible with sensitive substrates (plastic/rubber) are required. As the performance figures of FETs are intrinsically related to the ultimate electronic properties of the channel material and its interfaces, the need is to have materials responsive to all aforementioned demands.   At present, large area electronics on plastic and unusual format electronics use low performing materials such as organic conductors or metal oxide or alternatively newly emerging Si-like materials shaped into thin membranes, which are fabricated by multi step process exploiting the existing industry infrastructure with the related high costs. Graphene has been identified as a suitable electronic material for all kind of electronic technologies, owing to its mechanical, electrical, optical, chemical properties. Although graphene is the ideal material as a transparent electrode, it does not have a band gap hindering its use as a channel material. Here I propose studying a new range of 2D atomically thin materials, which share optical and mechanical properties of graphene, but in addition they are semiconducting with a band gap (1.1-1.9 eV).   Moreover, their high carrier mobility, uniquely distinguishes them from graphene as channel material for development of heterogeneous electronics. These materials are practically appealing as they offer realistic pathways to manufacture devices due to their 2 dimensional geometry facilitating integration, they do not have dangling bonds on the basal plane, allowing manipulation as individual particles in solution and they have unique mechanical features, with effects related to shape distortions and folding. Simultaneously, they present quantum and other size-dependent effects leading to a wealth of electronic, phonon dynamics and optical properties, not found in zero- and one-dimensional materials, offering opportunities to extend the frontiers of their applications to spintronics, photovoltaic, catalysis etc. The main objective is to demonstrate low-voltage n/p-type FETs operating in logic inverters, which are the elemental units of logic electronics.  Toward this end, the aim is to develop novel solution phase processing necessary to isolate monolayer flakes with preserved atomic and electronic structure, from their 3D counterpart and create stable inks of these platelets. These inks will be then exploited to establish a reliable array of scalable deterministic assembly techniques of the flakes onto any substrates in the form of highly uniform ultrathin films over large areas. These materials will be interfaced with the atomically thin organic dielectrics, which secure low-power operation. In addition both, 2D membranes as well as the organic components are transparent in the optical range due to their ultimately thin thickness leading to semitransparent devices.  The solution based processing at room temperature will ensure low cost manufacturing and compatibility with any plastic/rubber substrates, and reduction of energy employed for fabrication addressing also worldwide need for energy saving. The raw materials cost is also competitive in comparison with the existing materials for electronics. Overall the proposed research can lead to new economic benefits, extend the frontiers of the present electronic technology, and open new scenarios in fundamental science in respect to new quantum and other size-phenomena. "
	},
	{
		"grant":529,
		"ID": "EP/K033905/1",
		"Title": "Verification of resource-bounded multi-agent systems (VRBMAS)",
		"PIID": "71546",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "280864",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "104",
		"Investigators":[
		{"ID": "71546", "Role": "Principal Investigator"},
		{"ID": "88857", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A multi-agent system is a system which is comprised of multiple interacting agents. An agent is an autonomous entity that has the ability to collect information, reason about it, and perform actions based on it in pursuit of its own goals or on behalf of others. Examples of agents are controllers for automatic devices such as satellites, health care systems, non-driver transport systems such as UAVs, and even nodes in sensor networks. Multi-agent systems are ubiquitous.  Many distributed software and hardware systems can be naturally modelled as multi-agent systems. Such systems are by nature of their components extremely complex, and the interaction between components can lead to undesired behaviours that are not easy to detect. Hence, automated verification of multi-agent systems is a very important and thriving research area. We propose an important advance in this area, namely producing verification tools which allow users to specify which resources the agents need for their actions. Current tools do not offer a systematic framework for modelling resource requirements for agents' actions. However, such an ability is very important for specification of many systems, for example sensor networks where the nodes have very limited resources, e.g. energy. "
	},
	{
		"grant":530,
		"ID": "EP/K033948/1",
		"Title": "Evo-Bots - From Intelligent Building Blocks to Living Things",
		"PIID": "-361617",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/07/2013",
		"EndDate": "31/10/2014",
		"Value": "100905",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Automatic Control and Systems Eng",
		"OrgID": "116",
		"Investigators":[
		{"ID": "-361617", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "One of the grand challenges in robotics - and an enormous driver for technology - is to make robots more like living systems. If this was achieved, robots would act more autonomously, become more flexible, and be able to repair themselves. Life-like robots could even serve as models of natural organisms. They could be used to help answer three of the Top 25 Big Questions facing science over the next quarter-century (see 125th-anniversary issue of Science): Are we alone in the universe? How and where did life on Earth arise?  How far can we push self-assembly?  Our long-term vision is to create the first non-biological living system through evolution in the natural world. This project takes arguably a radical approach: creating living systems without using the building blocks of biological systems. Rather, the building blocks, called evo-bots, will be synthesised from scratch. It is expected that evo-bots, similar to RNA/DNA, can give rise to novel forms of life.   Evo-bots are expected to be a game changer in robotics. They are mobile robots that control when to move; however, their direction of motion is entirely dictated by their environment. We believe that this trade-off between mobility and extreme simplicity is an ideal compromise that will enable the fabrication of massively distributed robotic systems composed of millions of units. This will pave the way for a whole new range of applications, for example, in water engineering (micro-robots for inspection tasks) and clinical/healthcare technologies (micro- or nano-robots to operate inside the human body)."
	},
	{
		"grant":531,
		"ID": "EP/K033956/1",
		"Title": "Memoryless computation and network coding",
		"PIID": "-413398",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2015",
		"Value": "96437",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39",
		"Investigators":[
		{"ID": "-413398", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Information theory has recently undergone a formidable development, with the emergence of diverse techniques including iterative decoding and space-time codes. These have been used in different modern communication standards, thus indicating that information theory provides the most appropriate approach to design communication systems. However, it still faces many challenges, notably its possible application to different fields, such as biology, machine learning or complexity theory.  Arguably one of the recent great developments in the field is network coding, a revolutionary technique to transmit data through a network. Unlike routing, network coding lets the intermediate nodes combine the messages they receive, thus achieving a higher throughput. While routing treats information like an ordinary commodity, network coding transmits data by taking full advantage of the specific nature of information. As such, it can dramatically outperform routing in terms of throughput, robustness to network topology changes and packet losses. Network coding has attracted a large amount of research and has inspired other applications such as distributed storage and content distribution.   Memoryless computation is a new paradigm for computing functions, which offers two main innovations. First, it computes functions in a radically novel way. Unlike traditional computing, which views the registers as 'black boxes,' memoryless computation takes advantage of the nature of the information contained in those registers and combines the values of the different registers. Thus, it can be seen as the analogue of network coding for computing. The second innovation lies in the computational model, which offers to use any possible update of a register, without communicating with the memory. This model aims at emulating computations as they are carried out in a core, for they mostly involve manipulations of registers. An update is viewed as a quantum of complexity, hence the complexity measure is the number of updates required to compute a function, regardless of how complex each update could be.  Memoryless computation offers several advantages over traditional computing in theory. First, it offers a computational speed-up at the core level: for some classes of functions, we can obtain arbitrarily shorter programs than the traditional approach. Secondly, memoryless computation does not rely on additional buffers and hence performs computations in line. Memory management is a tedious task which can significantly slow down computations; this is particularly important for parallel architectures with shared memory. Although this problem can be alleviated by using different levels of cache, it still uses more hardware and brings a significant overhead. Memoryless computation offers a radical alternative: it uses no memory at all. It thus eases concurrent execution of different tasks by preventing memory conflicts. It also optimises the use of a crucial and expensive resource and offers another speed-up by avoiding any communication with the data memory.  Therefore, memoryless computation is an innovative approach to computing, with high potential to speed up computationally expensive problems and applications including multicore architectures and parallel computing. The long term objective of research in memoryless computation is to determine where it could be advantageous over traditional means of computing and to build hardware which could fully benefit from those principles. The proposed research is fundamental work which will lay the necessary foundations for a possible future implementation of memoryless computation ideas. In particular, we aim at evaluating the computational speed-up offered by memoryless computation, designing efficient instruction sets and extending the existing framework to take parallel threads into account."
	},
	{
		"grant":532,
		"ID": "EP/K034081/1",
		"Title": "Image Understanding for Alpine Environmental Monitoring",
		"PIID": "-372746",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2013",
		"EndDate": "30/11/2014",
		"Value": "98835",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "-372746", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In an era of anthropogenic global change, there is an increasing need to measure and monitor our natural environment. Time-lapse photography can provide compelling visualisations of how natural systems behave over time, for example, glacial recession due to global warming. However, in addition to visualisation, cameras can also be used as sensors, making measurements that aid the scientific understanding of natural systems.  The aim of this project is to develop novel computer vision techniques to monitor long-term change in outdoor environments, turning an SLR into a useful sensor for environmental monitoring. In particular we will use time-lapse imagery to extract detailed surface information and subtle motion in scenes, something that is not possible using existing satellite-based techniques. Reaching this goal will require improved techniques for robust image understanding that are capable of extracting useful information, such as very small scene motion, over long timescales and with wildly varying appearance and illumination conditions. "
	},
	{
		"grant":533,
		"ID": "EP/K034626/1",
		"Title": "Visits to University of California, Berkeley, Stanford University, and SRI International",
		"PIID": "122907",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "04/03/2013",
		"EndDate": "03/03/2014",
		"Value": "21059",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Telecommunications",
		"OrgID": "78",
		"Investigators":[
		{"ID": "122907", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During his visit to the Department of Statistics, University of California, Berkeley, Prof. Cvetkovic will be hosted by Prof. Yu, in the Statistical Machine Learning group. He will be focusing on topics of  the current activity of Prof. Yu's group, that can be broadly described as  statistical machine learning theory, methodologies, and algorithms for solving high-dimensional data problems. Particular problems covered include  sparse modelling (e.g. Lasso, compressed sensing), structured sparsity, analysis and methods for spectral clustering, and applications to  data which come from a diverse range of interdisciplinary areas, ranging from neuroscience to social networks. During this visit, Prof. Cvetkovic and Prof. Yu will set forth directions for  collaboration on  problems in learning in high-dimensions, leading to a research grant proposal.  During his previous EPSRC project, EP/D053005/1, Prof. Cvetkovic in collaboration with Prof. Sollich, Department of Mathematics, King's College London, and Prof. Yu developed an unorthodox approach to robust speech recognition in high-dimensional spaces of acoustic waveforms of speech. Dr. Horacio Franco, Director of Speech Technology and Research Laboratory of SRI International, who in 2010 won a major DARPA award for solving the problem of the  sensitivity of automatic speech recognition systems to additive noise, finds this approach groundbreaking and expresses a strong interest in exploring venues for collaboration. The purpose of this visit would be to investigate ways in which the approach developed by Prof. Cvetkovic and his collaborators can be brought closer to practice and based on that investigate the directions of long-term collaboration and possible joint grant proposals between SRI International, King's College London, and UC Berkeley.  At King's College, Prof. Cvetkovic has commenced work on a new multichannel audio technology, supported by EPSRC grant EP/F001142/1. The project produced a considerable  publication volume and patent portfolio. A visit to one of world leading centres for music and acoustics technologies, such as CCRMA, would be very beneficial for taking advantage of this gained momentum to penetrate the field, which is still a new application area for Prof. Cvetkovic, at a deeper level,  expand its scope, establish collaborations, and inform future grant proposals. At CCRMA, Prof. Cvetkovic will be interacting primarily with Prof. Julius Smith, working on multichannel audio technologies, and  other signal processing problems in audio and acoustics. A recent work of Prof. Cvetkovic  complements a large volume of work of Prof. Smith on ultra fast rendition of multichannel audio using  digital waveguide networks (DWNs). This is an area which is of a significant academic interest, requiring interdisciplinary approaches at the interface of signal processing, acoustics, psychoacoustic, and computer science, as well of a great relevance to virtual reality and gaming applications. While this would be the area of initial focus, at CCRMA there are  several other ongoing projects which are closely related to Prof. Cvetkovic's  research or research in the Institute of Telecommunications at King's (Mobile Phone Orchestra, Sound in Space, Music in Virtual Worlds),  as well as projects which could provide valuable inspiration for possible collaborative projects between the Department of Music and  the Institute of Telecommunications at King's and CCRMA  (Sound Waves on the Internet for Real-time Echoes, and the Historical Recordings). Finally, most of the largest companies which are potential licensees of Prof. Cvetkovic'c audio technology, such as DTS, Dolby, Microsoft, are based on the west  coast of the US. The presence of Prof. Cvetkovic at CCRMA would accelerate the exploration of licensing opportunities, as these and other relevant companies frequently visit CCRMA, and are situated in the Bay Area or not too far from it."
	},
	{
		"grant":534,
		"ID": "EP/K035959/1",
		"Title": "Symbolic Support for Scientific Discovery in Systems Biology",
		"PIID": "-107268",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/08/2013",
		"EndDate": "31/10/2014",
		"Value": "98673",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "22",
		"Investigators":[
		{"ID": "-107268", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The aim of this proposal is to advance the UK's world-leading research on the automation of science by developing novel Artificial Intelligence (AI) support for an existing laboratory robot called Eve (whose predecessor Adam was popularised by Time Magazine and Science in 2009). The purpose of this project is to develop a new logic-based reasoning tool that will allow robots to correct errors in their knowledge. Unlike prior work aimed at extending knowledge that is incomplete, we argue such machines also need the ability to revise knowledge that is incorrect. Indeed, we suggest the capacity to make (and learn from) mistakes is an indispensible part of scientific reasoning. Thus our goals are to realise this ability in a software system for automating intelligent inference about scientific theories and experiments and to demonstrate its benefit in a genuine application of Eve.  We believe this will pave the way to a new era in which Robot Scientists will be more productive, more cost-effective, and better able to assist humans in all parts of scientific method.  This project is based on the hypothesis that ground-breaking advances in a field of AI known as Answer Set Programming (ASP) can be used to develop a novel form of (multi-semantic meta-logical) reasoning that will give Robot Scientists the ability to continuously revise and extend their knowledge. Evidence to support this claim is provided by 2 preliminary studies which link the applicant's previous work on the integration of abductive and inductive inference with the robot Adam and a leading ASP system called Clasp. The 1st study showed how a combination of non-monotonic and non-deductive logic can be used to revise a state-of-the-art metabolic model of yeast metabolism in order to fit data seen by Adam; but it also showed a further combination of meta-logical and multi-semantic logic was needed to design new experiments for testing the proposed revisions. The 2nd study suggests how a combination of features recently included in Clasp can be used to do this. Hence this proposal affords a timely opportunity to draw together and build upon these complementary strands of research in a way that will open the door to exciting new opportunities for scientific discovery in systems biology.  The most direct beneficiaries of this work will be our collaborators in the Robot Scientist group (now at the Univ. of Manchester) as our software will enable their robot to correct mistakes in its knowledge and thereby allow the continual evolution of scientific models through many cycles of analysis and experiment. This will represent a major step towards Robot Scientists that participate more effectively in science. By making our tools portable we hope to facilitate their application in other tasks that will benefit from their enhanced reasoning abilities. These tasks include planned follow-on work in the modelling of social insect behaviour (previously studied by our research group) and the automation of some aspects of legal reasoning (recently formalised in argumentation theory). We also plan to study probabilistic extensions of this research that can be built on the logical foundations we will lay. Once our system has been deployed on the Robot Scientist, we also hope to use data generated by planned applications of Eve in high-throughput drug screens to improve our understanding of living organisms. "
	},
	{
		"grant":535,
		"ID": "EP/K037056/1",
		"Title": "Wavelength tunable, pulsewidth selectable, repetition rate variable, fibre based infra red source",
		"PIID": "15535",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2014",
		"Value": "111539",
		"ResearchArea": "Optical Devices and Subsystems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Physics",
		"OrgID": "77",
		"Investigators":[
		{"ID": "15535", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Using either passive or active mode locking techniques, short pulses are generated in conventional laser systems with a repetition rate that is primarily determined by the round trip time of the cavity. In the vast majority of cases the physical size of the cavity components results in the repetition rate being in the 10s of Megahertz regime. It is often difficult to vary and if it is variable it generally involves a reconfiguration of the cavity. Similarly, the duration of the pulses produced by the mode locking technique are predetermined by the cavity parameters and are not readily selectable over large ranges. Optical filters can be placed in the cavity to restrict the bandwidth and so affect the temporal format of the generated pulses. However, inserting such filters usually requires a reoptimisation of the cavity and pulse widths cannot be continuously selected. This places considerable restriction on the versatility and applicability of conventional laser systems. The configuration we propose employs a single pass technique. The repetition rate of the generated pulses is simply determined by the drive frequency to an in line phase modulator. In conjunction with a transmitting edge filter the signals are turned into a corresponding series of pulses. Since the input signal is also a continuously operating laser it can be easily tuned over the complete gain window of the seed laser.  In order to achieve continuously selectable pulse widths from this assembly, use is made of the fact that they can be made operate in the soliton supporting region of an optical fibre. Here the nonlinear intensity dependent non linear effects can be balanced by anomalous dispersion. The generated soliton has a fixed 'area'. The product of intensity and duration is constant, consequently if the soliton can be amplified, and this must be undertaken adiabatically, such that no energy disperses from the soliton, its duration will decrease. The slow, exponential gain provided by Raman gain in an optical fibre is ideal for this slow adiabatic amplification. Control of the gain, simply allows control of the pulse duration and by simply changing the pump power the output pulse duration is continuously selectable over a large range. We propose to seed the system from a narrow line thulium fibre laser and amplify in a specially designed high gain Raman amplifier fibre in an all fibre configuration that will allow average powers of up to 2 watts, pulse durations selectable between 20 picoseconds and 200 femtoseconds, at repetition rates between 5 and 10 GHz . Extension up to 20 GHz should be possible in the near term. Using additional follow on in fibre non linear optical processes should permit continuous tuning from 1950 nm to 2500 nm, providing unique performance characteristics in a region of importance for medical application and molecular fingerprinting.           "
	},
	{
		"grant":536,
		"ID": "EP/K037102/1",
		"Title": "Constrained low rank matrix recovery: from efficient algorithms to brain network imaging",
		"PIID": "-26511",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2014",
		"Value": "94392",
		"ResearchArea": "Digital Signal Processing",
		"Theme": "Information and Communication Technologies",
		"Department": "Faculty of Engineering & the Environment",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-26511", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The proposed research concerns the development of efficient computational algorithms to factorise matrix data into a low-rank representation, where the factors satisfy several constraints. Two scenarios are of interest:  1) The entire data-matrix is known and the goal is the decomposition of the data into explanatory components that reveal underlying data structure.  2) The data is only partially observed and the decomposition is also used to recover the un-observed data.  1) is often used to remove noise from data (e.g. to clean up images ) or to decompose data into several distinct components (e.g. to separation different speakers in a recording), while 2) is used, for example, by online retailers, who use recommender system to recommend products based on previous purchases or in medical imaging, where we want to reduce a patient's exposure to radiation. In general, better matrix factorisation techniques will enable us to a) acquire data faster, safer and cheaper; b) acquire data at a higher resolution; and c) find better interpretations of data in terms of meaningful underlying factors.   These improvements will be made possible through the development and exploitation of better data models. In particular, we will develop models and algorithms that are able to utilize a range of non-convex constraints, such as sparseness, smoothness, contiguity, block structure and low-rank. Each of these constraints has been individually exploited previously and each was found to be able to capture distinct data features. For example, the usefulness of sparse data models for data recovery has attracted significant attention (e.g. in medical imaging), whilst for matrix data, low-rank models are now becoming widely used (e.g. in recommender systems). We here build on our previous work on the efficient recovery and factorization of data and develop algorithms that can exploit more than one of these constraints. Instead of imposing either sparsity or low-rank, we will develop methods that will enable us to efficiently exploit several constraints jointly. This will have a transformative impact on many applications where data structure can be captured using several constraints, but where each single constraint is not strong enough to offer substantial benefits.   For example, in radio astronomy, observations might be missing, either due to inability to monitor certain regions of the sky or due to inability to physically store the vast amount of data generated by modern radio observatories. The structure in this data is only partially captured by any one constraint and can thus not be fully recovered with current approaches.  Here we are particularly inspired by our current work in functional brain imaging. Magnetic Resonance Imaging (MRI) techniques can be used to measure human brain activity whilst a person is at rest. This type of data provides crucial insights into information processing mechanisms in the living human brain and can also be used to reveal neural mechanisms underlying many brain disorders. Matrix factorization methods are already used as one of the main tools to analyse these data-sets. Current methods construct a low-rank approximation of the spatio-temporal data matrix, describing spatial regions that exhibit joint neural activity, thus revealing several distinct networks of connected brain regions.  Our new methods will significantly improve on current approaches. Advanced data models will allow us to better estimate functional neuro-anatomy and will provide better recovery of under-sampled fMRI data using far fewer measurements. This will speed up data acquisition, reduce cost and provide data of higher quality. This in turn will enable us to develop better techniques to study the healthy human brain as well as to detect and study neural processes that underlie different brain diseases. "
	},
	{
		"grant":537,
		"ID": "EP/K037633/1",
		"Title": "Semantic Types for Verified Program Behaviour",
		"PIID": "-247649",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "265061",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "7",
		"Investigators":[
		{"ID": "-247649", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Types help programmers to combine program components correctly, avoiding errors. Subtype polymorphism adds flexibility by allowing programs of different types to be used safely in the same context - as a means of structuring programs it has already found extremely widespread and fruitful application in object-oriented languages. Second-order features such as bounded quantification and type operators allows programmers further control over the type of code which is passed between programs, constituting powerful descriptive tools for the modular combination and reuse of code.  Denotational semantics is used to construct precise models of programming languages which abstract away from implementation detail and allows programs to be proved correct by reasoning about their interpretations as mathematical objects. This project aims to use semantics to understand the highly complex structures captured by second-order type systems, while developing the new capacity to use them to describe and reason about computational behaviour of programs and the environments in which they may be evaluated: for example, preventing malicious code from compromising security by constraining its access to control or information flow. It will develop the capacity to verify such properties and to use proofs directly and indirectly in the construction of programs. It will study types and type systems using game semantics, which describes programs in terms of their interaction with the environment, as a formal two player game. This reflects the behavioural properties that we wish to reason about, like control and information flow, elegantly captures key computational side-effects, like local state, and lends itself to powerful algorithmic and operational techniques for reasoning about them. Moreover, it allows a simple notion of intensional semantic subtyping to be formalised and investigated: a type S represents a subtype of T if program behaviour in T is available in S, and environment behaviour in S is available in T. By combining this with recently developed intensional representations of second-order types as games, the project will develop new models - of programming languages with higher-order state, dynamic binding, bounded quantifiers, and type constructors - new type theories using these features to represent program behaviour, computational effects and the environments in which code may be run - and new reasoning techniques for verifying program properties by model checking, type checking, and operational methods."
	},
	{
		"grant":538,
		"ID": "EP/K038125/1",
		"Title": "Active Quasi-Optics for High-Power THz Science",
		"PIID": "-110926",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "465158",
		"ResearchArea": "RF & Microwave Devices",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76",
		"Investigators":[
		{"ID": "-110926", "Role": "Principal Investigator"},
		{"ID": "117586", "Role": "Co Investigator"},
		{"ID": "12031", "Role": "Co Investigator"},
		{"ID": "126291", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Light is the most familiar manifestation of an electromagnetic wave. These waves extend continuously from radio and TV transmissions, through mobile communications and WiFi, to microwaves, infrared, light, an finally to ultraviolet and X-rays. For many of these waves, compact, high power, room temperature sources have been developed: the microwave oven and the laser are everyday examples. One part of this spectrum, the terahertz region, which lies between the microwave and infrared wavelengths, is technologically challenging as regards providing sources. Transistor devices, as used for radio, are not able to switch fast enough, and fundamental physics limits the power of sources that are bright in the infrared and optical regions. Our proposal aims to provide a source that will be compact, efficient, operate at room temperature in air, and which will be more powerful and cheaper than alternatives.  Realisation of the source will enable new science and facilitate technology developments. High power terahertz waves can be used in biochemistry,  the new field of bio-electromagnetics, and in chemical synthesis where the application of the terahertz wave affects the way that a chemical reaction proceeds. Higher power is needed for pulsed radars, for example future ground and spaceborne cloud radars that will provide input to national Met Offices and climate modellers. There are also potential military, security and industrial applications, where the possibility to transmit power through an absorbing material may be critical. This is the basis of scanners found at airports, where terahertz radiation is not believed to be a risk to public health associated with the ionising radiation from x-ray alternatives.   How will the source be made? The novel approach depends on microscopic semiconductor devices, called Schottky diodes, which are designed specifically to generate harmonics of an input frequency. In other words, the output wave is a distortion of the input. We shall specifically design dual purpose antennas for use with these diodes. These novel antennas (which we have called 'Multennas' - multiplying antennas) will receive an input signal from a lower frequency illuminating source antenna, couple it to the Schottky diode, and then preferentially retransmit the desired harmonic. As each diode can only handle a small amount of power, it will be necessary to combine the outputs of many diodes to create a powerful source. The proposed way is to pattern an array of flat antennas on a plate of terahertz dielectric material, and to solder the Schottky diodes in place. The driving terahertz waves will arrive through the plate, and the total emitted wave will be the sum of the contributions of tens or hundreds of elements.  Design and fabrication of the 'Multennas' is challenging precision work, and sophisticated software, dedicated apparatus and expertise is needed.   Scientists and engineers from two of the UK's leading research institutes, Queen Mary University of London (QMUL) and the STFC Rutherford Appleton Laboratory (RAL), have joined forces to tackle the terahertz source problem. A team of experienced personnel at QMUL, who possess the antenna design skills and test facilities, will undertake these aspects of the project. An initial challenge will be to improve existing software to be able to model novel multenna structures. At RAL, where the team specialises in the production of world class Schottky diode devices, bespoke diodes will be designed, fabricated and mounted to the antennas on the supporting plate. Other scientists at QMUL will add tiny light-activated tuning devices to the array,  made of a novel plastic whose properties can be changed by light. These tuners are needed to improve the performance as a whole, and to compensate for inevitable variations between the individual Schottky devices. The same material will be used to introduce tuneability to other elements in the network of the novel source."
	},
	{
		"grant":539,
		"ID": "EP/K038494/1",
		"Title": "Future-proof massively-parallel execution of multi-block applications",
		"PIID": "43886",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2016",
		"Value": "280147",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Oxford e-Research Centre",
		"OrgID": "106",
		"Investigators":[
		{"ID": "43886", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "For many years, increasing the clock frequency of microprocessors has led to steady improvements in performance of computer applications. This gave an almost free performance boost to the speed of applications without having to re-write software for each new generation of processors. However, increasing the performance of processors in this manner led to an unsustainable increase in energy consumption.  Thus, to gain higher performance chip developers now rely on multiple cores operating in parallel.  The latest CPUs have up to 10 cores, each with a vector unit producing up to 8 single precision floating point results per clock cycle, while the latest graphics processors (GPUs) have up to 2688 much simpler cores operating in groups of 32.  This move into manycore computing has led to considerable hardware innovation, and it is likely that the next 10 years will see further rapid evolution in computer architectures.  This poses huge challenges to application developers who naturally wish to concentrate on their engineering and scientific applications and how best to model them, without having to worry about the details of modern computer architectures. To address this, there are a range of efforts within scientific computing to develop high-level software packages or frameworks so that the application developer can specify what they want to be computed at a high level, and then the package takes care of the implementation details.  Building on prior EPSRC-funded research to develop a framework called OP2 for unstructured grid applications, this proposal aims to develop a future-proof extension called OPS to handle the needs of multi-block structured grid applications.  Developers' applications can be written in FORTRAN or C, using a carefully-designed application programming interface (API), and then OPS generates customised code for the implementation on different hardware target platforms.  As well as customising for the different hardware, two other optimisation approaches will be adopted.  One is the use of ``tiling'' to overlap the execution of parallel loops which are usually executed sequentially. This improves both performance and energy efficiency by reusing data within the cache, cutting down on the number of times data is moved between the processor and the main memory.  This is something which is becoming increasingly important on modern architectures because the energy cost and time taken for data movement is much greater than for floating point operations.   The other optimisation is the use of run-time optimisation for applications which execute for a long time.  The backend implementations are parameterised, with parameters controlling aspects such as the number of threads in a thread block, or the size of a ``tile'' in the tiling optimisation.  The optimal values for these parameters are not known a priori, and it could significantly affect the performance.  By dynamically varying the values, and timing the consequential changes in performance, we can implement heuristics to iteratively improve the parameter values during the execution.  The new OPS framework will be assessed, both for performance and ease-of-use, by applying it to two important academic CFD codes, ROTOR developed at Bristol by Prof. Chris Allen, and SBLI developed by at Southampton by Prof. Neil Sandham.  As well as being important codes in their own right, these are also representative of the needs of other codes within CCP12 (Computational Engineering), the UK Turbulence Consortium, and the UK Applied Aerodynamics Consortium.  "
	},
	{
		"grant":540,
		"ID": "EP/K038575/1",
		"Title": "Automated Game-Theoretic Verification of Security Systems",
		"PIID": "-149490",
		"Scheme": "First Grant Scheme",
		"StartDate": "04/11/2013",
		"EndDate": "03/11/2014",
		"Value": "98762",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-149490", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "We are surrounded by computerised systems, upon whose secure and reliable operation we are increasingly dependent. Yet flaws in these systems are common, from power plants to travel cards, and these come at high costs for individuals, companies and governments alike. So, rigorous, mathematically-sound techniques to check the security of computerised systems are essential. Security, though, is not absolute: we may only be able to guarantee that an attack on a system is possible with low probability, rather than impossible. Furthermore, system designs often need to trade off the degree of security or privacy offered against other practical concerns such as response time or power consumption. So, effective methods for the analysis of security also need to take these quantitative aspects into account.  This project will develop fully-automated techniques to formally verify the correctness of security systems, to identify flaws in their operation, and to fix or optimise aspects of their design. We will do so by bringing together techniques from several different areas: (i) game-theoretic methods, to reason about the interactions between a security system and its potential attackers; and (ii) automated verification and synthesis techniques, with a particular emphasis on quantitative aspects such as probability or resource usage. Building upon recent advances in these fields, and upon existing efforts to create efficient and scalable verification methods, this project will develop novel techniques to verify security systems, implement them in freely-available software tools and apply them to a variety of security applications, from electronic voting schemes to anonymous communication networks. "
	},
	{
		"grant":541,
		"ID": "EP/K039431/1",
		"Title": "Partial order semantics for concurrent program verification",
		"PIID": "-399067",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2013",
		"EndDate": "28/02/2015",
		"Value": "98000",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-399067", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Multiprocessor machines are now predominant, as most laptops, desktops, servers, mobile phones and aircrafts routinely have multiple to many cores. Unfortunately, concurrent programming is error-prone, which now affects everyone given this trend towards more and more concurrency.  Let us mention for example a recent concurrency bug found in the PostgreSQL database (see http://archives.postgresql.org/pgsql-hackers/2011-08/msg00330.php).  PostgreSQL is one of the most popular database nowadays, and many websites rely on its correct functioning.  This bug was particularly difficult to observe (and indeed is not fixed yet) because it only occurred on a multicore machine, and a particular hardware platform, IBM Power.  Reproducing such bugs is as hard as observing them; thus testing can hardly discover them. To prove a program free of errors, we would like to devise automated techniques that analyse the code without executing it.  Thus, we can relieve programmers from the burden of writing the proofs of their programs.  Yet, automatic verification of concurrent programs represents a challenge, whether it aims at proving the full correctness of a program (e.g. a program sorting a list actually sorts the list), or at checking specific properties (e.g. the program is free of data races) short of full correctness. We focus here on the latter: we would like to enhance the scalability of tools checking that a concurrent program does not violate certain safety-critical properties of interest.  We would like to show that scalable automatic verification can be achieved by exploiting the rich history of partial orders for modeling concurrency. "
	},
	{
		"grant":542,
		"ID": "EP/K040006/1",
		"Title": "Network Coded Modulation for Next Generation Wireless Access Networks",
		"PIID": "2320",
		"Scheme": "Standard Research",
		"StartDate": "01/09/2013",
		"EndDate": "31/08/2016",
		"Value": "585003",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics",
		"OrgID": "55",
		"Investigators":[
		{"ID": "2320", "Role": "Principal Investigator"},
		{"ID": "76902", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In view of the rapid increase in demand for mobile data services, next generation wireless access networks will have to provide greatly increased capacity density, up to 10 Gbps per square kilometre.  This will require a much larger density of very small, cheap and energy-efficient base stations, and will place increasing demand on the bandwidth and energy efficiency of the network, and especially the backhaul network.  Recent work on network MIMO, or coordinated multipoint (CoMP) has shown that by ensuring base stations cooperate to serve users, especially those close to cell edge, rather than interferring with one another, inter-user interference can be effectively eliminated, greatly increasing the efficiency of the network, in terms of both spectrum and energy.  However this tends to greatly increase the backhaul load.    This work proposes a form of wireless network coding, called network coded modulation, as an alternative to conventional CoMP.  This also enables base station cooperation, but instead of sending multiple separate information flows to each base station, flows are combined using network coding, which in principle allows cooperation with no increase in backhaul load compared to non-cooperative transmission, while gaining very similar advantages to CoMP in terms of bandwidth and energy efficiency.    The objective of the proposed work is to establish the practical feasibility of this approach, and evaluate its benefits, as applied to next generation wireless access networks.  To this end it will develop practical signalling schemes, network coordination and management protocols, and, with the help of industrial collaborators, will ensure compatibility with developing wireless standards."
	},
	{
		"grant":543,
		"ID": "EP/K040049/1",
		"Title": "Boosting Automated Verification Using Cyclic Proof",
		"PIID": "-95896",
		"Scheme": "Standard Research",
		"StartDate": "11/11/2013",
		"EndDate": "10/11/2016",
		"Value": "550181",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-95896", "Role": "Principal Investigator"},
		{"ID": "-230056", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Automatic verification tools based on separation logic have recently enabled the verification of code bases that scale into the millions of lines. Such analyses rely on the use of *inductive predicates* to describe data structures held in memory. However, such predicates are currently hard-coded into the analysis, which means that the analysis must fail when encountering an unknown data structure, not described by the hard-coded definitions. This results in reduced program coverage and increased rates of false negatives. Thus, methods for reasoning with *general* inductively defined predicates could greatly enhance the state of the art.  Cyclic proof, in essence, implements reasoning by infinite descent  la Fermat for general inductive definitions.  In contrast to traditional proofs by explicit induction, which force the prover to select the induction schema and hypotheses at the very beginning of a proof, cyclic proof allows these difficult decisions to be *postponed* until exploration of the proof search space makes suitable choices more evident.  This makes cyclic proof an attractive method for automatic proof search.  The main contention of this proposal is that cyclic proof techniques can add inductive reasoning capability, for general inductive predicates, to the many components of an interprocedural program analysis (theorem proving, abduction, frame-inference, abstraction) and thus can significantly extend the reach of current verification methods."
	},
	{
		"grant":544,
		"ID": "EP/K040057/1",
		"Title": "MEASURING AND REPRODUCING THE 3D APPEARANCE OF HUMAN FACIAL SKIN UNDER VARYING ILLUMINATION CONDITIONS: A 3D IMAGING SYSTEM FOR HUMAN FACIAL SKIN",
		"PIID": "42549",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "350358",
		"ResearchArea": "Image and Vision Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Psychology Health & Society",
		"OrgID": "68",
		"Investigators":[
		{"ID": "42549", "Role": "Principal Investigator"},
		{"ID": "55713", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Understanding human skin appearance is a subject of great interest in science, medicine and technology.  In medicine, skin appearance is a vital factor in surgical/prosthetic reconstruction, medical make-up/tattooing and disease diagnosis. The production of facial prostheses to replace missing facial structures requires the skills of highly trained anaplastologists to correctly match the shape and colour of the prosthesis to that of the host skin. With the 3D printing of human skin now available the process involved in matching natural and manufactured skin samples has become essential; a robust, accurate and efficient imaging system is required that acquires the relevant skin information and predicts a good match and translates this information through this new and innovative manufacturing process.     A major problem with manufactured skin is that the match to the individual's natural skin must hold not only be accurate under a particular ambient illumination but the match needs to be preserved when the individual is moving between different environments, e.g. when the individual moves from office or LED lighting into daylight. To achieve this illumination invariance, the physical properties of the skin need to be taken into account. A further requirement for successful skin reproduction is the development of appearance models. These can be considered as individual 'recipes' or 'blueprints' for each skin type and these not only represent inter-personal differences - different ethnic groups and age ranges, but also intra-personal differences - for each individual.  Features of the human skin (wrinkles, pores, freckles, spots etc) make human skin as individual as a finger prints and thus, for facial prosthetics applications, skin appearance models also need to be fine-tuned for each individual area.   The purpose of this work is to develop a complete spectral-based 3D imaging system which will allow us to additively manufacture soft tissue prosthetics or deliver predictable tattooing techniques that will exactly match the skin colour of a particular individual (Application 1) or have the capability to rapidly manufacture/3D print soft tissue replacements representative of a particular ethnic/age/gender group with a high degree of accuracy (Application 2).  In application 1, the input to this 3D imaging system will consist of a 3D colour skin image (of a particular individual) obtained with a 3D camera in conjunction other specific skin characteristics. The skin sample will then be printed using a printer profile that maximises the match between the natural and printed skin across different ambient illuminations. In application 2, the skin manufacturing process will not be fine-tuned for a particular individual, but input to the 3D imaging system will consist of basic information about the age, gender and ethnicity. Representative skin samples (colour; texture; translucency; geometry) for this group will then be loaded from a pre-computed library instead of using the measurements from an individual.  "
	},
	{
		"grant":545,
		"ID": "EP/K040561/1",
		"Title": "Relaxed Memory Model Design for Theory and Practice",
		"PIID": "-188637",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/08/2013",
		"EndDate": "31/07/2015",
		"Value": "98538",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing",
		"OrgID": "27",
		"Investigators":[
		{"ID": "-188637", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "In the past few years, computer processors have reached a speed limit imposed by semiconductor physics. Before, increased performance came from running a single program faster, but now it comes from running more programs concurrently, on multiple 'cores'. Multi-core processors also support low-power applications, and are becoming popular on mobile devices, such as smart phones, where several slow cores use less battery power than a single fast core. To write software for multi-core processors, programmers must decompose tasks into cooperating programs, ideally one per core. However, even experts cannot write these programs without tremendous effort, and the programs often have subtle bugs. Programmers have not been given the intellectual tools necessary for managing the complexity of multi-core computation.  This project focuses on a critical challenge posed by multi-core processors: their relaxed memory models. Conceptually, the processor's cores are connected to a single memory, and programs running on different cores communicate by writing data to the memory for others to read. In reality, the processor achieves good performance by not giving the programmer a globally consistent picture of the memory: at any point in time the cores can appear to disagree on its contents. The processor does make some guarantees about the memory, so that the programmer can write working programs, but it carefully avoids making others. A relaxed memory model specifies which guarantees are made and which are not. Our objectives are to improve the theory of relaxed memory models, and to apply this theory to a new model that is easier to understand in practice.  Most of the time, programming in a high-level language should have advantages over programming in the processor's low-level assembly language: advantages in, for example, reliability, security, and cost of development. However, this is not the case with relaxed memory models: the high-level language is more complicated because it has to account for the variety of significantly different processors that the high-level language can be compiled to, and it has to account for the compiler's optimisations too. The primary tension is between usability/security (for example, that sensitive data will not be leaked by a malicious program forging pointers to the data) and efficiency, with the latter driving existing designs. The Java Memory Model attempts to give basic security guarantees, but several underlying flaws have been discovered. On the other extreme, the new C and C++ models make no attempt to provide security guarantees. The design space for relaxed memory models has not been thoroughly explored.  In this project, we will design a relaxed memory model for a high level language that gives stronger guarantees to programmers, making it easier to write, reason about, and verify concurrent programs. Our approach to the design combines a focus on real-world concurrent algorithms, to ensure that it is practical, with mathematical rigor, to ensure that it supports robust reasoning principles that will ultimately help programmers to understand it and to write high quality concurrent software systems."
	},
	{
		"grant":546,
		"ID": "EP/K040863/1",
		"Title": "Automating Separation Logic Reasoning",
		"PIID": "-422034",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2013",
		"EndDate": "28/02/2015",
		"Value": "98055",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "81",
		"Investigators":[
		{"ID": "-422034", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Separation logic is a formalism designed to symbolically reason about computer programs that dynamically allocate data structures on memory. It allows to one mathematically _prove_ the correctness of such programs, for example showing the absence of memory leaks or violations. Using this proof system theoreticians could provide much cleaner and succinct proofs on the whiteboard, but their efforts to automate such reasoning process faced a number of limitations. Standard automated theorem proving techniques, based e.g. on resolution, did not seem to naturally apply to the new logic; thus researchers opted either to implement reasoning tools from scratch---missing the opportunity to exploit advances in modern theorem proving---or seek alternatives that avoid the perceived nuances of separation logic altogether---missing the conceptual advantages that separation logic _does_ provide. This research proposal seeks to demonstrate that modern theorem proving techniques are, in fact, compatible with separation logic reasoning. An approach that leads not only to much more efficient separation logic reasoners, but also broadens the scope of applications in which automated theorem proving can be practically applied. "
	},
	{
		"grant":547,
		"ID": "EP/L000555/1",
		"Title": "Abstraction-Level Energy Accounting and Optimisation in Many-core Programming Languages",
		"PIID": "-421253",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2016",
		"Value": "661061",
		"ResearchArea": "Programming Languages and Compilers",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10",
		"Investigators":[
		{"ID": "-421253", "Role": "Principal Investigator"},
		{"ID": "-421518", "Role": "Co Investigator"},
		{"ID": "51232", "Role": "Co Investigator"},
		{"ID": "-255992", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Energy efficiency is becoming increasingly important in today's world of battery powered mobile devices and power limited servers. While performance optimisation is a familiar topic for developers, few are even aware of the effects that source code changes will have on the energy profiles of their programs. Without knowledge of these effects, compiler and operating system writers cannot create automatic energy optimisers. To realise the needed energy savings, we require the capability to track energy consumption and associate it to code and data at a fine granularity. Furthermore, compilers and operating systems must exploit this capability to optimise applications automatically.  This proposal presents a novel approach to software-centric modelling, measurement, accounting and optimisation of energy-efficiency on many-core systems. Energy consumption will be matched against programming language abstractions, from basic-blocks to functions, loops, and parallel constructs, and from variables to data structures, providing developers with the information that they need. The project will use this fine grained accounting to build novel compiler optimisations that target energy consumption. It will create low energy runtime systems that adapt to environmental changes. It will develop energy efficient operating system scheduling that manages multi-tasking for heterogeneous many-cores. The project aims to improve performance per Watt by at least 40%."
	},
	{
		"grant":548,
		"ID": "EP/L000687/1",
		"Title": "Adaptive Just-In-Time Parallelisation (AJITPar)",
		"PIID": "28909",
		"Scheme": "Standard Research",
		"StartDate": "15/10/2013",
		"EndDate": "14/10/2016",
		"Value": "442468",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "28909", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "A key element of the multicore software crisis is a lack of abstraction: most parallel code mixes coordination and computation, and assumptions about the architecture are often hard-coded, tying the code to a specific architecture. For problems with regular parallelism, i.e. where the number and size of subproblems can be statically predicted, good parallel performance on an architecture can be obtained using static techniques including static compilation. However, many problems are irregular and require dynamic techniques such as we propose.  This project aims to address the challenges of programs with irregular parallelism by sharing the burden of parallel coordination between the programmer and an adaptive runtime system. The approach can be summarised in the slogan 'The programmer knows the problem, the runtime knows the hardware.' That is, the programmer merely exposes the parallelism in the problem domain by means of architecture-independent declarative constructs, and the runtime system decides how to map the parallelism to a specific architecture. The project aims for a framework combining portable parallel performance across a range of architectures with 'compile once, run anywhere' portable binaries. The project will test whether portable performance has been achieved by comparing the parallel performance of a suite benchmarks on several parallel architectures, from standard desktop workstations to high-end compute clusters.  The proposed research will advance the state-of-the-art in two areas.  (1) The development of runtime systems that tightly integrate dynamic scheduling of parallelism with dynamic trace-based just-in-time (JIT) compilation. (2) The systematic investigation into how to specialise architecture-independent declarative parallelism to a specific architecture at runtime by means of declarative and modular code transformations. Crucially, both novelties need to be combined; neither can deliver portable parallel performance on its own. The parallelising JIT runtime system may not find the 'right' amount of parallelism without dynamically applying code transformations, and the transformations cannot in general be applied statically because they require knowledge of runtime timing data.  The research is timely in exploiting emergent trace-based JIT technology. Moreover, the project has enormous transformative potential. It investigates JIT parallelisation using Haskell because its pure functional context enables safe code transformation. If successful, however, the techniques established can be used to transform the parallel portability of software in many programming languages with JIT compilers, not least 'functional second' languages like JavaScript, Python, Scala and C#. Moreover, a JIT compiler's innate ability to optimise traces spanning several layers of the software stack enables a parallelising JIT runtime to exploit potential parallelism uniformly across all software layers and components.  JIT compilers for languages like Java or JavaScript have spread widely in recent years, being deployed on a wide variety of architectures, from smart phones to desktop computers to web servers. Because of its transformative potential, the design and implementation of a parallelising JIT compiler for Haskell will be followed with interest by implementers of parallel languages in the UK and abroad, both academic and in industry, e.g. at Microsoft, Google, Mozilla.  This speculative project will be undertaken by an experienced and energetic team in a vibrant environment. The team will be led by Professor Phil Trinder, who has 20 year's experience in the field, delivering 13 successful research projects, and with over 100 publications. Dr. Maier contributes deep knowledge of parallel language implementation and program analysis."
	},
	{
		"grant":549,
		"ID": "EP/L000776/1",
		"Title": "Unifying audio signal processing and machine learning: a fundamental framework for machine hearing",
		"PIID": "-215861",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2015",
		"Value": "97101",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Engineering",
		"OrgID": "24",
		"Investigators":[
		{"ID": "-215861", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Modern technology is leading to a flood of audio data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute.  Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged for categorisation and search. Moreover, an increasing proportion of recordings are made on hand-held devices in challenging environments that contain multiple sound sources and noise. Such uncurated and noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. On a related note, devices for the hearing impaired currently perform poorly in noise. In fact, this is a major reason why six million people in the UK who would benefit from a hearing aid, do not use them (a market worth 18 billion p.a.). Patients fitted with cochlear implants suffer from similar limitations, and as the population ages more people are affected.   It is clear that audio recognition and enhancement methods are required to stop us drowning in audio-data, for processing in hearing devices, and to support new technological innovations. Current approaches to these problems use a combination of audio signal processing (which places the audio data into a convenient format and reduces the data-rate) and machine learning (which removes noise, separates sources, or classifies the content). It is widely believed that these two fields must become increasingly integrated in the future. However, this union is currently a troubled one, suffering from four problems.   Inefficiency: The methods are too inefficient when we have vast amounts of data (as is the case for audio-tracks on the web) or for real-time applications (such as is necessary in hearing aids) Impoverished models: The machine learning modules tend to be statistically limited. Unadapted: The signal processing modules are unadapted despite evidence from other fields, like computer vision, which suggests thatautomatic tuning leads to significant performance gains  Distorted mixtures: The signal processing modules introduce non-linear distortions which are not captured by the machine learning modules.  In this project we address these four limitations by introducing a new theoretical framework which unifies signal processing and machine learning. The key step is to view the signal processing module as solving an inference problem. Since the machine-learning modules are often framed in this way, the two modules can be integrated into a single coherent approach allowing technologies from the two fields to be completely integrated.  In the project we will then use the new approach to develop efficient, rich, adaptive, and distortion free approaches to audio denoising, source separation and recognition. We will evaluate the the noise reduction and source separations algorithms on the hearing impaired, and the audio recognition algorithms on audio-sound track data.  We believe this new framework will form a foundation of the emerging field of machine hearing. In the future, machine hearing will be deployed in a vast range of applications from music processing tasks to augmented reality systems (in conjunction with technologies from computer vision). We believe that this project will kick start this proliferation."
	},
	{
		"grant":550,
		"ID": "EP/L00206X/1",
		"Title": "Creative Code Generation for Interactive Media",
		"PIID": "98328",
		"Scheme": "Standard Research",
		"StartDate": "01/07/2013",
		"EndDate": "30/06/2015",
		"Value": "178501",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing Department",
		"OrgID": "97",
		"Investigators":[
		{"ID": "98328", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Computational Creativity research is a branch of Artificial Intelligence where we investigate ways in which software can enhance human creativity, as well as ways in which software can be autonomously creative. Researchers working in this field often build software that creates artefacts of some sort - from paintings to poems, soup recipes to sonatas. In more recent times, our attention has turned to higher level issues, such as how software can evaluate its work, show intentionality and imagination, and how it can frame its own work to add value.   Writing software is a difficult and creative skill which can only be performed by people after much training. As such, while it would seem a natural fit, there has been no serious study of automatic program generation from a Computational Creativity perspective. With this project, we will address this shortcoming, by applying the approaches and methodologies of more than a decade of work on simulating creative behaviour to the problem of automatically generating and testing interactive, multi-media software artefacts.   We will examine ways in which we can get software to write programs, and how we can make this process as creative possible. We will test our ideas by building a new system that can write interactive media programs (IMPs), ranging from videogames to first-person-perspective experiential art installations. This will be based on our successful existing co-operative co-evolution software which has generated well-received games in a fully autonomous way, but will hugely extend its creative abilities. In particular, the new system will plan, write and edit new program code directly, testing and evaluating what it writes before adding it to whatever IMP it is currently trying to create. This code might describe a new object, a new control or scoring mechanism, or how to produce a new musical or visual effect.   We will also look at how creative code generators like our IMP designer can create software that isn't quite finished. These unfinished IMPs will be able to rewrite their own code as people interact with them, to change themselves as they are used. They will self-modify not only in reaction to user responses, but also in reaction to external factors in the world, such as international news or social network trends. We hope to show that these programs are perceived as more surprising, inventive and novel, due to their ever-changing nature.  Importantly, we'll be trying to make automated code generation a creative process. In Computational Creativity research, the software we build does not just generate things - it can make decisions about how to generate something, and communicate why it made those decisions. We will look at how our IMP designer can decide whether something it has made is new and interesting or not, and give it ways to communicate with programmers and users, to tell them about what it has created. We will test our approaches via a crowd sourcing methodology, whereby the feedback from thousands of people interacting with the IMPs will be analysed and subjected to machine learning exercises in order to determine the truth of certain hypotheses, and to produce partial audience models to be used to improve the quality of the output.  We believe that this project will have much impact on Computational Creativity research and Artificial Intelligence in general, as it will bring to the fore new issues in the field, most notably questions around software writing software, the automatic production of 'unfinished' artefacts which self-modify and the spectrum from entertainment to thought provocation in interactive media. Moreover, we believe that this project will have much impact in the broader arenas of the public perception of computing and the creative industries, as it will highlight in a very tangible way - the automatic generation of games and artworks - the massive potential for computers to become our creative partners in the future. "
	},
	{
		"grant":551,
		"ID": "EP/L003112/1",
		"Title": "Visualising the UK General Election 2015 TV Debates",
		"PIID": "-161795",
		"Scheme": "Standard Research",
		"StartDate": "01/11/2013",
		"EndDate": "31/10/2016",
		"Value": "382053",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Institute of Communication Studies",
		"OrgID": "64",
		"Investigators":[
		{"ID": "-161795", "Role": "Principal Investigator"},
		{"ID": "-164761", "Role": "Co Investigator"},
		{"ID": "-248349", "Role": "Co Investigator"},
		{"ID": "55620", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During the 2010 general election the first ever televised leaders' debates to be held in the United Kingdom took place. A key argument in favour of these televised events was that they could reach a wider audience than is usual for politically-related content and that, after watching them, normally apolitical debate-watchers might be better informed about election issues; more likely to discuss policies with their friends and families; and more likely to vote than those who are not exposed to the debates. In short, televised election debates perform a heuristic function, providing voters with resources that enable them to carry out their normative role as informed and reflective citizens of a representative democracy. Research conducted by Coleman et al ('Leaders in the Living Room--The Prime Ministerial Debates of 2010', Oxford: Reuters Institute, 2011) after the 2010 UK debates showed that there was a significant public appetite for this means of learning about the candidates and their policies, but that many viewers were left feeling uncertain about the meaning of what they had witnessed. This prompted a group of scholars from different disciplines - information science, political communication and design - to get together with a view to exploring how future televised leaders' debates might be made more comprehensible to groups of viewers with specific information needs.  In considering the best approach to presenting complex arguments to citizens with a view to generating better informed public debate about political issues, we have turned to the field of Computer-Supported Argumentation Visualisation (CSAV) which has a track record of utilising innovative information techniques to present complex arguments that citizens can make sense of and reflect upon in the course of policy deliberations.  Our aim in this research is to develop an open-source web-based platform that incorporates a suite of visualisation tools and to develop a working model of how this platform can be embedded within a mixed-media ecology for covering and responding to issues of public political debate.  The platform will be designed with a view to i) responding to the information needs of audiences and specific types of audience member; ii) presenting the discursive content in ways that take account of the aesthetic and symbolic needs of information seekers; and iii) not only visualising the debaters' arguments by adopting innovative CSAV methods, but also visualising other features of the debates through the use of non-CSAV-specific techniques, such as word-cloud visualisations (where key words and phrases used in a debate are extracted and visualised such that graphical features like font-size, colour, and positioning are used to depict most import words and phrases) and time-series analyses (where the emphasis is on visualising the chronology of the various speech acts during the debate, so that the context 'in time' of key rhetorical events can be captured)."
	},
	{
		"grant":552,
		"ID": "EP/L003260/1",
		"Title": "LIQUID: Logic-based Integration and Querying of Unindexed Internet Data",
		"PIID": "-444482",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/10/2013",
		"EndDate": "31/03/2015",
		"Value": "89494",
		"ResearchArea": "Databases",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science and Information Systems",
		"OrgID": "73",
		"Investigators":[
		{"ID": "-444482", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Deep Web is constituted by data that are stored outside web pages, normally in databases or files, and are accessible by querying them through web forms. Deep Web data sources, apart from having local integrity constraints enforced on them, are characterised by so-called access limitations, due to the input requirements of the forms, and such limitations restrict the set of that that can be ``reached'' by queries.  Moreover, in general, recursive query plans are needed to provide the maximum information as answer to a query.  In the project LIQUID (Logic-based Integration of Unindexed Internet Data), we address the problem of integrating Deep Web sources and making them queryable through a global schema, representing all underlying data.  The global schema employs constraints to model properties of the domain of interest, which are not necessarily reflected by the data stored at the heterogeneous sources.  In this project we plan to study formalisms to represent Deep Web integration systems, and to devise scalable query processing techniques for such systems.  We plan to study static and dynamic techniques to reduce the query processing cost, by employing logic-based reasoning techniques on constraints and access limitations.  We plan to build a prototype implementing our findings.  LIQUID will face ambitious challenges, due to the variety of formalisms and problems into play. "
	},
	{
		"grant":553,
		"ID": "EP/L005255/1",
		"Title": "IMC2: Instrumentation Measurement and Control for the Cloud",
		"PIID": "-186860",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/01/2014",
		"EndDate": "31/12/2014",
		"Value": "98486",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computing Science",
		"OrgID": "49",
		"Investigators":[
		{"ID": "-186860", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The Internet landscape is changing rapidly, from a completely decentralised paradigm where distinct services were offered by different providers in a fully distributed and decentralised way, to a unified ICT environment where data, storage, and processing resources are co-located in the Cloud, and offered alongside connectivity. Although Cloud services and the underlying communication infrastructures are built on top of commodity Internet mechanisms (transport protocols, IP switching, multipath routing, etc.), it becomes apparent that the performance-agnostic and slow-converging operational assumptions of today's data communications are challenged by the new unified technological and business model. Massive overprovisioning of fully distributed resources that are managed in distinct and often long timescales (e.g., traffic aggregates over backbone networks) is not sustainable in an environment where connectivity and system resources need to be managed by a single unified ICT provider over a centralised infrastructure and in very short timescales. Cloud providers need to maximise return-on-investment from their infrastructures through rapid provisioning and elastic resource management, offering predictable services while operating at higher utilisation thresholds.  In order to achieve these goals, in this project we will design and develop an always-on Instrumentation, Measurement, and Control (IMC) framework that will dynamically and adaptively provision unified resources in a unified manner and in short timescales. Evidence has shown that distinct control loops typically employed to manage different resources in different timescales can themselves constitute factors of performance degradation over unified Cloud environments. For example, network-agnostic placement and migration of virtual machines can itself cause congestion in the underlying Data Centre topology. We will therefore revisit the one-dimensional, static or pseudo-random control loops that are typically employed over Cloud topologies, and develop an adaptive closed-loop system that will manage both server and network resources synergistically, in short timescales and based on temporal topology-wide performance. In doing so, we will exploit often controversial concepts such as non-shortest path routing for increasing load balancing while meeting flow completion deadlines, and network-aware dynamic virtual machine migration, to demonstrate the feasibility and also the benefits of combinatorial resource provisioning in achieving global performance optimisation and in increasing the usable capacity of future networks and services. One of the key aims of the proposed research is to investigate and to demonstrate the applicability of measurement-based processes to control and to admit resources in a unified manner and at appropriate, short timescales. Through the necessary system and network node instrumentation, we will devise a logically-centralised measurement and control closed-loop architecture that will be an integral part of the underlying infrastructure's data forwarding operation. The long-term impact of such endeavour will be to revisit the currently disjoint data and control planes in packet communications, and to transform next generation networked infrastructures from performance-agnostic to adaptive and self-managed, through synergy across the different layers and planes of the architecture.  The proposed research will be carried out at the University of Glasgow, and experiments will be conducted over a purpose-built programmable Cloud services testbed infrastructure, partly supported by EPSRC's first grant scheme and partly through a generous contribution from the host institution. The research will be conducted in close collaboration with Onyx Group, Microsoft Research and JANET(UK). "
	},
	{
		"grant":554,
		"ID": "EP/L005654/1",
		"Title": "Infinite-domain Constraint Satisfaction Problems",
		"PIID": "-441553",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2014",
		"EndDate": "31/07/2015",
		"Value": "100245",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Science and Technology",
		"OrgID": "85",
		"Investigators":[
		{"ID": "-441553", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Constraint Satisfaction Problems (CSPs) provide a powerful framework within which to phrase many computational problems from across Computer Science. In Combinatorics they are known as Homomorphism Problems and in Databases they appear as conjunctive-query containment. CSPs manifest in Artificial Intelligence in the form of temporal and spatial reasoning, and in Computational Linguistics in the guise of tree description languages. In Computational Biology, phylogenetic reconstruction is a CSP, and in Graph Theory it known as H-colouring.   We propose to study the computational complexity of CSPs given by a single constraint language that may have an infinite domain. Research into the finite-domain case is now quite advanced, yet a great many interesting problems, which may not be given as finite-domain CSPs, may be given as infinite-domain CSPs. For example, this is true for most of the CSPs associated with Artificial Intelligence and Computational Linguistics. The computational complexity of most natural finite-domain CSPs is now known, yet many interesting infinite-domain CSPs have open complexity. For example, this is true of the Max Atoms problem, very closely related to Model-checking the mu-calculus, a problem of open complexity from the Verification community. It is also true of the Concatenation problem for free algebras, a problem arising in the Rewriting community. Further, a CSP was recently given that is polynomially equivalent with the elusive problem of Integer factoring. The commonality of structure across CSPs gives hope that we might find generic methods with which to analyse the computational complexity of these diverse problems. We will work on these problems specifically, as well as seeking to map out landscapes of complexity in such areas as the following.   Linear Programming. Linear program feasibility is well-known to be polynomial-time solvable, How much extra expressive power can be added to linear program feasibility while maintaining its tractability?  Integer programming. Integer program feasibility is well-known to be (NP-)hard to solve. How little expressive power does one need to take away to reach tractability?"
	},
	{
		"grant":555,
		"ID": "EP/L006111/1",
		"Title": "Quantum Stochastic Analysis For Nanophotonic Circuit Design",
		"PIID": "94527",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "01/08/2013",
		"EndDate": "31/03/2015",
		"Value": "24494",
		"ResearchArea": "Mathematical Analysis",
		"Theme": "Information and Communication Technologies",
		"Department": "Inst of Mathematical and Physical Sci",
		"OrgID": "2",
		"Investigators":[
		{"ID": "94527", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The research is set against the context of the emergence of quantum technologies, and the desirability to establish a dedicated quantum control theory utilizing the primary feature of modern control -feedback - as an essential element in designing quantum components that operate in a robust, autonomous fashion. We believe that the next important stage in the development of computing will focus on optical platforms integrated on chip to perform low-power low-dissipation information storage, relaying and information processing, but requiring a quantum mechanical description of their dynamics, and their control.  The PI is an expert on quantum stochastic models based on the Hudson-Parthasarathy calculus, and has developed several key results in model reduction, synthesis and interconnection of quantum networks, and stochastic approximation techniques that are directly relevant to quantum optics. With Matthew James, the PI has pioneered the mathematical formalism of Quantum Feedback Networks for describing how component open quantum systems can be connected. This has recently been automated in a freely available visual programming and simulation implementation framework by researchers at MabuchiLab. The Quantum Feedback Network formalism lies at the heart of synthesis and design of quantum feedback control systems, and has been used to develop models of squeezing enhancement, autonomous quantum error correction schemes, optical switching devices, etc. At present, the MabuchiLab group are one of the main users of the theoretical framework developed by the PI. The aim of the visit is to identify the likely mathematical and conceptual problems that are currently arising.   The early theoretical suggestions for quantum coherent feedback control were:  i) The MabuchiLab treatment of the optimal least disturbance problem for optical signals passed through a quantum cavity (an exemplar for robust quantum control techniques developed by Nurdin, James and Peteresen),   ii) the Gough-Wildfeuer proposal to enhance the squeezing capability for degenerate parametric amplifiers (this was subsequently verified experimentally by the Tokyo group headed by Akira Furusawa, and a systems-theoretic improvement has been suggested by Zhang in Hong Kong based on the inclusion of active components). This proposal aims at drawing these strings together in an aligned way.  The quantum stochastic calculus has proved remarkably successful in modeling quantum optical systems, and rigorous limit results for these models, such as adiabatic elimination, are suggesting new strategies for engineering non-linear optical components, and the quantum feedback network framework has lead to an efficient technique leading directly to the master equation for compound quantum input-output systems. Coherent-feedback has been proposed as a mechanism for autonomous quantum memories, in particular, quantum-error correction schemes assembled from cavity QED devices linked through optical wave-guides could in principle work without external clocking or control. The goal of the visit is to explore likely areas of application for model abstraction using quantum stochastic analysis, with a view investigating the designability aspect of robustly controlled quantum systems.   As a principal direction for long term research investigation we would like to look at applying model-based graphical techniques from network analysis to quantum feedback networks. The PI proposes to visit Stanford University this year for three weeks (the start will coincide with the 2013 Principles and Applications of Control to Quantum Systems meeting organized in Monterey, CA). We also wish to connect this with recent work by Dr Guofeng Zhang in Hong Kong on control theoretic aspect of the Gough-Wildfeuer problem were a quantum mechanical dynamic compensator is included for robustness."
	},
	{
		"grant":556,
		"ID": "EP/L006251/1",
		"Title": "Network error control for Rapid and Reliable Data Delivery (R2D2)",
		"PIID": "-421189",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/09/2013",
		"EndDate": "28/02/2015",
		"Value": "98438",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Computing & Communications",
		"OrgID": "63",
		"Investigators":[
		{"ID": "-421189", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The widely anticipated fourth generation (4G) of mobile communications is expected to accommodate highly popular but bandwidth-demanding content on-the-move, ranging from high-definition multimedia streaming and online video gaming to software distribution and movie downloads. Long-Term Evolution (LTE), which is the dominant system for 4G networks, has introduced state-of-the-art fountain coding to support data broadcast and multicast services. The appealing property of fountain codes to adapt transmission rate to channel conditions, regardless of the number of receiving mobile users, is however constrained by a trade-off between latency and network overhead. The inherent requirement of content streaming for low latency can thus lead to a significant increase in overhead which, as recent research suggests, can be alleviated if adjacent mobile nodes form cooperative clusters. More specifically, 4G-enabled devices equipped with short-range communication capabilities, such as WiFi or Bluetooth, could request missing packets from neighbouring devices. The use of network coding in each cooperative cluster could further reduce traffic and energy consumption. Therefore, the combination of point-to-multipoint fountain coding and collaborative network coding at the application layer with point-to-point channel coding at the physical layer is an attractive alternative design to conventional automatic repeat request (ARQ) protocols, which can be less energy and bandwidth efficient.   Nevertheless, there is great scope for tailoring the proposed scheme to the requirements of upcoming 4G broadband technologies and developing radically new paradigms to satisfy the increasing demand for streaming and downloading of high quality media in next generation networks communications. The proposed project aims to develop a mathematical framework to identify key relationships between the various network parameters, contribute to the understanding of network dynamics and assist in the system-level optimisation of network-coded architectures. The benefits of low-level optimisation will also be harnessed if separate designs of channel coding and network coding are replaced by joint designs for content distribution, which better leverage the benefits of redundancy and boost reliability without requiring significant changes to existing infrastructures. The project also aspires to delve into the emerging research field of network error correction and develop practical implementations of unified channel and network codes that deliver high reliability, low decoding complexity and increased security from malicious users.  The proposed research will be carried out in consultation with international collaborators from universities in Portugal and Norway and its outcomes will benefit not only mobile ad-hoc architectures, but related systems too, such as broadband fixed wireless access networks, machine-to-machine communications and image & video processing. We anticipate that our work on network error control techniques will contribute to the reinforcement of the UK research base and play an important role in putting the UK at the forefront of developments in the arena of next-generation mobile technology."
	},
	{
		"grant":557,
		"ID": "EP/L00643X/1",
		"Title": "Testing Autonomous Vehicle Software using Situation Generation",
		"PIID": "-371484",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2014",
		"EndDate": "28/02/2015",
		"Value": "97100",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "55",
		"Investigators":[
		{"ID": "-371484", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Autonomous vehicles (AVs) must be controlled by software, and such software thus has responsibility for safe vehicle behaviour. It is therefore essential that we rigorously test such software. This is difficult to do for AVs, as they have to respond appropriately to a great diversity of external situations as they go about their missions.     It is possible to find faults in an AV software specification by testing its behaviour in a variety of external situations, either in reality or in computer simulation. Such testing may reveal that the specification ignores certain situations (e.g. negotiating a motorway contraflow lane) or defines behaviour that is unsafe in a subset of situations (e.g. its policy for adapting to icy surfaces leads to unsafe speed control in crowded urban environments).    This project will test the hypothesis that testing based on coverage of possible external situations ('situation coverage') is an effective means of finding AV specification faults. We will test the hypothesis by creating a tool that generates situations for simulated AVs, both randomly and using heuristic search, and assessing whether higher situation coverage correlates with greater success at revealing seeded specification faults. (For the search, the fitness function will be based on the situation coverage achieved)    The project will draw on previous work on test coverage measures, on search-based testing, and on automated scenario generation in training simulations. To assess the effectiveness of the approach, we will use a small but practically-motivated case study of an autonomous ground vehicle, informed by the advice of an advisory panel set up for this project.   "
	},
	{
		"grant":558,
		"ID": "EP/L006685/1",
		"Title": "Automatic Semantic Analysis of 3D Content in Digital Repositories",
		"PIID": "-173006",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2014",
		"EndDate": "31/07/2015",
		"Value": "97491",
		"ResearchArea": "Graphics and Visualisation",
		"Theme": "Information and Communication Technologies",
		"Department": "Cultural Informatics Research Group",
		"OrgID": "19",
		"Investigators":[
		{"ID": "-173006", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The increasing popularity of 3D technologies is having an impact on the amount of content that is being produced by users of these technologies. Witnessing the explosion of content such as images, music and videos available on the web, it is not difficult to predict that 3D will be the next type of content to undergo this effect. The research community has been taking action to ensure 3D content can be stored and managed in databases or repositories in order to be accessible to a wide variety of users. Nevertheless, searching for 3D content in these repositories is not an easy task. The main problem is that although a digital 3D representation of a physical object is a more accurate representation, the way that the information is stored means that automated solutions for understanding what the content represents is an unsolved challenged. To address this problem, the research community has created ways to tag or 'attach' additional information to the 3D content, as is done with 2D images, to support the computer's understanding of what the 3D content represents. However, this process is currently slow as it relies on mostly manual or semi-automatic techniques. This project will take these basic techniques forward by researching state of the art mechanisms to automate the enrichment of 3D content. This will be done by focusing on Cultural Heritage artefacts, in particular Regency architectural ornamental artefacts, to understand how the shape of an artefact might provide us with information about it (e.g. its origin, artistic style, production methods). It is currently very challenging to infer this high level information automatically. The project will thus combine expertise in shape analysis, the semantic web and Cultural Heritage in order to develop innovative techniques to automatically understand what the 3D content might represent. This process is referred to as 'automatic semantic enrichment' and will allow the 3D content to be linked to a vast amount of information and knowledge which will facilitate making connections with other pieces of information. As a result, searching for the most relevant item of 3D content amongst the petabytes of information stored in the database will be considerably improved. In turn, this will improve the availability and use of 3D content for different purposes. For instance, the project will demonstrate how the research can support the restoration of historical buildings."
	},
	{
		"grant":559,
		"ID": "EP/L007177/1",
		"Title": "p-Automata - Foundation for Probabilistic Model Checking",
		"PIID": "-215489",
		"Scheme": "First Grant Scheme",
		"StartDate": "01/02/2014",
		"EndDate": "30/09/2015",
		"Value": "99014",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "66",
		"Investigators":[
		{"ID": "-215489", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Stochastic systems, systems involving probabilistic aspects, are used in many sciences and engineering disciplines. In computer science, such systems are the underlying model for probabilistic algorithms, modeling of stochastic events, queue analysis, and much more. For example, communication protocols use randomness to break symmetry between equivalent processes and modeling mobile networks uses probabilities to capture the appearance, disappearance, and movement of nodes. The new computing paradigm is one of small devices and a high degree of mobility, across traditional boundaries. Therefore it is becoming more and more important to be able to analyze and reason about such systems.  One of the most successful techniques to reason about discrete (non-stochastic) systems has been model checking. Model checking is a formal method in which a mathematical model of a system is contrasted with a specification. The specification is usually given in a high level logical language that allows to write descriptions of wanted (or unwanted) behavior. Model checking either ascertains that the property holds (within a model) or produces an execution showing the failure of the specification. Model checking of discrete systems has been extremely successful. By now, it is a standard validation technique in hardware industry and has increasing importance in software industry. This success relied largely on two complementary concepts. First, the existence of an automata-theoretic framework within which systems and logical specifications live together and can be reasoned about. Second, the concept of abstraction, which allows to consider only information about the system that is relevant to the property that is being checked.   The success of the general approach of model checking has prompted researchers to explore its applicability to stochastic systems. In recent years much effort has been devoted to probabilistic model checking, where systems are modeled as Markov chains and logical specifications quantify over the probabilities of certain events. However, probabilistic model checking suffers even more than 'normal' model checking from the state-explosion problem, the fact that the size of the system (its number of states) is exponential in the number of its components. The state-explosion problem is a major challenge restricting the size of systems to which model checking is applicable, both with and without probabilities. As mentioned, the most successful approach to date to combat the state-explosion problem has been abstraction.   Recently I introduced p-automata, a new model of a computation device that reads labeled Markov chains as input. I have shown that these automata can constitute a framework for reasoning abstractly about Markov chains. Basic properties of p-automata were established showing that they support the most important features that constitute an automata-theoretic framework for reasoning about Markov chains. These two qualities together open the way to generalizing the successful abstraction approach from discrete model checking to probabilistic model checking. It is my belief that p-automata not only have the necessary features that can make it an automata-theoretic framework for reasoning about Markov chains but can also do so in practice.   In this grant I will start showing that p-automata can take on a major role in progressing probabilistic model checking and expanding its scope in practice, leading to the verification of complex stochastic systems. "
	},
	{
		"grant":560,
		"ID": "EP/L00738X/1",
		"Title": "Intelligent Management of Big Data Storage",
		"PIID": "6874",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "368053",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "6874", "Role": "Principal Investigator"},
		{"ID": "-304448", "Role": "Co Investigator"},
		{"ID": "95522", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The continuing revolutionary growth of data volumes and the increasing diversity of data-intensive applications demands an urgent investigation of effective means for efficient storage management.  In the summer of 2012, the volume of data in the world was around 10 to the power of 21 bytes, about 1.1TB per internet user, and this volume continues to increase at about 50% Compound Annual Growth Rate.  It has been said that 'By 2013, storage systems will no longer be manually tunable for performance or manual data placement. Similar to virtual memory management, the storage array's algorithms will determine data placement (The Future of Storage Management, Gartner 2010).  Meeting service-level objective/agreement (SLO/SLA) requirements for data-intensive applications is not straightforward and will become increasingly more challenging. In particular, there is an increasing need for intelligent mechanisms to manage the underlying architectures' infrastructure, taking into account the advent of new device technologies.  To cope with this challenge, we propose a research program in the mainstream of EPSRC's theme 'Towards an intelligent information infrastructure (TI3)', specifically with reference to the 'deluge of data' and the exploration of 'emerging technologies for low power, high speed, high density, low cost memory and storage solutions'.  Today, with the widespread distribution of storage, for example in cloud storage solutions, it is difficult for an infrastructure provider to decide where data resides, on what type of device, co-located with what other data owned by which other (maybe competing) user, and even in what country. The need to meet energy-consumption targets compounds this problem.  These decisional problems motivate the present research proposal, which aims at developing new model-based techniques and algorithms to facilitate the effective administration of data-intensive applications and their underlying storage device infrastructure.  We propose to develop techniques and tools for the quantitative analysis and optimisation of multi-tiered data storage systems.  The primary objective is to develop novel modelling approaches to define and facilitate the most appropriate data placement and data migration strategies.  These strategies share the common aim of placing data on the most effective target device in a tiered storage architecture. In the proposed research, the allocation algorithm will be able to decide the placement strategy and trigger data migrations to optimize an appropriate utility function. Our research will also take into account the likely quantitative impact of evolving storage and energy-efficiency technologies, by developing suitable models of these and integrating them into our tier-allocation methodologies. In essence, our models will be specialised for different storage and power technologies (e.g. fossil fuel, solar, wind).    The models, optimisers and methodologies that we produce will be tested in pilot implementations on our in-house cloud (already purchased); on Amazon EC2 resources; and finally in an industrial, controlled production environment as part of our collaboration with NetApp.  This will provide feedback to enable us to refine, enhance and extend our techniques, and hence to further improve the utility of the biggest of storage systems."
	},
	{
		"grant":561,
		"ID": "EP/L010534/1",
		"Title": "International Summer School on Social Human-Robot Interaction",
		"PIID": "-83565",
		"Scheme": "Training Schools",
		"StartDate": "26/08/2013",
		"EndDate": "25/09/2013",
		"Value": "12000",
		"ResearchArea": "Human-Computer Interaction",
		"Theme": "Information and Communication Technologies",
		"Department": "Sch of Computing & Mathematics",
		"OrgID": "109",
		"Investigators":[
		{"ID": "-83565", "Role": "Training Grant Holder"},
		{"ID": "58408", "Role": "Co Investigator"},
		{"ID": "126074", "Role": "Co Investigator"},
		{"ID": "95520", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The International Summer School on Human-Robot Interaction (26-30 August 2013, Cambridge, UK) is a 5 day event for young researchers offering a range of talks, lectures, tutorials and ateliers on the scientific and technical aspects of social Human-Robot Interaction. A carefully selected panel of lecturers will offer a current overview of a particular topic, with the aim of giving the summer school participants a balanced introduction. Tutorials will introduce techniques and technologies, such as software architectures, hardware platforms or experimental techniques. Finally, ateliers give the participants the opportunity to engage with hands-on activities, such as designing solutions or implementing prototypes.  Speakers and topics include: Fuhimide Tanaka (University of Tsukuba), child robot interaction; Brian Scassellati (Yale), social robotics; Maja Pantic (Imperial College), social signal processing; Kim Bard (University of Southampton), social interaction in primates and lessons for HRI; Mark Neerincx and Rosemarijn Looije (TNO), HRI experimental procedures; Karl MacDorman (Indiana University), Attitudes to human-like robots; Ben Krse (Universiteit van Amsterdam), Smart environments for social HRI; Ivana Kruijff-Korbayov (DFKI), Natural Language Interaction for HRI; Bram Vanderborght (Vrije Universiteit Brussel), Robot Assisted Therapy; Takayuki Kanda (ATR), long-term HRI with young users; Jean-Christophe Baillie (Aldebaran Robotics), the Nao robot platform; Aardman Animations; Designing life from lifeless material.  All speakers are internationally recognised scholars, who in addition are engaging speakers, able to inspire a young generation of researchers and entrepreneurs. "
	},
	{
		"grant":562,
		"ID": "EP/L010550/1",
		"Title": "Highly-parallel algorithms and architectures for high-throughput wireless receivers",
		"PIID": "-99841",
		"Scheme": "Standard Research",
		"StartDate": "01/01/2014",
		"EndDate": "31/12/2016",
		"Value": "480201",
		"ResearchArea": "Architecture and Operating Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronics and Computer Science",
		"OrgID": "117",
		"Investigators":[
		{"ID": "-99841", "Role": "Principal Investigator"},
		{"ID": "21686", "Role": "Co Investigator"},
		{"ID": "50450", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "During the past two decades, reliable wireless communication at near-theoretical-limit transmission throughputs has been facilitated by receivers that operate on the basis of the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. Most famously, this algorithm is employed for turbo error correction in the Long Term Evolution (LTE) standard for cellular telephony, as well as in its previous-generation predecessors. Looking forward, turbo error correction promises transmission throughputs in excess of 1 Gbit/s, which is the goal specified in the IMT-Advanced requirements for next-generation cellular telephony standards. Throughputs of this order have only very recently been achieved by State-Of-the-Art (SOA) LTE turbo decoder implementations. However, this has been achieved by exploiting every possible opportunity to increase the parallelism of the BCJR algorithm at an architectural level, implying that the SOA approach has reached its fundamental limit. This limit may be attributed to the data dependencies of the BCJR algorithm, resulting in an inherently serial nature that cannot be readily mapped to processing architectures having a high degree of parallelism.  Against this background, we propose to redesign turbo decoder implementations at an algorithmic level, rather than at the architectural level of the SOA approach. More specifically, we have recently been successful in devising an alternative to the BCJR algorithm, which has the same error correction capability, but does not have any data dependencies. Owing to this, our algorithm can be mapped to highly-parallel many-core processing architectures, facilitating an LTE turbo decoder processing throughput that is more than an order of magnitude higher than the SOA, satisfying future demands for gigabit throughputs. We will achieve this for the first time by developing a custom Field Programmable Gate Array (FPGA) architecture, comprising hundreds of processing cores that are interconnected using a reconfigurable Benes network. Furthermore, we will develop custom Network-on-Chip (NoC) architectures that facilitate different trade-offs between chip area, energy-efficiency, reconfigurability, processing throughput and latency. In parallel to developing these high-performance custom implementation architectures, we will apply our novel algorithm to both existing Graphics Processing Unit (GPU) and NoC architectures. This will grant us a rapid pace, allowing us to apply our novel algorithm to not only error correction, but to all aspects of receiver operation, including demodulation, equalisation, source decoding, channel estimation and synchronisation. Drawing upon our high-throughput algorithms and highly-parallel processing architectures, we will develop techniques for holistically optimising the algorithmic and implementational parameters of both the transmitter and receiver. This will facilitate practical high-performance schemes, which can pave the way for future generations of wireless communication.  This research addresses key EPSRC priorities in the Information and Communication Technologies theme (http://www.epsrc.ac.uk/ourportfolio/themes/ict), including 'Many-core architectures and concurrency in distributed and embedded systems' and 'Towards an intelligent information infrastructure'. The 'Working together' priority is also addressed, since this cross-disciplinary research will develop new knowledge that spans the gap between high-performance communication theory and high-performance hardware design. This research will offer new insights into the design of many-core architectures, which the hardware design community will be able to apply in the design of general purpose architectures. Furthermore, the communication theory community will be able to apply our algorithms across even wider aspects of receiver operation."
	},
	{
		"grant":563,
		"ID": "EP/L011018/1",
		"Title": "Algorithms for Finding Approximate Nash Equilibria",
		"PIID": "-95224",
		"Scheme": "Standard Research",
		"StartDate": "01/10/2013",
		"EndDate": "30/09/2016",
		"Value": "276828",
		"ResearchArea": "Maths of Computing",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "68",
		"Investigators":[
		{"ID": "-95224", "Role": "Principal Investigator"},
		{"ID": "-298121", "Role": "Co Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Game theory is the analysis of how people behave in strategic settings. It has risen to prominence because it can be applied in a wide variety of practical settings. For example, game theory was used in the late 1990s to design auctions for 3G telecommunications, which raised billions of pounds for the UK taxpayer. Game theory has also been used to design patrol routes for airport security, to design strategies for nuclear weapon inspectors, to match junior doctors to their hospital residencies, and to model road congestion. In the commercial world, game theory has found applications ranging from auctions for selling advertising in search engine results to designing the layout of circuits on microchips.   The Nash equilibrium is the most fundamental concept in game theory. An exact Nash equilibrium of a game predicts how the players of a game should play: in an exact Nash equilibrium, no player should have an incentive to deviate from their current strategy. This research considers algorithms for finding Nash equilibria. In Computer Science, we usually look for algorithms that are guaranteed to run quickly. Unfortunately, it has been shown that fast algorithms for finding exact Nash equilibria are unlikely to exist.   The fact that exact Nash equilibria are hard to compute poses a significant problem. For practical applications, people often want to find Nash equilibria in very large games, but the existing algorithms for finding exact Nash equilibria will take a prohibitively large amount of time to solve these games. The use of approximate Nash equilibria is one way to address this issue: whereas exact Nash equilibria require that all players have no incentive to deviate from their current strategies, approximate Nash equilibria allow the players to have a small incentive to deviate. Since it can be costly to change strategies, approximate Nash equilibria are often good enough for practical applications, and they can be significantly easier to compute.  In this research project, we will study algorithms for finding approximate Nash equilibria. Our goal is to study approximation algorithms for bimatrix games, which are a fundamental game model, and potential games, which encompass a wide variety of useful special cases. We will either find efficient approximation algorithms, or formally prove that efficient approximation algorithms are unlikely to exist.   We will also study algorithms from the payoff-query point of view. In real-world applications, it is unusual to be given a complete description of the game. Instead, we must find out the properties of the game through experimentation, and performing these experiments may be very costly. Payoff query complexity captures this, and algorithms with low payoff query complexity require few experiments before they give their answer. We will study the payoff query complexity of finding approximate Nash equilibria in bimatrix games and potential games, in order to find useful algorithms that can be applied in real-world settings."
	},
	{
		"grant":564,
		"ID": "EP/L018535/1",
		"Title": "CONCERT: A Context-Adaptive Content Ecosystem Under Uncertainty",
		"PIID": "62131",
		"Scheme": "Standard - NR1",
		"StartDate": "01/01/2014",
		"EndDate": "31/12/2016",
		"Value": "303887",
		"ResearchArea": "ICT Networks & Distributed Systems",
		"Theme": "Information and Communication Technologies",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81",
		"Investigators":[
		{"ID": "62131", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The CONCERT objective is to develop a content ecosystem, encompassing all relevant players, which will be able to perform intelligent content and network adaptation in highly dynamic conditions under uncertainty. This ecosystem will have as basis emerging information-/content-centric networking technologies which support intrinsic in-network content manipulation. The project will consider uncertainty aspects in the following two application domains: a) social media networks based on user generated content and b) CDN-like professional content distribution. Three dimensions of uncertainties will be addressed: heterogeneous and changing service requirements by end users, threats that may have adverse impacts on the content ecosystem, as well as opportunities that can be exploited by specific players in order to have their costs reduced.   In order to manage and exploit these uncertainty aspects, CONCERT defines a two-dimensional content and network adaptation framework that operates both cross-layer and cross-player. First, the decision on any single adaptation action needs to take into account context information from both the content application layer and the underlying network. Second, we consider joint content and network adaptation in order to simultaneously achieve optimised service performance and network resource utilisation. Finally, some complex uncertainty scenarios require coordinated content and network adaptation across different ecosystem players. In this case, inconsistent or even conflicting adaptation objectives and different levels of context knowledge need to be reconciled and are key research issues.  In order to achieve adaptation solutions capable of coping with these different uncertainties, the project will develop advanced learning, decision-making and negotiation techniques. Learning is required for deriving accurate system behavioural patterns according to the acquired context knowledge. This will then drive decision-making functions for taking the most appropriate adaptation actions to address these uncertainties. Negotiation techniques are required for resolving potential tussles between specific content/network adaptation objectives by different players in the content ecosystem. The project will consider both centralised and distributed approaches in which learning and decision-making processes on adaptation actions can be performed either at the central adaptation domain controller or in a decentralised manner across multiple network elements. In the latter case, emerging information-/content-centric networks will become much more intelligent, with content-aware devices performing self-adaptation according to their own context knowledge but through coordination in order to achieve global near-optimality and stability. "
	},
	{
		"grant":565,
		"ID": "EP/L018829/1",
		"Title": "MACACO: Mobile context-Adaptive CAching for COntent-centric networking",
		"PIID": "-289131",
		"Scheme": "Standard - NR1",
		"StartDate": "01/11/2013",
		"EndDate": "31/10/2016",
		"Value": "284801",
		"ResearchArea": "Artificial Intelligence Technologies",
		"Theme": "Information and Communication Technologies",
		"Department": "School of Computer Science",
		"OrgID": "14",
		"Investigators":[
		{"ID": "-289131", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Finding new ways to manage the increased data usage and to improve the level of service required by the new wave of smartphones applications is an essential issue. The MACACO project proposes an innovative solution to this problem by focusing on data offloading mechanisms that take advantage of context and content information. Our intuition is that if it is possible to extract and forecast the behaviour of mobile network users in the three dimensional space of time, location and interest (i.e. 'what', 'when' and 'where' users are pulling data from the network), it is possible to derive efficient data offloading protocols. Such protocols would pre-fetch the identified data and cache them at the network edge at an earlier time, preferably when the mobile network is less congested, or offers better quality of service. Caching can be done directly at the mobile terminals, as well as at the edge nodes of the network (e.g., femtocells or wireless access points).   Building on previous research efforts in the fields of social wireless networking, opportunistic communications and content networking, MACACO will address several issues in this space. The first one is to derive appropriate models for the correlation between user interests and their mobility. Lots of studies have characterised mobile nodes mobility based on real world data traces, but knowledge about the interactions with user interests in this context is still missing. To fill this gap, MACACO proposes to acquire real world data sets to model mobile node behaviour in the aforementioned three-dimensional space. The second issue addressed is the derivation of efficient data-offloading algorithms leveraging the large-scale data traces and corresponding models. Firstly, simple and efficient prediction algorithms will be derived to forecast the node's mobility and interests. Then, MACACO will provide data pre-fetching mechanisms that both improves the perceived quality of service of the mobile user and noticeably offloads peak bandwidth demands at the cellular network. A proof of concept will be exhibited though a federated testbed located in France, Switzerland and in the UK."
	},
	{
		"grant":566,
		"ID": "EP/L020750/1",
		"Title": "New aspects of the mu-calculus",
		"PIID": "7464",
		"Scheme": "Overseas Travel Grants",
		"StartDate": "01/12/2013",
		"EndDate": "31/07/2014",
		"Value": "7315",
		"ResearchArea": "Theory of Computation",
		"Theme": "Information and Communication Technologies",
		"Department": "Dept of Computing",
		"OrgID": "77",
		"Investigators":[
		{"ID": "7464", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Formal logic provides us with a mathematical language for specifying and describing systems, as well as powerful methods for reasoning about whether a logical statement is true in a given situation, which logical statements follow from which, and so on - and these methods can often be automated.    Model checking applies these ideas to provide powerful automated tools for verifying that software meets its specification.  This can save companies money and provide confidence to wide society of the reliability of software.  Conventional model checking has been enormously successful industrially, but much of it has considered time as discretely ticking - 0, 1, 2, and so on.  For some computer systems, continuous time as used in physics is more appropriate.  Part of this project will study the possibilities for model checking in situations concerning real-time models, using a very powerful logic, the temporal mu-calculus.  In the long term, it may lead to new ways of model checking more sophisticated systems.  As well as time, space is an important aspect of many modern applications, including databases, geographic information systems, and geometrical reasoning.  Logic can also be used to make statements about space, and reason with them.  Many different logical systems have been used, but again the very powerful mu-calculus has not been greatly investigated in this context.  This project aims to study the modal mu-calculus in spatial contexts, trying to ascertain its expressiveness, and what machinery is needed to reason correctly with it.  The work has the potential in the long run to improve our ability to specify and reason about situations involving space.  We will also take the opportunity to try to establish some fundamental facts about the mu-calculus's power to define classes of situations, echoing the so-called Goldblatt-Thomason theorem for simpler logics.  If successful, this will provide researchers with a basic tool usable in many areas requiring the mu-calculus.  Only three months is available for the research on this ambitious project, and it is likely that not all problems will be solved, but we hope that good progress will be made."
	},
	{
		"grant":567,
		"ID": "TS/H001816/1",
		"Title": "SATURN (Self-organising Adaptive Technology underlying Resilient Networks)",
		"PIID": "100470",
		"Scheme": "Technology Programme",
		"StartDate": "01/11/2009",
		"EndDate": "31/10/2012",
		"Value": "367541",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77",
		"Investigators":[
		{"ID": "100470", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "Within the SATURN project: (1) Imperial College will first conduct a comparative study of EU research capabilities and activities in Critical Network Infrastructures (CNI) so as to identify potential synergies, and possible significant differences, between UK and the broader EU CNI system. The comparative study will also identify potential collaborations with ongoing or planned EU research. The starting point for this step will be our existing involvement in the EU FP7 DIESIS project on the simulation of European interconnected critical infrastructures and our role as UK national representative in a new EU COST Action in the area of Critical Infrastructures which includes 22 participating EU countries..   (2) Then we will investigate and implement a mathematical and discrete event simulation model representation of the UK CNI topology in collaboration with BT, including typical network parameters and capacities, and also develop an emulation capability within our existing laboratory network test-bed. These models will include the effect of the middleware proposed by BT, whose role is to provide resilience and reliability to the CNI in the presence of failures and possible attacks. Cross-validation of the model, simulation and emulation will be conducted, and the representation of critical security threats will be examined within the mathematical model, the simulation and the emulation.  This modeling/simulation/emulation framework will also incorporate an appropriate representation of CNI service interactions. (3) We will then develop a predictive model for the stability analysis of the CNI in collaboration with Oxford and Warwick Universities. The predictive modeling effort should also include an analysis of techniques that mitigate the risks to the CNI, and the design and experimentation of the risk mitigation techniques and their impact on the stability of the CNI in the presence of failures, attacks and other security threats. (4) Finally, these techniques will be incorporated and tested, and the resulting dynamic behaviours will be visualised  on the Cyber Range in Collaboration with BT and Northrop Grumman.Imperial will actively pursue non-commercial and commercial dissemination of the results of this project. Commercial dissemination will be organised via patenting the intellectual property that is developed, and seeking license agreements both with BT and NG, and the SME network that will be part of SATURN. Non-commercial dissemination will occur via two PhD thesis, and especially via a series of conference papers and journal articles, and via two special issues of The Computer Journal (British Computer Society) that are being planned."
	},
	{
		"grant":568,
		"ID": "TS/H001832/1",
		"Title": "SATURN (Self-organising Adaptive Technology underlying Resilient Networks)",
		"PIID": "110005",
		"Scheme": "Technology Programme",
		"StartDate": "01/11/2009",
		"EndDate": "31/10/2012",
		"Value": "329328",
		"ResearchArea": "Complexity Science",
		"Theme": "Information and Communication Technologies",
		"Department": "Said Business School",
		"OrgID": "106",
		"Investigators":[
		{"ID": "110005", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The architecture and protocols that ensure safe operation of Critical National Infrastructure (CNI) networks are ultimately constrained by fundamental rules and principles. For instance, data flow in information networks is constrained by physical limits that apply to individual parts of the network as well as the network as a whole. The goals of SATURN can only be achieved if they are informed by such theoretical considerations.A number of tools have been created to analyze systems such as CNI networks in the field of Complex Networks, a critical growth area of Complexity Science. Unfortunately, most work has been done without data from the systems themselves due to the difficulty to obtain such data. SATURN will be an opportunity to address this problem, a test bench for these tools, and a source of development of new ones.The most relevant features of CNI networks are their structural resilience, and their transport performance and efficiency. The resilience of an CNI network is a measure of the percentage of the network that needs to fail before global failure occurs. The transport performance and efficiency corresponds to the amount of flow a network can cope with and with how much strain it does it. We aim to analyze the structural resilience and transport performance and efficiency of CNI networks under normal conditions, and under partial or global failure. Another aspect of the SATURN project is the interaction and integration of multiple CNI networks in an efficient way so they can be monitored and used simultaneously. For example, if the road network fails due to some generalized problem, the rail system becomes an alternative, and it is necessary to determine how to use it in an efficient way to partly or fully compensate for the other failure. Theory can help understand and plan for this situation by determining the transport performance and efficiency and resilience of the two networks combined together.For resilience, we employ the methods of percolation theory, including recent advances such as Limited Path Percolation (LPP), which attempts to address real-world problems. Transport performance and efficiency will be measured through the use of conductance, flow capacities, etc., to determine how much traffic/flow a given network can sustain, and what strain is being placed on its components due to the flow. A further theoretical aspect that can be addressed is the determination of how critical a given network element is to the correct functioning of the network. Our theoretical approaches in percolation theory, electrical conductance and other similar measures can provide criticality scores to network elements that should guide authorities and stakeholders in planning the maintenance and security of these CNI networks. We intend to conduct our research iteratively based on a close collaboration with the partners of the SATURN consortium. First, from our theoretical analysis we can predict possible failure and/or low transport efficiency regimes in CNI networks. Then, with the help of other partners we can study these scenarios in more realistic systems. The partners can then offer results, as well as propose other possible failure mechanisms that we need to incorporate into the theoretical models."
	},
	{
		"grant":569,
		"ID": "TS/H002138/2",
		"Title": "SATURN (Self-organising Adaptive Technology underlying Resilient Networks)",
		"PIID": "-137446",
		"Scheme": "Technology Programme",
		"StartDate": "01/10/2011",
		"EndDate": "31/12/2012",
		"Value": "167239",
		"ResearchArea": "Other",
		"Theme": "Information and Communication Technologies",
		"Department": "Computer Science",
		"OrgID": "106",
		"Investigators":[
		{"ID": "-137446", "Role": "Principal Investigator"}
		],
		"Collaborators":[
		],
		"Summary": "The overall project aims are to develop a middleware layer to provide an agile and dependable service-based information infrastructure capable of supporting critical functions.  The Warwick effort underpins this by fundamental research into the nature and communicability of interdependencies between organisations providing components of the infrastructure and analysis of the consequent risks.The complexity of systems today makes risk management a difficult task: often mitigation strategies become out of date as technology is installed and processes evolve. The agile nature of business and the fast-paced development of technology results in information infrastructures which are highly dynamic, changing both in configuration of their technology and in their use. The problem is aggravated when we consider a system of enterprises, where vulnerabilities are pervasive, and the infrastructures are interconnected via the Internet or some other common element (which might include members of staff). Here we need to be able to understand the potential impact of a single attack or set of events upon a whole system of systems, where the resilience mechanisms deployed may vary and where the risk management methods adopted may not be consistent. It is currently not possible to predict such an impact beyond the simplest cases (such as where the attack exploits an obvious vulnerability in a common piece of technology, which has a clear impact upon the organisations under attack, and where the potential for an attacker to exploit that vulnerability is known). However, where the potential impact of an attack is not known even within a single organisation, then understanding the cascade effects across a system of such organisations is exceptionally difficult. It is this interdependency problem which we seek to address.We will use our expertise in resilience, formal analysis, intelligence systems (and in particular data fusion) and threat modelling to develop an ontology and generic model of resilience mechanisms and risk-management methodologies, based on those in use in typical representatives of key domains, and to use this to assess the potential for cascade failures across a system of organisations, under a variety of vulnerability and threat models. Specifically, we will explore the relevant technology, process, standards, compliance, human and social (such as economics, WARP, social networking and other forms of collaboration) factors which could directly influence points of vulnerability and weakness. Logical abstractions will be developed and automated inference and model-checking tools will be used to explore the model and run 'what-if' scenarios in order to explore critical dependencies, and the degree of impact which attacks on vulnerabilities might have across the system of systems. The model will be extended to incorporate a formal representation of the SATURN self-healing functionality, allowing for 'what-if' scenarios to be explored and, in particular, potential conflicts between differing systems' strategies for healing and the overall impact on the CNI system of systems. The outputs of the mechanised analysis will be used to scope the experiments to be run on the Cyber Range; identifying potential areas of interest and concern, and healing strategies worthy of realistic simulation and further investigation using the range tools. The key outputs of this analysis will be: *  a capability to model (subsets of) CNI as systems of systems, to assess the impact of the varying risk management approaches upon the exposure to threats and vulnerabilities. This capability will be based upon a formal model and automated analysis tools, and will be validated via the Cyber Range experimentation. *  specific recommendations resulting directly from the experiments run on the Cyber Range on how organisations adopting differing risk management methodologies might collaborate to manage risk in a joined-up fashion."
	}
	],
	"persons":[
	{
		"person":0,
		"ID": "-444482",
		"Surname": "Cali",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science and Information Systems",
		"OrgID": "73"
	},
	{
		"person":1,
		"ID": "-441553",
		"Surname": "Martin",
		"Title": "Dr",
		"Initials": "B   D",
		"Department": "School of Science and Technology",
		"OrgID": "85"
	},
	{
		"person":2,
		"ID": "-422034",
		"Surname": "Navarro Perez",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":3,
		"ID": "-421970",
		"Surname": "Peters",
		"Title": "Dr",
		"Initials": "W   T   M",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":4,
		"ID": "-421255",
		"Surname": "Balocco",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":5,
		"ID": "-421253",
		"Surname": "Nikolopoulos",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10"
	},
	{
		"person":6,
		"ID": "-421189",
		"Surname": "Chatzigeorgiou",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":7,
		"ID": "-420725",
		"Surname": "Mattevi",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Materials",
		"OrgID": "77"
	},
	{
		"person":8,
		"ID": "-420555",
		"Surname": "Schockaert",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "28"
	},
	{
		"person":9,
		"ID": "-418850",
		"Surname": "Rodrigues",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":10,
		"ID": "-417031",
		"Surname": "Fraser",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":11,
		"ID": "-413742",
		"Surname": "Mosley",
		"Title": "Dr",
		"Initials": "P   J",
		"Department": "Physics",
		"OrgID": "7"
	},
	{
		"person":12,
		"ID": "-413398",
		"Surname": "Gadouleau",
		"Title": "Dr",
		"Initials": "M   R",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":13,
		"ID": "-408781",
		"Surname": "Latora",
		"Title": "Professor",
		"Initials": "V",
		"Department": "Sch of Mathematical Sciences",
		"OrgID": "76"
	},
	{
		"person":14,
		"ID": "-408327",
		"Surname": "Alatise",
		"Title": "Dr",
		"Initials": "O",
		"Department": "Sch of Engineering",
		"OrgID": "31"
	},
	{
		"person":15,
		"ID": "-403442",
		"Surname": "Mertzios",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":16,
		"ID": "-402956",
		"Surname": "Sviridenko",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "31"
	},
	{
		"person":17,
		"ID": "-401877",
		"Surname": "Sastry",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Institute of Telecommunications",
		"OrgID": "78"
	},
	{
		"person":18,
		"ID": "-401519",
		"Surname": "Grierson",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Design Manufacture and Engineering Man",
		"OrgID": "48"
	},
	{
		"person":19,
		"ID": "-399067",
		"Surname": "Alglave",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":20,
		"ID": "-394954",
		"Surname": "Doucet",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Statistics",
		"OrgID": "106"
	},
	{
		"person":21,
		"ID": "-394342",
		"Surname": "Navaie",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":22,
		"ID": "-393162",
		"Surname": "Rogers",
		"Title": "Dr",
		"Initials": "D   J",
		"Department": "Sch of Engineering",
		"OrgID": "28"
	},
	{
		"person":23,
		"ID": "-390041",
		"Surname": "Ridge",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Computer Science",
		"OrgID": "66"
	},
	{
		"person":24,
		"ID": "-388198",
		"Surname": "Nash",
		"Title": "Professor",
		"Initials": "G R",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46"
	},
	{
		"person":25,
		"ID": "-386104",
		"Surname": "Kim",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77"
	},
	{
		"person":26,
		"ID": "-384825",
		"Surname": "Hao",
		"Title": "Dr",
		"Initials": "F",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":27,
		"ID": "-382671",
		"Surname": "Williams",
		"Title": "Dr",
		"Initials": "O A",
		"Department": "School of Physics and Astronomy",
		"OrgID": "28"
	},
	{
		"person":28,
		"ID": "-372746",
		"Surname": "Brown",
		"Title": "Dr",
		"Initials": "M A",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":29,
		"ID": "-371484",
		"Surname": "Alexander",
		"Title": "Dr",
		"Initials": "R   D",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":30,
		"ID": "-366390",
		"Surname": "Bhaskaran",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Materials",
		"OrgID": "106"
	},
	{
		"person":31,
		"ID": "-361617",
		"Surname": "Gross",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Automatic Control and Systems Eng",
		"OrgID": "116"
	},
	{
		"person":32,
		"ID": "-354377",
		"Surname": "Gan",
		"Title": "Dr",
		"Initials": "L",
		"Department": "Sch of Engineering and Design",
		"OrgID": "129"
	},
	{
		"person":33,
		"ID": "-302138",
		"Surname": "Richtarik",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Sch of Mathematics",
		"OrgID": "41"
	},
	{
		"person":34,
		"ID": "-298121",
		"Surname": "Gairing",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":35,
		"ID": "-295955",
		"Surname": "Specia",
		"Title": "Dr",
		"Initials": "L",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":36,
		"ID": "-289131",
		"Surname": "Musolesi",
		"Title": "Dr",
		"Initials": "M",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":37,
		"ID": "-283783",
		"Surname": "Markham",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":38,
		"ID": "-283417",
		"Surname": "Sutton",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":39,
		"ID": "-269821",
		"Surname": "Carr",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Computing",
		"OrgID": "64"
	},
	{
		"person":40,
		"ID": "-266029",
		"Surname": "Lukasiewicz",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":41,
		"ID": "-264123",
		"Surname": "Cadar",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":42,
		"ID": "-260000",
		"Surname": "Baptista",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Physics",
		"OrgID": "0"
	},
	{
		"person":43,
		"ID": "-258244",
		"Surname": "Razavi",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":44,
		"ID": "-258048",
		"Surname": "Brostow",
		"Title": "Dr",
		"Initials": "G J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":45,
		"ID": "-253807",
		"Surname": "Harris",
		"Title": "Professor",
		"Initials": "K D",
		"Department": "Institute of Neurology",
		"OrgID": "81"
	},
	{
		"person":46,
		"ID": "-253397",
		"Surname": "Barnes",
		"Title": "Dr",
		"Initials": "P   R   F",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":47,
		"ID": "-252393",
		"Surname": "Gunes",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":48,
		"ID": "-251433",
		"Surname": "Tu",
		"Title": "Dr",
		"Initials": "W",
		"Department": "School of Science & Technology",
		"OrgID": "103"
	},
	{
		"person":49,
		"ID": "-251053",
		"Surname": "Ying",
		"Title": "Dr",
		"Initials": "Y",
		"Department": "Computer Science",
		"OrgID": "46"
	},
	{
		"person":50,
		"ID": "-250817",
		"Surname": "Takashina",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Physics",
		"OrgID": "7"
	},
	{
		"person":51,
		"ID": "-250741",
		"Surname": "Stankovic",
		"Title": "Dr",
		"Initials": "V",
		"Department": "Centre for Software Reliability",
		"OrgID": "87"
	},
	{
		"person":52,
		"ID": "-249907",
		"Surname": "Kristensson",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Computer Science",
		"OrgID": "119"
	},
	{
		"person":53,
		"ID": "-249686",
		"Surname": "Capretta",
		"Title": "Dr",
		"Initials": "V",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":54,
		"ID": "-249634",
		"Surname": "Sach",
		"Title": "Dr",
		"Initials": "B G",
		"Department": "Computer Science",
		"OrgID": "31"
	},
	{
		"person":55,
		"ID": "-249414",
		"Surname": "Pedersen",
		"Title": "Dr",
		"Initials": "M D",
		"Department": "School of Biological Sciences",
		"OrgID": "24"
	},
	{
		"person":56,
		"ID": "-249355",
		"Surname": "Sarkar",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "119"
	},
	{
		"person":57,
		"ID": "-249197",
		"Surname": "Soares Indrusiak",
		"Title": "Dr",
		"Initials": "L",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":58,
		"ID": "-247649",
		"Surname": "Laird",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":59,
		"ID": "-247515",
		"Surname": "Lin",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":60,
		"ID": "-245462",
		"Surname": "Porta",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Architecture",
		"OrgID": "48"
	},
	{
		"person":61,
		"ID": "-241485",
		"Surname": "Wang",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Communications Systems Res CCSR",
		"OrgID": "51"
	},
	{
		"person":62,
		"ID": "-240925",
		"Surname": "Lesanovsky",
		"Title": "Dr",
		"Initials": "I   W",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104"
	},
	{
		"person":63,
		"ID": "-238808",
		"Surname": "Schewe",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":64,
		"ID": "-228147",
		"Surname": "Muskens",
		"Title": "Dr",
		"Initials": "O",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117"
	},
	{
		"person":65,
		"ID": "-227295",
		"Surname": "Hinze",
		"Title": "Dr",
		"Initials": "R T W",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":66,
		"ID": "-223789",
		"Surname": "Weyrich",
		"Title": "Dr",
		"Initials": "T A",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":67,
		"ID": "-220230",
		"Surname": "Prodromakis",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":68,
		"ID": "-219232",
		"Surname": "Johann",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Computer and Information Sciences",
		"OrgID": "48"
	},
	{
		"person":69,
		"ID": "-219198",
		"Surname": "Mitrovic",
		"Title": "Dr",
		"Initials": "I   Z",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68"
	},
	{
		"person":70,
		"ID": "-215861",
		"Surname": "Turner",
		"Title": "Dr",
		"Initials": "R E",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":71,
		"ID": "-215835",
		"Surname": "Cohn",
		"Title": "Dr",
		"Initials": "T A",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":72,
		"ID": "-215718",
		"Surname": "Donaldson",
		"Title": "Dr",
		"Initials": "A F",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":73,
		"ID": "-215489",
		"Surname": "Piterman",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Computer Science",
		"OrgID": "66"
	},
	{
		"person":74,
		"ID": "-214796",
		"Surname": "Wojtczak",
		"Title": "Dr",
		"Initials": "D K",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":75,
		"ID": "-214321",
		"Surname": "Oliveto",
		"Title": "Dr",
		"Initials": "P S",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":76,
		"ID": "-209266",
		"Surname": "Coja-Oghlan",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Mathematics",
		"OrgID": "31"
	},
	{
		"person":77,
		"ID": "-203954",
		"Surname": "Phan",
		"Title": "Professor",
		"Initials": "R C",
		"Department": "UNLISTED",
		"OrgID": "23"
	},
	{
		"person":78,
		"ID": "-203779",
		"Surname": "Goldwater",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":79,
		"ID": "-202908",
		"Surname": "Coles",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":80,
		"ID": "-202368",
		"Surname": "Kroening",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":81,
		"ID": "-201783",
		"Surname": "Yates",
		"Title": "Professor",
		"Initials": "J M",
		"Department": "Dentistry",
		"OrgID": "93"
	},
	{
		"person":82,
		"ID": "-201108",
		"Surname": "Mitrofanov",
		"Title": "Dr",
		"Initials": "O",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":83,
		"ID": "-197015",
		"Surname": "Teh",
		"Title": "Professor",
		"Initials": "Y W",
		"Department": "Statistics",
		"OrgID": "106"
	},
	{
		"person":84,
		"ID": "-196798",
		"Surname": "He",
		"Title": "Dr",
		"Initials": "Y",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12"
	},
	{
		"person":85,
		"ID": "-194260",
		"Surname": "Krysta",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":86,
		"ID": "-193871",
		"Surname": "Tanaka",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computing Department",
		"OrgID": "97"
	},
	{
		"person":87,
		"ID": "-193862",
		"Surname": "Sun",
		"Title": "Dr",
		"Initials": "X",
		"Department": "Computer Science",
		"OrgID": "28"
	},
	{
		"person":88,
		"ID": "-193341",
		"Surname": "MAIR",
		"Title": "Professor",
		"Initials": "C",
		"Department": "London College of Fashion",
		"OrgID": "84"
	},
	{
		"person":89,
		"ID": "-190482",
		"Surname": "Wang",
		"Title": "Dr",
		"Initials": "W",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51"
	},
	{
		"person":90,
		"ID": "-190287",
		"Surname": "Whelan-Curtin",
		"Title": "Dr",
		"Initials": "J W",
		"Department": "Physics and Astronomy",
		"OrgID": "119"
	},
	{
		"person":91,
		"ID": "-190135",
		"Surname": "Wang",
		"Title": "Dr",
		"Initials": "Q",
		"Department": "School of Computing",
		"OrgID": "107"
	},
	{
		"person":92,
		"ID": "-189982",
		"Surname": "Power",
		"Title": "Dr",
		"Initials": "A J",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":93,
		"ID": "-189859",
		"Surname": "Ding",
		"Title": "Dr",
		"Initials": "Z",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98"
	},
	{
		"person":94,
		"ID": "-189537",
		"Surname": "Gerardot",
		"Title": "Dr",
		"Initials": "B D",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":95,
		"ID": "-188808",
		"Surname": "Martinez",
		"Title": "Dr",
		"Initials": "A E",
		"Department": "School of Engineering",
		"OrgID": "125"
	},
	{
		"person":96,
		"ID": "-188637",
		"Surname": "Owens",
		"Title": "Dr",
		"Initials": "S A",
		"Department": "Sch of Computing",
		"OrgID": "27"
	},
	{
		"person":97,
		"ID": "-188465",
		"Surname": "Barrett",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":98,
		"ID": "-188117",
		"Surname": "Subramanian",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":99,
		"ID": "-187034",
		"Surname": "Groth",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":100,
		"ID": "-186860",
		"Surname": "Pezaros",
		"Title": "Dr",
		"Initials": "D",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":101,
		"ID": "-186548",
		"Surname": "Cuenca Grau",
		"Title": "Dr",
		"Initials": "B",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":102,
		"ID": "-185954",
		"Surname": "Komendantskaya",
		"Title": "Dr",
		"Initials": "E",
		"Department": "School of Computing",
		"OrgID": "37"
	},
	{
		"person":103,
		"ID": "-185805",
		"Surname": "Sadrzadeh",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":104,
		"ID": "-185626",
		"Surname": "Benedikt",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":105,
		"ID": "-185357",
		"Surname": "Li",
		"Title": "Dr",
		"Initials": "F W B",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":106,
		"ID": "-185268",
		"Surname": "Lestas",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":107,
		"ID": "-183749",
		"Surname": "Wood",
		"Title": "Dr",
		"Initials": "J D",
		"Department": "Information Science",
		"OrgID": "83"
	},
	{
		"person":108,
		"ID": "-182177",
		"Surname": "Ballard",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Wolfson Centre for Age Related Diseases",
		"OrgID": "78"
	},
	{
		"person":109,
		"ID": "-181039",
		"Surname": "Ang",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Sch of Engineering & Digital Arts",
		"OrgID": "27"
	},
	{
		"person":110,
		"ID": "-179793",
		"Surname": "MIZUTA",
		"Title": "Professor",
		"Initials": "H",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":111,
		"ID": "-178077",
		"Surname": "Tanner",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Mathematical Institute",
		"OrgID": "106"
	},
	{
		"person":112,
		"ID": "-177354",
		"Surname": "Ni",
		"Title": "Professor",
		"Initials": "Q",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":113,
		"ID": "-175388",
		"Surname": "Moram",
		"Title": "Dr",
		"Initials": "M A",
		"Department": "Materials",
		"OrgID": "77"
	},
	{
		"person":114,
		"ID": "-173006",
		"Surname": "Rodriguez Echavarria",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Sch of Computing, Engineering & Maths",
		"OrgID": "19"
	},
	{
		"person":115,
		"ID": "-172924",
		"Surname": "Azzopardi",
		"Title": "Dr",
		"Initials": "L",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":116,
		"ID": "-171807",
		"Surname": "Copner",
		"Title": "Professor",
		"Initials": "N J",
		"Department": "Faculty of Advanced Technology",
		"OrgID": "127"
	},
	{
		"person":117,
		"ID": "-170498",
		"Surname": "Nowotny",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Sch of Engineering and Informatics",
		"OrgID": "20"
	},
	{
		"person":118,
		"ID": "-170420",
		"Surname": "Yamagishi",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Centre for Speech Technology Research",
		"OrgID": "41"
	},
	{
		"person":119,
		"ID": "-170360",
		"Surname": "Singh",
		"Title": "Dr",
		"Initials": "S S",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":120,
		"ID": "-169518",
		"Surname": "Cain",
		"Title": "Dr",
		"Initials": "R",
		"Department": "WMG",
		"OrgID": "31"
	},
	{
		"person":121,
		"ID": "-165743",
		"Surname": "Hrkac",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46"
	},
	{
		"person":122,
		"ID": "-164856",
		"Surname": "Cheng",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Civil Environmental and Geomatic Eng",
		"OrgID": "81"
	},
	{
		"person":123,
		"ID": "-162851",
		"Surname": "Cassidy",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Sch of Engineering & Built Environment",
		"OrgID": "50"
	},
	{
		"person":124,
		"ID": "-162039",
		"Surname": "Peacock",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":125,
		"ID": "-161956",
		"Surname": "YANG",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":126,
		"ID": "-161795",
		"Surname": "Coleman",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Institute of Communication Studies",
		"OrgID": "64"
	},
	{
		"person":127,
		"ID": "-161185",
		"Surname": "Czumaj",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "31"
	},
	{
		"person":128,
		"ID": "-160701",
		"Surname": "BORDAS",
		"Title": "Professor",
		"Initials": "S P A",
		"Department": "Sch of Engineering",
		"OrgID": "28"
	},
	{
		"person":129,
		"ID": "-156057",
		"Surname": "Missier",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":130,
		"ID": "-153897",
		"Surname": "Choi",
		"Title": "Professor",
		"Initials": "J",
		"Department": "School of Engineering",
		"OrgID": "125"
	},
	{
		"person":131,
		"ID": "-150560",
		"Surname": "Siddharthan",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computing Science",
		"OrgID": "0"
	},
	{
		"person":132,
		"ID": "-150371",
		"Surname": "Clifford",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":133,
		"ID": "-149756",
		"Surname": "Kurlin",
		"Title": "Dr",
		"Initials": "V",
		"Department": "Mathematical Sciences",
		"OrgID": "39"
	},
	{
		"person":134,
		"ID": "-149587",
		"Surname": "Silva",
		"Title": "Dr",
		"Initials": "R B d",
		"Department": "Statistical Science",
		"OrgID": "81"
	},
	{
		"person":135,
		"ID": "-149490",
		"Surname": "Parker",
		"Title": "Dr",
		"Initials": "D",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":136,
		"ID": "-149220",
		"Surname": "Hadfield",
		"Title": "Professor",
		"Initials": "R H",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":137,
		"ID": "-149095",
		"Surname": "Apostolopoulos",
		"Title": "Dr",
		"Initials": "V",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117"
	},
	{
		"person":138,
		"ID": "-147964",
		"Surname": "Chakraborty",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":139,
		"ID": "-145924",
		"Surname": "McBride",
		"Title": "Dr",
		"Initials": "C T",
		"Department": "Computer and Information Sciences",
		"OrgID": "48"
	},
	{
		"person":140,
		"ID": "-145660",
		"Surname": "Worrell",
		"Title": "Professor",
		"Initials": "J B",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":141,
		"ID": "-140023",
		"Surname": "Mortier",
		"Title": "Dr",
		"Initials": "R M",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":142,
		"ID": "-137446",
		"Surname": "Creese",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":143,
		"ID": "-136653",
		"Surname": "Chothia",
		"Title": "Dr",
		"Initials": "T C",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":144,
		"ID": "-130158",
		"Surname": "Ferguson",
		"Title": "Dr",
		"Initials": "A J",
		"Department": "Physics",
		"OrgID": "24"
	},
	{
		"person":145,
		"ID": "-128441",
		"Surname": "Whittow",
		"Title": "Dr",
		"Initials": "W G",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90"
	},
	{
		"person":146,
		"ID": "-128335",
		"Surname": "Pietzuch",
		"Title": "Dr",
		"Initials": "P R",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":147,
		"ID": "-124855",
		"Surname": "Jones",
		"Title": "Dr",
		"Initials": "M W",
		"Department": "College of Science",
		"OrgID": "125"
	},
	{
		"person":148,
		"ID": "-124398",
		"Surname": "Hayne",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Physics",
		"OrgID": "63"
	},
	{
		"person":149,
		"ID": "-124229",
		"Surname": "De Bie",
		"Title": "Dr",
		"Initials": "T E P",
		"Department": "Engineering Mathematics",
		"OrgID": "22"
	},
	{
		"person":150,
		"ID": "-120856",
		"Surname": "Lagoudakis",
		"Title": "Professor",
		"Initials": "P",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117"
	},
	{
		"person":151,
		"ID": "-120390",
		"Surname": "Kautz",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":152,
		"ID": "-120115",
		"Surname": "Dearden",
		"Title": "Dr",
		"Initials": "R W",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":153,
		"ID": "-119314",
		"Surname": "McAllister",
		"Title": "Dr",
		"Initials": "J P",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10"
	},
	{
		"person":154,
		"ID": "-118653",
		"Surname": "Stark",
		"Title": "Dr",
		"Initials": "B H",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":155,
		"ID": "-117490",
		"Surname": "Guglielmi",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":156,
		"ID": "-117299",
		"Surname": "Libkin",
		"Title": "Professor",
		"Initials": "L",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":157,
		"ID": "-116900",
		"Surname": "Doran",
		"Title": "Professor",
		"Initials": "N J",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12"
	},
	{
		"person":158,
		"ID": "-116873",
		"Surname": "Brown",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Oxford Internet Institute",
		"OrgID": "106"
	},
	{
		"person":159,
		"ID": "-116626",
		"Surname": "Distefano",
		"Title": "Dr",
		"Initials": "D",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":160,
		"ID": "-115276",
		"Surname": "Mikolajczyk",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51"
	},
	{
		"person":161,
		"ID": "-115235",
		"Surname": "Skylaris",
		"Title": "Dr",
		"Initials": "C",
		"Department": "School of Chemistry",
		"OrgID": "117"
	},
	{
		"person":162,
		"ID": "-115131",
		"Surname": "Patras",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":163,
		"ID": "-115066",
		"Surname": "Kowalski",
		"Title": "Dr",
		"Initials": "D R",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":164,
		"ID": "-114981",
		"Surname": "Ouaknine",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":165,
		"ID": "-114156",
		"Surname": "Clark",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":166,
		"ID": "-113600",
		"Surname": "Qin",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Sch of Computing",
		"OrgID": "94"
	},
	{
		"person":167,
		"ID": "-111291",
		"Surname": "Kashefi",
		"Title": "Dr",
		"Initials": "E",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":168,
		"ID": "-111126",
		"Surname": "McMinn",
		"Title": "Dr",
		"Initials": "P S",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":169,
		"ID": "-110997",
		"Surname": "Debattista",
		"Title": "Dr",
		"Initials": "K",
		"Department": "WMG",
		"OrgID": "31"
	},
	{
		"person":170,
		"ID": "-110926",
		"Surname": "Dubrovka",
		"Title": "Dr",
		"Initials": "R F",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":171,
		"ID": "-109524",
		"Surname": "O'Brien",
		"Title": "Professor",
		"Initials": "J L",
		"Department": "Physics",
		"OrgID": "22"
	},
	{
		"person":172,
		"ID": "-108516",
		"Surname": "Kaiser",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":173,
		"ID": "-108456",
		"Surname": "Maffeis",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":174,
		"ID": "-108007",
		"Surname": "Adams",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "43"
	},
	{
		"person":175,
		"ID": "-107268",
		"Surname": "Ray",
		"Title": "Dr",
		"Initials": "O",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":176,
		"ID": "-106108",
		"Surname": "Gottlob",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":177,
		"ID": "-105665",
		"Surname": "Bordewich",
		"Title": "Dr",
		"Initials": "M J R",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":178,
		"ID": "-105075",
		"Surname": "Raimondi",
		"Title": "Dr",
		"Initials": "F",
		"Department": "School of Science and Technology",
		"OrgID": "85"
	},
	{
		"person":179,
		"ID": "-100599",
		"Surname": "Montanaro",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Applied Maths and Theoretical Physics",
		"OrgID": "24"
	},
	{
		"person":180,
		"ID": "-99841",
		"Surname": "Maunder",
		"Title": "Dr",
		"Initials": "R G",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":181,
		"ID": "-99600",
		"Surname": "Brumby",
		"Title": "Dr",
		"Initials": "D P",
		"Department": "UCL Interaction Centre",
		"OrgID": "81"
	},
	{
		"person":182,
		"ID": "-97308",
		"Surname": "WATTS",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":183,
		"ID": "-95896",
		"Surname": "Brotherston",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":184,
		"ID": "-95250",
		"Surname": "Thompson",
		"Title": "Dr",
		"Initials": "M G",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":185,
		"ID": "-95224",
		"Surname": "Savani",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":186,
		"ID": "-94516",
		"Surname": "Dean",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":187,
		"ID": "-93157",
		"Surname": "Gao",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Surrey Space Centre Academic",
		"OrgID": "51"
	},
	{
		"person":188,
		"ID": "-87745",
		"Surname": "Robinson",
		"Title": "Professor",
		"Initials": "I K",
		"Department": "Physics and Astronomy",
		"OrgID": "81"
	},
	{
		"person":189,
		"ID": "-86462",
		"Surname": "de Cesare",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Business School",
		"OrgID": "129"
	},
	{
		"person":190,
		"ID": "-83565",
		"Surname": "Belpaeme",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Sch of Computing & Mathematics",
		"OrgID": "109"
	},
	{
		"person":191,
		"ID": "-81370",
		"Surname": "Gong",
		"Title": "Dr",
		"Initials": "Y",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90"
	},
	{
		"person":192,
		"ID": "-80756",
		"Surname": "Laurand",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Inst of Photonics",
		"OrgID": "48"
	},
	{
		"person":193,
		"ID": "-80240",
		"Surname": "Lubeigt",
		"Title": "Dr",
		"Initials": "W",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "48"
	},
	{
		"person":194,
		"ID": "-79875",
		"Surname": "Thomsen",
		"Title": "Dr",
		"Initials": "B C",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":195,
		"ID": "-79823",
		"Surname": "So",
		"Title": "Dr",
		"Initials": "D K C",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":196,
		"ID": "-79586",
		"Surname": "Ghica",
		"Title": "Dr",
		"Initials": "D R",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":197,
		"ID": "-79136",
		"Surname": "Fish",
		"Title": "Dr",
		"Initials": "A G",
		"Department": "Sch of Computing, Engineering & Maths",
		"OrgID": "19"
	},
	{
		"person":198,
		"ID": "-64990",
		"Surname": "Adams",
		"Title": "Dr",
		"Initials": "W J",
		"Department": "School of Psychology",
		"OrgID": "117"
	},
	{
		"person":199,
		"ID": "-53274",
		"Surname": "Wong",
		"Title": "Dr",
		"Initials": "K K",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":200,
		"ID": "-47636",
		"Surname": "Soldatova",
		"Title": "Dr",
		"Initials": "L",
		"Department": "Information Systems Computing and Maths",
		"OrgID": "129"
	},
	{
		"person":201,
		"ID": "-44623",
		"Surname": "Mullins",
		"Title": "Dr",
		"Initials": "R D",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":202,
		"ID": "-37554",
		"Surname": "Mauthe",
		"Title": "Dr",
		"Initials": "A U",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":203,
		"ID": "-29547",
		"Surname": "Newell",
		"Title": "Dr",
		"Initials": "C H",
		"Department": "Scarborough School of Arts and New Media",
		"OrgID": "57"
	},
	{
		"person":204,
		"ID": "-27192",
		"Surname": "Bradley",
		"Title": "Dr",
		"Initials": "J T",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":205,
		"ID": "-26511",
		"Surname": "Blumensath",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Faculty of Engineering & the Environment",
		"OrgID": "117"
	},
	{
		"person":206,
		"ID": "-26308",
		"Surname": "Gallant",
		"Title": "Dr",
		"Initials": "A J",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":207,
		"ID": "-26269",
		"Surname": "Paulusma",
		"Title": "Dr",
		"Initials": "D",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":208,
		"ID": "-25791",
		"Surname": "Hawes",
		"Title": "Dr",
		"Initials": "N",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":209,
		"ID": "-25558",
		"Surname": "Kurz",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "66"
	},
	{
		"person":210,
		"ID": "-23788",
		"Surname": "Welbourne",
		"Title": "Dr",
		"Initials": "S R",
		"Department": "Psychological Sciences",
		"OrgID": "93"
	},
	{
		"person":211,
		"ID": "-22951",
		"Surname": "Keller",
		"Title": "Professor",
		"Initials": "F",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":212,
		"ID": "-19880",
		"Surname": "Ng",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":213,
		"ID": "-19876",
		"Surname": "Liu",
		"Title": "Professor",
		"Initials": "H",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":214,
		"ID": "-19872",
		"Surname": "Groom",
		"Title": "Dr",
		"Initials": "K M",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":215,
		"ID": "-17667",
		"Surname": "Cavallaro",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":216,
		"ID": "-16489",
		"Surname": "Maier",
		"Title": "Professor",
		"Initials": "S A",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":217,
		"ID": "-16288",
		"Surname": "Novikov",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104"
	},
	{
		"person":218,
		"ID": "-13398",
		"Surname": "Sushko",
		"Title": "Dr",
		"Initials": "P V",
		"Department": "Physics and Astronomy",
		"OrgID": "81"
	},
	{
		"person":219,
		"ID": "-12722",
		"Surname": "Stockman",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Institute of Ophthalmology",
		"OrgID": "81"
	},
	{
		"person":220,
		"ID": "-11022",
		"Surname": "Nejabati",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":221,
		"ID": "-10366",
		"Surname": "Konev",
		"Title": "Dr",
		"Initials": "B",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":222,
		"ID": "-10093",
		"Surname": "Ratnarajah",
		"Title": "Dr",
		"Initials": "T",
		"Department": "Sch of Engineering",
		"OrgID": "41"
	},
	{
		"person":223,
		"ID": "-9996",
		"Surname": "Lock",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Sch of Art & Media",
		"OrgID": "109"
	},
	{
		"person":224,
		"ID": "-8997",
		"Surname": "Bodanese",
		"Title": "Dr",
		"Initials": "E L",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":225,
		"ID": "-8384",
		"Surname": "Fieldhouse",
		"Title": "Professor",
		"Initials": "E",
		"Department": "Social Sciences",
		"OrgID": "93"
	},
	{
		"person":226,
		"ID": "-7328",
		"Surname": "Lomuscio",
		"Title": "Professor",
		"Initials": "A R",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":227,
		"ID": "-5678",
		"Surname": "Green",
		"Title": "Dr",
		"Initials": "P N",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":228,
		"ID": "-2868",
		"Surname": "Hogg",
		"Title": "Professor",
		"Initials": "R A",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":229,
		"ID": "-2799",
		"Surname": "Haseloff",
		"Title": "Dr",
		"Initials": "J P",
		"Department": "Plant Sciences",
		"OrgID": "24"
	},
	{
		"person":230,
		"ID": "-1912",
		"Surname": "Glennerster",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Sch of Psychology and Clinical Lang Sci",
		"OrgID": "113"
	},
	{
		"person":231,
		"ID": "35",
		"Surname": "Abramsky",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":232,
		"ID": "45",
		"Surname": "Ackland",
		"Title": "Professor",
		"Initials": "G J",
		"Department": "Sch of Physics and Astronomy",
		"OrgID": "41"
	},
	{
		"person":233,
		"ID": "273",
		"Surname": "Allsopp",
		"Title": "Dr",
		"Initials": "D",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "7"
	},
	{
		"person":234,
		"ID": "602",
		"Surname": "Atwell",
		"Title": "Dr",
		"Initials": "E S",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":235,
		"ID": "783",
		"Surname": "Balkan",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30"
	},
	{
		"person":236,
		"ID": "873",
		"Surname": "Bard",
		"Title": "Dr",
		"Initials": "E",
		"Department": "Sch of Philosophy Psychology & Language",
		"OrgID": "41"
	},
	{
		"person":237,
		"ID": "1799",
		"Surname": "Bradley",
		"Title": "Professor",
		"Initials": "D D C",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":238,
		"ID": "2022",
		"Surname": "Brown",
		"Title": "Professor",
		"Initials": "A D",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":239,
		"ID": "2212",
		"Surname": "Bull",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":240,
		"ID": "2220",
		"Surname": "Buller",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":241,
		"ID": "2244",
		"Surname": "Bundy",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":242,
		"ID": "2307",
		"Surname": "Burns",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":243,
		"ID": "2320",
		"Surname": "Burr",
		"Title": "Professor",
		"Initials": "A G",
		"Department": "Electronics",
		"OrgID": "55"
	},
	{
		"person":244,
		"ID": "2698",
		"Surname": "Chadwick",
		"Title": "Professor",
		"Initials": "D W",
		"Department": "Sch of Computing",
		"OrgID": "27"
	},
	{
		"person":245,
		"ID": "2755",
		"Surname": "Chantler",
		"Title": "Professor",
		"Initials": "M",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40"
	},
	{
		"person":246,
		"ID": "2851",
		"Surname": "Cherns",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Physics",
		"OrgID": "22"
	},
	{
		"person":247,
		"ID": "3257",
		"Surname": "Constantinou",
		"Title": "Dr",
		"Initials": "C C",
		"Department": "Electronic, Electrical and Computer Eng",
		"OrgID": "14"
	},
	{
		"person":248,
		"ID": "3618",
		"Surname": "Crowcroft",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":249,
		"ID": "3661",
		"Surname": "Culverhouse",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Sch of Computing & Mathematics",
		"OrgID": "109"
	},
	{
		"person":250,
		"ID": "3819",
		"Surname": "Davenport",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":251,
		"ID": "4140",
		"Surname": "Derrick",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":252,
		"ID": "4597",
		"Surname": "Dyer",
		"Title": "Professor",
		"Initials": "M E",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":253,
		"ID": "4633",
		"Surname": "Eason",
		"Title": "Professor",
		"Initials": "R W",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":254,
		"ID": "4697",
		"Surname": "Edwards",
		"Title": "Dr",
		"Initials": "A D N",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":255,
		"ID": "4822",
		"Surname": "Elliott",
		"Title": "Professor",
		"Initials": "S R",
		"Department": "Chemistry",
		"OrgID": "24"
	},
	{
		"person":256,
		"ID": "4995",
		"Surname": "Evans",
		"Title": "Professor",
		"Initials": "B",
		"Department": "Communications Systems Res CCSR",
		"OrgID": "51"
	},
	{
		"person":257,
		"ID": "5179",
		"Surname": "Ferrari",
		"Title": "Professor",
		"Initials": "A C",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":258,
		"ID": "5526",
		"Surname": "Fraser",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Physics and Astronomy",
		"OrgID": "66"
	},
	{
		"person":259,
		"ID": "5587",
		"Surname": "Friend",
		"Title": "Professor Sir",
		"Initials": "R",
		"Department": "Physics",
		"OrgID": "24"
	},
	{
		"person":260,
		"ID": "5628",
		"Surname": "Furber",
		"Title": "Professor",
		"Initials": "S   B",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":261,
		"ID": "6007",
		"Surname": "Goble",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":262,
		"ID": "6534",
		"Surname": "Gwilliam",
		"Title": "Professor",
		"Initials": "R",
		"Department": "ATI Electronics",
		"OrgID": "51"
	},
	{
		"person":263,
		"ID": "6605",
		"Surname": "Hall",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68"
	},
	{
		"person":264,
		"ID": "6674",
		"Surname": "Hamilton",
		"Title": "Professor",
		"Initials": "B",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":265,
		"ID": "6874",
		"Surname": "Harrison",
		"Title": "Professor",
		"Initials": "P G",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":266,
		"ID": "7164",
		"Surname": "Henini",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104"
	},
	{
		"person":267,
		"ID": "7464",
		"Surname": "Hodkinson",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":268,
		"ID": "7875",
		"Surname": "Humphreys",
		"Title": "Professor Sir",
		"Initials": "C",
		"Department": "Materials Science & Metallurgy",
		"OrgID": "24"
	},
	{
		"person":269,
		"ID": "7923",
		"Surname": "Hurlbert",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Institute of Neuroscience",
		"OrgID": "98"
	},
	{
		"person":270,
		"ID": "8103",
		"Surname": "Jackman",
		"Title": "Professor",
		"Initials": "R B",
		"Department": "London Centre for Nanotechnology",
		"OrgID": "81"
	},
	{
		"person":271,
		"ID": "8305",
		"Surname": "Jerrum",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Sch of Mathematical Sciences",
		"OrgID": "76"
	},
	{
		"person":272,
		"ID": "8369",
		"Surname": "Johnson",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":273,
		"ID": "8411",
		"Surname": "Johnstone",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "44"
	},
	{
		"person":274,
		"ID": "8600",
		"Surname": "Kale",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Sch of Electronics and Computer Science",
		"OrgID": "71"
	},
	{
		"person":275,
		"ID": "8620",
		"Surname": "Kar",
		"Title": "Professor",
		"Initials": "A K",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":276,
		"ID": "8710",
		"Surname": "Kelly",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":277,
		"ID": "8761",
		"Surname": "Kent",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104"
	},
	{
		"person":278,
		"ID": "8936",
		"Surname": "Kittler",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51"
	},
	{
		"person":279,
		"ID": "9032",
		"Surname": "Koutny",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":280,
		"ID": "9043",
		"Surname": "Krier",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Physics",
		"OrgID": "63"
	},
	{
		"person":281,
		"ID": "9121",
		"Surname": "Lambert",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Physics",
		"OrgID": "63"
	},
	{
		"person":282,
		"ID": "9148",
		"Surname": "Lane",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":283,
		"ID": "9162",
		"Surname": "Langley",
		"Title": "Professor",
		"Initials": "R J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":284,
		"ID": "9896",
		"Surname": "Maciejowski",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":285,
		"ID": "10226",
		"Surname": "Martin",
		"Title": "Professor",
		"Initials": "R R",
		"Department": "Computer Science",
		"OrgID": "28"
	},
	{
		"person":286,
		"ID": "10482",
		"Surname": "McDermid",
		"Title": "Professor",
		"Initials": "J A",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":287,
		"ID": "10573",
		"Surname": "McKane",
		"Title": "Professor",
		"Initials": "A J",
		"Department": "Physics and Astronomy",
		"OrgID": "93"
	},
	{
		"person":288,
		"ID": "10612",
		"Surname": "McLaughlin",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":289,
		"ID": "10744",
		"Surname": "Mellish",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Computing Science",
		"OrgID": "0"
	},
	{
		"person":290,
		"ID": "10928",
		"Surname": "Milne",
		"Title": "Professor",
		"Initials": "W I",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":291,
		"ID": "10962",
		"Surname": "Mirshekar",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30"
	},
	{
		"person":292,
		"ID": "11374",
		"Surname": "Murray",
		"Title": "Professor",
		"Initials": "D W",
		"Department": "Engineering Science",
		"OrgID": "106"
	},
	{
		"person":293,
		"ID": "11377",
		"Surname": "Murray",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":294,
		"ID": "11400",
		"Surname": "Mycroft",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":295,
		"ID": "11552",
		"Surname": "Newton",
		"Title": "Professor",
		"Initials": "M E",
		"Department": "Physics",
		"OrgID": "31"
	},
	{
		"person":296,
		"ID": "11775",
		"Surname": "O'Neill",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98"
	},
	{
		"person":297,
		"ID": "11778",
		"Surname": "O'Neill",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Physical Sciences",
		"OrgID": "57"
	},
	{
		"person":298,
		"ID": "12034",
		"Surname": "Parish",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90"
	},
	{
		"person":299,
		"ID": "12101",
		"Surname": "Parr",
		"Title": "Professor",
		"Initials": "G P",
		"Department": "Sch of Computing & Information Eng",
		"OrgID": "9"
	},
	{
		"person":300,
		"ID": "12193",
		"Surname": "Paulson",
		"Title": "Professor",
		"Initials": "L C",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":301,
		"ID": "12201",
		"Surname": "Payne",
		"Title": "Dr",
		"Initials": "F",
		"Department": "Engineering Science",
		"OrgID": "106"
	},
	{
		"person":302,
		"ID": "12208",
		"Surname": "Payne",
		"Title": "Professor Sir",
		"Initials": "D N",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":303,
		"ID": "12308",
		"Surname": "Penty",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":304,
		"ID": "12400",
		"Surname": "Phelps",
		"Title": "Professor",
		"Initials": "A D R",
		"Department": "Physics",
		"OrgID": "48"
	},
	{
		"person":305,
		"ID": "12495",
		"Surname": "Pipe",
		"Title": "Professor",
		"Initials": "A G",
		"Department": "Bristol Robotics Laboratory",
		"OrgID": "21"
	},
	{
		"person":306,
		"ID": "12545",
		"Surname": "Plotkin",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":307,
		"ID": "12851",
		"Surname": "Pulman",
		"Title": "Professor",
		"Initials": "S G",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":308,
		"ID": "13096",
		"Surname": "Reed",
		"Title": "Professor",
		"Initials": "G T",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":309,
		"ID": "13969",
		"Surname": "Sawyer",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":310,
		"ID": "14115",
		"Surname": "Seeds",
		"Title": "Professor",
		"Initials": "A J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":311,
		"ID": "14151",
		"Surname": "Sergot",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":312,
		"ID": "14826",
		"Surname": "Soraghan",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "48"
	},
	{
		"person":313,
		"ID": "15129",
		"Surname": "Stewart",
		"Title": "Professor",
		"Initials": "I A",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":314,
		"ID": "15237",
		"Surname": "Strangeways",
		"Title": "Dr",
		"Initials": "H",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98"
	},
	{
		"person":315,
		"ID": "15494",
		"Surname": "Tate",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":316,
		"ID": "15524",
		"Surname": "Taylor",
		"Title": "Professor",
		"Initials": "D M",
		"Department": "Sch of Electronics",
		"OrgID": "6"
	},
	{
		"person":317,
		"ID": "15535",
		"Surname": "Taylor",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":318,
		"ID": "15550",
		"Surname": "Taylor",
		"Title": "Professor",
		"Initials": "R A",
		"Department": "Oxford Physics",
		"OrgID": "106"
	},
	{
		"person":319,
		"ID": "15688",
		"Surname": "Calder",
		"Title": "Professor",
		"Initials": "M",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":320,
		"ID": "16022",
		"Surname": "Tropper",
		"Title": "Professor",
		"Initials": "A",
		"Department": "School of Physics and Astronomy",
		"OrgID": "117"
	},
	{
		"person":321,
		"ID": "16159",
		"Surname": "Tyrrell",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Electronics",
		"OrgID": "55"
	},
	{
		"person":322,
		"ID": "16234",
		"Surname": "Vardaxoglou",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90"
	},
	{
		"person":323,
		"ID": "16932",
		"Surname": "White",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":324,
		"ID": "17757",
		"Surname": "Zheludev",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":325,
		"ID": "17768",
		"Surname": "Zisserman",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Engineering Science",
		"OrgID": "106"
	},
	{
		"person":326,
		"ID": "18960",
		"Surname": "Rarity",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":327,
		"ID": "21502",
		"Surname": "Plumbley",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":328,
		"ID": "21646",
		"Surname": "Dawson",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Physics and Astronomy",
		"OrgID": "93"
	},
	{
		"person":329,
		"ID": "21686",
		"Surname": "Hanzo",
		"Title": "Professor",
		"Initials": "L",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":330,
		"ID": "21837",
		"Surname": "Veres",
		"Title": "Professor",
		"Initials": "S M",
		"Department": "Automatic Control and Systems Eng",
		"OrgID": "116"
	},
	{
		"person":331,
		"ID": "21886",
		"Surname": "Sutton",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Sch of Marine Science & Engineering",
		"OrgID": "109"
	},
	{
		"person":332,
		"ID": "21923",
		"Surname": "Moore",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Inst of Eng Surveying & Space Geodesy",
		"OrgID": "104"
	},
	{
		"person":333,
		"ID": "22017",
		"Surname": "Yakovlev",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98"
	},
	{
		"person":334,
		"ID": "22018",
		"Surname": "Chen",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Oxford e-Research Centre",
		"OrgID": "106"
	},
	{
		"person":335,
		"ID": "22036",
		"Surname": "Galbraith",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":336,
		"ID": "22235",
		"Surname": "Sandler",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":337,
		"ID": "25929",
		"Surname": "Barber",
		"Title": "Dr",
		"Initials": "Z",
		"Department": "Materials Science & Metallurgy",
		"OrgID": "24"
	},
	{
		"person":338,
		"ID": "27122",
		"Surname": "Hogan",
		"Title": "Professor",
		"Initials": "S J",
		"Department": "Engineering Mathematics",
		"OrgID": "22"
	},
	{
		"person":339,
		"ID": "27169",
		"Surname": "Kelly",
		"Title": "Professor",
		"Initials": "M J",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":340,
		"ID": "27394",
		"Surname": "Voronkov",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":341,
		"ID": "27588",
		"Surname": "Henderson",
		"Title": "Dr",
		"Initials": "R K",
		"Department": "Sch of Engineering",
		"OrgID": "41"
	},
	{
		"person":342,
		"ID": "27910",
		"Surname": "Cockburn",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Physics and Astronomy",
		"OrgID": "116"
	},
	{
		"person":343,
		"ID": "27964",
		"Surname": "Jones",
		"Title": "Professor",
		"Initials": "R E",
		"Department": "Sch of Computing",
		"OrgID": "27"
	},
	{
		"person":344,
		"ID": "27991",
		"Surname": "Connor",
		"Title": "Professor",
		"Initials": "R C H",
		"Department": "Computer and Information Sciences",
		"OrgID": "48"
	},
	{
		"person":345,
		"ID": "28022",
		"Surname": "Gaizauskas",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":346,
		"ID": "28149",
		"Surname": "Cox",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computing Sciences",
		"OrgID": "102"
	},
	{
		"person":347,
		"ID": "28187",
		"Surname": "Ritchie",
		"Title": "Professor",
		"Initials": "D A",
		"Department": "Physics",
		"OrgID": "24"
	},
	{
		"person":348,
		"ID": "28909",
		"Surname": "Trinder",
		"Title": "Professor",
		"Initials": "P",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":349,
		"ID": "29233",
		"Surname": "Barnes",
		"Title": "Professor",
		"Initials": "W L",
		"Department": "Physics",
		"OrgID": "46"
	},
	{
		"person":350,
		"ID": "29356",
		"Surname": "Jennings",
		"Title": "Professor",
		"Initials": "N R",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":351,
		"ID": "29466",
		"Surname": "Bradfield",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":352,
		"ID": "29563",
		"Surname": "Meeson",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Physics",
		"OrgID": "43"
	},
	{
		"person":353,
		"ID": "29766",
		"Surname": "Fisher",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "68"
	},
	{
		"person":354,
		"ID": "29870",
		"Surname": "Lucas",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30"
	},
	{
		"person":355,
		"ID": "30452",
		"Surname": "Pomiankowski",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Genetics Evolution and Environment",
		"OrgID": "81"
	},
	{
		"person":356,
		"ID": "30827",
		"Surname": "Henning",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Computer Sci and Electronic Engineering",
		"OrgID": "30"
	},
	{
		"person":357,
		"ID": "30887",
		"Surname": "McCluskey",
		"Title": "Professor",
		"Initials": "T L",
		"Department": "Sch of Computing and Engineering",
		"OrgID": "56"
	},
	{
		"person":358,
		"ID": "31980",
		"Surname": "Allerton",
		"Title": "Professor",
		"Initials": "D J",
		"Department": "Automatic Control and Systems Eng",
		"OrgID": "116"
	},
	{
		"person":359,
		"ID": "32038",
		"Surname": "Hailes",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":360,
		"ID": "32082",
		"Surname": "Pratt-Hartmann",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":361,
		"ID": "32110",
		"Surname": "Martin",
		"Title": "Professor",
		"Initials": "R W",
		"Department": "Physics",
		"OrgID": "48"
	},
	{
		"person":362,
		"ID": "32288",
		"Surname": "Clark",
		"Title": "Professor",
		"Initials": "J A",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":363,
		"ID": "32627",
		"Surname": "Kelsall",
		"Title": "Professor",
		"Initials": "R W",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":364,
		"ID": "33271",
		"Surname": "Brennan",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":365,
		"ID": "34198",
		"Surname": "Davies",
		"Title": "Professor",
		"Initials": "M E",
		"Department": "Sch of Engineering",
		"OrgID": "41"
	},
	{
		"person":366,
		"ID": "34203",
		"Surname": "Ong",
		"Title": "Professor",
		"Initials": "C H L",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":367,
		"ID": "34584",
		"Surname": "Warrington",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Engineering",
		"OrgID": "66"
	},
	{
		"person":368,
		"ID": "34855",
		"Surname": "Andrews",
		"Title": "Dr",
		"Initials": "S R",
		"Department": "Physics",
		"OrgID": "7"
	},
	{
		"person":369,
		"ID": "35966",
		"Surname": "McGuigan",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Welsh School of Pharmacy",
		"OrgID": "28"
	},
	{
		"person":370,
		"ID": "36060",
		"Surname": "Marshall",
		"Title": "Professor",
		"Initials": "A J",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10"
	},
	{
		"person":371,
		"ID": "36081",
		"Surname": "Alford",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Materials",
		"OrgID": "77"
	},
	{
		"person":372,
		"ID": "36281",
		"Surname": "Keane",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":373,
		"ID": "36580",
		"Surname": "Pym",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":374,
		"ID": "36938",
		"Surname": "Chambers",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Electronic, Electrical & Systems Enginee",
		"OrgID": "90"
	},
	{
		"person":375,
		"ID": "37134",
		"Surname": "Elmirghani",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":376,
		"ID": "37267",
		"Surname": "Yeates",
		"Title": "Professor",
		"Initials": "S G",
		"Department": "Chemistry",
		"OrgID": "93"
	},
	{
		"person":377,
		"ID": "37325",
		"Surname": "Canagarajah",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":378,
		"ID": "37788",
		"Surname": "Morrison",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Sch of Computing, Science & Engineering",
		"OrgID": "114"
	},
	{
		"person":379,
		"ID": "37926",
		"Surname": "Crochemore",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":380,
		"ID": "38924",
		"Surname": "Coles",
		"Title": "Professor",
		"Initials": "H J",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":381,
		"ID": "39488",
		"Surname": "Ireland",
		"Title": "Professor",
		"Initials": "A",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40"
	},
	{
		"person":382,
		"ID": "39898",
		"Surname": "Rodger",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Chemistry",
		"OrgID": "31"
	},
	{
		"person":383,
		"ID": "40034",
		"Surname": "Mowbray",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Physics and Astronomy",
		"OrgID": "116"
	},
	{
		"person":384,
		"ID": "40042",
		"Surname": "Kenyon",
		"Title": "Dr",
		"Initials": "A J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":385,
		"ID": "40151",
		"Surname": "Luk",
		"Title": "Professor",
		"Initials": "W",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":386,
		"ID": "40395",
		"Surname": "Hewak",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":387,
		"ID": "40551",
		"Surname": "Tasker",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Sch of Engineering",
		"OrgID": "28"
	},
	{
		"person":388,
		"ID": "41081",
		"Surname": "MacKay",
		"Title": "Professor",
		"Initials": "R S",
		"Department": "Mathematics",
		"OrgID": "31"
	},
	{
		"person":389,
		"ID": "41202",
		"Surname": "Richardson",
		"Title": "Professor",
		"Initials": "D J",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":390,
		"ID": "41641",
		"Surname": "Krauss",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Physics",
		"OrgID": "55"
	},
	{
		"person":391,
		"ID": "41844",
		"Surname": "Huang",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Electrical Engineering and Electronics",
		"OrgID": "68"
	},
	{
		"person":392,
		"ID": "41915",
		"Surname": "Emerson",
		"Title": "Professor",
		"Initials": "D R",
		"Department": "Scientific Computing Department",
		"OrgID": "36"
	},
	{
		"person":393,
		"ID": "42450",
		"Surname": "Finnis",
		"Title": "Professor",
		"Initials": "M W",
		"Department": "Materials",
		"OrgID": "77"
	},
	{
		"person":394,
		"ID": "42549",
		"Surname": "Wuerger",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Psychology",
		"OrgID": "68"
	},
	{
		"person":395,
		"ID": "43134",
		"Surname": "Wiggins",
		"Title": "Professor",
		"Initials": "G A",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":396,
		"ID": "43826",
		"Surname": "Samuel",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Physics and Astronomy",
		"OrgID": "119"
	},
	{
		"person":397,
		"ID": "43886",
		"Surname": "Giles",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Mathematical Institute",
		"OrgID": "106"
	},
	{
		"person":398,
		"ID": "44068",
		"Surname": "Longley",
		"Title": "Professor",
		"Initials": "P A",
		"Department": "Geography",
		"OrgID": "81"
	},
	{
		"person":399,
		"ID": "44683",
		"Surname": "Cohen",
		"Title": "Professor",
		"Initials": "L F",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":400,
		"ID": "45280",
		"Surname": "Knight",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Physics",
		"OrgID": "7"
	},
	{
		"person":401,
		"ID": "45306",
		"Surname": "Dawson",
		"Title": "Professor",
		"Initials": "M D",
		"Department": "Inst of Photonics",
		"OrgID": "48"
	},
	{
		"person":402,
		"ID": "45311",
		"Surname": "Wilkinson",
		"Title": "Dr",
		"Initials": "T D",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":403,
		"ID": "45348",
		"Surname": "Halsall",
		"Title": "Professor",
		"Initials": "M P",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":404,
		"ID": "45542",
		"Surname": "Hilton",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51"
	},
	{
		"person":405,
		"ID": "45577",
		"Surname": "Whitelock",
		"Title": "Dr",
		"Initials": "D",
		"Department": "Institute of Educational Technology",
		"OrgID": "95"
	},
	{
		"person":406,
		"ID": "45632",
		"Surname": "Simeonidou",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":407,
		"ID": "45650",
		"Surname": "Dholakia",
		"Title": "Professor",
		"Initials": "K",
		"Department": "Physics and Astronomy",
		"OrgID": "119"
	},
	{
		"person":408,
		"ID": "45744",
		"Surname": "Cacialli",
		"Title": "Professor",
		"Initials": "F",
		"Department": "Physics and Astronomy",
		"OrgID": "81"
	},
	{
		"person":409,
		"ID": "45941",
		"Surname": "Paul",
		"Title": "Professor",
		"Initials": "D J",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":410,
		"ID": "46045",
		"Surname": "Moore",
		"Title": "Dr",
		"Initials": "S W",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":411,
		"ID": "46128",
		"Surname": "Reid",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":412,
		"ID": "46162",
		"Surname": "Thomson",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":413,
		"ID": "46163",
		"Surname": "Fox",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":414,
		"ID": "46590",
		"Surname": "Jones",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":415,
		"ID": "47044",
		"Surname": "Long",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":416,
		"ID": "47424",
		"Surname": "Baumberg",
		"Title": "Professor",
		"Initials": "J J",
		"Department": "Physics",
		"OrgID": "24"
	},
	{
		"person":417,
		"ID": "47895",
		"Surname": "Harman",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":418,
		"ID": "48487",
		"Surname": "Audsley",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":419,
		"ID": "48924",
		"Surname": "Leadley",
		"Title": "Professor",
		"Initials": "D R",
		"Department": "Physics",
		"OrgID": "31"
	},
	{
		"person":420,
		"ID": "48970",
		"Surname": "Honary",
		"Title": "Professor",
		"Initials": "F",
		"Department": "Physics",
		"OrgID": "63"
	},
	{
		"person":421,
		"ID": "49195",
		"Surname": "Greenham",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Physics",
		"OrgID": "24"
	},
	{
		"person":422,
		"ID": "49445",
		"Surname": "Brewster",
		"Title": "Professor",
		"Initials": "S A",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":423,
		"ID": "49796",
		"Surname": "King",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":424,
		"ID": "49970",
		"Surname": "Goldberg",
		"Title": "Professor",
		"Initials": "L A",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":425,
		"ID": "49980",
		"Surname": "Finlayson",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Computing Sciences",
		"OrgID": "102"
	},
	{
		"person":426,
		"ID": "50283",
		"Surname": "Dawar",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":427,
		"ID": "50450",
		"Surname": "Al-Hashimi",
		"Title": "Professor",
		"Initials": "B",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":428,
		"ID": "50579",
		"Surname": "Hand",
		"Title": "Professor",
		"Initials": "D P",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":429,
		"ID": "51118",
		"Surname": "Hall",
		"Title": "Dr",
		"Initials": "P M",
		"Department": "Computer Science",
		"OrgID": "7"
	},
	{
		"person":430,
		"ID": "51232",
		"Surname": "O'Boyle",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":431,
		"ID": "51433",
		"Surname": "Braunstein",
		"Title": "Professor",
		"Initials": "S L",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":432,
		"ID": "51628",
		"Surname": "d'Inverno",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computing Department",
		"OrgID": "97"
	},
	{
		"person":433,
		"ID": "51931",
		"Surname": "Gilbert",
		"Title": "Professor",
		"Initials": "G N",
		"Department": "Sociology",
		"OrgID": "51"
	},
	{
		"person":434,
		"ID": "52001",
		"Surname": "Lidzey",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Physics and Astronomy",
		"OrgID": "116"
	},
	{
		"person":435,
		"ID": "53062",
		"Surname": "Gavaghan",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":436,
		"ID": "53237",
		"Surname": "Smith",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Physics and Astronomy",
		"OrgID": "119"
	},
	{
		"person":437,
		"ID": "53920",
		"Surname": "Cowling",
		"Title": "Professor",
		"Initials": "P I",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":438,
		"ID": "54046",
		"Surname": "Gutin",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Computer Science",
		"OrgID": "44"
	},
	{
		"person":439,
		"ID": "54148",
		"Surname": "Stevens",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":440,
		"ID": "54555",
		"Surname": "Ryan",
		"Title": "Professor",
		"Initials": "M",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":441,
		"ID": "54563",
		"Surname": "Scott",
		"Title": "Dr",
		"Initials": "J A",
		"Department": "Scientific Computing Department",
		"OrgID": "36"
	},
	{
		"person":442,
		"ID": "54639",
		"Surname": "John",
		"Title": "Professor",
		"Initials": "R I",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":443,
		"ID": "54662",
		"Surname": "Van Deemter",
		"Title": "Professor",
		"Initials": "K",
		"Department": "Computing Science",
		"OrgID": "0"
	},
	{
		"person":444,
		"ID": "54937",
		"Surname": "Lemon",
		"Title": "Professor",
		"Initials": "O",
		"Department": "S of Mathematical and Computer Sciences",
		"OrgID": "40"
	},
	{
		"person":445,
		"ID": "55263",
		"Surname": "Guo",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":446,
		"ID": "55316",
		"Surname": "Roy",
		"Title": "Professor",
		"Initials": "S",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":447,
		"ID": "55374",
		"Surname": "Batchelor",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Sch of Engineering & Digital Arts",
		"OrgID": "27"
	},
	{
		"person":448,
		"ID": "56407",
		"Surname": "Althoefer",
		"Title": "Professor",
		"Initials": "K",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":449,
		"ID": "56494",
		"Surname": "Boiten",
		"Title": "Dr",
		"Initials": "E A",
		"Department": "Sch of Computing",
		"OrgID": "27"
	},
	{
		"person":450,
		"ID": "56547",
		"Surname": "Kuball",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Physics",
		"OrgID": "22"
	},
	{
		"person":451,
		"ID": "56861",
		"Surname": "Clarkson",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":452,
		"ID": "56943",
		"Surname": "Ochieng",
		"Title": "Professor",
		"Initials": "W Y",
		"Department": "Civil & Environmental Engineering",
		"OrgID": "77"
	},
	{
		"person":453,
		"ID": "56944",
		"Surname": "Rorison",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":454,
		"ID": "57008",
		"Surname": "Kerber",
		"Title": "Dr",
		"Initials": "M",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":455,
		"ID": "57256",
		"Surname": "Bhatti",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "119"
	},
	{
		"person":456,
		"ID": "57264",
		"Surname": "Cooper",
		"Title": "Professor",
		"Initials": "C D",
		"Department": "Informatics",
		"OrgID": "78"
	},
	{
		"person":457,
		"ID": "58111",
		"Surname": "Schmidt",
		"Title": "Dr",
		"Initials": "R A",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":458,
		"ID": "58408",
		"Surname": "Dautenhahn",
		"Title": "Professor",
		"Initials": "K",
		"Department": "Science and Technology RI",
		"OrgID": "53"
	},
	{
		"person":459,
		"ID": "59559",
		"Surname": "Goldberg",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":460,
		"ID": "59852",
		"Surname": "Shepherd",
		"Title": "Professor",
		"Initials": "D P",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":461,
		"ID": "60141",
		"Surname": "Curzon",
		"Title": "Professor",
		"Initials": "P",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":462,
		"ID": "60733",
		"Surname": "Manandhar",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":463,
		"ID": "61326",
		"Surname": "Sewell",
		"Title": "Professor",
		"Initials": "P M",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":464,
		"ID": "61373",
		"Surname": "Probert",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Physics",
		"OrgID": "55"
	},
	{
		"person":465,
		"ID": "62131",
		"Surname": "Pavlou",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":466,
		"ID": "62710",
		"Surname": "Thomas",
		"Title": "Dr",
		"Initials": "N A",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":467,
		"ID": "65573",
		"Surname": "Hierons",
		"Title": "Professor",
		"Initials": "R M",
		"Department": "Information Systems Computing and Maths",
		"OrgID": "129"
	},
	{
		"person":468,
		"ID": "66486",
		"Surname": "Sezer",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Electronics Electrical Eng and Comp Sci",
		"OrgID": "10"
	},
	{
		"person":469,
		"ID": "67244",
		"Surname": "Sandham",
		"Title": "Professor",
		"Initials": "N D",
		"Department": "Faculty of Engineering & the Environment",
		"OrgID": "117"
	},
	{
		"person":470,
		"ID": "67555",
		"Surname": "Smowton",
		"Title": "Professor",
		"Initials": "P M",
		"Department": "School of Physics and Astronomy",
		"OrgID": "28"
	},
	{
		"person":471,
		"ID": "68356",
		"Surname": "Cross",
		"Title": "Dr",
		"Initials": "A . W .",
		"Department": "Physics",
		"OrgID": "48"
	},
	{
		"person":472,
		"ID": "68959",
		"Surname": "Yang",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":473,
		"ID": "69020",
		"Surname": "Kemp",
		"Title": "Dr",
		"Initials": "A H",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "64"
	},
	{
		"person":474,
		"ID": "69411",
		"Surname": "Ghani",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Computer and Information Sciences",
		"OrgID": "48"
	},
	{
		"person":475,
		"ID": "69511",
		"Surname": "Eder",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":476,
		"ID": "70023",
		"Surname": "Beanland",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Physics",
		"OrgID": "31"
	},
	{
		"person":477,
		"ID": "70737",
		"Surname": "Luckin",
		"Title": "Professor",
		"Initials": "R",
		"Department": "London Knowledge Lab",
		"OrgID": "79"
	},
	{
		"person":478,
		"ID": "71158",
		"Surname": "Cumming",
		"Title": "Professor",
		"Initials": "D R S",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":479,
		"ID": "71179",
		"Surname": "Horrocks",
		"Title": "Professor",
		"Initials": "I",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":480,
		"ID": "71452",
		"Surname": "Reed",
		"Title": "Professor",
		"Initials": "C",
		"Department": "School of Computing",
		"OrgID": "37"
	},
	{
		"person":481,
		"ID": "71480",
		"Surname": "Gales",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":482,
		"ID": "71546",
		"Surname": "Alechina",
		"Title": "Dr",
		"Initials": "N",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":483,
		"ID": "71851",
		"Surname": "Tanner",
		"Title": "Dr",
		"Initials": "G",
		"Department": "Sch of Mathematical Sciences",
		"OrgID": "104"
	},
	{
		"person":484,
		"ID": "72333",
		"Surname": "Cangelosi",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Computing & Mathematics",
		"OrgID": "109"
	},
	{
		"person":485,
		"ID": "72414",
		"Surname": "Flach",
		"Title": "Professor",
		"Initials": "P A",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":486,
		"ID": "72518",
		"Surname": "Fernandez",
		"Title": "Dr",
		"Initials": "F A",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":487,
		"ID": "72946",
		"Surname": "Torr",
		"Title": "Professor",
		"Initials": "P H",
		"Department": "Faculty of Tech, Design and Environment",
		"OrgID": "54"
	},
	{
		"person":488,
		"ID": "73003",
		"Surname": "Cannon",
		"Title": "Professor",
		"Initials": "P S",
		"Department": "Electronic, Electrical and Computer Eng",
		"OrgID": "14"
	},
	{
		"person":489,
		"ID": "73455",
		"Surname": "Honda",
		"Title": "Dr",
		"Initials": "K H",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":490,
		"ID": "73952",
		"Surname": "Sazio",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":491,
		"ID": "73981",
		"Surname": "Cronin",
		"Title": "Professor",
		"Initials": "L",
		"Department": "School of Chemistry",
		"OrgID": "49"
	},
	{
		"person":492,
		"ID": "74001",
		"Surname": "Huggard",
		"Title": "Dr",
		"Initials": "P",
		"Department": "RAL Space",
		"OrgID": "36"
	},
	{
		"person":493,
		"ID": "74076",
		"Surname": "Tian",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Electrical, Electronic & Computer Eng",
		"OrgID": "98"
	},
	{
		"person":494,
		"ID": "74080",
		"Surname": "Hirst",
		"Title": "Professor",
		"Initials": "J D",
		"Department": "Sch of Chemistry",
		"OrgID": "104"
	},
	{
		"person":495,
		"ID": "74172",
		"Surname": "Chalmers",
		"Title": "Dr",
		"Initials": "M",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":496,
		"ID": "74374",
		"Surname": "Glass",
		"Title": "Professor",
		"Initials": "C A",
		"Department": "Actuarial Science and Insurance",
		"OrgID": "83"
	},
	{
		"person":497,
		"ID": "74459",
		"Surname": "Gibbons",
		"Title": "Professor",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":498,
		"ID": "75997",
		"Surname": "Podoleanu",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Physical Sciences",
		"OrgID": "27"
	},
	{
		"person":499,
		"ID": "76174",
		"Surname": "Yao",
		"Title": "Professor",
		"Initials": "X",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":500,
		"ID": "76432",
		"Surname": "Skryabin",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Physics",
		"OrgID": "7"
	},
	{
		"person":501,
		"ID": "76609",
		"Surname": "Johnston",
		"Title": "Dr",
		"Initials": "M B",
		"Department": "Oxford Physics",
		"OrgID": "106"
	},
	{
		"person":502,
		"ID": "77266",
		"Surname": "Mitchell",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "7"
	},
	{
		"person":503,
		"ID": "77274",
		"Surname": "Romanovsky",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":504,
		"ID": "77443",
		"Surname": "Jackson",
		"Title": "Dr",
		"Initials": "P B",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":505,
		"ID": "77575",
		"Surname": "Chen",
		"Title": "Professor",
		"Initials": "W",
		"Department": "Aeronautical and Automotive Engineering",
		"OrgID": "90"
	},
	{
		"person":506,
		"ID": "77900",
		"Surname": "Rafailov",
		"Title": "Professor",
		"Initials": "E U",
		"Department": "Electronic Engineering and Physics",
		"OrgID": "37"
	},
	{
		"person":507,
		"ID": "78607",
		"Surname": "Toni",
		"Title": "Dr",
		"Initials": "F",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":508,
		"ID": "78847",
		"Surname": "Power",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Computing",
		"OrgID": "95"
	},
	{
		"person":509,
		"ID": "79530",
		"Surname": "Popescu",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Physics",
		"OrgID": "22"
	},
	{
		"person":510,
		"ID": "79623",
		"Surname": "Stevenson",
		"Title": "Dr",
		"Initials": "R M",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":511,
		"ID": "79845",
		"Surname": "Sheel",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Sch of Computing, Science & Engineering",
		"OrgID": "114"
	},
	{
		"person":512,
		"ID": "79853",
		"Surname": "Hussain",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Computing Science and Mathematics",
		"OrgID": "121"
	},
	{
		"person":513,
		"ID": "80116",
		"Surname": "Qiu",
		"Title": "Dr",
		"Initials": "G",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":514,
		"ID": "80552",
		"Surname": "Osborne",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":515,
		"ID": "80862",
		"Surname": "Hao",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":516,
		"ID": "81125",
		"Surname": "Weir",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Sch of Engineering and Informatics",
		"OrgID": "20"
	},
	{
		"person":517,
		"ID": "83531",
		"Surname": "Krokhin",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Engineering and Computing Sciences",
		"OrgID": "39"
	},
	{
		"person":518,
		"ID": "84023",
		"Surname": "Hart",
		"Title": "Professor",
		"Initials": "E",
		"Department": "Computing",
		"OrgID": "42"
	},
	{
		"person":519,
		"ID": "84313",
		"Surname": "Girolami",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Statistical Science",
		"OrgID": "81"
	},
	{
		"person":520,
		"ID": "84339",
		"Surname": "Adamatzky",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Faculty of Environment and Technology",
		"OrgID": "21"
	},
	{
		"person":521,
		"ID": "84749",
		"Surname": "Yoshida",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Dept of Computing",
		"OrgID": "77"
	},
	{
		"person":522,
		"ID": "84767",
		"Surname": "Cliff",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":523,
		"ID": "85034",
		"Surname": "Vuskovic",
		"Title": "Professor",
		"Initials": "K",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":524,
		"ID": "85889",
		"Surname": "Johnson",
		"Title": "Dr",
		"Initials": "C G",
		"Department": "Sch of Computing",
		"OrgID": "27"
	},
	{
		"person":525,
		"ID": "87124",
		"Surname": "Bogdanov",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":526,
		"ID": "87220",
		"Surname": "Mandic",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77"
	},
	{
		"person":527,
		"ID": "87358",
		"Surname": "Zakharyaschev",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science and Information Systems",
		"OrgID": "73"
	},
	{
		"person":528,
		"ID": "88227",
		"Surname": "Campbell",
		"Title": "Dr",
		"Initials": "I C G",
		"Department": "Engineering Mathematics",
		"OrgID": "22"
	},
	{
		"person":529,
		"ID": "88715",
		"Surname": "Neely",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Engineering",
		"OrgID": "24"
	},
	{
		"person":530,
		"ID": "89709",
		"Surname": "Ciravegna",
		"Title": "Professor",
		"Initials": "F",
		"Department": "Computer Science",
		"OrgID": "116"
	},
	{
		"person":531,
		"ID": "90168",
		"Surname": "Cheverst",
		"Title": "Dr",
		"Initials": "K",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":532,
		"ID": "90639",
		"Surname": "Dudek",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":533,
		"ID": "91905",
		"Surname": "Webb",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12"
	},
	{
		"person":534,
		"ID": "92211",
		"Surname": "Curry",
		"Title": "Dr",
		"Initials": "R J",
		"Department": "ATI Electronics",
		"OrgID": "51"
	},
	{
		"person":535,
		"ID": "92216",
		"Surname": "Brown",
		"Title": "Dr",
		"Initials": "C T A",
		"Department": "Physics and Astronomy",
		"OrgID": "119"
	},
	{
		"person":536,
		"ID": "93100",
		"Surname": "Whitaker",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Computer Science",
		"OrgID": "28"
	},
	{
		"person":537,
		"ID": "93129",
		"Surname": "Mascolo",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":538,
		"ID": "94527",
		"Surname": "Gough",
		"Title": "Professor",
		"Initials": "J E",
		"Department": "Inst of Mathematical and Physical Sci",
		"OrgID": "2"
	},
	{
		"person":539,
		"ID": "94747",
		"Surname": "Waterson",
		"Title": "Dr",
		"Initials": "B J",
		"Department": "Faculty of Engineering & the Environment",
		"OrgID": "117"
	},
	{
		"person":540,
		"ID": "94763",
		"Surname": "Riis",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":541,
		"ID": "94934",
		"Surname": "Bowden",
		"Title": "Professor",
		"Initials": "R",
		"Department": "Vision Speech and Signal Proc CVSSP",
		"OrgID": "51"
	},
	{
		"person":542,
		"ID": "96083",
		"Surname": "Manlove",
		"Title": "Dr",
		"Initials": "D F",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":543,
		"ID": "96326",
		"Surname": "Andrieu",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Mathematics",
		"OrgID": "22"
	},
	{
		"person":544,
		"ID": "96850",
		"Surname": "Kalna",
		"Title": "Dr",
		"Initials": "K",
		"Department": "School of Engineering",
		"OrgID": "125"
	},
	{
		"person":545,
		"ID": "97323",
		"Surname": "Walmsley",
		"Title": "Professor",
		"Initials": "I A",
		"Department": "Oxford Physics",
		"OrgID": "106"
	},
	{
		"person":546,
		"ID": "97324",
		"Surname": "Zheng",
		"Title": "Professor",
		"Initials": "F C",
		"Department": "Sch of Systems Engineering",
		"OrgID": "113"
	},
	{
		"person":547,
		"ID": "97751",
		"Surname": "Patane",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Sch of Physics & Astronomy",
		"OrgID": "104"
	},
	{
		"person":548,
		"ID": "98122",
		"Surname": "Stevens",
		"Title": "Professor",
		"Initials": "R D",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":549,
		"ID": "98328",
		"Surname": "Colton",
		"Title": "Professor",
		"Initials": "S",
		"Department": "Computing Department",
		"OrgID": "97"
	},
	{
		"person":550,
		"ID": "98457",
		"Surname": "Hong",
		"Title": "Dr",
		"Initials": "X",
		"Department": "Sch of Systems Engineering",
		"OrgID": "113"
	},
	{
		"person":551,
		"ID": "98852",
		"Surname": "McIntosh-Smith",
		"Title": "Mr",
		"Initials": "S",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":552,
		"ID": "98871",
		"Surname": "Pradhan",
		"Title": "Professor",
		"Initials": "D",
		"Department": "Computer Science",
		"OrgID": "22"
	},
	{
		"person":553,
		"ID": "98879",
		"Surname": "Levy",
		"Title": "Dr",
		"Initials": "P B",
		"Department": "School of Computer Science",
		"OrgID": "14"
	},
	{
		"person":554,
		"ID": "98932",
		"Surname": "Altenkirch",
		"Title": "Dr",
		"Initials": "T",
		"Department": "School of Computer Science",
		"OrgID": "104"
	},
	{
		"person":555,
		"ID": "99595",
		"Surname": "Shakhlevich",
		"Title": "Dr",
		"Initials": "N",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":556,
		"ID": "99939",
		"Surname": "de Groot",
		"Title": "Professor",
		"Initials": "C",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":557,
		"ID": "100470",
		"Surname": "Gelenbe",
		"Title": "Professor",
		"Initials": "S E",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77"
	},
	{
		"person":558,
		"ID": "100570",
		"Surname": "Aquino",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Division of Infrastructure and Geomatics",
		"OrgID": "104"
	},
	{
		"person":559,
		"ID": "100961",
		"Surname": "Constantinides",
		"Title": "Professor",
		"Initials": "G A",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "77"
	},
	{
		"person":560,
		"ID": "101189",
		"Surname": "Petillot",
		"Title": "Professor",
		"Initials": "Y",
		"Department": "Sch of Engineering and Physical Science",
		"OrgID": "40"
	},
	{
		"person":561,
		"ID": "102151",
		"Surname": "Lapata",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Sch of Informatics",
		"OrgID": "41"
	},
	{
		"person":562,
		"ID": "103370",
		"Surname": "Cavalcanti",
		"Title": "Dr",
		"Initials": "A L C",
		"Department": "Computer Science",
		"OrgID": "55"
	},
	{
		"person":563,
		"ID": "103570",
		"Surname": "Belz",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Sch of Computing, Engineering & Maths",
		"OrgID": "19"
	},
	{
		"person":564,
		"ID": "103599",
		"Surname": "Keylock",
		"Title": "Dr",
		"Initials": "C J",
		"Department": "Civil and Structural Engineering",
		"OrgID": "116"
	},
	{
		"person":565,
		"ID": "103654",
		"Surname": "Sweeney",
		"Title": "Professor",
		"Initials": "S J",
		"Department": "ATI Physics",
		"OrgID": "51"
	},
	{
		"person":566,
		"ID": "103767",
		"Surname": "Foody",
		"Title": "Professor",
		"Initials": "G",
		"Department": "Sch of Geography",
		"OrgID": "104"
	},
	{
		"person":567,
		"ID": "105791",
		"Surname": "Sorel",
		"Title": "Dr",
		"Initials": "M",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":568,
		"ID": "105814",
		"Surname": "Setzer",
		"Title": "Dr",
		"Initials": "A",
		"Department": "College of Science",
		"OrgID": "125"
	},
	{
		"person":569,
		"ID": "105958",
		"Surname": "Hauser",
		"Title": "Dr",
		"Initials": "R",
		"Department": "Mathematical Institute",
		"OrgID": "106"
	},
	{
		"person":570,
		"ID": "106108",
		"Surname": "Booth",
		"Title": "Dr",
		"Initials": "M J",
		"Department": "Engineering Science",
		"OrgID": "106"
	},
	{
		"person":571,
		"ID": "106606",
		"Surname": "Moore",
		"Title": "Dr",
		"Initials": "A W",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":572,
		"ID": "106698",
		"Surname": "Morris",
		"Title": "Dr",
		"Initials": "K A",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":573,
		"ID": "107145",
		"Surname": "Horak",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":574,
		"ID": "107181",
		"Surname": "Wilson",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Electronics and Computer Science",
		"OrgID": "117"
	},
	{
		"person":575,
		"ID": "107187",
		"Surname": "Wassell",
		"Title": "Dr",
		"Initials": "I J",
		"Department": "Computer Laboratory",
		"OrgID": "24"
	},
	{
		"person":576,
		"ID": "107748",
		"Surname": "Fiadeiro",
		"Title": "Professor",
		"Initials": "J L",
		"Department": "Computer Science",
		"OrgID": "43"
	},
	{
		"person":577,
		"ID": "107816",
		"Surname": "Rudolph",
		"Title": "Professor",
		"Initials": "T G",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":578,
		"ID": "107856",
		"Surname": "Miguel",
		"Title": "Dr",
		"Initials": "I J",
		"Department": "Computer Science",
		"OrgID": "119"
	},
	{
		"person":579,
		"ID": "108534",
		"Surname": "Cohen",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":580,
		"ID": "108792",
		"Surname": "Cryan",
		"Title": "Professor",
		"Initials": "M J",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "22"
	},
	{
		"person":581,
		"ID": "108991",
		"Surname": "coecke",
		"Title": "Professor",
		"Initials": "B",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":582,
		"ID": "109541",
		"Surname": "Wasige",
		"Title": "Dr",
		"Initials": "E",
		"Department": "School of Engineering",
		"OrgID": "49"
	},
	{
		"person":583,
		"ID": "109610",
		"Surname": "Jackson",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Information Science",
		"OrgID": "90"
	},
	{
		"person":584,
		"ID": "109735",
		"Surname": "Newman",
		"Title": "Professor",
		"Initials": "P M",
		"Department": "Engineering Science",
		"OrgID": "106"
	},
	{
		"person":585,
		"ID": "109935",
		"Surname": "Bryan-Kinns",
		"Title": "Dr",
		"Initials": "N J",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":586,
		"ID": "110005",
		"Surname": "Reed-Tsochas",
		"Title": "Dr",
		"Initials": "F P",
		"Department": "Said Business School",
		"OrgID": "106"
	},
	{
		"person":587,
		"ID": "110324",
		"Surname": "Mackenzie",
		"Title": "Dr",
		"Initials": "J I",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":588,
		"ID": "110348",
		"Surname": "Wilson",
		"Title": "Professor",
		"Initials": "A",
		"Department": "Comparative Biomedical Sciences CBS",
		"OrgID": "86"
	},
	{
		"person":589,
		"ID": "110493",
		"Surname": "Krasnogor",
		"Title": "Professor",
		"Initials": "N",
		"Department": "Computing Sciences",
		"OrgID": "98"
	},
	{
		"person":590,
		"ID": "111741",
		"Surname": "Wang",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":591,
		"ID": "111934",
		"Surname": "Purver",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Sch of Electronic Eng & Computer Science",
		"OrgID": "76"
	},
	{
		"person":592,
		"ID": "114122",
		"Surname": "Magee",
		"Title": "Dr",
		"Initials": "D",
		"Department": "Sch of Computing",
		"OrgID": "64"
	},
	{
		"person":593,
		"ID": "115433",
		"Surname": "Hess",
		"Title": "Professor",
		"Initials": "O",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":594,
		"ID": "115824",
		"Surname": "Migliorato",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Electrical and Electronic Engineering",
		"OrgID": "93"
	},
	{
		"person":595,
		"ID": "115843",
		"Surname": "Hastie",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Inst of Photonics",
		"OrgID": "48"
	},
	{
		"person":596,
		"ID": "116096",
		"Surname": "Lujan",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "93"
	},
	{
		"person":597,
		"ID": "116973",
		"Surname": "He",
		"Title": "Dr",
		"Initials": "J",
		"Department": "Computer Science",
		"OrgID": "2"
	},
	{
		"person":598,
		"ID": "117106",
		"Surname": "Penders",
		"Title": "Professor",
		"Initials": "J s j",
		"Department": "Faculty of Arts Computing Eng and Sci",
		"OrgID": "115"
	},
	{
		"person":599,
		"ID": "117266",
		"Surname": "Yellowlees",
		"Title": "Professor",
		"Initials": "L",
		"Department": "Sch of Chemistry",
		"OrgID": "41"
	},
	{
		"person":600,
		"ID": "117271",
		"Surname": "Vanderbauwhede",
		"Title": "Dr",
		"Initials": "W",
		"Department": "School of Computing Science",
		"OrgID": "49"
	},
	{
		"person":601,
		"ID": "118538",
		"Surname": "Derevyanko",
		"Title": "Dr",
		"Initials": "S",
		"Department": "Sch of Engineering and Applied Science",
		"OrgID": "12"
	},
	{
		"person":602,
		"ID": "119263",
		"Surname": "Marshall",
		"Title": "Dr",
		"Initials": "J A R",
		"Department": "Kroto Research Institute",
		"OrgID": "116"
	},
	{
		"person":603,
		"ID": "119760",
		"Surname": "Renaud",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":604,
		"ID": "119985",
		"Surname": "Ziebart",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Civil Environmental and Geomatic Eng",
		"OrgID": "81"
	},
	{
		"person":605,
		"ID": "120114",
		"Surname": "Pontil",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Computer Science",
		"OrgID": "81"
	},
	{
		"person":606,
		"ID": "120483",
		"Surname": "Tan",
		"Title": "Dr",
		"Initials": "C",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "116"
	},
	{
		"person":607,
		"ID": "121458",
		"Surname": "Brosnan",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Psychology",
		"OrgID": "7"
	},
	{
		"person":608,
		"ID": "122242",
		"Surname": "Flechais",
		"Title": "Dr",
		"Initials": "I",
		"Department": "Computer Science",
		"OrgID": "106"
	},
	{
		"person":609,
		"ID": "122907",
		"Surname": "Cvetkovic",
		"Title": "Professor",
		"Initials": "Z",
		"Department": "Institute of Telecommunications",
		"OrgID": "78"
	},
	{
		"person":610,
		"ID": "124327",
		"Surname": "Zhu",
		"Title": "Professor",
		"Initials": "M",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46"
	},
	{
		"person":611,
		"ID": "124474",
		"Surname": "Keedwell",
		"Title": "Dr",
		"Initials": "E",
		"Department": "Engineering Computer Science and Maths",
		"OrgID": "46"
	},
	{
		"person":612,
		"ID": "125902",
		"Surname": "Zvelindovsky",
		"Title": "Professor",
		"Initials": "A V",
		"Department": "Sch of Comput Engin and Physical Sci",
		"OrgID": "112"
	},
	{
		"person":613,
		"ID": "126042",
		"Surname": "Guy",
		"Title": "Dr",
		"Initials": "O J",
		"Department": "School of Engineering",
		"OrgID": "125"
	},
	{
		"person":614,
		"ID": "126618",
		"Surname": "Savory",
		"Title": "Dr",
		"Initials": "S J",
		"Department": "Electronic and Electrical Engineering",
		"OrgID": "81"
	},
	{
		"person":615,
		"ID": "126765",
		"Surname": "Oulton",
		"Title": "Dr",
		"Initials": "R F M",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":616,
		"ID": "126832",
		"Surname": "Anthopoulos",
		"Title": "Professor",
		"Initials": "T",
		"Department": "Dept of Physics",
		"OrgID": "77"
	},
	{
		"person":617,
		"ID": "127032",
		"Surname": "Murawski",
		"Title": "Dr",
		"Initials": "A",
		"Department": "Computer Science",
		"OrgID": "31"
	},
	{
		"person":618,
		"ID": "128182",
		"Surname": "Mosses",
		"Title": "Professor",
		"Initials": "P",
		"Department": "College of Science",
		"OrgID": "125"
	},
	{
		"person":619,
		"ID": "129562",
		"Surname": "Hazas",
		"Title": "Dr",
		"Initials": "M",
		"Department": "Computing & Communications",
		"OrgID": "63"
	},
	{
		"person":620,
		"ID": "132133",
		"Surname": "Xiao",
		"Title": "Dr",
		"Initials": "P",
		"Department": "Communications Systems Res CCSR",
		"OrgID": "51"
	},
	{
		"person":621,
		"ID": "132139",
		"Surname": "Oliver",
		"Title": "Dr",
		"Initials": "R A",
		"Department": "Materials Science & Metallurgy",
		"OrgID": "24"
	},
	{
		"person":622,
		"ID": "132142",
		"Surname": "Price",
		"Title": "Dr",
		"Initials": "J H V",
		"Department": "Optoelectronics Research Centre",
		"OrgID": "117"
	},
	{
		"person":623,
		"ID": "132444",
		"Surname": "Buchanan",
		"Title": "Dr",
		"Initials": "G R",
		"Department": "Centre for HCI Design",
		"OrgID": "87"
	}
	],
	"organisations":[
	{
		"organisation":0,
		"OrgID": "0",
		"Name": "University of Aberdeen",
		"City": "Aberdeen",
		"Region": "Aberdeen",
		"Country": "Scotland",
		"Postcode": "AB24 3FX",
		"Latitude": "57.16476",
		"Longitude": "-2.1015257"
	},
	{
		"organisation":1,
		"OrgID": "1",
		"Name": "The Robert Gordon University",
		"City": "Aberdeen",
		"Region": "Aberdeen",
		"Country": "Scotland",
		"Postcode": "AB10 7QB",
		"Latitude": "57.1189259",
		"Longitude": "-2.1373874"
	},
	{
		"organisation":2,
		"OrgID": "2",
		"Name": "Aberystwyth University",
		"City": "Aberystwyth",
		"Region": "Dyfed",
		"Country": "Wales",
		"Postcode": "SY23 3AL",
		"Latitude": "52.4155239",
		"Longitude": "-4.0630663"
	},
	{
		"organisation":3,
		"OrgID": "3",
		"Name": "EURATOM/CCFE",
		"City": "Abingdon",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX14 3DB",
		"Latitude": "51.6577685",
		"Longitude": "-1.2286693"
	},
	{
		"organisation":4,
		"OrgID": "4",
		"Name": "University of Auckland",
		"City": "Auckland",
		"Region": "Auckland",
		"Country": "New Zealand",
		"Postcode": "1010",
		"Latitude": "-36.8523378",
		"Longitude": "174.7691073"
	},
	{
		"organisation":5,
		"OrgID": "5",
		"Name": "SRUC",
		"City": "Ayr",
		"Region": "Ayrshire and Arran",
		"Country": "Scotland",
		"Postcode": "KA8 0SX",
		"Latitude": "55.458564",
		"Longitude": "-4.629179"
	},
	{
		"organisation":6,
		"OrgID": "6",
		"Name": "Bangor University",
		"City": "Bangor",
		"Region": "Gwynedd",
		"Country": "Wales",
		"Postcode": "LL57 2DG",
		"Latitude": "53.2295205",
		"Longitude": "-4.1299874"
	},
	{
		"organisation":7,
		"OrgID": "7",
		"Name": "University of Bath",
		"City": "Bath",
		"Region": "Somerset",
		"Country": "England",
		"Postcode": "BA2 7AY",
		"Latitude": "51.3777431",
		"Longitude": "-2.3263779"
	},
	{
		"organisation":8,
		"OrgID": "8",
		"Name": "Cranfield University",
		"City": "Bedford",
		"Region": "Bedfordshire",
		"Country": "England",
		"Postcode": "MK43 0AL",
		"Latitude": "52.074389",
		"Longitude": "-0.629225"
	},
	{
		"organisation":9,
		"OrgID": "9",
		"Name": "University of Ulster",
		"City": "Belfast",
		"Region": "Antrim",
		"Country": "Northern Ireland",
		"Postcode": "BT15 1ED",
		"Latitude": "54.6040056",
		"Longitude": "-5.9294107"
	},
	{
		"organisation":10,
		"OrgID": "10",
		"Name": "Queen's University of Belfast",
		"City": "Belfast",
		"Region": "Antrim",
		"Country": "Northern Ireland",
		"Postcode": "BT7 1NN",
		"Latitude": "54.584038",
		"Longitude": "-5.933196"
	},
	{
		"organisation":11,
		"OrgID": "11",
		"Name": "Birmingham City University",
		"City": "Birmingham",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "B42 2SU",
		"Latitude": "52.5170469",
		"Longitude": "-1.8973091"
	},
	{
		"organisation":12,
		"OrgID": "12",
		"Name": "Aston University",
		"City": "Birmingham",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "B4 7ET",
		"Latitude": "52.4870177",
		"Longitude": "-1.8888033"
	},
	{
		"organisation":13,
		"OrgID": "13",
		"Name": "Adelan Limited",
		"City": "Birmingham",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "B17 9HD",
		"Latitude": "52.4632561",
		"Longitude": "-1.9467781"
	},
	{
		"organisation":14,
		"OrgID": "14",
		"Name": "University of Birmingham",
		"City": "Birmingham",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "B15 2TT",
		"Latitude": "52.4508168",
		"Longitude": "-1.9305135"
	},
	{
		"organisation":15,
		"OrgID": "15",
		"Name": "London Sch of Hygiene and Trop Medicine",
		"City": "Bloomsbury",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1E 7HT",
		"Latitude": "51.520925",
		"Longitude": "-0.1306868"
	},
	{
		"organisation":16,
		"OrgID": "16",
		"Name": "University of Bolton",
		"City": "Bolton",
		"Region": "Greater Manchester",
		"Country": "England",
		"Postcode": "BL3 5AB",
		"Latitude": "53.572854",
		"Longitude": "-2.437891"
	},
	{
		"organisation":17,
		"OrgID": "17",
		"Name": "University of Bradford",
		"City": "Bradford",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "BD7 1DP",
		"Latitude": "53.7910694",
		"Longitude": "-1.7670961"
	},
	{
		"organisation":18,
		"OrgID": "18",
		"Name": "Institute of Development Studies",
		"City": "Brighton",
		"Region": "East Sussex",
		"Country": "England",
		"Postcode": "BN1 9RE",
		"Latitude": "50.865958",
		"Longitude": "-0.090113"
	},
	{
		"organisation":19,
		"OrgID": "19",
		"Name": "University of Brighton",
		"City": "Brighton",
		"Region": "East Sussex",
		"Country": "England",
		"Postcode": "BN2 4AT",
		"Latitude": "50.8590874",
		"Longitude": "-0.086689"
	},
	{
		"organisation":20,
		"OrgID": "20",
		"Name": "University of Sussex",
		"City": "Brighton",
		"Region": "East Sussex",
		"Country": "England",
		"Postcode": "BN1 9RH",
		"Latitude": "50.8670895",
		"Longitude": "-0.087914"
	},
	{
		"organisation":21,
		"OrgID": "21",
		"Name": "University of the West of England",
		"City": "Bristol",
		"Region": "Bristol",
		"Country": "England",
		"Postcode": "BS16 1QY",
		"Latitude": "51.4842362",
		"Longitude": "-2.5422165"
	},
	{
		"organisation":22,
		"OrgID": "22",
		"Name": "University of Bristol",
		"City": "Bristol",
		"Region": "Bristol",
		"Country": "England",
		"Postcode": "BS8 1TH",
		"Latitude": "51.4584172",
		"Longitude": "-2.6029792"
	},
	{
		"organisation":23,
		"OrgID": "23",
		"Name": "Multimedia University",
		"City": "Bukit Beruang",
		"Region": "Malaysia",
		"Country": "Malaysia",
		"Postcode": "75450",
		"Latitude": "2.249312",
		"Longitude": "102.2760061"
	},
	{
		"organisation":24,
		"OrgID": "24",
		"Name": "University of Cambridge",
		"City": "Cambridge",
		"Region": "Cambridgeshire",
		"Country": "England",
		"Postcode": "CB2 1TN",
		"Latitude": "52.2042666",
		"Longitude": "0.1149085"
	},
	{
		"organisation":25,
		"OrgID": "25",
		"Name": "MRC Centre Cambridge",
		"City": "Cambridge",
		"Region": "Cambridgeshire",
		"Country": "England",
		"Postcode": "CB2 OQH",
		"Latitude": "52.1756204",
		"Longitude": "0.1333257"
	},
	{
		"organisation":26,
		"OrgID": "26",
		"Name": "National Inst of Agricultural Botany",
		"City": "Cambridge",
		"Region": "Cambridgeshire",
		"Country": "England",
		"Postcode": "CB3 0LE",
		"Latitude": "52.2218966",
		"Longitude": "0.0963045"
	},
	{
		"organisation":27,
		"OrgID": "27",
		"Name": "University of Kent",
		"City": "Canterbury",
		"Region": "Kent",
		"Country": "England",
		"Postcode": "CT2 7NZ",
		"Latitude": "51.296336",
		"Longitude": "1.0639901"
	},
	{
		"organisation":28,
		"OrgID": "28",
		"Name": "Cardiff University",
		"City": "Cardiff",
		"Region": "Cardiff",
		"Country": "Wales",
		"Postcode": "CF10 3XQ",
		"Latitude": "51.4866271",
		"Longitude": "-3.1788641"
	},
	{
		"organisation":29,
		"OrgID": "29",
		"Name": "Cardiff Metropolitan University",
		"City": "Cardiff",
		"Region": "Cardiff",
		"Country": "Wales",
		"Postcode": "CF5 2YB",
		"Latitude": "51.4827084",
		"Longitude": "-3.1658813"
	},
	{
		"organisation":30,
		"OrgID": "30",
		"Name": "University of Essex",
		"City": "Colchester",
		"Region": "Essex",
		"Country": "England",
		"Postcode": "CO4 3SQ",
		"Latitude": "51.8777259",
		"Longitude": "0.9472069"
	},
	{
		"organisation":31,
		"OrgID": "31",
		"Name": "University of Warwick",
		"City": "Coventry",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "CV4 7AL",
		"Latitude": "52.379411",
		"Longitude": "-1.563191"
	},
	{
		"organisation":32,
		"OrgID": "32",
		"Name": "Coventry University",
		"City": "Coventry",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "CV1 5FB",
		"Latitude": "52.4071747",
		"Longitude": "-1.5037461"
	},
	{
		"organisation":33,
		"OrgID": "33",
		"Name": "University of Western Australia",
		"City": "Crawley",
		"Region": "Western Austalia",
		"Country": "Australia",
		"Postcode": "WA 6009",
		"Latitude": "-31.981179",
		"Longitude": "115.8199096"
	},
	{
		"organisation":34,
		"OrgID": "34",
		"Name": "University of Derby",
		"City": "Derby",
		"Region": "Derbyshire",
		"Country": "England",
		"Postcode": "DE22 1GB",
		"Latitude": "52.9380607",
		"Longitude": "-1.4964366"
	},
	{
		"organisation":35,
		"OrgID": "35",
		"Name": "Diamond Light Source",
		"City": "Didcot",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX11 0QX",
		"Latitude": "51.5742765",
		"Longitude": "-1.3099598"
	},
	{
		"organisation":36,
		"OrgID": "36",
		"Name": "STFC - Laboratories",
		"City": "Didcot",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX11 0QX",
		"Latitude": "51.5725756",
		"Longitude": "-1.3159229"
	},
	{
		"organisation":37,
		"OrgID": "37",
		"Name": "University of Dundee",
		"City": "Dundee",
		"Region": "Dundee",
		"Country": "Scotland",
		"Postcode": "DD1 4HN",
		"Latitude": "56.4582447",
		"Longitude": "-2.9821428"
	},
	{
		"organisation":38,
		"OrgID": "38",
		"Name": "University of Abertay Dundee",
		"City": "Dundee",
		"Region": "Dundee",
		"Country": "Scotland",
		"Postcode": "DD1 1HG",
		"Latitude": "56.4633067",
		"Longitude": "-2.9739172"
	},
	{
		"organisation":39,
		"OrgID": "39",
		"Name": "Durham University",
		"City": "Durham",
		"Region": "Durham",
		"Country": "England",
		"Postcode": "DH1 3LE",
		"Latitude": "54.768139",
		"Longitude": "-1.5718583"
	},
	{
		"organisation":40,
		"OrgID": "40",
		"Name": "Heriot-Watt University",
		"City": "Edinburgh",
		"Region": "Edinburgh",
		"Country": "Scotland",
		"Postcode": "EH14 4AS",
		"Latitude": "55.909403",
		"Longitude": "-3.3206991"
	},
	{
		"organisation":41,
		"OrgID": "41",
		"Name": "University of Edinburgh",
		"City": "Edinburgh",
		"Region": "Edinburgh",
		"Country": "Scotland",
		"Postcode": "EH8 9YL",
		"Latitude": "55.9445158",
		"Longitude": "-3.1892413"
	},
	{
		"organisation":42,
		"OrgID": "42",
		"Name": "Edinburgh Napier University",
		"City": "Edinburgh",
		"Region": "Edinburgh",
		"Country": "Scotland",
		"Postcode": "EH10 5LG",
		"Latitude": "55.922944",
		"Longitude": "-3.2280758"
	},
	{
		"organisation":43,
		"OrgID": "43",
		"Name": "Royal Holloway, Univ of  London",
		"City": "Egham",
		"Region": "Surrey",
		"Country": "England",
		"Postcode": "TW20 0EX",
		"Latitude": "51.425673",
		"Longitude": "-0.5630625"
	},
	{
		"organisation":44,
		"OrgID": "44",
		"Name": "Royal Holloway, Univ of London",
		"City": "Egham",
		"Region": "Surrey",
		"Country": "England",
		"Postcode": "TW20 0EX",
		"Latitude": "51.425673",
		"Longitude": "-0.5630625"
	},
	{
		"organisation":45,
		"OrgID": "45",
		"Name": "University of Duisburg-Essen",
		"City": "Essen",
		"Region": "Essen",
		"Country": "Germany",
		"Postcode": "45141",
		"Latitude": "51.4634841",
		"Longitude": "7.0037632"
	},
	{
		"organisation":46,
		"OrgID": "46",
		"Name": "University of Exeter",
		"City": "Exeter",
		"Region": "Devon",
		"Country": "England",
		"Postcode": "EX4 4PY",
		"Latitude": "50.7352616",
		"Longitude": "-3.5338323"
	},
	{
		"organisation":47,
		"OrgID": "47",
		"Name": "Justus-Liebig University Giessen",
		"City": "Gieen",
		"Region": "Gieen",
		"Country": "Germany",
		"Postcode": "35390",
		"Latitude": "50.57368",
		"Longitude": "8.700465"
	},
	{
		"organisation":48,
		"OrgID": "48",
		"Name": "University of Strathclyde",
		"City": "Glasgow",
		"Region": "Glasgow",
		"Country": "Scotland",
		"Postcode": "G1 1XQ",
		"Latitude": "55.8624195",
		"Longitude": "-4.2425876"
	},
	{
		"organisation":49,
		"OrgID": "49",
		"Name": "University of Glasgow",
		"City": "Glasgow",
		"Region": "Glasgow",
		"Country": "Scotland",
		"Postcode": "G12 8QQ",
		"Latitude": "55.8721211",
		"Longitude": "-4.2882005"
	},
	{
		"organisation":50,
		"OrgID": "50",
		"Name": "Glasgow Caledonian University",
		"City": "Glasgow",
		"Region": "Glasgow",
		"Country": "Scotland",
		"Postcode": "G4 0BA",
		"Latitude": "55.8660103",
		"Longitude": "-4.2514152"
	},
	{
		"organisation":51,
		"OrgID": "51",
		"Name": "University of Surrey",
		"City": "Guildford",
		"Region": "Surrey",
		"Country": "England",
		"Postcode": "GU2 7XH",
		"Latitude": "51.242722",
		"Longitude": "-0.5895144"
	},
	{
		"organisation":52,
		"OrgID": "52",
		"Name": "Rothamsted Research",
		"City": "Harpenden",
		"Region": "Hertfordshire",
		"Country": "England",
		"Postcode": "AL5 2JQ",
		"Latitude": "51.8092738",
		"Longitude": "-0.3546325"
	},
	{
		"organisation":53,
		"OrgID": "53",
		"Name": "University of Hertfordshire",
		"City": "Hatfield",
		"Region": "Hertfordshire",
		"Country": "England",
		"Postcode": "AL10 9AB",
		"Latitude": "51.7516063",
		"Longitude": "-0.239013"
	},
	{
		"organisation":54,
		"OrgID": "54",
		"Name": "Oxford Brookes University",
		"City": "Headington",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX3 0BP",
		"Latitude": "51.7546132",
		"Longitude": "-1.2228302"
	},
	{
		"organisation":55,
		"OrgID": "55",
		"Name": "University of York",
		"City": "Heslington",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "YO10 5DD",
		"Latitude": "53.9496217",
		"Longitude": "-1.0453804"
	},
	{
		"organisation":56,
		"OrgID": "56",
		"Name": "University of Huddersfield",
		"City": "Huddersfield",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "HD1 3DH",
		"Latitude": "53.6433316",
		"Longitude": "-1.7790149"
	},
	{
		"organisation":57,
		"OrgID": "57",
		"Name": "University of Hull",
		"City": "Hull",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "HU6 7RX",
		"Latitude": "53.7714552",
		"Longitude": "-0.3673875"
	},
	{
		"organisation":58,
		"OrgID": "58",
		"Name": "University of the Highlands and Islands",
		"City": "Inverness",
		"Region": "Highland",
		"Country": "Scotland",
		"Postcode": "IV3 5SQ",
		"Latitude": "57.470943",
		"Longitude": "-4.230444"
	},
	{
		"organisation":59,
		"OrgID": "59",
		"Name": "Keele University",
		"City": "Keele",
		"Region": "West Midlands",
		"Country": "England",
		"Postcode": "ST5 5BG",
		"Latitude": "53.0039516",
		"Longitude": "-2.2745983"
	},
	{
		"organisation":60,
		"OrgID": "60",
		"Name": "NERC British Geological Survey",
		"City": "Keyworth",
		"Region": "Nottingham",
		"Country": "England",
		"Postcode": "NG12 5GG",
		"Latitude": "52.8789969",
		"Longitude": "-1.0780878"
	},
	{
		"organisation":61,
		"OrgID": "61",
		"Name": "Kingston University",
		"City": "Kingston upon Thames",
		"Region": "Surrey",
		"Country": "England",
		"Postcode": "KT1 2EE",
		"Latitude": "51.4038595",
		"Longitude": "-0.3038292"
	},
	{
		"organisation":62,
		"OrgID": "62",
		"Name": "Kobe University",
		"City": "Kobe",
		"Region": "Hyogo",
		"Country": "Japan",
		"Postcode": "657-0013",
		"Latitude": "34.7262714",
		"Longitude": "135.2345373"
	},
	{
		"organisation":63,
		"OrgID": "63",
		"Name": "Lancaster University",
		"City": "Lancaster",
		"Region": "Lancashire",
		"Country": "England",
		"Postcode": "LA1 4YW",
		"Latitude": "54.0103942",
		"Longitude": "-2.7877294"
	},
	{
		"organisation":64,
		"OrgID": "64",
		"Name": "University of Leeds",
		"City": "Leeds",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "LS2 9JT",
		"Latitude": "53.8066815",
		"Longitude": "-1.5550328"
	},
	{
		"organisation":65,
		"OrgID": "65",
		"Name": "De Montfort University",
		"City": "Leicester",
		"Region": "Leicestershire",
		"Country": "England",
		"Postcode": "LE1 9BH",
		"Latitude": "52.6294673",
		"Longitude": "-1.1380354"
	},
	{
		"organisation":66,
		"OrgID": "66",
		"Name": "University of Leicester",
		"City": "Leicester",
		"Region": "Leicestershire",
		"Country": "England",
		"Postcode": "LE1 7RH",
		"Latitude": "52.6277507",
		"Longitude": "-1.1213276"
	},
	{
		"organisation":67,
		"OrgID": "67",
		"Name": "University Of Lincoln",
		"City": "Lincoln",
		"Region": "Lincolnshire",
		"Country": "England",
		"Postcode": "LN6 7TS",
		"Latitude": "53.268697",
		"Longitude": "-0.527901"
	},
	{
		"organisation":68,
		"OrgID": "68",
		"Name": "University of Liverpool",
		"City": "Liverpool",
		"Region": "Merseyside",
		"Country": "England",
		"Postcode": "L69 3BX",
		"Latitude": "53.405936",
		"Longitude": "-2.9655722"
	},
	{
		"organisation":69,
		"OrgID": "69",
		"Name": "Liverpool John Moores University",
		"City": "Liverpool",
		"Region": "Merseyside",
		"Country": "England",
		"Postcode": "L3 5UA",
		"Latitude": "53.4033933",
		"Longitude": "-2.9728283"
	},
	{
		"organisation":70,
		"OrgID": "70",
		"Name": "University of East London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "E16 2RD",
		"Latitude": "51.5067736",
		"Longitude": "0.068631"
	},
	{
		"organisation":71,
		"OrgID": "71",
		"Name": "University of Westminster",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "W1W 6UW",
		"Latitude": "51.5207093",
		"Longitude": "-0.1399728"
	},
	{
		"organisation":72,
		"OrgID": "72",
		"Name": "Royal College of Art",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SW7 2EU",
		"Latitude": "51.5011441",
		"Longitude": "-0.1792847"
	},
	{
		"organisation":73,
		"OrgID": "73",
		"Name": "Birkbeck College",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1E 7HX",
		"Latitude": "51.5219508",
		"Longitude": "-0.1302037"
	},
	{
		"organisation":74,
		"OrgID": "74",
		"Name": "London School of Economics & Pol Sci",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC2A 2AE",
		"Latitude": "51.5139664",
		"Longitude": "-0.1167323"
	},
	{
		"organisation":75,
		"OrgID": "75",
		"Name": "University of Greenwich",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SE10 9LS",
		"Latitude": "51.4833162",
		"Longitude": "-0.0039603"
	},
	{
		"organisation":76,
		"OrgID": "76",
		"Name": "Queen Mary, University of London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "E1 4NS",
		"Latitude": "51.5240671",
		"Longitude": "-0.0403745"
	},
	{
		"organisation":77,
		"OrgID": "77",
		"Name": "Imperial College London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SW7 2AZ",
		"Latitude": "51.4987835",
		"Longitude": "-0.1748876"
	},
	{
		"organisation":78,
		"OrgID": "78",
		"Name": "King's College London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC2R 2LS",
		"Latitude": "51.5114864",
		"Longitude": "-0.115997"
	},
	{
		"organisation":79,
		"OrgID": "79",
		"Name": "Institute of Education",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1H 0AL",
		"Latitude": "51.5229872",
		"Longitude": "-0.1279678"
	},
	{
		"organisation":80,
		"OrgID": "80",
		"Name": "School of Pharmacy",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1N 1AX",
		"Latitude": "51.5251331",
		"Longitude": "-0.1224535"
	},
	{
		"organisation":81,
		"OrgID": "81",
		"Name": "University College London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1E 6BT",
		"Latitude": "51.5247725",
		"Longitude": "-0.1334268"
	},
	{
		"organisation":82,
		"OrgID": "82",
		"Name": "London South Bank University",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SE1 0AA",
		"Latitude": "51.498831",
		"Longitude": "-0.101827"
	},
	{
		"organisation":83,
		"OrgID": "83",
		"Name": "City University",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "EC1V 0HB",
		"Latitude": "51.5277383",
		"Longitude": "-0.1023356"
	},
	{
		"organisation":84,
		"OrgID": "84",
		"Name": "University of the Arts London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "WC1V 7EY",
		"Latitude": "51.5234286",
		"Longitude": "-0.0960729"
	},
	{
		"organisation":85,
		"OrgID": "85",
		"Name": "Middlesex University",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "NW4 4BT",
		"Latitude": "51.5897302",
		"Longitude": "-0.2287137"
	},
	{
		"organisation":86,
		"OrgID": "86",
		"Name": "Royal Veterinary College",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "NW1 0TU",
		"Latitude": "51.536454",
		"Longitude": "-0.1335335"
	},
	{
		"organisation":87,
		"OrgID": "87",
		"Name": "City University London",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "EC1V 0HB",
		"Latitude": "51.5277383",
		"Longitude": "-0.1023356"
	},
	{
		"organisation":88,
		"OrgID": "88",
		"Name": "Zoological Soc London Inst of Zoology",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "NW1 4RY",
		"Latitude": "51.5357184",
		"Longitude": "-0.1557324"
	},
	{
		"organisation":89,
		"OrgID": "89",
		"Name": "Institute of Cancer Research",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SM2 5NG",
		"Latitude": "51.3446698",
		"Longitude": "-0.1890745"
	},
	{
		"organisation":90,
		"OrgID": "90",
		"Name": "Loughborough University",
		"City": "Loughborough",
		"Region": "Leicestershire",
		"Country": "England",
		"Postcode": "LE11 3TU",
		"Latitude": "52.7641408",
		"Longitude": "-1.2333126"
	},
	{
		"organisation":91,
		"OrgID": "91",
		"Name": "University of Bedfordshire",
		"City": "Luton",
		"Region": "Bedfordshire",
		"Country": "England",
		"Postcode": "LU1 3JU",
		"Latitude": "51.8783065",
		"Longitude": "-0.4113524"
	},
	{
		"organisation":92,
		"OrgID": "92",
		"Name": "Manchester Metropolitan University",
		"City": "Manchester",
		"Region": "Greater Manchester",
		"Country": "England",
		"Postcode": "M13 9PL",
		"Latitude": "53.4668498",
		"Longitude": "-2.2338837"
	},
	{
		"organisation":93,
		"OrgID": "93",
		"Name": "The University of Manchester",
		"City": "Manchester",
		"Region": "Greater Manchester",
		"Country": "England",
		"Postcode": "M13 9PL",
		"Latitude": "53.4668498",
		"Longitude": "-2.2338837"
	},
	{
		"organisation":94,
		"OrgID": "94",
		"Name": "University of Teesside",
		"City": "Middlesbrough",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "TS1 3BA",
		"Latitude": "54.5720764",
		"Longitude": "-1.2346941"
	},
	{
		"organisation":95,
		"OrgID": "95",
		"Name": "Open University",
		"City": "Milton Keynes",
		"Region": "Buckinghamshire",
		"Country": "England",
		"Postcode": "MK7 6AA",
		"Latitude": "52.0248254",
		"Longitude": "-0.7077008"
	},
	{
		"organisation":96,
		"OrgID": "96",
		"Name": "Queen Margaret University Edinburgh",
		"City": "Musselburgh",
		"Region": "Lothians",
		"Country": "Scotland",
		"Postcode": "EH21 6UU",
		"Latitude": "55.9224977",
		"Longitude": "-3.06635"
	},
	{
		"organisation":97,
		"OrgID": "97",
		"Name": "Goldsmiths College",
		"City": "London",
		"Region": "London",
		"Country": "England",
		"Postcode": "SE14 6NW",
		"Latitude": "51.4749823",
		"Longitude": "-0.0371372"
	},
	{
		"organisation":98,
		"OrgID": "98",
		"Name": "Newcastle University",
		"City": "Newcastle upon Tyne",
		"Region": "Tyne and Wear",
		"Country": "England",
		"Postcode": "NE1 7RU",
		"Latitude": "54.9791871",
		"Longitude": "-1.6146608"
	},
	{
		"organisation":99,
		"OrgID": "99",
		"Name": "Northumbria University",
		"City": "Newcastle upon Tyne",
		"Region": "Tyne and Wear",
		"Country": "England",
		"Postcode": "NE1 8ST",
		"Latitude": "54.9774905",
		"Longitude": "-1.6066714"
	},
	{
		"organisation":100,
		"OrgID": "100",
		"Name": "Transport Research Laboratory Ltd",
		"City": "Norwich",
		"Region": "Norfolk",
		"Country": "England",
		"Postcode": "NR1 2RH",
		"Latitude": "52.618954",
		"Longitude": "1.2944096"
	},
	{
		"organisation":101,
		"OrgID": "101",
		"Name": "John Innes Centre",
		"City": "Norwich",
		"Region": "Norfolk",
		"Country": "England",
		"Postcode": "NR4 7UH",
		"Latitude": "52.622271",
		"Longitude": "1.221381"
	},
	{
		"organisation":102,
		"OrgID": "102",
		"Name": "University of East Anglia",
		"City": "Norwich",
		"Region": "Norfolk",
		"Country": "England",
		"Postcode": "NR4 7TJ",
		"Latitude": "52.6219215",
		"Longitude": "1.2391761"
	},
	{
		"organisation":103,
		"OrgID": "103",
		"Name": "Nottingham Trent University",
		"City": "Nottingham",
		"Region": "East Midlands",
		"Country": "England",
		"Postcode": "NG1 4BU",
		"Latitude": "52.9563632",
		"Longitude": "-1.1520043"
	},
	{
		"organisation":104,
		"OrgID": "104",
		"Name": "University of Nottingham",
		"City": "Nottingham",
		"Region": "East Midlands",
		"Country": "England",
		"Postcode": "NG7 2RD",
		"Latitude": "52.9370093",
		"Longitude": "-1.1847503"
	},
	{
		"organisation":105,
		"OrgID": "105",
		"Name": "Scottish Association For Marine Science",
		"City": "Oban",
		"Region": "Argyll and Bute",
		"Country": "Scotland",
		"Postcode": "PA37 1QA",
		"Latitude": "56.4511526",
		"Longitude": "-5.4407411"
	},
	{
		"organisation":106,
		"OrgID": "106",
		"Name": "University of Oxford",
		"City": "Oxford",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX1 2JD",
		"Latitude": "51.7566341",
		"Longitude": "-1.2547037"
	},
	{
		"organisation":107,
		"OrgID": "107",
		"Name": "University of the West of Scotland",
		"City": "Paisley",
		"Region": "Renfrewshire",
		"Country": "Scotland",
		"Postcode": "PA1 2BE",
		"Latitude": "55.844268",
		"Longitude": "-4.4304783"
	},
	{
		"organisation":108,
		"OrgID": "108",
		"Name": "Edith Cowan University",
		"City": "Perth",
		"Region": "Perth",
		"Country": "Australia",
		"Postcode": "WA 6050",
		"Latitude": "-31.9199089",
		"Longitude": "115.8660544"
	},
	{
		"organisation":109,
		"OrgID": "109",
		"Name": "University of Plymouth",
		"City": "Plymouth",
		"Region": "Devon",
		"Country": "England",
		"Postcode": "PL4 8AA",
		"Latitude": "50.3746445",
		"Longitude": "-4.1384841"
	},
	{
		"organisation":110,
		"OrgID": "110",
		"Name": "Bournemouth University",
		"City": "Poole",
		"Region": "Dorset",
		"Country": "England",
		"Postcode": "BH12 5BB",
		"Latitude": "50.743294",
		"Longitude": "-1.8971579"
	},
	{
		"organisation":111,
		"OrgID": "111",
		"Name": "University of Portsmouth",
		"City": "Portsmouth",
		"Region": "Hampshire",
		"Country": "England",
		"Postcode": "PO1 2UP",
		"Latitude": "50.7952771",
		"Longitude": "-1.0935841"
	},
	{
		"organisation":112,
		"OrgID": "112",
		"Name": "University of Central Lancashire",
		"City": "Preston",
		"Region": "Lancashire",
		"Country": "England",
		"Postcode": "PR1 2HE",
		"Latitude": "53.76278",
		"Longitude": "-2.707783"
	},
	{
		"organisation":113,
		"OrgID": "113",
		"Name": "University of Reading",
		"City": "Reading",
		"Region": "Berkshire",
		"Country": "England",
		"Postcode": "RG6 6AH",
		"Latitude": "51.4414205",
		"Longitude": "-0.9418157"
	},
	{
		"organisation":114,
		"OrgID": "114",
		"Name": "University of Salford",
		"City": "Salford",
		"Region": "Lancashire",
		"Country": "England",
		"Postcode": "M5 4WT",
		"Latitude": "53.4872074",
		"Longitude": "-2.2742964"
	},
	{
		"organisation":115,
		"OrgID": "115",
		"Name": "Sheffield Hallam University",
		"City": "Sheffield",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "S1 1WB",
		"Latitude": "53.3784202",
		"Longitude": "-1.4656122"
	},
	{
		"organisation":116,
		"OrgID": "116",
		"Name": "University of Sheffield",
		"City": "Sheffield",
		"Region": "Yorkshire",
		"Country": "England",
		"Postcode": "S10 2TN",
		"Latitude": "53.3809409",
		"Longitude": "-1.4879469"
	},
	{
		"organisation":117,
		"OrgID": "117",
		"Name": "University of Southampton",
		"City": "Southampton",
		"Region": "Hampshire",
		"Country": "England",
		"Postcode": "SO17 1BJ",
		"Latitude": "50.928762",
		"Longitude": "-1.402183"
	},
	{
		"organisation":118,
		"OrgID": "118",
		"Name": "National Oceanography Centre",
		"City": "Southampton",
		"Region": "Hampshire",
		"Country": "England",
		"Postcode": "SO14 3ZH",
		"Latitude": "50.8927085",
		"Longitude": "-1.395092"
	},
	{
		"organisation":119,
		"OrgID": "119",
		"Name": "University of St Andrews",
		"City": "St Andrews",
		"Region": "Fife",
		"Country": "Scotland",
		"Postcode": "KY16 9AJ",
		"Latitude": "56.3416934",
		"Longitude": "-2.7927522"
	},
	{
		"organisation":120,
		"OrgID": "120",
		"Name": "GlaxoSmithKline",
		"City": "Stevenage",
		"Region": "Hertfordshire",
		"Country": "England",
		"Postcode": "SG1 2NY",
		"Latitude": "51.8881864",
		"Longitude": "-0.2000009"
	},
	{
		"organisation":121,
		"OrgID": "121",
		"Name": "University of Stirling",
		"City": "Stirling",
		"Region": "Stirling",
		"Country": "Scotland",
		"Postcode": "FK9 4LA",
		"Latitude": "56.1459667",
		"Longitude": "-3.9197915"
	},
	{
		"organisation":122,
		"OrgID": "122",
		"Name": "Staffordshire University",
		"City": "Stoke-on-Trent",
		"Region": "Staffordshire",
		"Country": "England",
		"Postcode": "ST4 2DE",
		"Latitude": "53.0090658",
		"Longitude": "-2.1761383"
	},
	{
		"organisation":123,
		"OrgID": "123",
		"Name": "Stuttgart University",
		"City": "Stuttgart",
		"Region": "Stuttgart",
		"Country": "Germany",
		"Postcode": "70174",
		"Latitude": "48.7820533",
		"Longitude": "9.1742086"
	},
	{
		"organisation":124,
		"OrgID": "124",
		"Name": "University of Sunderland",
		"City": "Sunderland",
		"Region": "Cumbria",
		"Country": "England",
		"Postcode": "SR1 3SD",
		"Latitude": "54.9044941",
		"Longitude": "-1.391347"
	},
	{
		"organisation":125,
		"OrgID": "125",
		"Name": "Swansea University",
		"City": "Swansea",
		"Region": "Glamorganshire",
		"Country": "Wales",
		"Postcode": "SA2 8PP",
		"Latitude": "51.6096175",
		"Longitude": "-3.9806297"
	},
	{
		"organisation":126,
		"OrgID": "126",
		"Name": "Swansea Metropolitan University",
		"City": "Swansea",
		"Region": "Glamorganshire",
		"Country": "Wales",
		"Postcode": "SA1 6ED",
		"Latitude": "51.6246538",
		"Longitude": "-3.9494278"
	},
	{
		"organisation":127,
		"OrgID": "127",
		"Name": "University of Glamorgan",
		"City": "Pontypridd",
		"Region": "Glamorganshire",
		"Country": "Wales",
		"Postcode": "CF37 1DL",
		"Latitude": "51.5887376",
		"Longitude": "-3.3257747"
	},
	{
		"organisation":128,
		"OrgID": "128",
		"Name": "Brookhaven National Laboratory",
		"City": "Upton",
		"Region": "New York",
		"Country": "USA",
		"Postcode": "NY 11973",
		"Latitude": "40.8707596",
		"Longitude": "-72.8800783"
	},
	{
		"organisation":129,
		"OrgID": "129",
		"Name": "Brunel University",
		"City": "Uxbridge",
		"Region": "London",
		"Country": "England",
		"Postcode": "UB8 3PH",
		"Latitude": "51.5328475",
		"Longitude": "-0.4728554"
	},
	{
		"organisation":130,
		"OrgID": "130",
		"Name": "H R Wallingford Ltd",
		"City": "Wallingford",
		"Region": "Oxfordshire",
		"Country": "England",
		"Postcode": "OX10 8BA",
		"Latitude": "51.5987148",
		"Longitude": "-1.1238704"
	},
	{
		"organisation":131,
		"OrgID": "131",
		"Name": "Glyndwr University",
		"City": "Wrexham",
		"Region": "Clwyd",
		"Country": "Wales",
		"Postcode": "LL11 2AW",
		"Latitude": "53.052963",
		"Longitude": "-3.0056395"
	},
	{
		"organisation":132,
		"OrgID": "132",
		"Name": "University of Wuerzburg",
		"City": "Wrzburg",
		"Region": "Wrzburg",
		"Country": "Germany",
		"Postcode": "97070",
		"Latitude": "49.7830083",
		"Longitude": "9.9708462"
	}
	]
}